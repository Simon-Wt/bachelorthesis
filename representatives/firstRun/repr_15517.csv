ID_Article,communityId,ID_RelatedVenue,title,year,abstract
1978583,15517,517,Estimation of Clock Drift in HiL Testing by Property-Based Conformance Check,2011,"Hardware-in-the-loop testing (HiL) requires exact timing of measurements of continuous signals. Although great effort is put into design and manufacturing of measurement devices, inaccuracies might occur. In industrial practice differently fast clocks can occur and lead to drifting sampling rates. This drift leads to stretched and distorted signal data. Based on a methodology for analysing test data of continuous systems, we suggest to estimate the drift in HiL measurements with a property-based approach. Data of a reference device and the device exhibiting the drift is compared with help of signal properties. The shifted occurrences of one property depends on the clock's drift."
1308060,15517,517,A Query Driven Security Testing Framework for Enterprise Network,2013,"Due to extensive use of various network services and web based applications and heterogeneous organizational security requirements; enterprise network configuration is becoming very complex that imposes high operational workload on both regular and experienced administrators. This complexity extensively reduces overall network assurability and usability which in turn make the network vulnerable to various cyber-attacks. Network Access Control Lists (ACLs) is a standard for implementing security configurations in enterprise networks. However, the size and distributed placement of the ACLs in the network impose significant complexity as well as introduce potential scope of security misconfigurations. In this paper, we present a query driven security testing framework to assess the correctness and consistency of the access control list (ACL) based security implementations in an enterprise network. It will allow the network administrators to systematically test the ACL configurations with various interactive service access queries. The framework is built on top of a satisfiability analysis (SAT) engine. The efficacy of the framework is evaluated with extensive experimentations on real and synthetic networks."
2465683,15517,122,Extending a C-like language for portable SIMD programming,2012,"SIMD instructions are common in CPUs for years now. Using these instructions effectively requires not only vectorization of  code , but also modifications to the  data layout . However, automatic vectorization techniques are often not powerful enough and suffer from restricted scope of applicability; hence, programmers often vectorize their programs manually by using intrinsics: compiler-known functions that directly expand to machine instructions. They significantly decrease programmer productivity by enforcing a very error-prone and hard-to-read assembly-like programming style. Furthermore, intrinsics are not portable because they are tied to a specific instruction set.   In this paper, we show how a C-like language can be extended to allow for portable and efficient SIMD programming. Our extension puts the programmer in total control over where and how control-flow vectorization is triggered. We present a type system and a formal semantics of our extension and prove the soundness of the type system. Using our prototype implementation IVL that targets Intel's MIC architecture and SSE instruction set, we show that the generated code is roughly on par with handwritten intrinsic code."
1937182,15517,122,Thread contracts for safe parallelism,2011,"We build a framework of thread contracts, called Accord, that allows programmers to annotate their concurrency co-ordination strategies. Accord annotations allow programmers to declaratively specify the parts of memory that a thread may read or write into, and the locks that protect them, reflecting the concurrency co-ordination among threads and the reason why the program is free of data-races. We provide automatic tools to check if the concurrency co-ordination strategy ensures race-freedom, using constraint-solvers (SMT solvers). Hence programmers using Accord can both formally state and prove their co-ordination strategies ensure race freedom. The programmer's implementation of the co-ordination strategy may however be correct or incorrect. We show how the formal Accord contracts allow us to automatically insert runtime assertions that serve to check, during testing, whether the implementation conforms to the contract. Using a large class of data-parallel programs that share memory in intricate ways, we show that natural and simple contracts suffice to document the co-ordination strategy amongst threads, and that the task of showing that the strategy ensures race-freedom can be handled efficiently and automatically by an existing SMT solver (Z3). While co-ordination strategies can be proved race-free in our framework, failure to prove the co-ordination strategy race-free, accompanied by counter-examples produced by the solver, indicates the presence of races. Using such counterexamples, we report hitherto undiscovered data-races that we found in the long-tested applu_l benchmark in the Spec OMP2001 suite."
1742166,15517,507,On the BDD/FC conjecture,2013,"Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are two properties of sets of datalog rules and tuple generating dependencies (known as Datalog 3  programs), which recently attracted some attention. We conjecture that the first of these properties implies the second, and support this conjecture by some evidence proving, among other results, that it holds true for all theories over binary signature."
2172129,15517,517,Monitoring and Testing with Case Observer Automata: An Industry Report,2011,"In a highly configurable system, new or changed configurations may have to be tested in a running environment. To elicit high level information from such system, a monitoring solution must be able to: sort out the interesting data, connect the related data, draw conclusions about the data, and report the conclusion. The conclusions can be helpful to verify that the new configuration has the desired functionality. To continuously monitor requirements and try to find specific erroneous pattern is helpful during testing but when the system is deployed. In this industry report we present the Case Observer Automata modeling language (COAml) that aims to solve these issues. The COAml works with workflows where related data are put into cases, which can be seen as instances of the same observation pattern. In a case, parallel activities can be traced with synchronization points and timers. The Case Observer Language can be integrated into most forms of tracing/monitoring such as business rule monitoring, coverage tracing, static program analysis, log analysis, etc. We are currently integrating the language in the xTrade Alarm Server at Xware, and there are plans to reintegrate the COAml into UPPAAL Cover."
1366963,15517,517,Testing of Evolving Protocols,2012,"A common assumption for the state-of-the-art methods of protocol testing is that the protocol description is precise, unambiguous and fully determined. Unfortunately, in many practical situations this assumption turns out to be unrealistic. We propose an architecture of a testing framework where different aspects and facets of protocols are separated in a clear manner so that the adaptation of the framework to amendments in protocol description is relatively straightforward. This architecture is realised in a testing framework for RCS mobile phone protocol suite we developed in cooperation with Samsung Electronics. The framework successfully went through a number of adjustments to accommodate new interpretations of RCS protocols as assimilated by the developers."
1286021,15517,339,Automating Information Flow Analysis of Low Level Code,2014,"Low level code is challenging: It lacks structure, it uses jumps and symbolic addresses, the control flow is often highly optimized, and registers and memory locations may be reused in ways that make typing extremely challenging. Information flow properties create additional complications: They are hyperproperties relating multiple executions, and the possibility of interrupts and concurrency, and use of devices and features like memory-mapped I/O requires a departure from the usual initial-state final-state account of noninterference. In this work we propose a novel approach to relational verification for machine code. Verification goals are expressed as equivalence of traces decorated with observation points. Relational verification conditions are propagated between observation points using symbolic execution, and discharged using first-order reasoning. We have implemented an automated tool that integrates with SMT solvers to automate the verification task. The tool transforms ARMv7 binaries into an intermediate, architecture-independent format using the BAP toolset by means of a verified translator. We demonstrate the capabilities of the tool on a separation kernel system call handler, which mixes hand-written assembly with gcc-optimized output, a UART device driver and a crypto service modular exponentiation routine."
2384154,15517,8385,RADA: a tool for reasoning about algebraic data types with abstractions,2013,"We present RADA, a portable, scalable tool for reasoning about formulas containing algebraic data types using catamorphism (fold) functions. It can work as a back-end for reasoning about recursive programs that manipulate algebraic types. RADA operates by successively unrolling catamorphisms and uses either CVC4 and Z3 as reasoning engines. We have used RADA for reasoning about functional implementations of complex data structures and to reason about guard applications that determine whether XML messages should be allowed to cross network security domains. Promising experimental results demonstrate that RADA can be used in several practical contexts."
1738188,15517,517,A Case Study of Automating User Experience-Oriented Performance Testing on Smartphones,2013,"We have developed a platform named Advanced Test Environment (ATE) for supporting the design and the automatic execution of UX tests for applications running on Android smartphones. The platform collects objective metrics used to estimate the UX. In this paper, we investigate the extent that the metrics captured by ATE are able to approximate the results that are obtained from UX testing with real human users. Our findings suggest that ATE produces UX estimations that are comparable to those reported by human users. We have also compared ATE with three widespread benchmark tools that are commonly used in the industry, and the results show that ATE outperforms these tools."
1480010,15517,517,Estimating Fault Detection Effectiveness,2014,"A t-way covering array can detect t-way faults, however they generally include other combinations beyond t-way as well. For example, a particular test set of all 5-way combinations is shown capable of detecting all seeded faults in a test program, despite the fact that it contains up to 9-way faults. This poster gives an overview of methods for estimating fault detection effectiveness of a test set based on combinatorial coverage for a class of software.Detection effectiveness depends on the distribution of t-way faults, which is not known. However based on past experience one could say for example the fraction of 1-way faults is F1 = 60 %, 2- way faults F2 = 25 % F3 = 10 % and F4 = 5 %. Such information could be used in determining the required strength t. It is shown that the fault detection effectiveness of a test set may be affected significantly by the t-way fault distribution, overall, simple coverage at each level of t, number of values per variable, and minimum t-way coverage. Using these results, we develop practical guidance for testers."
1528930,15517,507,Verification of relational data-centric dynamic systems with external services,2013,"Data-centric dynamic systems are systems where both the process controlling the dynamics and the manipulation of data are equally central. We study verification of (first-order) mu-calculus variants over  relational data-centric dynamic systems , where data are maintained in a relational database, and the process is described in terms of atomic actions that evolve the database. Action execution may involve calls to external services, thus inserting fresh data into the system. As a result such systems are infinite-state. We show that verification is undecidable in general, and we isolate notable cases where decidability is achieved. Specifically we start by considering service calls that return values deterministically (depending only on passed parameters). We show that in a mu-calculus variant that preserves knowledge of objects appeared along a run we get decidability under the assumption that the fresh data introduced along a run are bounded, though they might not be bounded in the overall system. In fact we tie such a result to a notion related to weak acyclicity studied in data exchange. Then, we move to nondeterministic services and we investigate decidability under the assumption that knowledge of objects is preserved only if they are continuously present. We show that if infinitely many values occur in a run but do not accumulate in the same state, then we get again decidability. We give syntactic conditions to avoid this accumulation through the novel notion of generate-recall acyclicity, which ensures that every service call activation generates new values that cannot be accumulated indefinitely."
2540861,15517,374,A systematic analysis of XSS sanitization in web application frameworks,2011,"While most research on XSS defense has focused on techniques for securing existing applications and re-architecting browser mechanisms, sanitization remains the industry-standard defense mechanism. By streamlining and automating XSS sanitization, web application frameworks stand in a good position to stop XSS but have received little research attention. In order to drive research on web frameworks, we systematically study the security of the XSS sanitization abstractions frameworks provide. We develop a novel model of the web browser and characterize the challenges of XSS sanitization. Based on the model, we systematically evaluate the XSS abstractions in 14 major commercially-used web frameworks. We find that frameworks often do not address critical parts of the XSS conundrum. We perform an empirical analysis of 8 large web applications to extract the requirements of sanitization primitives from the perspective of realworld applications. Our study shows that there is a wide gap between the abstractions provided by frameworks and the requirements of applications."
2215580,15517,517,An Empirical Evaluation of Assertions as Oracles,2011,"In software testing, an oracle determines whether a test case passes or fails by comparing output from the program under test with the expected output. Since the identification of faults through testing requires that the bug is both exercised and the resulting failure is recognized, it follows that oracles are critical to the efficacy of the testing process. Despite this, there are few rigorous empirical studies of the impact of oracles on effectiveness. In this paper, we report the results of one such experiment in which we exercise seven core Java classes and two sample programs with branch-adequate, input only(i.e., no oracle) test suites and collect the failures observed by different oracles. For faults, we use synthetic bugs created by the muJava mutation testing tool. In this study we evaluate two oracles: (1) the implicit oracle (or null oracle) provided by the runtime system, and (2) runtime assertions embedded in the implementation (by others) using the Java Modeling Language. The null oracle establishes a baseline measurement of the potential benefit of rigorous oracles, while the assertions represent a more rigorous approach that is sometimes used in practice. The results of our experiments are interesting. First, on a per-method basis, we observe that the null oracle catches less than 11% of the faults, leaving more than 89% uncaught. Second, we observe that the runtime assertions in our subjects are effective at catching about 53% of the faults not caught by null oracle. Finally, by analyzing the data using data mining techniques, we observe that simple, code-based metrics can be used to predict which methods are amenable to the use of assertion-based oracles with a high degree of accuracy."
2440850,15517,517,Finding Software Vulnerabilities by Smart Fuzzing,2011,"Nowadays, one of the most effective ways to identify software vulnerabilities by testing is the use of fuzzing, whereby the robustness of software is tested against invalid inputs that play on implementation limits or data boundaries. A high number of random combinations of such inputs are sent to the system through its interfaces. Although fuzzing is a fast technique which detects real errors, its efficiency should be improved. Indeed, the main drawbacks of fuzz testing are its poor coverage which involves missing many errors, and the quality of tests. Enhancing fuzzing with advanced approaches such as: data tainting and coverage analysis would improve its efficiency and make it smarter. This paper will present an idea on how these techniques when combined give better error detection by iteratively guiding executions and generating the most pertinent test cases able to trigger potential vulnerabilities and maximize the coverage of testing."
1836639,15517,517,Test Selection for Data-Flow Reactive Systems Based on Observations,2011,"Conformance testing amounts to verifying adequacy between the behaviors and the specified behaviors of an implementation. In this paper, we handle model-based conformance testing for data-flow critical systems with time constraints. Specifications are described with a formal model adapted for such systems and called Variable Driven Timed Automata (VDTA). VDTA are inspired by timed automata but they use input/output communication variables, allowing clear and short specifications. We present a conformance relation for this model and we propose a symbolic test selection algorithm based on a test purpose. The selection algorithm computes the variations on inputs allowing to reach an expected state of the implementation. Then we propose an on-line testing algorithm."
2372280,15517,517,Search-Based Application Security Testing: Towards a Structured Search Space,2011,"This position paper outlines a staged approach to search-based application security testing. In the first stage one searches for candidate tests in the input space that have a chance of leading to good security tests. In the second stage one selects individual candidates and uses them to select and parametrize specialized search techniques. This approach has its roots in exploratory security testing. In the first stage, the fitness of tests depends on their ability to provoke vulnerability symptoms at all, and on their relation to other tests in a test suite. In the second stage, the fittest tests are those that come closest to an exploit of a specific type of vulnerability. To evaluate the performance of such a staged approach one might use web application vulnerability scanners as a baseline."
1446851,15517,517,Systematic Testing for Detecting Concurrency Errors in Erlang Programs,2013,"We present the techniques used in Concuerror, a systematic testing tool able to find and reproduce a wide class of concurrency errors in Erlang programs. We describe how we take advantage of the characteristics of Erlang's actor model of concurrency to selectively instrument the program under test and how we subsequently employ a stateless search strategy to systematically explore the state space of process interleaving sequences triggered by unit tests. To ameliorate the problem of combinatorial explosion, we propose a novel technique for avoiding process blocks and describe how we can effectively combine it with preemption bounding, a heuristic algorithm for reducing the number of explored interleaving sequences. We also briefly discuss issues related to soundness, completeness and effectiveness of techniques used by Concuerror."
2156783,15517,517,Protocol Security Testing with SPIN and TTCN-3,2011,"Protocol security testing is an important technique to ensure the security of communication protocols. However, methods considering both effective detection to specification vulnerabilities and efficient testing on protocol implementations are not well developed. In this paper, we present a general method for protocol security testing including protocol verification with SPIN model checker and protocol testing with formal testing language TTCN-3. We use threat model to model malicious entities and import the classification of information security to achieve a complete analysis of security requirements for protocol verification. We also develop a SPIN Trail to TTCN-3(st2ttcn) conversion tool to generate test cases automatically from counter examples obtained from model checking. As a case study, we apply our approach to the security testing of Source Address Validation Improvements (SAVI) protocol. We test two versions of SAVI-DHCP protocol. Security vulnerabilities have been found and tested in corresponding implementations."
1592980,15517,517,Multicore SDK: A Practical and Efficient Deadlock Detector for Real-World Applications,2011,"Traditional deadlock detection algorithms depend on finding cycles in lock graphs created from the application code. Usually, these approaches suffer from scalability and performance problems and cannot handle large industrial strength applications. The main problem for lack of scalability and poor performance is caused by the size of the lock graphs that have to be analyzed. In this paper we show a new two-phase deadlock detection algorithm that is efficient both in terms of memory utilization and time and also very scalable. In the first phase of the algorithm we consider a reduced lock graph based on program locations. We filter out certain locks that cannot participate in a deadlock by analyzing the lock graph created in the first phase. In the second phase of the algorithm we create an even smaller lock graph considering only those locks that were not filtered in the first phase. Finally, the second phase lock graph is analyzed for cycles to find potential deadlocks in the application. We present results from various open-source and commercial software, and also compare the performance of our algorithm with traditional approach."
955014,15517,517,The Forth International Workshop on Security Testing (SECTEST 2013),2013,"To improve software security, several techniques, including vulnerability modelling and security testing, have been developed but the problem remains unsolved. On one hand, the SECTEST workshop tries to answer how vulnerability modelling can help users understand the occurrence of vulnerabilities so to avoid them, and what the advantages and drawbacks of the existing models are to represent vulnerabilities. At the same time, the workshop tries to understand how to solve the challenging security testing problem given that testing the mere functionality of a system alone is already a fundamentally critical task, how security testing is different from and related to classical functional testing, and how to assess the quality of security testing. The objective of this workshop is to share ideas, methods, techniques, and tools about vulnerability modelling and security testing to improve the state of the art."
2134022,15517,517,Reconfigurable Model-Based Test Program Generator for Microprocessors,2011,"Automatic generation and simulation of test programs is known to be the main means for verifying microprocessors. The problem is that test program generators for new designs are often developed from scratch with little reuse of well-tried components. State-of-the-art tools, like Genesys-Pro and RAVEN, meet the challenge by using a model-based approach, where a microprocessor model is separated from a platform-independent generation core. However, there is still a problem. Developing a microprocessor model is rather difficult and usually requires deep knowledge of the inner-core structure and interfaces. In this paper, we describe a concept of a reconfigurable test program generator being customized with the help of architecture specifications and configuration files, which describe parameters of the microprocessor subsystems (pipeline, memory, and others). The suggested approach eases the model development and makes it possible to apply the model-based testing in the early design stages when the microprocessor architecture is frequently modified."
2341179,15517,339,Fortifying web-based applications automatically,2011,"Browser designers create security mechanisms to help web developers protect web applications, but web developers are usually slow to use these features in web-based applications (web apps). In this paper we introduce Zan, a browser-based system for applying new browser security mechanisms to legacy web apps automatically. Our key insight is that web apps often contain enough information, via web developer source-code patterns or key properties of web-app objects, to allow the browser to infer opportunities for applying new security mechanisms to existing web apps. We apply this new concept to protect authentication cookies, prevent web apps from being framed unwittingly, and perform JavaScript object deserialization safely. We evaluate Zan on up to the 1000 most popular websites for each of the three cases. We find that Zan can provide complimentary protection for the majority of potentially applicable websites automatically without requiring additional code from the web developers and with negligible incompatibility impact."
1782663,15517,517,"Automatic Validation and Correction of Formalized, Textual Requirements",2011,"Nowadays requirements are mostly specified in unrestricted natural language so that each stakeholder understands them. To ensure high quality and to avoid misunderstandings, the requirements have to be validated. Because of the ambiguity of natural language and the resulting absence of an automatic mechanism, this has to be done manually. Such manual validation techniques are time-consuming, error-prone, and repetitive because hundreds or thousands of requirements must be checked. With an automatic validation the requirements engineering process can be faster and can produce requirements of higher quality. To realize an automatism, we propose a controlled natural language (CNL) for the documentation of requirements. On basis of the CNL, a concept for an automatic requirements validation is developed for the identification of inconsistencies and incomplete requirements. Additionally, automated correction operations for such defective requirements are presented. The approach improves the quality of the requirements and therefore the quality of the whole development process."
2229027,15517,517,Challenges in Audit Testing of Web Services,2011,"Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit."
1472381,15517,517,Towards a Methodology for Verifying Partial Model Refinements,2012,"Models are good at expressing information that is known but do not typically have support for representing what information a modeler does not know or does not care about at a particular stage in the software development process. Partial models address this by being able to precisely represent uncertainty about model content. In previous work, we have defined a general approach for defining partial model semantics using a first order logic encoding. In this paper, we use this FO encoding to formally define the conditions for partial model refinement in the manner of the refinement of algebraic specifications. We use this approach to verify both manual refinements and automated transformation-based refinements. We illustrate our approach using example models and transformations."
2139867,15517,122,Trace driven dynamic deadlock detection and reproduction,2014,"Dynamic analysis techniques have been proposed to detect potential deadlocks. Analyzing and comprehending each potential deadlock to determine whether the deadlock is feasible in a real execution requires significant programmer effort. Moreover, empirical evidence shows that existing analyses are quite imprecise. This imprecision of the analyses further void the manual effort invested in reasoning about non-existent defects.   In this paper, we address the problems of imprecision of existing analyses and the subsequent manual effort necessary to reason about deadlocks. We propose a novel approach for deadlock detection by designing a dynamic analysis that intelligently leverages execution traces. To reduce the manual effort, we replay the program by making the execution follow a schedule derived based on the observed trace. For a real deadlock, its feasibility is automatically verified if the replay causes the execution to deadlock.   We have implemented our approach as part of WOLF and have analyzed many large (upto 160KLoC) Java programs. Our experimental results show that we are able to identify 74% of the reported defects as true (or false) positives automatically leaving very few defects for manual analysis. The overhead of our approach is negligible making it a compelling tool for practical adoption."
1536288,15517,517,"When a GUI Regression Test Failed, What Should be Blamed?",2012,"Script-based automated regression testing is widely used in industry. In this work, we focus on failed tests in a real regression test project. The causes of 197 failed tests produced in automated testing are examined and categorized based on an analysis procedure presented. The result shows that incorrect scripts, oracle mismatches, test tool bugs and misconfigurations involved in testing contribute most to failures instead of product bugs. Detecting and fixing the false positives are laborious and time-consuming. Our empirical study demonstrates that the benefits of test automation are obliterated to some extent in practical settings. To improve the effectiveness of automated regression testing further, some issues should receive more attention."
1008804,15517,517,Considering Context Events in Event-Based Testing of Mobile Applications,2013,"A relevant complexity factor in developing and testing mobile apps is given by their sensibility to changes in the context in which they run. As an example, apps running on a smartphone can be influenced by location changes, phone calls, device movements and many other typologies of context events. In this paper, we address the problem of testing a mobile app as an event-driven system by taking into account both context events and GUI events. We present approaches based on the definition of reusable event patterns for the manual and automatic generation of test cases for mobile app testing. One of the proposed testing techniques, based on a systematic and automatic exploration of the behaviour of an Android app, has been implemented and some preliminary case studies on real apps have been carried out in order to explore their effectiveness."
2010882,15517,517,Extracting Properties from Test Cases by Refactoring,2011,"A Quick Check property is a logical statement of aspects of the behaviour of a system. We report on how similar test cases in a test suite written in Erlang can be identified and then refactored into properties, giving a generalisation of the specification implicit in the test suite. Properties give more concise, easier to maintain test suites and better test coverage. A preliminary evaluation of the techniques in industry demonstrates feasibility as well as potential benefits."
2028663,15517,517,Implementing and Evaluating a Runtime Conformance Checker for Mobile Agent Systems,2011,"A Mobile Agent System (MAS) is a special kind of distributed system in which the agent software can move from one physical host to another. This paper describes a new approach, together with its implementation and evaluation, for checking the conformance of a MAS with respect to an executable model. In order to check the effectiveness of our conformance check, we have built a mutation-based evaluation framework. Part of the framework is a set of 29 new mutation operators for mobile agent systems. Our conformance checking approach is used to compare the mutated agents with the executable model and determine nonconformance. Our experimental results suggest that our approach holds promise for the generation and detection of non-equivalent mutants."
2014844,15517,517,Towards Formal Description of Standards for Automotive Operating Systems,2013,"The OSEK/VDX specification is a standard for automotive operating systems, i.e., operating systems for mobile vehicles. The specification is described in a natural language. Thus, it is difficult to verify the conformity that the automotive operating systems follow this standard due to its ambiguity. We think that such standard has to be formally described enough to ensure that final products conform to the standard. In this paper, we propose a framework for formalization of the OSEK/VDX specification."
2215442,15517,517,Towards Search-Based Testing for Event-B Models,2011,"This position paper discusses the challenges and opportunities of applying search-based techniques to a formal environment of abstract state machines defined using a language called Event-B. Event-B is based on a formal abstract machine notation that has a mature tool support and gets continuous feedback from industry. Although search-based techniques recently developed for extended finite state machines may be adapted to this context, new challenges such as implicit states, non-determinism, non-numerical data types and hierarchical models are still to be solved for test data generation for Event-B models."
1167899,15517,517,Searching the Boundaries of a Modeling Space to Test Metamodels,2012,"Model-driven software development relies on metamodels to formally capture modeling spaces. Metamodels specify concepts and relationships between them in order to represent either a specific business domain model or the input and output domains for operations on models (e.g., model refinement). In all cases, a metamodel is a finite description of a possibly infinite set of models, i.e. the set of all models which structure conforms to the description specified in the metamodel. However, there is currently no systematic method to test that a metamodel captures all the correct models of the domain and no more. In this paper, we focus on the automatic selection of a set of models in the modeling space captured by a metamodel. The selected set should both cover as many representative situations as possible and be kept small as possible for further manual analysis. We use simulated annealing to select a set of models that satisfies those two objectives and report on results using two metamodels from two different domains."
2791795,15517,22232,CPAchecker with Sequential Combination of Explicit-Value Analyses and Predicate Analyses - (Competition Contribution).,2014,������������������ ���������������� ����� ����������������� ����� ����� ����� ���� ���������������� ������������� ���� ����������������� ���������� ���� ���� ���� ���� ����� ����� ����� ������� ������� ������� �������� �������� ������� ����������������� ������ ������� ���� ����� �������� �������� �������� ��������
24268,15517,9438,Experimentation in Executable Enterprise Architecture Models,2012,���������� �������� ������������� ��������������� � ���������� ������������� �� ������� �������� � ��������� ��������� ���� �������� ��� ������ ��������������� � �������� � �������� ������������� ��� �� � �������� ������ ���������� ��� ������������� � ������ ������������� � ������������ ������������� � �������������� ���� ��������� ���� ������ ���������� ����� ���� ������� �������� ������� ������� ���������� ���������� �������� �������� ����������� ���� ��������� ����� ������� � ���� � � � ����������� � �������� ����������� � �������������� � ����������� � ����������� ����������� � ���������������� ��� �������� ����� � ����������� � ������������� ��������� �������� ������� ������� ��������� ����������� � � ������� �������� ��������� ������������ ����� ������� ��������� ���������� ���� � ���� ���� ������������� � ������� � ��������� ���������� � ������� � ��������� ������ ������������� ������ ������� ������� ��������� ��������� ���������� �������� ������������ ���������������� �� ������������������������ ��������������� � ���������������� ������������ �������� ���������������� ���������������� �� ������������������� �� ����������������������������� �� ������� ������������� ������ ���������������� ���������������� � ���������������� �� � ! ��������� �������� ����� ���������� ������ ����� ������������ �������������� ��� ��������������� �������� �������� ��������������� ������������� �� � �������������� ��� ��������������� ��������� �������������� ��� ����!��������������������� ����� ���������� ��#����$�� ����������� %&� ���������� ��#������������� ����������� �&$ ���������� ��# ��������������� ������������ ����������� ��'% ����������� ������'$��� ��������� ��� ���������� ��#����(�������� ��&�������� ��������� ������� ���������� ���������� ��)� ���� ���������� ���������� ��)� ���� ��)� ���� !���� �������� ������*�� �������� �������� �������� ������*�� ������*�� ������*�� ���������� ���������� ���������� ���������� ���������� ���������� ���������� ���������� ���� ���� ���� �������������� ��� ���������������� ����
440284,15517,9438,Exploring Features of a Full-Coverage Integrated Solution for Business Process Compliance,2011,������ ��������� ����������� ��������� ���������� � (����'������������� ��������� (��$������%� ������������������� (����������������� (���!�������� �������������!��� (������������� ��!��� ���������%� (������������������(#��� ��������������� (�������%����������������� ������������������� ���������� �������� (������������� ���� (�������%������� (������������ %���� (����������� ���������� (��!���������� ����� (���������������� (������%�������������� ���!���� (�����(�!���� �%���� ������
2093079,15517,10973,A Why-on-Earth Tutorial on Finite Model Theory,2011,This note advertises the topics that will be covered in the tutorial on finite model theory.
95130,15517,9438,An Application of Philosophy in Software Modelling and Future Information Systems Development,2013,Proceedings of the workshops of the 25th International Conference on Advanced Information Systems Engineering.
1700267,15517,8385,Managing lots of models: the FaMine approach,2014,In this paper we present recent developments in reverse engineering variability for block-based data-flow models.
1031100,15517,8385,Empirical answers to fundamental software engineering problems (panel),2013,Can the methods of empirical software engineering give us answers to the truly important open questions in the field?
2144858,15517,9438,On the Applicability of Concepts from Variability Modelling in Capability Modelling: Experiences from a Case in Business Process Outsourcing,2014,On the Applicability of Concepts from Variability Modelling in Capability Modelling : Experiences from a Case in Business Process Outsourcing
855554,15517,10973,Turing's Password: What Internet Cannot Leak,2012,"I use a revised, more robust, notion of computability to answer bothersome questions that are not even easy to ask with any clarity."
730239,15517,23827,Strawberries are nuts,2011,Separation of concerns is a central element for program comprehension. This note briefly explains why human categorization can be interesting for program comprehension.
2567019,15517,23827,Third international workshop on software engineering in healthcare (SEHC 2011),2011,"This paper briefly describes the 3rd International Workshop on Software Engineering in Healthcare, held at the 2011 International Conference on Software Engineering."
119509,15517,9438,Towards A Collaborative Framework for Image Annotation and Search,2011,"This paper was presented at CAiSE 2011 International Workshops, London, UK, June 20-24, 2011 and published in the proceedings."
1311253,15517,517,Statechart Analysis with Symbolic PathFinder,2012,We report here on our on-going work that addresses the automated analysis and test case generation for software systems modeled using multiple State chart formalisms.
368080,15517,9438,Towards a Sociomaterial Ontology,2013,"The management of social phenomena in conceptual modelling requires a novel understanding of the notion of representation. In particular, the principles for the existence and identification of obje ..."
2612712,15517,22232,SatAbs: A Bit-Precise Verifier for C Programs - (Competition Contribution),2012,SatAbs is a bit-precise software model checker for ANSI-C programs. It implements sound predicate-abstraction based algorithms for both sequential and concurrent software.
1972715,15517,517,Online Test Generation with PathCrawler: Tool Demo,2011,This demonstration presents a new version of Path Crawler developed in an entirely novel form: that of a test-case server which is freely accessible online.
2559410,15517,9438,Integrating the goal and business process perspectives in information system analysis,2014,The final publication is available at Springer via  http://dx.doi.org/ 10.1007/978-3-319-07881-6_23
136859,15517,8422,CSolve: verifying c with liquid types,2012,"We present CSolve, an automated verifier for C programs based on Liquid Type inference. We show how CSolve verifies memory safety through an example and describe its architecture and interface."
2093561,15517,122,Automatic safety proofs for asynchronous memory operations,2011,"We present a work-in-progress proof system and tool, based on separation logic, for analysing memory safety of multicore programs that use asynchronous memory operations."
1502831,15517,23827,MDSheet: a framework for model-driven spreadsheet engineering,2012,"In this paper, we present MDSheet, a framework for the embedding, evolution and inference of spreadsheet models. This framework offers a model-driven software development mechanism for spreadsheet users."
738889,15517,20524,Automated documentation inference to explain failed tests,2011,A failed test reveals a potential bug in the tested code. Developers need to understand which parts of the test are relevant to the failure before they start bug-fixing.
47188,15517,9438,Towards Consumer Preference-Aware Requirements,2012,"From the business perspective, one of the core concerns within Business-IT alignment is coordinating strategic initiatives and plans with Information Systems (IS). However, while substantial work h ..."
2072394,15517,9438,Modeling Competition-Driven Business Strategy for Business IT Alignment,2011,"Business strategy aims at supporting the vision of an enterprise, by paving the way to achieve it through goals that direct the strategy’s execution. Aligning business strategy to system requiremen ..."
1328714,15517,23827,Model-driven development of diverse user interfaces,2014,Developing and maintaining user interfaces of an application for various devices is usually laborious. This paper discusses how to build diverse user interfaces based on model-driven development.
1875783,15517,8912,WaRR: A tool for high-fidelity web application record and replay,2011,"We introduce WaRR, a tool that records and replays with high fidelity the interaction between users and modern web applications. WaRR consists of two independent components: the WaRR Recorder and the WaRR Replayer."
2692797,15517,22232,HSF(C): A Software Verifier Based on Horn Clauses (Competition Contribution),2012,HSF(C) is a tool that automates verification of safety and liveness properties for C programs. This paper describes the verification approach taken by HSF(C) and provides instructions on how to install and use the tool.
1296298,15517,23827,"Identifying caching opportunities, effortlessly",2014,Memory consumption is a great concern for most non trivial software. In this paper we introduce a dedicated code profiler that identifies opportunities to reduce memory consumption by introducing caches.
811656,15517,20524,Keynote talk: EasyChair,2014,"The design and architecture of every very large Web service is unique, and EasyChair is not an exception. This talk overviews design features of EasyChair, which may be interesting for the software engineering community."
187834,15517,9438,Model-Centric Strategy-IT Alignment: An Empirical Study in Progress,2013,"IT pervades all sectors of today's organizations. To support efficient business solutions, business-IT alignment has been long-time discussed as a solution. Given the complexity of achieving alignm ..."
2539207,15517,122,SCRATCH: a tool for automatic analysis of dma races,2011,"We present the SCRATCH tool, which uses bounded model checking and k-induction to automatically analyse software for multicore processors such as the Cell BE, in order to detect DMA races."
2522749,15517,23827,Portfolio: a search engine for finding functions and their usages,2011,"In this demonstration, we present a code search system called  Portfolio  that retrieves and visualizes relevant functions and their usages. We will show how chains of relevant functions and their usages can be visualized to users in response to their queries."
1063274,15517,23865,MSR 2012 keynote: Software analytics in practice — Approaches and experiences,2012,Summary form only given. A list of the plenary sessions speakers and tracks is given. Following that are abstracts for all full papers published on the original conference proceedings CD.
146874,15517,9438,Tool Integration beyond Wasserman,2011,"The typical development environment today consists of many specialized development tools, which are partially integrated, forming a complex tool landscape with partial integration. Traditional appr ..."
1420849,15517,11330,Author retrospective for PTRAN's analysis and optimization techniques,2014,"The PTRAN (Parallel Translator) system at IBM had as its goal the analysis and optimization of sequential programs for parallel architectures. In this paper, we give our perspective on what has changed since PTRAN, and what is still relevant."
736410,15517,23827,Automated memory leak detection for production use,2014,"This paper presents Sniper, an automated memory leak detection tool for C/C++ production software. To track the staleness of allocated memory (which is a clue to potential leaks) with little overhead (mostly"
1987105,15517,23827,Modelling and Verifying Smell-Free Architectures with the Archery Language,2014,"This work was funded by ERDF - European Regional Development Fund, through#R##N#the COMPETE Programme, and by National Funds through FCT within project#R##N#FCOMP-01-0124-FEDER-028923."
954715,15517,23827,Software engineering in CS 2013,2013,This paper discusses how software engineering topics are included in the CS2013 Curriculum Guidelines and presents several challenges to be addressed in collaboration with the software engineering community before final publication of the CS2013 volume.
2391078,15517,9436,UML4PF — A tool for problem-oriented requirements analysis,2011,"We present UML4PF, a tool for requirements analysis based on problem frames. It consists of a UML profile and an Eclipse-Plugin to model and analyze problem diagrams, derive specifications, and develop architectures."
1174324,15517,23827,Developing verified programs with dafny,2013,Dafny is a programming language and program verifier. The language includes specification constructs and the verifier checks that the program lives up to its specifications. These tutorial notes give some Dafny programs used as examples in the tutorial.
1135409,15517,23620,A galois connection calculus for abstract interpretation,2014,"We introduce a Galois connection calculus for language independent specification of abstract interpretations used in programming language semantics, formal verification, and static analysis. This Galois connection calculus and its type system are typed by abstract interpretation."
1615,15517,23827,Fuzzy Models for Complex Social Systems Using Distributed Agencies in Poverty Studies,2011,"There are several ways to model a complex social system, as is the poverty of an entity, the object of this paper is to present a methodology consisting of several techniques that offers to solve complex social problems with soft computing."
2180983,15517,8912,Architecting resilient computing systems: Overall approach and open issues,2011,"Resilient systems [1] are expected to continuously provide trustworthy services despite changes coming from the environment or from their specifications. This definition is based on the work carried out within the European Network of Excellence, ReSIST [2]."
1306179,15517,517,Measuring T2 against SBST 2013 Benchmark Suite,2013,"T2 is a light-weight, on-the-fly random-based automated testing tool to test Java classes. This paper presents its recent benchmarking result against SBST 2013 Contest's test suite."
192329,15517,9438,Risk accelerators in disasters : insights from the typhoon Haiyan response on humanitarian information management and decision support,2014,Published version of a chapter in the book: Advanced Information Systems Engineering. Also available from the publisher at: http://dx.doi.org/10.1007/978-3-319-07881-6_2
1750818,15517,23827,"Tool to Support Constructing, Analyzing, Comparing and Composing Models",2011,"This paper describes the basis and functionality of a semantic database system that supports building, analyzing, and comparing any models that can be described in English. The examples are from the domain of system development but the tool can be used for models in any domain."
123042,15517,23634,Kleene theorem in partial conway theories with applications,2011,Partial Conway theories are algebraic theories equipped with a partially defined dagger operation satisfying some natural identities. We prove a Kleene type theorem for partial Conway theories and discuss several applications of this result.
2230651,15517,21089,Initial Experiments with TPTP-style Automated Theorem Provers on ACL2 Problems,2014,This paper reports our initial experiments with using external ATP on some corpora built with the ACL2 system. This is intended to provide the first estimate about the usefulness of such external reasoning and AI systems for solving ACL2 problems.
563027,15517,20332,Adaptive Performance Optimization over Crowd Labor Channels,2014,We describe a system which monitors the performance of labor channels within a crowdsourcing platform in an online manner. This allows us to automatically determine if and when to switch between labor channels in order to improve overall performance of crowd tasks.
2318711,15517,23827,When open source turns cold on innovation - the challenges of navigating licensing complexities in new research domains,2012,"In this poster, we review the limitations open source licences introduce to the application of Linked Data in Software Engineering. We investigate whether open source licences support special requirements to publish source code as Linked Data on the Internet."
1104147,15517,23827,Merging Multi-view Feature Models by Local Rules,2011,In this paper we consider merging of feature models arising from different viewpoints. We propose a normative procedure to merge basic feature models by applying local rules. Our procedure can merge basic feature models and feature models with cross-tree relationships between sibling features.
436642,15517,8422,SLAYER: memory safety for systems-level code,2011,"SLAyer is a program analysis tool designed to automatically prove memory safety of industrial systems code. In this paper we describe SLAyer's implementation, and its application to Windows device drivers. This paper accompanies the first release of SLAYER."
572344,15517,8422,Equality-based translation validator for LLVM,2011,"We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks."
2214718,15517,9436,PABRE-Man: Management of a requirement patterns catalogue,2011,"Software requirement patterns have been proposed as an artifact for fostering requirements reuse. In this paper we present PABRE-Man, a software subsystem aimed at managing a catalogue of patterns ready to be applied in requirements engineering projects."
1139233,15517,517,Sixth International Workshop on Search-Based Software Testing (SBST 2013): Workshop Summary,2013,"This paper summarizes the presentations and discussions at the 6th International Workshop on Search-Based Software Testing (SBST 2013), held in conjunction with the IEEE International Conference on Software Testing, Verification and Validation (ICST 2013)."
1054614,15517,10973,Duality in Logic and Computation,2013,I give a brief introduction to Stone duality and then survey a number of duality theories that arise in logic and computer science. I mention some more unfamiliar dualities at the end which may be of importance to emerging fields within computer science.
996617,15517,23876,Toward structured location of features,2012,"This paper proposes structured location, a semiautomatic technique and its supporting tool both for locating features and exposing their structures in source code, using a combination of dynamic analysis, sequential pattern mining and formal concept analysis."
1216989,15517,8385,Known unknowns: testing in the presence of uncertainty,2014,"Uncertainty is becoming more prevalent in the software systems we build, introducing challenges in the way we develop software, especially in software testing. In this work we explore how uncertainty affects software testing, how it is managed currently, and how it could be treated more effectively."
2122356,15517,23827,Augmenting test suites automatically,2012,We present an approach to augment test suites with automatically generated integration test cases. Our approach utilizes existing test cases to direct generation towards producing complex object interactions and execution sequences that have not been observed before.
2410142,15517,23827,Writing dynamic service orchestrations with DSOL,2012,"We present the workflow language DSOL, its runtime system and the tools available to support the development of dynamic service orchestrations. DSOL aims at supporting dynamic, self-managed service compositions that can adapt to changes occurring at runtime."
1579514,15517,122,TeamWork: synchronizing threads globally to detect real deadlocks for multithreaded programs,2013,"This paper presents the aim of TeamWork, our ongoing effort to develop a comprehensive dynamic deadlock confirmation tool for multithreaded programs. It also presents a refined object abstraction algorithm that refines the existing stack hash abstraction."
1368988,15517,23827,Some non-usage data for a distributed editor: the saros outreach,2011,"After contacting more than 40 companies and 11 OSS projects regarding using our distributed editor Saros, we find that almost all of those many who have a use case for it, are reluctant to even try it out. It appears that distance matters even by anticipatory obedience."
1605299,15517,20524,Local vs. global models for effort estimation and defect prediction,2011,Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules.
1721864,15517,10973,Capsules and Separation,2012,"We study a formulation of separation logic using capsules, a representation of the state of a computation in higher-order programming languages with mutable variables. We prove soundness of the frame rule in this context and investigate alternative formulations with weaker side conditions."
2314154,15517,517,IPEG: Utilizing Infeasibility,2011,"Infeasible paths are an hindrance to path-oriented test input generators. IPEG is a tool that takes a C program, and such an infeasible path of the program as input, and infers a possibly infinite family of infeasible paths. This paper gives a short description of the tool and the technique behind it."
1662481,15517,23827,Increasing SEC Effectiveness through Mobile and Web Applications,2011,This document proposes an architecture to produce software for use by U.S. soldiers on commercially available smart phones and other mobile hardware to increase their productivity. The architecture decreases time to production while maintaining Department of Defense Information Assurance standards.
2652215,15517,22232,Wolverine: Battling Bugs with Interpolants (Competition Contribution),2012,"Wolverine is a software verifier that checks safety proper- ties of sequential ANSI-C and C++ programs, deploying Craig interpola- tion to derive program invariants. We describe the underlying approach and the architecture, and provide instructions for installation and usage."
86939,15517,8422,TRACER: a symbolic execution tool for verification,2012,"We present tracer, a verifier for safety properties of sequential C programs. It is based on symbolic execution (se) and its unique features are in how it makes se finite in presence of unbounded loops and its use of interpolants from infeasible paths to tackle the path-explosion problem."
167255,15517,8422,The TAMARIN prover for the symbolic analysis of security protocols,2013,"The Tamarin prover supports the automated, unbounded, symbolic analysis of security protocols. It features expressive languages for specifying protocols, adversary models, and properties, and support for efficient deduction and equational reasoning. We provide an overview of the tool and its applications."
2253373,15517,23620,A typed store-passing translation for general references,2011,We present a store-passing translation of System  F  with general references into an extension of System  F  ω  with certain well-behaved recursive kinds. This seems to be the first type-preserving store-passing translation for general references. It can be viewed as a purely syntactic account of a possible worlds model.
2203030,15517,9438,Designing technical action research and generalizing from real-world cases,2012,"This tutorial presents a sound methodology for technical action research, which consist of testing a new artifact by using it to solve a real problem. Such a test would be useless if we could not generalize from it, and the tutorial introduces architectural inference as a way of supporting generalizations by technical action research."
5598,15517,22232,BULL: a library for learning algorithms of boolean functions,2013,"We present the tool BULL (Boolean fUnction Learning Library), the first publicly available implementation of learning algorithms for Boolean functions. The tool is implemented in C with interfaces to C++, JAVA and OCAML. Experimental results show significant advantages of Boolean function learning algorithms over all variants of the L* learning algorithm for regular languages."
422357,15517,22232,Reliable software development: analysis-aware design,2011,The application of formal methods in software development does not have to be an all-or-nothing proposition. Progress can be made with the introduction of relatively unobtrusive techniques that simplify analysis. This approach is meant replace traditional analysis-agnostic coding with an analysis-aware style of software development.
1205111,15517,20524,JPF-AWT: Model checking GUI applications,2011,Verification of Graphical User Interface (GUI) applications presents many challenges. GUI applications are open systems that are driven by user events. Verification of such applications by means of model checking therefore requires a user model in order to close the state space.
2704768,15517,10973,Concurrent Strategies,2011,"A bi category of very general nondeterministic concurrent games and strategies is presented. The intention is to formalize distributed games in which both Player (or a team of players) and Opponent (or a team of opponents) can interact in highly distributed fashion, without, for instance, enforcing that their moves alternate."
2432326,15517,23865,Apples vs. oranges?: an exploration of the challenges of comparing the source code of two software systems,2011,"We attempt to compare the source code of two Java IDE systems: Netbeans and Eclipse. The result of this experiment shows that many factors, if ignored, could risk a bias in the results, and we posit various observations that should be taken into consideration to minimize such risk."
1469934,15517,8385,Good technology makes the difficult task easy,2013,A new language for chip design is presented. The main advantages of the language are explicit conveyer and parallel features fully controlled by the author of chip design. Non trivial industrial example is under discussion. There are run-time estimations and comparison with traditional programming in C.
1153926,15517,517,EvoSuite at the SBST 2013 Tool Competition,2013,"EvoSuite is a mature research prototype that automatically generates unit tests for Java code. This paper summarizes the results and experiences in participating at the unit testing competition held at SBST 2013, where EvoSuite ranked first with a score of 156.95."
986195,15517,20524,Abstraction-aware verifying compiler for yet another MDD,2014,"This paper rethinks both modularity and compilation in the light of abstraction between design and implementation. We propose a new compilation approach called  abstraction-aware verifying compiler , in which abstraction is the target of compilation. Both a design model and its code are inputted as the first-class software modules to the compiler."
1544332,15517,20524,Automatic assessment of software documentation quality,2011,"In this paper I describe the concept of my dissertation, which aims at adapting the Continuous Quality Monitoring Method (CQMM) for the automated quality assessment of software documentation using a developed document quality analysis framework and software document quality rules which represent best practices for software documentation."
1063747,15517,8385,REDACT: preventing database deadlocks from application-based transactions,2013,"In this demonstration, we will present a database deadlocks prevention system that visualizes our algorithm for detecting hold-and-wait cycles that specify how resources (e.g., database tables) are locked and waited on to be locked during executions of SQL statements and utilizes those cycles information to prevent database deadlocks automatically."
1647285,15517,10973,"Non-definability of Languages by Generalized First-order Formulas over (N,+)",2012,"We consider first-order logic with monoidal quantifiers over words. We show that all languages with a neutral letter, definable using the addition predicate are also definable with the order predicate as the only numerical predicate. Let S be a subset of monoids. Let L be the logic closed under quantification over the monoids in S. Then we prove that L["
2081401,15517,23827,Log-based testing,2012,"This thesis presents an ongoing research on using logs for software testing. We propose a complex and generic logging and diagnosis framework, that can be efficiently used for continuous testing of future Internet applications. To simplify the diagnosis of logs we suggest to reduce its size by means of rewriting."
1441068,15517,23827,UML in practice,2013,UML has been described by some as “the lingua franca of software engineering”. Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry - if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.
1831083,15517,23827,A study on improving static analysis tools: why are we not using them?,2012,"Using static analysis tools for automating code inspections can be beneficial for software engineers. Despite the benefits of using static analysis tools, research suggests that these tools are underused. In this research, we propose to investigate why developers are not widely using static analysis tools and how current tools could potentially be improved to increase usage."
237793,15517,23827,Modelling and Knowledge Management for Sustainable Development,2012,"This paper introduces the motivation and aim of the 1st International Symposium on Modelling and Knowledge Management for Sustainable Development MoKMaSD 2012, inspired by the POST-2015 UN Development Agenda. Then the keynote paper and the four contributed papers presented at the Symposium are summarised and related to the POST-2015 UN Development Agenda."
51618,15517,23827,Idealized fault-tolerant components in requirements engineering,2011,"We have previously proposed a requirements development process, DREP, for dependable systems. In this paper, we draw a parallel between the notions defined in DREP and the elements of idealized fault-tolerant components (IFTCs). We show how the key ideas of IFTCs can be re-interpreted at the requirements engineering level and mapped to DREP concepts."
2564048,15517,23827,Summary of the ICSE 2012 workshops,2012,The workshops of ICSE 2012 provide a forum for researchers and practitioners to exchange and discuss scientific ideas before they have matured to warrant conzference or journal publication. ICSE Workshops also serve as incubators for scientific communities that form and share a particular research agenda.
1067587,15517,21102,Development of distributed fuzzy systems with a runtime-adaptable mobile components framework,2012,"A distributed fuzzy system is a real-time fuzzy system in which the input, output and computation may be located on different networked computing nodes. The ability for a distributed software application, such as a distributed fuzzy system, to adapt to changes in the computing network at runtime can provide real-time performance improvement and fault-tolerance."
1965642,15517,8385,SecuriTAS: a tool for engineering adaptive security,2012,"This paper presents SecuriTAS, a tool to engineer adaptive security. It allows software designers to model security concerns together with the requirements of a system. This model is then used at runtime to analyze changes in security concerns and select the best set of security controls necessary to protect the system."
1965377,15517,517,What Can We Learn from In-process Metrics on Issue Management? -- Insights from an Industrial Case Study,2013,In this article we point out that issue management in an industrial context is an excellent foundation for addressing prevailing research questions on state of the practice software engineering. Further we briefly report on challenges in implementing a dashboard for issue management and discuss the main research questions in exploiting historical data.
680014,15517,9080,An interpolation based crossover operator for genetic programming,2013,"This paper presents a new crossover operator for genetic programming. We exploit two concepts of formal methods: Weakest precondition and Craig interpolation, to perform semantically aware crossover. Weakest preconditions are used to locate faulty parts of a program and Craig interpolation is utilized to correct these ones."
1119288,15517,9080,Automated test generation for multi-state systems,2013,This paper describes a genetic algorithm based on mutation testing to generate test cases for classes with multiple states. The fitness function is based on the  coverability  and the  killability  of the individuals. The paper includes a small empirical section that shows evidences of the ability of the algorithm to generate good test cases.
2227280,15517,23827,APISynth: a new graph-based API recommender system,2014,"Current API recommendation tools yield either good recall ratio or good precision, but not both. A tool named APISynth is proposed in this paper by utilizing a new graph based approach. Preliminary evaluation demonstrates that APISynth wins over the state of the art with respect to both the two criteria."
993276,15517,23827,Calibrating use case points,2014,An approach to calibrate the complexity weights of the use cases in the Use Case Points (UCP) model is put forward. The size metric used is the Use Case Points (UCP) which can be calculated from the use case diagram along with its use case scenario as described in the UCP model. The approach uses a neural network with fuzzy logic to tune the complexity weights.
2107168,15517,23827,A framework for self-healing software systems,2013,"I present an approach to avoid functional failures at runtime in component-based application systems. The approach exploits the intrinsic redundancy of components to find workarounds as alternative sequences of operations to avoid a failure. A first Java prototype is presented, and an evaluation plan, as some preliminary results, are discussed."
1460293,15517,339,"Protection, usability and improvements in reflected XSS filters",2012,"Due to the high popularity of Cross-Site Scripting (XSS) attacks, most major browsers now include or support filters to protect against reflected XSS attacks. Internet Explorer and Google Chrome provide built-in filters, while Firefox supports extensions that provide this functionality. However, these filters all have limitations."
943183,15517,517,Introducing Test Case Derivation Techniques into Traditional Software Development: Obstacles and Potentialities,2011,"In traditional development, extracting test cases manually is an effort-consuming and error-prone process. To examine whether automation techniques can be integrated into such traditional development, we implemented our previously proposed method to TesMa, a test case generation tool. We had a case study to evaluate the effectiveness and the cost."
2157385,15517,23827,JDeodorant: identification and application of extract class refactorings,2011,"Evolutionary changes in object-oriented systems can result in large, complex classes, known as God Classes. In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from God Classes and automatically apply the refactoring chosen by the developer."
269279,15517,8884,A hybrid natural language approach to manage semantic interoperability for public health analytics,2013,"This paper discusses the integration of an ontology with a natural language query engine to calculate and interpret epidemiological indicators for population health assessment. In this paper, we discuss the application of this approach to one type of possible query, which retrieves health determinants, causally associated with diabetes mellitus."
804098,15517,517,TAIC PART 2014 Workshop Overview,2014,The ninth Workshop on Testing: Academic and Industrial Conference - Practice and Research Techniques (TAIC PART 2014) provides an opportunity for academics and practitioners to exchange ideas and viewpoints and discuss possible avenues for collaboration in software testing. This paper presents an overview of the workshop and its contributions.
1310106,15517,517,TAIC PART 2013 Workshop Summary,2013,The 8th Workshop on Testing: Academic and Industrial Conference - Practice and Research Techniques (TAIC PART 2013) brings together representatives of both industry and academia to foster discussion and collaboration on software testing problems of high practical relevance. This paper summarizes the workshop TAIC PART 2013 and its contributions.
2196887,15517,23827,A bidirectional model-driven spreadsheet environment,2012,"In this extended abstract we present a bidirectional model-driven framework to develop spreadsheets. By being model driven, our approach allows to evolve a spreadsheet model and automatically have the data co-evolved. The bidirectional component achieves precisely the inverse, that is, to evolve the data and automatically obtain a new model to which the data conforms."
145411,15517,23634,Automated Support for the Investigation of Paraconsistent and Other Logics,2013,"We automate the construction of analytic sequent calculi and effective semantics for a large class of logics formulated as Hilbert calculi. Our method applies to infinitely many logics, which include the family of paraconsistent C-systems, as well as to other logics for which neither analytic calculi nor suitable semantics have so far been available."
1454509,15517,20524,mbeddr: instantiating a language workbench in the embedded software domain,2013,"Tools can boost software developer productivity, but building custom tools is prohibitively expensive, especially for small organizations. For example, embedded programmers often have to use low-level C with limited IDE support, and integrated into an off-the-shelf tool chain in an ad-hoc way."
2379601,15517,23827,Automatic performance modeling of multithreaded programs,2014,"Multithreaded programs express a complex non-linear dependency between their configuration and the performance. To better understand this dependency performance prediction models are used. However, building performance models manually is time-consuming and error-prone. We present a novel methodology for automatically building performance models of industrial multithreaded programs."
2273819,15517,22288,Towards choreography-based process distribution in the cloud,2011,"Choreographies provide means to describe collaborations. Each partner runs its own processes. To reduce the amount of data exchanged and to save resources, part of the choreography can be run on a community cloud. We show how private parts of a choreography can still be run on-premise and how non-private parts can be merged to make use of the cloud infrastructure."
1483854,15517,23827,Service descriptions and linked data for integrating WSNs into enterprise IT,2012,This paper presents our ongoing work on enterprise IT integration of sensor networks based on the idea of service descriptions and applying linked data principles to them. We argue that using linked service descriptions facilitates a better integration of sensor nodes into enterprise IT systems and allows SOA principles to be used within the enterprise IT and within the sensor network itself.
2099804,15517,517,Making the Case for MORTO: Multi Objective Regression Test Optimization,2011,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.
173354,15517,22113,An epistemic Halpern-Shoham logic,2013,We define a family of epistemic extensions of Halpern-Shoham logic for reasoning about temporal-epistemic properties of multi-agent systems. We exemplify their use and study the complexity of their model checking problem. We show a range of results ranging from PTIME to PSPACE-hard depending on the logic considered.
1708324,15517,23827,Town hall discussion of SE 2004 revisions (panel),2013,"This panel will engage participants in a discussion of recent changes in software engineering practice that should be reflected in curriculum guidelines for undergraduate software engineering programs. Current progress in revising the guidelines will be presented, including suggestions to update coverage of agile methods, security and service-oriented computing."
999925,15517,23827,Proposing a theory of gamification effectiveness,2014,"Gamification informally refers to making a system more game-like. More specifically, gamification denotes applying game mechanics to a non-game system. We theorize that gamification success depends on the game mechanics employed and their effects on user motivation and immersion. The proposed theory may be tested using an experiment or questionnaire study."
1656351,15517,11058,Laws of concurrent programming,2014,"The talk extends the Laws of Programming [1] by four laws governing concurrent composition of programs. This operator is associative and commutative and distributive through union; and it has the same unit (do nothing) as sequential composition. Furthermore, sequential and concurrent composition distribute through each other, in accordance with an exchange law; this permits an implementation of concurrency by partial interleaving."
794331,15517,23876,SeByte: A semantic clone detection tool for intermediate languages,2012,SeByte is a semantic clone detection tool which accepts Java bytecode (binary) as input. SeByte provides a complementary approach to traditional pattern-based source code level clone detection. It is capable of detecting clones missed by existing clone detection tools since it exploits both pattern and content similarity at binary level.
1703131,15517,22288,Mining and analyzing the characteristic of projects collaborative relationship in open source software,2012,"This paper focus on the relationship between projects that have same participants. A definition of projects collaborative relationship has been presented, and a projects collaborative network model is constructed. By mining and analyzing more than 10000 projects from sourceforge.net, we find that the projects collaborative relationship of software has a great impact on software trustworthiness. The number of project's collaborative project and the proportion of project's participants, who also join in other projects, are mostly important factors of projects collaborative network."
180993,15517,22232,Demonstrating learning of register automata,2012,"We will demonstrate the impact of the integration of our most recently developed learning technology for inferring Register Automata into the LearnLib, our framework for active automata learning. This will not only illustrate the unique power of Register Automata, which allows one to faithfully model data independent systems, but also the ease of enhancing the LearnLib with new functionality."
1823021,15517,9080,Transition coverage testing for simulink/stateflow models using messy genetic algorithms,2011,This paper introduces a messy-GA for transition coverage of Simulink/StateFlow models. We introduce a tool that implements our approach and evaluate it on three benchmark embedded system Simulink models. Our messy-GA is able to achieve statistically significantly better coverage when compared to both random search and to a commercial tool for Simulink/StateFlow model Testing.
2150954,15517,22232,Lazy-CSeq: a lazy sequentialization tool for c (competition contribution),2014,"We describe a version of the lazy sequentialization schema by La Torre, Madhusudan, and Parlato that is optimized for bounded programs, and avoids the re-computation of the local state of each process at each context switch. Lazy-CSeq implements this sequentialization schema for sequentially consistent C programs using POSIX threads. Experiments show that it is very competitive."
1616579,15517,10973,Modular Construction of Cut-free Sequent Calculi for Paraconsistent Logics,2012,This paper makes a substantial step towards automatization of Para consistent reasoning by providing a general method for a systematic and modular generation of cut-free calculi for thousands of Para consistent logics known as Logics of Formal (In)consistency. The method relies on the use of non-deterministic semantics for these logics.
1518608,15517,23827,Eliminative induction: a basis for arguing system confidence,2013,"Assurance cases provide a structured method of explaining why a system has some desired property, e.g., that the system is safe. But there is no agreed approach for explaining what degree of confidence one should have in the conclusions of such a case. In this paper, we use the principle of eliminative induction to provide a justified basis for assessing how much confidence one should have in an assurance case argument."
1472387,15517,8806,Combining research and education of software testing: a preliminary study,2014,"This paper reports a preliminary study on combining research and education of software testing. We introduce some industrial-strength programs, from the open-source projects for research, into the assignments of system testing. Research assistants and teaching assistants work together to establish and evaluate the assignments of system testing. Our preliminary results show that research and education of software testing can benefit each other in this way."
2343626,15517,23827,A decision support system for the classification of software coding faults: a research abstract,2011,"A decision support system for fault classification is presented. The fault classification scheme is developed to provide guidance in process improvement and fault-based testing. The research integrates results in fault classification, source code analysis, and fault-based testing research. Initial results indicate that existing change type and fault classification schemes are insufficient for this purpose. Development of sufficient schemes and their evaluation are discussed."
1084517,15517,8385,A publication culture in software engineering (panel),2013,"This panel will discuss what characterizes the publication process in the software engineering community and debate how it serves the needs of the community, whether it is fair - e.g. valuable work gets published and mediocre work rejected - and highlight the obstacles for young scientists. The panel will conclude with a discussion on suggested next steps."
1871183,15517,23827,StakeSource2.0: using social networks of stakeholders to identify and prioritise requirements,2011,"Software projects typically rely on system analysts to conduct requirements elicitation, an approach potentially costly for large projects with many stakeholders and requirements. This paper describes StakeSource2.0, a web-based tool that uses social networks and collaborative filtering, a crowdsourcing approach, to identify and prioritise stakeholders and their requirements."
864476,15517,517,Analysis of Test Clusters for Regression Testing,2012,Samples from clusters of tests are used to automatically predict if tests are to be re-clustered for a modified program. Tests likely to be affected by line level changes are included in the samples by analyzing their execution profiles based on spatial locality. Initial experiments show that the approach can potentially avoid re-clustering in many cases.
1393971,15517,10973,Turing Machines with Atoms,2013,"We study Turing machines over sets with atoms, also known as nominal sets. Our main result is that deterministic machines are weaker than nondeterministic ones; in particular, P6=NP in sets with atoms. Our main construction is closely related to the Cai-Furer-Immerman graphs used in descriptive complexity theory."
2378713,15517,8422,Software Model Checking for People Who Love Automata,2013,"In this expository paper, we use automata for software model checking in a new way. The starting point is to fix the alphabet: the set of statements of the given program. We show how automata over the alphabet of statements can help to decompose the main problem in software model checking, which is to find the right abstraction of a program for a given correctness property."
2165848,15517,8385,Inferring data polymorphism in systems code,2011,"We describe techniques for analyzing data polymorphism in C, and show that understanding data polymorphism is important for statically verifying type casts in the Linux kernel, where our techniques prove the safety of 75% of downcasts to structure types, out of a population of 28767. We also discuss prevalent patterns of data polymorphism in Linux, including code patterns we can handle and those we cannot."
2413240,15517,517,Agent-Based Test Case Prioritization,2011,In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.
167339,15517,22232,Proving Nontermination via safety,2014,"We show how the problem of nontermination proving can be reduced to a question of underapproximation search guided by a safety prover. This reduction leads to new nontermination proving implementation strategies based on existing tools for safety proving. Our preliminary implementation beats existing tools. Furthermore, our approach leads to easy support for programs with unbounded nondeterminism."
1200308,15517,23620,Game semantics for interface middleweight Java,2014,"We consider an object calculus in which open terms interact with the environment through interfaces. The calculus is intended to capture the essence of contextual interactions of Middleweight Java code. Using game semantics, we provide fully abstract models for the induced notions of contextual approximation and equivalence. These are the first denotational models of this kind."
3083707,15517,8967,A study of Linux file system evolution,2013,"We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux file-system changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools."
48963,15517,9438,Ten open challenges at the boundaries of software engineering and information systems,2011,"In this talk, intended to provoke discussion, I will suggest ten important open challenges at boundaries where Software Engineering & Information Systems meet. I will focus on challenges are both intellectually demanding and of industrial importance. I will suggest some approaches to meeting these challenges and will lay stress upon the interdisciplinary opportunities they give rise to."
2131715,15517,23827,Managing evolution of software product line,2012,"In software product line engineering, core assets are shared among multiple products. Core assets and products generally evolve independently. Developers need to capture evolution in both contexts and to propagate changes in both directions between the core assets and the products. We propose a version control system to support product line engineering by supporting the evolution of product line, product derivation, and change propagation from core assets to products and vice versa."
2326031,15517,23827,Synchronizing asynchronous conformance testing,2011,We present several theorems and their proofs which enable using synchronous testing techniques such as input output conformance testing (ioco) in order to test implementations only accessible through asynchronous communication channels. These theorems define when the synchronous test-cases are sufficient for checking all aspects of conformance that are observable by asynchronous interaction with the implementation under test.
1804667,15517,23827,Restructuring unit tests with TestSurgeon,2012,"The software engineering community has produced great techniques for software maintainability, however, less effort is dedicated to have unit tests modular and extensible. TestSurgeon is a profiler for unit tests which collects information from tests execution. It proposes a metric for similarity between tests and provides a visualization to help developers restructure their unit tests."
2202045,15517,22232,CSeq: A sequentialization tool for C (Competition Contribution),2013,Sequentialization translates concurrent programs into equivalent non- deterministic sequential programs so that the different concurrent schedules no longer need to be handled explicitly. It can thus be used as a concurrency pre- processor for many sequential program verification techniques. CSeq implements sequentialization for C and uses ESBMC as sequential verification backend (5).
1780891,15517,23827,Program transformations to fix C buffer overflows,2014,"This paper describes two program transformations to fix buffer overflows originating from unsafe library functions and bad pointer operations. Together, these transformations fixed all buffer overflows featured in 4,505 programs of NIST’s SAMATE reference dataset, making the changes automatically on over 2.3 million lines of C code."
1762515,15517,20524,"Decomposing feature models: language, environment, and applications",2011,"Variability in software product lines is often expressed through feature models (FMs). To handle the complexity of increasingly larger FMs, we propose semantically meaningful decomposition support through a slicing operator. We describe how the slicing operator is integrated into the FAMILIAR environment and how it can be combined with other operators to support complex tasks over FMs in different case studies."
2547447,15517,23865,"Why, when, and what: Analyzing Stack Overflow questions by topic, type, and code",2013,"Questions from Stack Overflow provide a unique opportunity to gain insight into what programming concepts are the most confusing. We present a topic modeling analysis that combines question concepts, types, and code. Using topic modeling, we are able to associate programming concepts and identifiers (like the String class) with particular types of questions, such as, “how to perform encoding”."
1102393,15517,23827,Predicting Faculty Performance Using Regression Model in Data Mining,2011,This paper investigates the different attributes used in evaluating faculty performance to come up with a regression model that predicts faculty performance. The main objective of this paper is to develop a model for predicting faculty performance and design a framework of data mining implementing ETL. The outcome of this research could be used as basis in improving the instruction in an academic institution.
1364066,15517,23827,Distributed parallel algorithm for numerical solving of 3D problem of fluid dynamics in anisotropic elastic porous medium using MapReduce and MPI technologies,2014,"Paper presents an advanced iterative MapReduce solution that employs Hadoop and MPI technologies. First, we present an overview of working implementations that make use of the same technologies. Then we define an academic example of numeric problem with an emphasis on its computational features. The named definition is used to justify the proposed solution design."
1724740,15517,10973,Stone Duality for Markov Processes,2013,"We define Aumann algebras, an algebraic analog of probabilistic modal logic. An Aumann algebra consists of a Boolean algebra with operators modeling probabilistic transitions. We prove a Stone-type duality theorem between countable Aumann algebras and countably-generated continuous-space Markov processes. Our results subsume existing results on completeness of probabilistic modal logics for Markov processes."
1606524,15517,8806,Process-aware web programming with Jolie,2013,"We present a programming framework, based upon the Jolie language, for the native modelling of process-aware web information systems. Our major contribution is to offer a unifying approach for the programming of distributed architectures based on HTTP that support typical features of the process-oriented paradigm, such as structured communication flows and multiparty sessions."
1171278,15517,23827,Statistical learning of API mappings for language migration,2014,"The process of migrating software between languages is called language migration or code migration. To reduce manual effort in defining the rules of API mappings for code migration, we propose StaMiner, a data-driven model that statistically learns the mappings between API usages from the corpus of the corresponding methods in the client code of the APIs in two languages."
154524,15517,8422,Better termination proving through cooperation,2013,One of the difficulties of proving program termination is managing the subtle interplay between the finding of a termination argument and the finding of the argument's supporting invariant. In this paper we propose a new mechanism that facilitates better cooperation between these two types of reasoning. In an experimental evaluation we find that our new method leads to dramatic performance improvements.
2571986,15517,9436,Unknown knowns: Tacit knowledge in requirements engineering,2011,"Summary form only given. Due to increasing complexity of software in embedded systems, the software development requires approaches that can manage that complexity in a similar way as this is done in general-purpose software, but at the same time provide support for embedded systems specifics. In this paper we give a short overview of a component-based approach that meets these requirements."
727205,15517,517,A Method of Making Single Function Tests for Constituting Scenario Tests,2013,"In designing scenario tests, it is difficult to suppose how the target software will be used. We regard a scenario test as testing a flow of multiple functions. And we propose a method to design scenario tests by combining single function tests in a structured way. In our method, a single-function test consists of one trigger and some conditions. In this way, the single-function tests are easily combined for designing scenario tests. Our method can generate single-function tests for each function and scenario tests in a systematic manner at low cost."
24168,15517,22232,Concurrent Depth-First Search Algorithms,2014,"We present concurrent algorithms, based on depth-first search, for three problems relevant to model checking: given a state graph, to find its strongly connected components, which states are in loops, and which states are in lassos. Our algorithms typically exhibit about a four-fold speed-up over the corresponding sequential algorithms on an eight-core machine."
1793662,15517,20524,Querying source code with natural language,2011,One common task of developing or maintaining software is searching the source code for information like specific method calls or write accesses to certain fields. This kind of information is required to correctly implement new features and to solve bugs. This paper presents an approach for querying source code with natural language.
2229287,15517,9080,Generation of tests for programming challenge tasks using evolution algorithms,2011,"In this paper, an automated method for generation of tests in order to detect inefficient (slow) solutions for programming challenge tasks is proposed. The method is based on genetic algorithms.   The proposed method was applied to a task from the Internet problem archive - the Timus Online Judge. For this problem, none of the existed solutions passed the generated set of tests."
1845284,15517,20524,Automatic recovery of statecharts from procedural code,2012,"We have developed a static-analysis algorithm that extracts statecharts from procedural implementations of state machines. The extracted statecharts are semantically-equivalent to the original program, and can be used for further development instead of the procedural code. We have implemented this algorithm in a tool called StatRec. We report on the results of running StatRec on a number of examples, including an implementation of the TCP protocol."
1352764,15517,8385,Checking conformance of a producer and a consumer,2011,"This paper addresses the problem of identifying incompatibilities between two programs that operate in a producer/consumer relationship. It describes the techniques that are incorporated in a tool called PCCA (Producer-Consumer Conformance Analyzer), which attempts to (i) determine whether the consumer is prepared to accept all messages that the producer can emit, or (ii) find a counter-example: a message that the producer can emit and the consumer considers ill-formed."
1748443,15517,20561,End-User Involvement and Team Factors in Business Process Modeling,2012,"We study the impact of end-user involvement and team factors on model quality and consensus. By end-user involvement we mean the degree to which participants of a modeling session are involved in the active creation of models, i.e. the drawing of the diagram. We find that higher end-user involvement, facilitated by tool support, increases model quality. Complementary teams achieve better consensus than matched teams."
2512796,15517,517,An Experience Report on Using Code Smells Detection Tools,2011,"Detecting code smells in the code and consequently applying the right refactoring steps when necessary is very important to improve the quality of the code. Different tools have been proposed for code smell detection, each one characterized by particular features. The aim of this paper is to describe our experience on using different tools for code smell detection. We outline the main differences among them and the different results we obtained."
2080821,15517,9436,Vacuous real-time requirements,2011,"We introduce the property of vacuity for requirements. A requirement is vacuous in a set of requirements if it is equivalent to a simpler requirement in the context of the other requirements. For example, the requirement “if A then B” is vacuous together with the requirement “not A”. The existence of a vacuous requirement is likely to indicate an error. We give an algorithm that proves the absence of this kind of error for real-time requirements. A case study in an industrial context demonstrates the practical potential of the algorithm."
2261068,15517,23827,Summary of the ICSE 2012 tutorials and technical briefings,2012,"This year ICSE is offering a mix of half-day and full day tutorials in addition to shorter technical briefings in selected domains. Whereas tutorials cover a wide range of mature topics of both academic and practical interest, technical briefings are intended to provide a compact introduction to the state-of-the-art in an emerging area."
1504077,15517,23827,1st FME workshop on formal methods in software engineering (FormaliSE 2013),2013,"Despite their significant advantages, formal methods are not widely used in industrial software development. Following the successful workshops we organized at ICSE 2103 in San Francisco, and ICSE 2014 in Hyderabad, we organize a third edition of the FormaliSE workshop with the main goal to promote the integration between the formal methods and the software engineering communities."
1362216,15517,8385,Toward measuring program comprehension with functional magnetic resonance imaging,2012,"Program comprehension is an often evaluated, internal cognitive process. In neuroscience,  functional magnetic resonance imaging (fMRI)  is used to visualize such internal cognitive processes. We propose an experimental design to measure program comprehension based on fMRI. In the long run, we hope to answer questions like  What distinguishes good programmers from bad programmers?  or  What makes a good programmer?"
688970,15517,10973,Dynamic Logic of Propositional Assignments: A Well-Behaved Variant of PDL,2013,"We study a version of Propositional Dynamic Logic (PDL) that we call Dynamic Logic of Propositional Assignments (DL-PA). The atomic programs of DL-PA are assignments of propositional variables to true or to false. We show that DL-PA behaves better than PDL, having e.g. compactness and eliminability of the Kleene star. We establish tight complexity results: both satisfiability and model checking are EXPTIME-complete."
1195027,15517,23827,Recent Trends in Graduate Software Engineering,2013,"This panel will discuss recent trends in graduate software engineering programs, including adoption of GSwE2009, cooperative programs between schools, increasing use of distance education formats, and specialization of programs for industry partners. Panelists will also discuss the evolving relationship of software engineering to other disciplines, such as computer science and systems engineering."
1490633,15517,23620,Verified squared: does critical software deserve verified tools?,2011,"The formal verification of programs has progressed tremendously in the last decade. In this talk, I review some of the obstacles that [6, 8, 15, 18] remain to be lifted before source-level verification tools can be taken really seriously in the critical software industry. A direction I advocate is the systematic formal verification of the development tools that participate in the production and verification of critical software."
1574336,15517,9080,A new framework for scalable genetic programming,2012,"This paper presents a novel framework for scalable multi-objective genetic programming. We introduce a new program modeling aiming at facilitating programs' creation, execution and improvement. The proposed modeling allows making symbolic executions in such a way to reduce drastically the time of programs' executions and to allow well-founded programs recombination."
47533,15517,9438,Ontologies for Security Requirements: A Literature Survey and Classification,2012,"Despite existing methodologies in the field, most requirements engineers are poorly trained to define security requirements. This is due to a considerable lack of security knowledge. Some security ontologies have been proposed, but a gap still exists between the two fields of security requirement engineering and ontologies. This paper is a survey, it proposes an analysis and a typology of existing security ontologies and their use for requirements definition."
403142,15517,23827,Logical Abstract Domains and Interpretations,2011,"We give semantic foundations to abstract domains consisting in first order logic formulae in a theory, as used in verification tools or methods using SMT-solvers or theorem provers. We exhibit conditions for a sound usage of such methods with respect to multi-interpreted semantics and extend their usage to automatic invariant generation by abstract interpretation."
2187473,15517,23876,The Concept of Stratified Sampling of Execution Traces,2011,"Execution traces can be overwhelmingly large. To reduce their size, sampling techniques, especially the ones based on random sampling, have been extensively used. Random sampling, however, may result in samples that are not representative of the original trace. We propose a trace sampling framework based on stratified sampling that not only reduces the size of a trace but also results in a sample that is representative of the original trace by ensuring that the desired characteristics of an execution are distributed similarly in both the sampled and the original trace."
1537505,15517,20524,Toward consistency checking of natural language temporal requirements,2011,"We discuss the problem of identifying inconsistencies in temporal requirements expressed as natural language text. We propose a partially automated approach that aims to minimize analysts' workload and improve accuracy. As one of the ingredients of the approach, we introduce a formal language to represent temporal requirements precisely and unambiguously. We call this language Temporal Action Language (TAL)."
64632,15517,8422,Cubicle: a parallel SMT-based model checker for parameterized systems: tool paper,2012,Cubicle is a new model checker for verifying safety properties of parameterized systems. It implements a parallel symbolic backward reachability procedure using Satisfiabilty Modulo Theories. Experiments done on classic and challenging mutual exclusion algorithms and cache coherence protocols show that Cubicle is effective and competitive with state-of-the-art model checkers.
2517103,15517,23827,ConcernReCS: finding code smells in software aspectization,2012,"Refactoring object-oriented (OO) code to aspects is an error-prone task. To support this task, this paper presents ConcernReCS, an Eclipse plug-in to help developers to avoid recurring mistakes during software aspectization. Based on a map of concerns, ConcernReCS automatically finds and reports error-prone scenarios in OO source code; i.e., before the concerns have been refactored to aspects."
1542760,15517,10973,Bisimilarity of Pushdown Automata is Nonelementary,2013,"Given two pushdown automata, the bisimilarity problem asks whether the infinite transition systems they induce are bisimilar. While this problem is known to be decidable our main result states that it is nonelementary, improving EXPTIME-hardness, which was the best previously known lower bound for this problem. Our lower bound result holds for normed pushdown automata as well."
2018917,15517,23827,Fourth international workshop on multicore software engineering (IWMSE 2011),2011,"This paper summarizes the highlights of the Fourth International Workshop on Multicore Software Engineering (IWMSE 2011). The workshop addresses the software engineering and parallel programming challenges that come with the wide availability of multicore processors. Researchers and practitioners have come together to present and discuss new work on programming techniques, refactoring, performance engineering, and applications."
1633302,15517,20754,Driving Secure Software Development Experience in a Diverse Product Environment,2012,"Siemens' central security team drives secure software development across a diverse product portfolio. From factory automation to wind turbines, Siemens builds security in by activities including standardizing roles and responsibilities, threat and risk analysis, and product security risk management across Siemens' 15,000 software developers."
1738709,15517,10973,A Compositional Semantics for the Reversible p-Calculus,2013,"We introduce a labelled transition semantics for the reversible p-calculus. It is the first account of a compositional definition of a reversible calculus, that has both concurrency primitives and name mobility. The notion of reversibility is strictly linked to the notion of causality. We discuss the notion of causality induced by our calculus, and we compare it with the existing notions in the literature, in particular for what concerns the syntactic feature of scope extrusion, typical of the p-calculus."
1299282,15517,23827,Multi-Agent control system,2014,"The paper deals with distributed planning in a Multi-Agent System (MAS) constituted by several intelligent agents each one has to interact with the other autonomous agents. The problem faced is how to ensure a distributed planning through the cooperation in our multi-agent system. Finally, we use JADE platform to create agents and ensure the communication between them. A Benchmark Production System is used as a running example to explain our contribution."
671840,15517,8422,Engineering a Static Verification Tool for GPU Kernels,2014,"We report on practical experiences over the last 2.5 years related to the engineering of GPUVerify, a static verification tool for OpenCL and CUDA GPU kernels, plotting the progress of GPUVerify from a prototype to a fully functional and relatively efficient analysis tool. Our hope is that this experience report will serve the verification community by helping to inform future tooling efforts."
1127944,15517,8385,IWPSE-EVOL 2011: 12th international workshop on principles on software evolution and 7th ERCIM workshop on software evolution,2011,"The IWPSE-EVOL'2011 workshop is the merger of the 12th International Workshop on Principles of Software Evolution (IWPSE) and the 7th annual ERCIM Workshop on Software Evolution (EVOL). The objectives of this joint event is to provide a forum to discuss a wide range of topics in software evolution, to foster the better understanding of the nature of software evolution, and to accelerate research activities on the subject."
2183736,15517,23827,Integrating tools and frameworks in undergraduate software engineering curriculum,2012,"We share our experience over the last 10 years for finding, deploying and evaluating software engineering (SE) technologies in an undergraduate program at the ETS in Montreal, Canada. We identify challenges and propose strategies to integrate technologies into an SE curriculum. We demonstrate how technologies are integrated throughout our program, and provide details of the integration in two specific courses."
1974028,15517,23827,Petri nets state space analysis in the cloud,2012,"Several techniques for addressing the state space explosion problem in model checking have been studied. One of these is to use distributed memory and computation for storing and exploring the state space of the model of a system. In this report, we present and compare different multi- thread, distributed, and cloud approaches to face the state-space explosion problem. The experiments report shows the convenience (in particular) of cloud approaches."
2652337,15517,22232,"Symbiotic: Synergy of Instrumentation, Slicing, and Symbolic Execution - (Competition Contribution)",2013,"Symbiotic is a tool for detection of bugs described by finite#N#state machines in C programs. The tool combines three#N#well-known techniques: instrumentation, program slicing, and#N#symbolic execution. This paper briefly describes the approach#N#of Symbiotic including its strengths, weaknesses, and#N#modifications for SV-COMP 2013. Architecture and installation#N#of the tool are described as well."
2134531,15517,517,Looking for Patterns in Code Bad Smells Relations,2011,"Code smells are the named design anomalies that indicate to a need for refactoring. Due to their diverse and ambiguous nature, their identification in code requires complex methods. In the paper we formulate a hypothesis that some smells make associations that are repeatable and can be treated as patterns. We present also early results of investigation of two Large Class-related patterns."
1834931,15517,8385,Estimating the effectiveness of spectrum-based fault localization,2014,"Spectrum-Based Fault Localization (SBFL) techniques calculate risk values to predict buggy units in a program,but they may cause heavy manual work when the calculated risk values are not reasonable on some application scenarios. In this paper, presents a preliminary study to estimate the effectiveness of SBFL before manual code walk through, so that we can decide whether to adopt SBFL for a given application."
2772987,15517,9438,Transforming enterprise ontologies into SBVR formalizations,2013,In 2007 the Object Management Group (OMG) adopted the Se- mantics of Business Vocabulary and Business Rules (SBVR) specification. The languages specified by this specification must be used to create business vocab- ularies and business rules of all kinds of business activities of all kinds of or- ganizations. This paper describes and demonstrates how enterprise ontologies can be transformed into SBVR formalizations.
892577,15517,20524,Fault-localization using dynamic slicing and change impact analysis,2011,"Spectrum-based fault-localization tools, such as Tarantula, have been developed to help guide developers towards faulty statements in a system under test. These tools report statements ranked in order of suspiciousness. Unfortunately, the reported statements can often be unrelated to the error. This paper evaluates the impact of several approaches to ignoring such unrelated statements in order to improve the effectiveness of fault-localization tools."
1329549,15517,20754,The Tests-versus-Proofs Conundrum,2014,"Fully proving the correctness of entire real-world software stacks is still not practical, despite impressive recent advances. At the same time, classic system testing is increasingly insufficient to make modern systems reliable and secure. To make progress, we must fuse formal methods with traditional testing practice into a unified approach."
1649854,15517,8806,Feedback-driven points-to analysis,2011,"In this paper, we present  feedback-driven  points-to analysis where any classical points-to analysis has its points-to results at certain program points guarded by a-priori upper bounds. Such upper bounds can come from other points-to analyses -- this is of interest when different approaches are not strictly ordered in terms of accuracy -- and from human insight, i.e., manual proofs that certain points-to relations are infeasible for every program run."
1365453,15517,23827,Education impact of evolutionary software development,2013,"The paradigm shift from waterfall to evolutionary software development (that includes agile development) has been widespread in industry, but academia is still struggling with it. This position paper reiterates the reasons for the paradigm shift that illustrate the importance of the shift. The position paper then discusses what the shift means for the software engineering education. As an example, it briefly presents how software engineering is taught at Wayne State University."
349124,15517,23827,Applying the framework of Turing machine developing system for implementing Java virtual machine,2012,"The Turing machine can be used for rigorous study of the computation theory. In theory the Turing machine can simulate modern computers, but the actual implementation would be very difficult. This paper proposes a framework of Turing machine developing system. This developing system has been used successfully to implement a special Turing machine that is capable of executing Java bytecode. Demonstrations of this paper have shown the effectiveness of adopting the Turing machine for practical computation. In addition, the developing system can be applied to demonstrate the principle and concept of Turing machines."
1107040,15517,10973,Game Semantics in String Diagrams,2012,"A dialogue category is a symmetric monoidal category equipped with a notion of tensorial negation. We establish that the free dialogue category is a category of dialogue games and total innocent strategies. The connection clarifies the algebraic and logical nature of dialogue games, and their intrinsic connection to linear continuations. The proof of the statement is based on an algebraic presentation of dialogue categories inspired by knot theory, and a factorization theorem established by rewriting techniques."
1581566,15517,517,Algorithms for Basic Compliance Problems,2013,"The present paper focuses on the problems of verifying compliance for global achievement and maintenance obligations. We first introduce the elements needed to identify and study compliance to these two classes of obligations in processes. Additionally, we define procedures and algorithms to efficiently deal with the identified compliance problem. We finally show that both algorithms proposed in the paper belong to the complexity class P."
2271491,15517,23827,Workshop on games and software engineering (GAS 2011),2011,"At the core of video games are complex interactions leading to emergent behaviors. This complexity creates difficulties architecting components, predicting their behaviors and testing the results. The Workshop on Games and Software Engineering (GAS 2011) provides an opportunity for software engineering researchers and practitioners who work with games to come together and discuss how these two areas can be intertwined."
813481,15517,20524,Domain and value checking of web application invocation arguments,2011,"Invocations are widely used by many web applications, but have been found to be a common source of errors. This paper presents a new technique that can statically verify that an invocation's set of argument names, types, and request method match the constraints of a target interface. An empirical evaluation of the technique shows that it is successful at identifying previously unknown errors in web applications."
507894,15517,22232,Alternating Runtime and Size Complexity Analysis of Integer Programs,2014,"We present a modular approach to automatic complexity analysis. Based on a novel alternation between finding symbolic time bounds for program parts and using these to infer size bounds on program variables, we can restrict each analysis step to a small part of the program while maintaining a high level of precision. Extensive experiments with the implementation of our method demonstrate its performance and power in comparison with other tools."
2445440,15517,23827,Refounding software engineering: the semat initiative (invited presentation),2012,"The new software engineering initiative, Semat, is in the process of developing a kernel for software engineering that stands on a solid theoretical basis. So far, it has suggested a set of kernel elements for software engineering and basic language constructs for defining the elements and their usage. This paper describes a session during which Semat results and status will be presented. The presentation will be followed by a discussion panel."
253424,15517,22232,Transition invariants and transition predicate abstraction for program termination,2011,"Originally, the concepts of transition invariants and transition predicate abstraction were used to formulate a proof rule and an abstraction-based algorithm for the verification of liveness properties of concurrent programs under fairness assumptions. This note applies the two concepts for proving termination of sequential programs. We believe that the specialized setting exhibits the underlying principles in a more direct way."
1865235,15517,23827,Using impact analysis in industry,2011,"Software is subjected to continuous change, and with increasing size and complexity performing changes becomes more critical. Impact analysis assists in estimating the consequences of a change, and is an important research topic. Nevertheless, until now researchers have not applied and evaluated those techniques in industry. This paper contributes an approach suitable for an industrial setting, and an evaluation of its application in a large software system."
2397142,15517,517,Refactoring as Testability Transformation,2011,"This paper briefly reviews the theory of Testability Transformation and outlines its implications for and relationship to refactoring for testing. The paper introduces testability refactorings, a subclass of Testability Transformations and discusses possible examples of testability refactorings. Several approaches to testability refactoring are also introduced. These include the novel concept of test-carrying code and the use of pare to optimization for balancing the competing needs of machine and human in search based testability refactoring."
967251,15517,8385,Artifact evaluation (summary),2013,The artifact evaluation committee (AEC) is in charge of evaluating data and software that accompany papers accepted at the ESEC/FSE'13 research track. Authors of more than 40% of the accepted papers have submitted an artifact (22 out of the 51 accepted papers). The AEC has positively evaluated more than 50% of the submitted artifacts. 12 out of the 22 artifacts have been graded as ``met expectations'' or ``exceeded expectations''.
1815070,15517,8912,Smart checklists for human-intensive medical systems,2012,"Human-intensive cyber-physical systems involve software applications and hardware devices, but also depend upon the expertise of human participants to achieve their goal. In this paper. we describe a project we have started to improve the effectiveness of such systems by providing Smart Checklists to support and guide human participants in carrying out their tasks, including their interactions with the devices and software applications."
136129,15517,8422,CacBDD: A BDD Package with Dynamic Cache Management,2013,"In this paper, we present CacBDD, a new efficient BDD Binary Decision Diagrams package. It implements a dynamic cache management algorithm, which takes account of the hit-rate of computed table and available memory. Experiments on the BDD benchmarks of both combinational circuits and model checking show that CacBDD is more efficient compared with the state-of-the-art BDD package CUDD."
2098510,15517,8806,WAVE-CIA: a novel CIA approach based on call graph mining,2013,"Software change impact analysis (CIA) is a key technique to identify the potential effects caused by software changes. In this paper, a new graph mining based CIA technique is proposed, which takes the interference among multiple proposed changes into account to improve the precision of the impact results. Empirical evaluations on two real-world software projects demonstrate the effectiveness of our CIA technique."
1490848,15517,8385,From software engineering to software systems (invited talk),2014,"I began my career in software engineering research and now find myself working more in software systems research. Is there a difference? In this talk I reflect on this question by recalling the stream of ideas, students, and colleagues that have shaped my path. I present an overview of the current projects in which I am involved to understand at a technical level where the two research communities, software engineering and software systems, connect and diverge."
146256,15517,8422,"Acacia+, a tool for LTL synthesis",2012,"We present Acacia+, a tool for solving the LTL realizability and synthesis problems. We use recent approaches that reduce these problems to safety games, and can be solved efficiently by symbolic incremental algorithms based on antichains. The reduction to safety games offers very interesting properties in practice: the construction of compact solutions (when they exist) and a compositional approach for large conjunctions of LTL formulas."
2417142,15517,8494,CNF encodings of cardinality in formal methods for robustness checking of gate-level circuits,2011,"With decreasing transistor sizes, the susceptibility of digital circuits to soft errors will increase. Thus, the need to efficiently evaluate the robustness of a gate-level circuit to multiple simultaneous soft errors. We compare the efficiency of various CNF schemes for encoding of cardinality constraints, which control the number of simultaneously injected soft errors in a gate-level circuit, when the robustness of the circuit is computed with SAT-based formal methods."
2677256,15517,8806,Locking fast,2014,"This article presents several independent optimizations of operations on monitors. They do not involve the low-level mutual exclusion mechanisms but rather their integration with and usage within higher-level constructs of the language. The paper reports acceleration of Hop, the Web programming language for which these optimizations have been created. The paper shows that other languages such as C and Java would also benefit from these optimizations."
981471,15517,517,RAILROADMAP: An Agile Security Testing Framework for Web-application Development,2013,"We propose a model-assisted security testing framework for developing Web applications. We devised a tool called “RailroadMap” that automatically extracts a behavior model from the code base of Ruby-on-Rails. This model provides a unified point of view for analyzing security problems by representing an application's behavior, which includes all security functions and possible attack scenarios."
7731,15517,8422,Using coverage to deploy formal verification in a simulation world,2011,"Formal verification technology has today advanced to the stage that it can complement or replace simulation effort for selected hardware designs. Yet the completion of a formal verification effort is rarely a requirement for hardware tapeout. Simulation remains the primary verification methodology, and means of deciding when verification is complete. In this paper we discuss how formal verification can be deployed using simulation-based coverage in a simulation-based verification schedule."
2438874,15517,23827,Mental models and parallel program maintenance,2011,"Parallel programs are difficult to write, test, and debug. This thesis explores how programmers build mental models about parallel programs, and demonstrates, through user evaluations, that maintenance activities can be improved by incorporating theories based on such models. By doing so, this work aims to increase the reliability and performance of today's information technology infrastructure by improving the practice of maintaining and testing parallel software."
1741460,15517,23827,Teaching software development processes by simulation: Wuality assurance as a factor of success,2013,"This half-day tutorial shows how a flexible simulation environment can link the various topic areas of software engineering in the same way they are interwoven in the daily work of practitioners. Based on the simulation goals, the participants act as project managers determining the simulated development process by adequate staffing and allocating software development as well as quality assurance tasks."
1675192,15517,10973,Decidability of Weak Simulation on One-Counter Nets,2013,"One-counter nets (OCN) are Petri nets with exactly one unbounded place. They are equivalent to a subclass of one-counter automata with only a weak test for zero. We show that weak simulation preorder is decidable for OCN and that weak simulation approximants do not converge at level ω, but only at ω 2 . In contrast, other semantic relations like weak bisimulation are undecidable for OCN [1], and so are weak (and strong) trace inclusion (Sec. VII)."
1782104,15517,517,Towards Software Quality and User Satisfaction through User Interfaces,2011,"With this PhD we expect to provide the community and the industry with a solid basis for the development, integration, and deployment of software testing tools. As a solid basis we mean, on one hand, a set of guidelines, recommendations, and clues to better comprehend, analyze, and perform software testing processes, and on the other hand, a set of robust software frameworks that serve as a starting point for the development of future testing tools."
601387,15517,8422,Learning boolean functions incrementally,2012,"Classical learning algorithms for Boolean functions assume that unknown targets are Boolean functions over fixed variables. The assumption precludes scenarios where indefinitely many variables are needed. It also induces unnecessary queries when many variables are redundant. Based on a classical learning algorithm for Boolean functions, we develop two learning algorithms to infer Boolean functions over enlarging sets of ordered variables. We evaluate their performance in the learning-based loop invariant generation framework."
1501044,15517,10973,Atomic Lambda Calculus: A Typed Lambda-Calculus with Explicit Sharing,2013,"An explicit -- sharing lambda-calculus is presented, based on a Curry -- Howard-style interpretation of the deep inference proof formalism. Duplication of subterms during reduction proceeds 'atomically', i.e. on individual constructors, similar to optimal graph reduction in the style of Lamping. The calculus preserves strong normalisation with respect to the lambdacalculus, and achieves fully lazy sharing."
1824278,15517,23827,Locating features in dynamically configured avionics software,2012,"Locating features in software is an important activity for program comprehension and to support software reengineering. We present a novel automated approach to locate features in source code based on static analysis and model checking. The technique is aimed at dynamically configured software, which is software in which the activation of specific features is controlled by configuration variables. The approach is evaluated on an industrial avionics system."
315462,15517,9438,The Business Behavior Model: A Revised Version,2011,The problem of aligning the strategy of an enterprise and supporting IT resources has been recognized since decades. In this paper we revise a model - The Business Behavior Model (BBM) - which is envisioned to help in the alignment between the goal layer and the business layer of an enterprise. A number of problems are identified in the original BBM and solutions for those problems are proposed. A short illustrative case is provided that shows the applicability of the revised BBM.
1727097,15517,517,Automated Performance Model Construction through Event Log Analysis,2012,Locating performance bottlenecks in modern distributed systems can be difficult because direct measurement is often not available. Performance models are often very beneficial in these situations because far more information can be extracted as a result. This work demonstrates the generation of a Layered Queueing Network performance model through the analysis of trace information from a live system. The model can then be analyzed to locate bottlenecks in both the hardware and software.
2333178,15517,517,Property-Driven Software Engineering Approach,2012,"We present a research roadmap that defines an enhanced model-driven software engineering approach focused on non-functional properties models. Currently, we have implemented two sub-processes of this roadmap: Property Modeling and Monitoring. We provide a property-driven approach to runtime monitoring based on a comprehensive Property Meta- Model (PMM) and on a generic configurable event-based monitoring infrastructure."
1557769,15517,23865,MSR 2012 keynote: The evolution of the social programmer,2012,"Summary form only given. Software architecture is a complex domain and one of the cornerstones to successful software development. The complexity appears in learning and teaching as well as in real-life projects, where architectural thinking emerges as a process of life-long learning. In order to develop a structured and comprehensive architectural awareness, we present an explanatory model for dealing with architecture."
2043501,15517,23865,A dataset for pull-based development research,2014,"Pull requests form a new method for collaborating in distributed software development. To study the pull request distributed development model, we constructed a dataset of almost 900 projects and 350,000 pull requests, including some of the largest users of pull requests on Github. In this paper, we describe how the project selection was done, we analyze the selected features and present a machine learning tool set for the R statistics environment."
2155058,15517,23827,ICSE 2011 technical briefings,2011,"The better we meet the interest of our community, the better we can help bringing ourselves up-to-date with the latest and greatest in and around software engineering. To this purpose, ICSE 2011 for the first time featured  technical briefings , an all-day venue for communicating the state of topics related to software engineering, thus providing an exchange of ideas as well as an introduction to the main conference itself."
1589346,15517,8385,SelfMotion: a declarative language for adaptive service-oriented mobile apps,2012,"In this demo we present SelfMotion: a declarative language and a run-time system conceived to support the development of adaptive, mobile applications, built as compositions of ad-hoc components, existing services and third party applications. The advantages of the approach and the adaptive capabilities of SelfMotion are demonstrated in the demo by designing and executing a mobile application inspired by an existing, worldwide distributed, mobile application."
1887196,15517,23827,FireDetective: understanding ajax client/server interactions,2011,"Ajax-enabled web applications are a new breed of highly interactive, highly dynamic web applications. Although Ajax allows developers to create rich web applications, Ajax applications can be difficult to comprehend and thus to maintain. FireDetective aims to facilitate the understanding of Ajax applications. It uses dynamic analysis at both the client (browser) and server side and subsequently connects both traces for further analysis."
145612,15517,8422,SeLoger: a tool for graph-based reasoning in separation logic,2013,"This paper introduces the tool SeLoger, which is a reasoner for satisfiability and entailment in a fragment of separation logic with pointers and linked lists. SeLoger builds upon and extends graph-based algorithms that have recently been introduced in order to settle both decision problems in polynomial time. Running SeLoger on standard benchmarks shows that the tool outperforms current state-of-the-art tools by orders of magnitude."
2080153,15517,8806,Towards a colored reflective Petri-net approach to model self-evolving service-oriented architectures,2012,"Service-based software systems could require to evolve during their execution. To support this, we need to consider system evolving since the design phase. Reflective Petri nets separate the system from its evolution by describing it and how it can evolve. However, reflective Petri nets have some expressivity limits and render overcomplicated the consistency checking necessary during service evolution. In this paper, we extend the reflective Petri nets approach to overcome such limits and show that on a case study."
2129998,15517,517,A Framework to Test Advanced Web Services Transactions,2011,"Transactions are a key issue in the reliability of distributed applications because they ensure all the participants achieve a mutually agreed outcome. However, current research has given little attention to testing transactions in web services. This paper presents a conceptual framework, inspired in risk-based methodologies, to address this gap. It also reports on preliminary results and identifies future work."
1817624,15517,23827,xMapper: an architecture-implementation mapping tool,2012,"xMapper is an Eclipse-based tool that implements a new architecture-implementation mapping approach called 1.x-way mapping. xMapper is able to record various architecture changes during software development, and automatically map specific kinds of architecture changes to code in specific ways. In addition, xMapper supports the mapping of behavioral architecture specifications modeled as UML-like sequence diagrams and state diagrams."
2383392,15517,20754,Verifying Cyber-Physical Interactions in Safety-Critical Systems,2013,"Safety-compromising bugs in software-controlled systems are often hard to detect. In a 2007 DARPA Urban Challenge vehicle, such a defect remained hidden during more than 300 miles of test-driving, manifesting for the first time during the competition. With this incident as an example, the authors discuss formalisms and techniques available for safety analysis of cyber-physical systems."
1137017,15517,8806,Translating event-B to JML-specified Java programs,2014,We present a translation from Event-B machines to JML-specified Java class implementations and the EventB2Java Rodin plug-in that automates the translation. Producing JML specifications in addition to Java implementations enables users to write bespoke implementations that can then be checked for correctness using existing JML tools. We have validated the proposed translation by applying the EventB-2Java tool to various programs and systems.
238583,15517,20332,Synthesizing strategies for epistemic goals by epistemic model checking: an application to pursuit evasion games,2012,"The paper identifies a special case in which the complex problem of synthesis from specifications in temporal-epistemic logic can be reduced to the simpler problem of model checking such specifications. An application is given of strategy synthesis in pursuit-evasion games, where one or more pursuers with incomplete information aim to discover the existence of an evader. Experimental results are provided to evaluate the feasibility of the approach."
2272067,15517,10973,"Logic in Software, Dynamical and Biological Systems",2011,"Formal methods is concerned with analyzing systems formally. Here, we focus on three different systems:software systems, dynamical control systems, and biological systems. The analysis questions can be broadly classified into verification and synthesis questions. We focus on both these aspects here. Logic and logical methods play a key role in the tool sand techniques across this whole range of systems and analyses."
1719515,15517,23876,OnionUML: An Eclipse plug-in for visualizing UML class diagrams in onion graph notation,2013,"This paper presents OnionUML, an Eclipse plug-in that reduces the number of visible classes in a UML class diagram while preserving structure and semantics of the UML elements. Compaction of class elements is done using onion graph notation. The goal is that developers will be able to view and understand subsystems of a large software system while being able to visualize how that subsystem fits into the whole system."
576476,15517,8422,MCMAS-SLK: A Model Checker for the Verification of Strategy Logic Specifications,2014,"Model checking has come of age. A number of techniques are increasingly used in industrial setting to verify hardware and software systems, both against models and concrete implementations. While it is generally accepted that obstacles still remain, notably handling infinite state systems efficiently, much of current work involves refining and improving existing techniques such as predicate abstraction."
2444208,15517,11058,Precise and compact modular procedure summaries for heap manipulating programs,2011,"We present a strictly bottom-up, summary-based, and precise heap analysis targeted for program verification that performs strong updates to heap locations at call sites. We first present a theory of heap decompositions that forms the basis of our approach; we then describe a full analysis algorithm that is fully symbolic and efficient. We demonstrate the precision and scalability of our approach for verification of real C and C++ programs."
232164,15517,23827,Innovation and Sustainability in Education,2012,"This paper aims at proposing a founding manifesto for the International Symposia on Innovation and Sustainability in Education InSuEdu. We analyze two possible interpretations of Innovation and Sustainability in Education: 1 sustainability of methodological and technological innovation processes in education, and 2 creation and implementation of innovation processes that can make education a key driver in sustainable development. After a discussion of these two interpretations we briefly visit the ten contributions to the 1st International Symposium on Innovation and Sustainability in Education and show how the solutions they propose address one or both of these interpretations."
1652079,15517,517,Unit Testing Tool Competition,2013,"This paper describes the Java Unit Testing Tool Competition that ran in the context of the Search Based Software Testing (SBST) workshop at ICST 2013. It describes the main objective of the benchmark, the Java classes that were selected, the data that was collected, the tools that were used for data collection, the protocol that was carried out to execute the benchmark and how the final benchmark score for each participating tool can be calculated."
1817658,15517,10973,Higher-Order Model Checking: From Theory to Practice,2011,"The model checking of higher-order recursion schemes (higher-order model checking for short) has been actively studied in the last decade, and has seen significant progress in both theory and practice. From a practical perspective, higher-order model checking provides a foundation for software model checkers for functional programming languages such as ML and Haskell. This short article aims to provide an overview of the recent progress in higher-order model checking and discuss future directions."
356664,15517,22113,Model checking knowledge in pursuit evasion games,2011,"In a pursuit-evasion game, one or more pursuers aim to discover the existence of, and then capture, an evader. The paper studies pursuit-evasion games in which players may have incomplete information concerning the game state. A methodology is presented for the application of a model checker for the logic of knowledge and time to verify epistemic properties in such games. Experimental results are provided from a number of case studies that validate the feasibility of the approach."
1910664,15517,8806,A catalogue of functional software requirement patterns for the domain of content management systems,2013,"Software requirement patterns have been proposed as an artifact for fostering requirements reuse. When we define these patterns for the functional part of a software system, we realize that most of patterns are specific of a software domain. This paper presents and analyzes a catalogue of functional software requirement patterns for the domain of content management, and gives an overview of how this catalogue has been constructed from the systematic analysis of 6 existing software specification documents with the support of expert assessment."
1801991,15517,22232,Predicate generation for learning-based quantifier-free loop invariant inference,2011,"We address the predicate generation problem in the context of loop invariant inference. Motivated by the interpolation-based abstraction refinement technique, we apply the interpolation theorem to synthesize predicates implicitly implied by program texts. Our technique is able to improve the effectiveness and efficiency of the learning-based loop invariant inference algorithm in [14]. Experiments excerpted from Linux, SPEC2000, and Tar source codes are reported."
2214386,15517,10973,On the Significance of the Collapse Operation,2012,"We show that deterministic collapsible pushdown automata of second level can recognize a language which is not recognizable by any deterministic higher order pushdown automaton (without collapse) of any level. This implies that there exists a tree generated by a second level collapsible pushdown system (equivalently: by a recursion scheme of second level), which is not generated by any deterministic higher order pushdown system (without collapse) of any level (equivalently: by any safe recursion scheme of any level). As a side effect, we present a pumping lemma for deterministic higher order pushdown automata, which potentially can be useful for other applications."
1957379,15517,23827,AutoBlackTest: a tool for automatic black-box testing,2011,"In this paper we present  AutoBlackTest , a tool for the automatic generation of test cases for interactive applications.  AutoBlackTest  interacts with the application though its GUI, and uses reinforcement learning techniques to understand the interaction modalities and to generate relevant testing scenarios. Early results show that the tool has the potential of automatically discovering bugs and generating useful system and regression test suites."
570754,15517,8884,A linked data platform adapter for the Bugzilla issue tracker,2014,The W3C Linked Data Platform (LDP) specification defines a standard HTTP-based protocol for read/write Linked Data and provides the basis for application integration using Linked Data. This paper presents an LDP adapter for the Bugzilla issue tracker and demonstrates how to use the LDP protocol to expose a traditional application as a read/write Linked Data application. This approach provides a fl exible LDP adoption strategy with minimal changes to existing applications.
443280,15517,8422,SYMDIFF: a language-agnostic semantic diff tool for imperative programs,2012,"In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C# and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs."
1724915,15517,23827,GROPG: a graphical on-phone debugger,2013,"Debugging mobile phone applications is hard, as current debugging techniques either require multiple computing devices or do not support graphical debugging. To address this problem we present GROPG, the first graphical on-phone debugger. We implement GROPG for Android and perform a preliminary evaluation on third-party applications. Our experiments suggest that GROPG can lower the overall debugging time of a comparable text-based on-phone debugger by up to 2/3."
1084488,15517,23827,Understanding the dynamics of test-driven development,2014,Test-driven development (TDD) has been the subject of several software engineering experiments. However the controversial results about its effects still need to be contextualized. This doctoral research will show how TDD could be better assessed by studying to what extent developers follow its cycle and for what kind of development tasks. This knowledge is foreseen to be beneficial for software industries willing to adopt or adapt TDD.
1453205,15517,8385,A framework for defining the dynamic semantics of DSLs,2013,"In this research abstract we describe our project on a common reference framework for defining domain specific languages (DSLs). The framework is meant for defining the dynamic semantics of DSLs and allows for mapping the DSL definition to the various platforms, such as verification, validation and simulation. The objectives of the project are to make a DSL dynamic semantics definition explicit and to use this definition for bridging technological diversity of various platforms, used in the DSLs development."
1280299,15517,517,Random Grammar-Based Testing for Covering All Non-terminals,2013,"In the context of software testing, generating complex data inputs is frequently performed using a grammar-based specification. For combinatorial reasons, an exhaustive generation of the data - of a given size - is practically impossible, and most approaches are either based on random techniques or on coverage criteria. In this paper, we show how to combine these two techniques by biasing the random generation in order to optimise the probability of satisfying a coverage criterion."
2015017,15517,9436,Emotional requirements engineering,2011,This mini tutorial reviews application of psychological theories in requirements engineering. Theories from psychology of emotion and motivation are introduced and applied in a scenario-based process to analyse affective situations which might be produced by user-oriented RE. Use of agent technology in storyboards and scenario analysis of affective situations is described and illustrated with case studies in health informatics for persuasive technology applications.
2152134,15517,23827,Requirements Engineering Current Practice and Capability in Small and Medium Software Development Enterprises in New Zealand,2011,This paper presents research on current industry practices with respect to requirements engineering as implemented within software development companies in New Zealand. A survey instrument is designed and deployed. The results are analysed and compared against what is internationally considered best practice and previous New Zealand and Australian studies. An attempt is made to assess the requirements engineering capability of New Zealand companies using both formal and informal frameworks
1798231,15517,9436,Disambiguation of industrial standards through formalization and graphical languages,2011,"Natural language safety requirements in industrial standards pose risks for ambiguities which need to be resolved by the system manufacturer in concertation with the certificate authority. This is especially challenging for small and medium-sized enterprises (SME). In this paper we report on our experiences with applying traditional requirements engineering techniques, formal methods, and visual narratives in an exploratory case-study in an SME."
1089270,15517,23827,Compiler error notifications revisited: an interaction-first approach for helping developers more effectively comprehend and resolve error notifications,2014,"Error notifications and their resolutions, as presented by modern IDEs, are still cryptic and confusing to developers. We propose an interaction-first approach to help developers more effectively comprehend and resolve compiler error notifications through a conceptual interaction framework. We propose novel taxonomies that can serve as controlled vocabularies for compiler notifications and their resolutions. We use preliminary taxonomies to demonstrate, through a prototype IDE, how the taxonomies make notifications and their resolutions more consistent and unified."
175303,15517,9438,Secure and privacy-preserving execution model for data services,2013,"Data services have almost become a standard way for data publishing and sharing on top of the Web. In this paper, we present a secure and privacy-preserving execution model for data services. Our model controls the information returned during service execution based on the identity of the data consumer and the purpose of the invocation. We implemented and evaluated the proposed model in the healthcare application domain. The obtained results are promising."
1538047,15517,23827,Extending a general theory of software to engineering,2014,"In this paper, we briefly describe a general theory of software used in order to model and predict the current and future quality of software systems and their environment. The general theory is described using a class model containing classes such as application component, business service, and infrastructure function as well as attributes such as modifiability, cost, and availability. We also elaborate how this general theory of software can be extended into a general theory of software engineering by adding engineering activities, roles, and requirements."
1926939,15517,23827,Model translations among big-step modeling languages,2012,"Model Driven Engineering (MDE) is a progressive area that tries to fill the gap between problem definition and software development. There are many modeling languages proposed for use in MDE. A challenge is how to provide automatic analysis for these models without having to create new analyzers for each different language. In this research, we tackle this problem for a family of modeling languages using a semantically configurable model translation framework."
2683787,15517,8385,Con2colic testing,2013,"In this paper, we describe (con)2colic testing - a systematic testing approach for concurrent software. Based on concrete and symbolic executions of a concurrent program, (con)2colic testing derives inputs and schedules such that the execution space of the program under investigation is systematically explored. We introduce interference scenarios as key concept in (con)2colic testing. Interference scenarios capture the flow of data among different threads and enable a unified representation of path and interference constraints. We have implemented a (con)2colic testing engine and demonstrate the effectiveness of our approach by experiments."
31514,15517,9438,Managing the evolution and customization of database schemas in information system ecosystems,2013,"We present an approach that supports the customization and evolution of a database schema in a software ecosystem context. The approach allows for the creation of customized database schemas according to selected, supported feature packs and can be used in an ecosystem context, where third-party providers and customers augment the system with their own capabilities.#R##N##R##N#The creation of the final database schema is automatic and also the relevant updates of individual feature packs can be automatically handled by the system."
1079952,15517,8806,On the exploitation of process mining for security audits: the process discovery case,2013,"This paper reports on the potential of process mining as a basis for security audits of business process and corresponding business process management systems. In particular, it focuses on process discovery as a means to reconstruct process-related structures from event logs, such as the process' control flow, social network and data flows. Based on this information, security analysis to determine the compliance with security and privacy requirements can be automated."
1952828,15517,23827,View infinity: a zoomable interface for feature-oriented software development,2011,"Software product line engineering provides efficient means to develop variable software. To support program comprehension of  software product lines (SPLs) , we developed  View Infinity , a tool that provides seamless and semantic zooming of different abstraction layers of an SPL. First results of a qualitative study with experienced SPL developers are promising and indicate that View Infinity is useful and intuitive to use."
276283,15517,22113,A cutoff technique for the verification of parameterised interpreted systems with parameterised environments,2013,We put forward a cutoff technique for determining the number of agents that is sufficient to consider when checking temporal-epistemic specifications on a system of any size. We identify a special class of interleaved interpreted systems for which we give a parameterised semantics and an abstraction methodology. This enables us to overcome the significant limitations in expressivity present in the state-of-the-art. We present an implementation and discuss experimental results.
88721,15517,9438,Clouding services for linked data exploration,2012,"Exploration of linked data aims at providing tools and techniques that enable to effectively explore a dataset through concepts, relationships, and properties by means of SPARQL endpoints and visual interfaces. In this paper, we present a set of clouding services for linked data exploration, to enable the end-user to personalize and focus her/his exploration by interactively configuring high-level conceptual structures called inClouds."
1353820,15517,23876,Leveraging clone detection for Internet-scale source code search,2012,"Different approaches to search for source code on the Internet exist. In this paper, we propose techniques that support a special type of Internet-scale code search using two novel syntactical and semantic clone search and detection methods. The syntactical search focuses on providing a scalable real-time engine, while the semantic clone detection is being used to enrich our knowledge base during the offline processing step."
2470314,15517,23827,Language modularity with the MPS language workbench,2012,"JetBrains MPS is a comprehensive environment for language engineering. New languages can be defined as standalone languages or as modular extensions of existing languages. Since MPS is a projectional editor, syntactic forms other than text are possible, including tables or mathematical symbols. This demo will show MPS based on mbeddr C, a novel approach for embedded software development that makes use of incremental language extension on the basis of C."
604282,15517,8422,Recent developments in FDR,2012,We describe and report upon various substantial extensions of the CSP refinement checker FDR including (i) the direct ability to handle real-time processes; (ii) the incorporation of bounded model checking technology; (iii) the development of conservative and highly efficient static analysis algorithms for guaranteeing livelock-freedom; and (iv) the development of automated CEGAR technology.
1579533,15517,517,Test Quality Measurement Using TBPP-R,2013,"Software test quality measurement is the key for software release decision. It is hard to evaluate software test quality because full coverage test is impossible in practice. In this paper, we compare the existing methods for the software test quality measurement and discuss their drawbacks. Then we propose TBPP - a weighted tree based partition and proration method to evaluate software test quality. Furthermore, we evolve this method to TBPP-R by introducing a risk distribution function, which can reveal software test quality more accurately. In our experiments, we compare TBPP-R and TBPP with existing methods in our test projects. The results show that TPBB-R can measure software test quality much more accurately."
956003,15517,23876,Supporting comprehension experiments with human subjects,2012,"Experiments with human subjects become more and more important in software engineering. To support planning, conducting, and replicating experiments targeting program comprehension, we developed PROPHET. It allows experimenters to easily define and customize experimental settings as well as to export settings such that others can replicate their results. Furthermore, PROPHET provides extension points, which allow users to integrate additional functionality."
636078,15517,9438,Value-oriented coordination process model engineering,2011,"One of the most important aspects of a business collaboration is the value aspect. Analyzing a business collaboration form the value point of view enables us to understand the value-creation and sustainability of the collaboration. Therefore, having a business collaboration up and running for some time, the stakeholders involved in the collaboration can asses its performance by designing a value model of the collaboration and analyzing it. A value model is an abstract and easy to understand image of the collaboration from the value point of view. In this paper we elaborate on producing a business value model from a coordination process model."
1844076,15517,20524,Keynote talk: the logic of information design,2014,"At least since the sixties, there have been many attempts to understand the logic of design, when the latter is broadly understood as a purposeful way of realizing an artefact. In this talk, I shall explore current methodologies to see how they may be adapted to cases in which what is being designed is information, in the sense of both a semantic artefact (e.g. a train timetable) and a communication process (e.g. the announcement that a specific train is leaving from particular platform)."
2147401,15517,23827,Workshop on flexible modeling tools (FlexiTools 2011),2011,"Modeling tools are often not used for tasks during the software lifecycle for which they should be more helpful; instead free-from approaches, such as office tools and white boards, are frequently used. Prior workshops explored why this is the case and what might be done about it. The goal of this workshop is to continue those discussions and also to form an initial set of challenge problems and research challenges that researchers and developers of flexible modeling tools should address."
2476634,15517,23827,1.x-Way architecture-implementation mapping,2011,"A new architecture-implementation mapping approach, 1.x-way mapping, is presented to address architecture-implementation conformance. It targets maintaining conformance of structure and behavior, providing a solution to architecture changes, and protecting architecture-prescribed code from being manually changed. Technologies developed in this work include deep separation of generated and non-generated code, an architecture change model, architecture-based code regeneration, and architecture change notification."
1373164,15517,8385,Software programmer management: a machine learning and human computer interaction framework for optimal task assignment,2014,This paper attempts optimal task assignment at the enterprise-level by assigning complexity metrics to the programming tasks and predicting task completion times for each of these tasks based on a machine learning framework that factors in programmer attributes. The framework also considers real-time programmer state by using a simple EEG device to detect programmer mood. A final task assignment is made using a PDTS solver.
112535,15517,22232,Verifying pCTL model checking,2012,Probabilistic model checkers like PRISM check the satisfiability of probabilistic CTL (pCTL) formulas against discrete-time Markov chains. We prove soundness and completeness of their underlying algorithm in Isabelle/HOL. We define Markov chains given by a transition matrix and formalize the corresponding probability measure on sets of paths. The formalization of pCTL formulas includes unbounded cumulated rewards.
961127,15517,8385,A software lifecycle process for context-aware adaptive systems,2011,"It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated."
779613,15517,23827,Using GSwE2009 in the creation and modification of graduate software engineering programs and related curricula,2013,"The current ACM and IEEE Computer Society recommended guidelines for graduate software engineering programs (GSwE2009) were created in 2009 by an international group of experts from industry, government and academia. We report here on some of the early experiences using GSwE2009 to create a new curriculum in software assurance and to create and/or improve 4 different academic programs in 3 countries. All of these experiences have been positive, confirming the validity and usefulness of GSwE2009."
149478,15517,22232,MU-CSeq: Sequentialization of C Programs by Shared Memory Unwindings (Competition Contribution),2014,We implement a new sequentialization algorithm for multi-threaded C programs with dynamic thread creation as a new CSeq module. The novel basic idea of this algorithm is to fix (by a nondeterministic guess) the sequence of write operations in the shared memory and then simulate the behavior of the program according to any scheduling that respects this choice. Simulation is done thread-by-thread and the thread creation mechanism is replaced by function calls.
900079,15517,20524,Test suite selection based on traceability annotations,2012,"This paper describes the Tobias tool. Tobias is a combinatorial test generator which unfolds a test pattern provided by the test engineer, and performs various combinations and repetitions of test parameters and methods. Tobias is available on-line at tobias.liglab.fr . This website features recent improvements of the tool including a new input language, a traceability mechanism, and the definition of various ``selectors'' which achieve test suite reduction."
716284,15517,8385,Inference and checking of context-sensitive pluggable types,2012,"Pluggable types can help find bugs such as null-pointer dereference or unwanted mutation (or they can prove the absence of such bugs). Unfortunately, pluggable types require annotations, which imposes a burden on programmers.   We propose a framework for specifying,  inferring  and checking of context-sensitive pluggable types. Programmers can use the framework to plug existing context-sensitive type systems (e.g., Immutability, Ownership) as well as to build new systems."
2184919,15517,23876,Is the derivation of a model easier to understand than the model itself,2012,"Software architectures can be presented by graphs with components as nodes and connectors as edges. These graphs, or models, typically encode expert domain knowledge, which makes them difficult to understand. Hence, instead of presenting a complete complex model, we can derive it from a simple, easy-to-understand model by a set of easy-to-understand transformations. In two controlled experiments, we evaluate whether a derivation of a model is easier to understand than the model itself."
571185,15517,8422,TTP: Tool for Tumor Progression,2013,"In this work we present a flexible tool for tumor progression, which simulates the evolutionary dynamics of cancer. Tumor progression implements a multi-type branching process where the key parameters are the fitness landscape, the mutation rate, and the average time of cell division. The fitness of a cancer cell depends on the mutations it has accumulated. The input to our tool could be any fitness landscape, mutation rate, and cell division time, and the tool produces the growth dynamics and all relevant statistics."
2062025,15517,517,A Controlled Experiment to Evaluate Effectiveness and Efficiency of Three Software Testing Methods,2013,"In this paper, we describe a controlled experiment carried out to compare three software testing methods: code reading, functional testing and structural testing. The experiment was carried out with eighteen subjects who applied three techniques to three C programs in a fractional factorial experimental design. The main results of the study are that all testing techniques are equivalent in terms of effectiveness; however the techniques differ partially in terms of efficiency."
2162247,15517,23827,Improving open source software patch contribution process: methods and tools,2011,"The patch contribution process (PCP) is very important to the sustainability of OSS projects. Nevertheless, there are several issues on patch contribution in mature OSS projects, which include time consuming process, lost and ignored patches, slow review process. These issues are recognized by researchers and OSS projects, but have not been addressed. In this dissertation, I apply Kanban method to guide process improvement and tools development to reduce PCP cycle time."
1142386,15517,23865,Innovation diffusion in open source software: preliminary analysis of dependency changes in the gentoo portage package database,2014,"In this paper we make the case that software dependencies are a form of innovation adoption. We then test this on the time-evolution of the Gentoo package dependency graph. We find that the Bass model of innovation diffusion fits the growth of the number of packages depending on a given library. Interestingly, we also find that low-level packages have a primarily imitation driven adoption and multimedia libraries have primarily innovation driven growth."
2501317,15517,20524,MaramaAI: tool support for capturing and managing consistency of multi-lingual requirements,2012,"Requirements captured by Requirements Engineers are commonly inconsistent with their client&#x2019;s intended requirements and are often error prone especially if the requirements are written in multiple languages. We demonstrate the use of our automated inconsistency-checking tool MaramaAI to capture and manage the consistency of multi-lingual requirements in both the English and Malay languages for requirements engineers and clients using a round-trip, rapid prototyping approach."
974879,15517,10973,A Computational Interpretation of Parametricity,2012,"Reynolds' abstraction theorem has recently been extended to lambda-calculi with dependent types. In this paper, we show how this theorem can be internalized. More precisely, we describe an extension of the Pure Type Systems with a special parametricity rule (with computational content), and prove fundamental properties such as Church-Rosser's and strong normalization. All instances of the abstraction theorem can be both expressed and proved in the calculus itself. Moreover, one can apply parametricity to the parametricity rule: parametricity is itself parametric."
1776652,15517,23827,Probabilistic Petri Net and its Logical Semantics,2011,"There are many variants of Petri net at present, and some of them can model system with both function and performance specification, such as stochastic Petri net, and generalized stochastic Petri net. In order to address the issue of modeling system with probabilistic behaviors, a kind of Petri net with probability (probabilistic Petri net, PPN) is proposed in this paper. Then an action-based PCTL is developed to interpret logical semantics for PPN system. The usefulness of PPN system is illustrated by modeling and specifying an elaborate model of travel arrangements workflow."
159646,15517,23827,Predictability and evolution in resilient systems,2011,"This paper gives a short overview of the talk related to the challenges in software development of resilient systems. The challenges come of the resilience characteristic as such; it a system emerging lifecycle property, neither directly measurable nor computable.While software is an essential part of a system, its analysis not enough for determining the system resilience. The talk will discuss about system resilience reasoning, its limitations, and possible approaches in the software design that include resilience analysis."
882371,15517,8385,Data hard with a vengeance (invited talk),2014,"Action flicks and the analysis of software data in industry have more in common than you think. Both action heroes and development teams are on tight deadlines to save the day. Getting wrong information can lead to disastrous outcomes. In this talk, I will share experiences from my six years of research in the Empirical Software Engineering Group working with engineers towards sound data-driven decision about software."
641118,15517,20561,Computing Requirements: Cognitive Approaches to Distributed Requirements Engineering,2012,We present a study of on the goal-oriented modeling of RE processes executed by a practicing systems development team. The research combines an empirical case study of RE practices with the evaluation and simulation capability of i* modeling. Our analysis focuses on a system implementation project at a mid-size U.S. university and applies the theory of distributed cognition to generate a range of design insights for goal identification and process enhancement.
2430067,15517,8228,Service Protocol Replaceability Assessment in Mediated Service Interactions,2011,"In this paper we propose a technique called replaceability assessment where, according to the adaptation mechanisms of a certain adapter, it provides a set of condition pairs that determine when one protocol can be replaced by another, and computes a replacement degree that specifies how replaceable two protocols are. The set of condition pairs and the replacement degree are complementary criteria to be used by the requestor for identifying the most suitable provider service from a set of functionally equivalent candidates."
2534815,15517,23827,Empirically researching development of international software,2012,"Software localization is an important process for international acceptance of software products. However, software development and localization does not always come together without friction. In our empirical software engineering research, we examine the interplay of software development and software localization by gathering and analyzing qualitative and quantitative data from professionals in relevant roles. Our aim is to co-validate issues and inform practice about the development of international software."
2030028,15517,23827,MT-Scribe: an end-user approach to automate software model evolution,2011,"Model evolution is an essential activity in software system modeling, which is traditionally supported by manual editing or writing model transformation rules. However, the current state of practice for model evolution presents challenges to those who are unfamiliar with model transformation languages or metamodel definitions. This demonstration presents a demonstration-based approach that assists end-users through automation of model evolution tasks (e.g., refactoring, model scaling, and aspect weaving)."
2658416,15517,11058,Algorithmic profiling,2012,"Traditional profilers identify where a program spends most of its resources. They do not provide information about why the program spends those resources or about how resource consumption  would change  for different program inputs. In this paper we introduce the idea of  algorithmic profiling . While a traditional profiler determines a set of measured cost  values , an algorithmic profiler determines a cost  function . It does that by automatically determining the inputs of a program, by measuring the program's cost for any given input, and by inferring an empirical cost function."
1221619,15517,8385,Towards a theory of architectural styles,2014,"Architectural styles and patterns play an important role in software architectures. However, they are usually only stated informally, which may cause problems such as ambiguity and wrong conclusions. A rigorous theory of architectural styles --- consisting of (i) mathematical models for each style; (ii) axioms to identify different variants of a style; and (iii) rigorous analyses by means of mathematical proofs --- would address these problems. With this work we report on our progress towards such a rigorous theory of architectural styles."
1321232,15517,10973,Converging to the Chase -- A Tool for Finite Controllability,2013,"We solve a problem, stated in [CGP10], showing that Sticky Datalog exists, defined in the cited paper as an element of the Datalog pm project, has the finite controllability property. In order to do that, we develop a technique, which we believe can have further applications, of approximating Chase(D, cal T), for a database instance D and a set of tuple generating dependencies and data log rules cal T, by an infinite sequence of finite structures, all of them being models of cal T and D."
730301,15517,23827,A Rule-Based Fuzzy Diagnostics Decision Support System for Tuberculosis,2011,The system is specialized for pulmonary physicians focusing on tuberculosis and for patients already diagnosed with tuberculosis. The main focus for the development of the system is on the architecture and algorithm used to find the probable class of tuberculosis a patient may have. The class of tuberculosis is determined by using a rule base populated by rules made for the different classes of tuberculosis. The clinical decision support system integrated with Fuzzy Logic and Rule-based method that generates classes of tuberculosis suits the needs of pulmonary physicians and lessens the time consumed in generating diagnosis.
2760920,15517,22232,Ultimate Automizer with Unsatisfiable Cores - (Competition Contribution).,2014,"Ultimate Automizer is an automatic software verification tool for C programs. This tool is a prototype implementation of an automata-theoretic approach that allows a modular verification of pro- grams. Furthermore, this is the first implementation of a novel interpola- tion technique where interpolants are not obtained from an interpolating theorem prover but from a combination of a live variable analysis, inter- procedural predicate transformers and unsatisfiable cores."
168132,15517,8422,"IC3 and beyond: incremental, inductive verification",2012,"IC3, a SAT-based safety model checking algorithm introduced in 2010 [1, 2], is considered among the best safety model checkers. This tutorial discusses its essential ideas: the use of concrete states, called counterexamples to induction, to motivate lemma discovery; the incremental application of induction to generate the lemmas; and the use of stepwise assumptions to allow dynamic shifting between inductive lemma generation and propagation of lemmas as predicates."
2267406,15517,23827,The American law institute's principles on software contracts and their ramifications for software engineering research (NIER track),2011,"The American Law Institute has recently published principles of software contracts that may have profound impact on changing the software industry. One of the principles implies a nondisclaimable liability of software vendors for any hidden material defects. In this paper, we describe the new principle, first from a legal and then from a software engineering point of view. We point out potential ramifications and research directions for the software engineering community."
2094993,15517,23827,Ten tips to succeed in global software engineering education,2012,"The most effective setting for training in Global Software Engineering is to provide a distributed environment for students. In such an environment, students will meet challenges in recognizing problems first-hand. Teaching in a distributed environment is, however, very demanding, challenging and unpredictable compared to teaching in a local environment. Based on nine years of experience, in this paper we present the most important issues that should be taken into consideration to increase the probability of success in teaching a Global Software Engineering course."
2469423,15517,517,Validating Service Value Propositions Regarding Stakeholder Preferences,2011,"In service science, the design of services' value propositions is a major concern. A service's value proposition needs to best possibly meet the customers' requirements and preferences. We envision an approach of validating value propositions regarding stakeholder preferences. The approach proposes the modeling of value propositions using Service Feature Modeling. Stakeholder preferences are then collected based on the created model. Finally, the model and the collected preferences are unitedly evaluated and optimizations are identified under consideration of potential adaptation expenses."
1583311,15517,23827,Model-driven development and the future of software engineering education,2013,"This paper argues that the research area of model-driven development is likely to lead before long to tools being produced that will automate most of the construction activity within software development processes. It therefore discusses the impact that such developments would have on the software engineering curriculum, in terms of both the body of knowledge and the structures of courses, drawing on analogies with the impact during the 1960s of the development of problem-oriented programming languages and compilers for them."
827475,15517,517,XSS Vulnerability Detection Using Model Inference Assisted Evolutionary Fuzzing,2012,"We present an approach to detect web injection vulnerabilities by generating test inputs using a combination of model inference and evolutionary fuzzing. Model inference is used to obtain a knowledge about the application behavior. Based on this understanding, inputs are generated using genetic algorithm (GA). GA uses the learned formal model to automatically generate inputs with better fitness values towards triggering an instance of the given vulnerability."
237826,15517,22113,What is an ideal logic for reasoning with inconsistency,2011,"Many AI applications are based on some underlying logic that tolerates inconsistent information in a non-trivial way. However, it is not always clear what should be the exact nature of such a logic, and how to choose one for a specific application. In this paper, we formulate a list of desirable properties of ideal logics for reasoning with inconsistency, identify a variety of logics that have these properties, and provide a systematic way of constructing, for every n > 2, a family of such n-valued logics."
2105652,15517,23827,Improving failure-inducing changes identification using coverage analysis,2012,"Delta debugging has been proposed for failure-inducing changes identification. Despite promising results, there are two practical factors that thwart the application of delta debugging: large number of tests and misleading false positives. To address the issues, we present a combination of coverage analysis and delta debugging that automatically isolates failure-inducing changes. Evaluations on twelve real regressions in GNU software demonstrate both the speed gain and effectiveness improvements."
2009425,15517,23827,MeCC: memory comparison-based clone detector,2011,"In this paper, we propose a new semantic clone detection technique by comparing programs' abstract memory states, which are computed by a semantic-based static analyzer.   Our experimental study using three large-scale open source projects shows that our technique can detect semantic clones that existing syntactic- or semantic-based clone detectors miss. Our technique can help developers identify inconsistent clone changes, find refactoring candidates, and understand software evolution related to semantic clones."
1827265,15517,517,Assessing Oracle Quality with Checked Coverage,2011,"A known problem of traditional coverage metrics is that they do not assess oracle quality -- that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage -- the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality -- and even more sensitive than mutation testing, its much more demanding alternative."
2417829,15517,8422,JBernstein: A Validity Checker for Generalized Polynomial Constraints,2013,"Efficient and scalable verification of nonlinear real arithmetic constraints is essential in many automated verification and synthesis tasks for hybrid systems, control algorithms, digital signal processors, and mixed analog/digital circuits. Despite substantial advances in verification technology, complexity issues with classical decision procedures for nonlinear real arithmetic are still a major obstacle for formal verification of real-world applications."
1596214,15517,23827,Studying the effect of co-change dispersion on software quality,2013,"Software change history plays an important role in measuring software quality and predicting defects. Co-change metrics such as number of files changed together has been used as a predictor of bugs. In this study, we further investigate the impact of specific characteristics of co-change dispersion on software quality. Using statistical regression models we show that co-changes that include files from different subsystems result in more bugs than co-changes that include files only from the same subsystem. This can be used to improve bug prediction models based on co-changes."
748307,15517,23865,The GHTorent dataset and tool suite,2013,"During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it."
841419,15517,517,Mutant Subsumption Graphs,2014,"Mutation testing researchers have long known that many generated mutants are not needed. This paper develops a graph model to describe redundancy among mutations. We define true subsumption, a relation that practicing test engineers would like to have, but cannot due to issues of computability. We also define dynamic subsumption and static subsumption as approximations of true subsumption. We explore the properties of the approximate subsumption relations in the context of a small example. We suggest possible uses for subsumption graphs."
2437154,15517,517,Customer-Oriented Regression Testing: An Initial Discussion,2011,"Software systems are becoming more complex each year. User-configurable software is becoming more prevalent, leading to additional forms of complexity for regression testing, as these systems have configurations that affect execution. In this paper, we present a number of challenges and problems that need to be solved to allow better regression testing. We also present the results of a preliminary study of a user-configurable system developed at ABB to show how large these problems are in industry."
2091009,15517,23827,Does bug prediction support human developers? findings from a google case study,2013,"While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code."
93502,15517,22232,Weighted pushdown systems with indexed weight domains,2013,"The reachability analysis of weighted pushdown systems is a very powerful technique in verification and analysis of recursive programs. Each transition rule of a weighted pushdown system is associated with an element of a bounded semiring representing the weight of the rule. However, we have realized that the restriction of the boundedness is too strict and the formulation of weighted pushdown systems is not general enough for some applications.#R##N##R##N#To generalize weighted pushdown systems, we first introduce the notion of stack signatures that summarize the effect of a computation of a pushdown system and formulate pushdown systems as automata over the monoid of stack signatures. We then generalize weighted pushdown systems by introducing semirings indexed by the monoid and weaken the boundedness to local boundedness."
42105,15517,8422,The BINCOA framework for binary code analysis,2011,"This paper presents the BINCOA framework, whose goal is to ease the development of binary code analysers by providing an open formal model for low-level programs (typically: executable files), an XML format for easy exchange of models and some basic tool support. The BINCOA framework already comes with three different analysers, including simulation, test generation and Control-Flow Graph reconstruction."
97718,15517,8422,Automated termination proofs for Java programs with cyclic data,2012,"In earlier work, we developed a technique to prove termination of Java programs automatically: first, Java programs are automatically transformed to term rewrite systems (TRSs) and then, existing methods and tools are used to prove termination of the resulting TRSs. In this paper, we extend our technique in order to prove termination of algorithms on cyclic data such as cyclic lists or graphs automatically. We implemented our technique in the tool AProVE and performed extensive experiments to evaluate its practical applicability."
1843408,15517,517,Reconstructing Core Dumps,2013,"When a software failure occurs in the field, it is often difficult to reproduce. Guided by a memory dump at the moment of failure (a “core dump”), our RECORE test case generator searches for a series of events that precisely reconstruct the failure from primitive data. Applied on seven non-trivial Java bugs, RECORE reconstructs the exact failure in five cases without any runtime overhead in production code."
1764445,15517,23827,"3rd international workshop on games and software engineering: engineering computer games to enable positive, progressive change (GAS 2013)",2013,"We present a summary of the 3rd ICSE Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change in this article. The full day workshop is planned to include a keynote speaker, panel discussion, and paper presentations on game software engineering topics related to requirements specification and verification, software engineering education, re-use, and infrastructure. An overview of the accepted papers is included in this summary."
2783225,15517,20332,Symbolic model checking epistemic strategy logic,2014,"This paper presents a symbolic BDD-based model checking algorithm for an epistemic strategy logic with observational semantics. The logic has been shown to be more expressive than several variants of ATEL and therefore the algorithm can also be used for ATEL model checking. We implement the algorithm in a model checker and apply it to an application on train control system. The performance of the algorithm is also reported, with a comparison showing improved results over a previous partially symbolic approach for ATEL model checking."
237660,15517,23827,The Role of Peer Review in Supporting the Sustainability of Technology-Enhanced Learning Environments,2012,"We present a series of three studies on the peer review method. We argue that the implementation of such a method in a technology-enhanced learning environment can enhance the learning experience for the students, and consequently affect the sustainability of the learning environment itself. In the context of the paper, sustainability is viewed through a pedagogy lens, describing the property of a learning environment to maintain an active group of students. Finally, we present how different aspects of the peer review process can affect a learning activity."
875510,15517,507,"The relational model is dead, SQL is dead, and I don't feel so good myself",2013,"We report the opinions expressed by well-known database researchers on the future of the relational model and SQL during a panel at the International Workshop on Non-Conventional Data Access (NoCoDa 2012), held in Florence, Italy in October 2012 in conjunction with the 31st International Conference on Conceptual Modeling. The panelists include: Paolo Atzeni (Universita Roma Tre, Italy), Umeshwar Dayal (HP Labs, USA), Christian S. Jensen (Aarhus University, Denmark), and Sudha Ram (University of Arizona, USA). Quotations from movies are used as a playful though effective way to convey the dramatic changes that database technology and research are currently undergoing."
1866635,15517,339,Scriptless attacks: stealing the pie without touching the sill,2012,"Due to their high practical impact, Cross-Site Scripting (XSS) attacks have attracted a lot of attention from the security community members. In the same way, a plethora of more or less effective defense techniques have been proposed, addressing the causes and effects of XSS vulnerabilities. NoScript, and disabling scripting code in non-browser applications such as e-mail clients or instant messengers.   As a result, an adversary often can no longer inject or even execute arbitrary scripting code in several real-life scenarios.   In this paper, we examine the attack surface that remains after XSS and similar scripting attacks are supposedly mitigated by preventing an attacker from executing JavaScript code. We address the question of whether an attacker really needs JavaScript or similar functionality to perform attacks aiming for information theft. The surprising result is that an attacker can also abuse Cascading Style Sheets (CSS) in combination with other Web techniques like plain HTML, inactive SVG images or font files. Through several case studies, we introduce the so called  scriptless attacks  and demonstrate that an adversary might not need to execute code to preserve his ability to extract sensitive information from well protected websites. More precisely, we show that an attacker can use seemingly benign features to build side channel attacks that measure and exfiltrate almost arbitrary data displayed on a given website.   We conclude this paper with a discussion of potential mitigation techniques against this class of attacks. In addition, we have implemented a browser patch that enables a website to make a vital determination as to being loaded in a detached view or pop-up window. This approach proves useful for prevention of certain types of attacks we here discuss."
761718,15517,339,Polyglots: crossing origins by crossing formats,2013,"In a heterogeneous system like the web, information is exchanged between components in versatile formats. A new breed of attacks is on the rise that exploit the mismatch between the expected and provided content. This paper focuses on the root cause of a large class of attacks: polyglots. A polyglot is a program that is valid in multiple programming languages. Polyglots allow multiple interpretation of the content, providing a new space of attack vectors. We characterize what constitutes a dangerous format in the web setting and identify particularly dangerous formats, with PDF as the prime example. We demonstrate that polyglot-based attacks on the web open up for insecure communication across Internet origins. The paper presents novel attack vectors that infiltrate the trusted origin by syntax injection across multiple languages and by content smuggling of malicious payload that appears formatted as benign content. The attacks lead to both cross-domain leakage and cross-site request forgery. We perform a systematic study of PDF-based injection and content smuggling attacks. We evaluate the current practice in client/server content filtering and PDF readers for polyglot-based attacks, and report on vulnerabilities in the top 100 Alexa web sites. We identify five web sites to be vulnerable to syntax injection attacks. Further, we have found two major enterprise cloud storage services to be susceptible to content smuggling attacks. Our recommendations for protective measures on server side, in browsers, and in content interpreters (in particular, PDF readers) show how to mitigate the attacks."
1892639,15517,122,Concurrency testing using schedule bounding: an empirical study,2014,"We present the first independent empirical study on schedule bounding techniques for systematic concurrency testing (SCT). We have gathered 52 buggy concurrent software benchmarks, drawn from public code bases, which we call SCTBench. We applied a modified version of an existing concurrency testing tool to SCTBench to attempt to answer several research questions, including: How effective are the two main schedule bounding techniques, preemption bounding and delay bounding, at bug finding? What challenges are associated with applying SCT to existing code? How effective is schedule bounding compared to a naive random scheduler at finding bugs? Our findings confirm that delay bounding is superior to preemption bounding and that schedule bounding is more effective at finding bugs than unbounded depth-first search. The majority of bugs in SCTBench can be exposed using a small bound (1-3), supporting previous claims, but there is at least one benchmark that requires 5 preemptions. Surprisingly, we found that a naive random scheduler is at least as effective as schedule bounding for finding bugs. We have made SCTBench and our tools publicly available for reproducibility and use in future work."
2374602,15517,339,Code Reuse Attacks in PHP: Automated POP Chain Generation,2014,"Memory corruption vulnerabilities that lead to control-flow hijacking attacks are a common problem for binary executables and such attacks are known for more than two decades. Over the last few years, especially code reuse attacks attracted a lot of attention. In such attacks, an adversary does not need to inject her own code during the exploitation phase, but she reuses existing code fragments (so called gadgets) to build a code chain that performs malicious computations on her behalf. Return-oriented programming (ROP) is a well-known technique that bypasses many existing defenses. Surprisingly, code reuse attacks are also a viable attack vector against web applications.   In this paper, we study code reuse attacks in the context of PHP-based web applications. We analyze how PHP object injection (POI) vulnerabilities can be exploited via property-oriented programming (POP) and perform a systematic analysis of available gadgets in common PHP applications. Furthermore, we introduce an automated approach to statically detect POI vulnerabilities in object-oriented PHP code. Our approach is also capable of generating POP chains in an automated way. We implemented a prototype of the proposed approach and evaluated it with 10 well-known applications. Overall, we detected 30 new POI vulnerabilities and 28 new gadget chains."
1994904,15517,517,T-Fuzz: Model-Based Fuzzing for Robustness Testing of Telecommunication Protocols,2014,"Telecommunication networks are crucial in today's society since critical socio-economical and governmental functions depend upon them. High availability requirements, such as the five nines uptime availability, permeate the development of telecommunication applications from their design to their deployment. In this context, robustness testing plays a fundamental role in software quality assurance. We present T-Fuzz - a novel fuzzing framework that integrates with existing conformance testing environment. Automated model extraction of telecommunication protocols is provided to enable better code testing coverage. The T-Fuzz prototype has been fully implemented and tested on the implementation of a common LTE protocol within existing testing facilities. We provide an evaluation of our framework from both a technical and a qualitative point of view based on feedback from key testers. T-Fuzz has shown to enhance the existing development already in place by finding previously unseen unexpected behaviour in the system. Furthermore, according to the testers, T-Fuzz is easy to use and would likely result in time savings as well as more robust code."
1488572,15517,517,SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software,2014,"Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation."
1875087,15517,339,You are what you include: large-scale evaluation of remote javascript inclusions,2012,"JavaScript is used by web developers to enhance the interactivity of their sites, offload work to the users' browsers and improve their sites' responsiveness and user-friendliness, making web pages feel and behave like traditional desktop applications. An important feature of JavaScript, is the ability to combine multiple libraries from local and remote sources into the same page, under the same namespace. While this enables the creation of more advanced web applications, it also allows for a malicious JavaScript provider to steal data from other scripts and from the page itself. Today, when developers include remote JavaScript libraries, they trust that the remote providers will not abuse the power bestowed upon them.   In this paper, we report on a large-scale crawl of more than three million pages of the top 10,000 Alexa sites, and identify the trust relationships of these sites with their library providers. We show the evolution of JavaScript inclusions over time and develop a set of metrics in order to assess the maintenance-quality of each JavaScript provider, showing that in some cases, top Internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious JavaScript. In this process, we identify four, previously unknown, types of vulnerabilities that attackers could use to attack popular web sites. Lastly, we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought."
2432777,15517,517,Test Generation for X-machines with Non-terminal States and Priorities of Operations,2011,"Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition."
1828001,15517,339,AppInk: watermarking android apps for repackaging deterrence,2013,"With increased popularity and wide adoption of smartphones and mobile devices, recent years have seen a new burgeoning economy model centered around mobile apps. However, app repackaging, among many other threats, brings tremendous risk to the ecosystem, including app developers, app market operators, and end users. To mitigate such threat, we propose and develop a watermarking mechanism for Android apps. First, towards automatic watermark embedding and extraction, we introduce the novel concept of  manifest app , which is a companion of a target Android app under protection. We then design and develop a tool named  AppInk , which takes the source code of an app as input to automatically generate a new app with a transparently-embedded watermark and the associated manifest app. The manifest app can be later used to reliably recognize embedded watermark with zero user intervention. To demonstrate the effectiveness of AppInk in preventing app repackaging, we analyze its robustness in defending against distortive, subtractive, and additive attacks, and then evaluate its resistance against two open source repackaging tools. Our results show that AppInk is easy to use, effective in defending against current known repackaging threats on Android platform, and introduces small performance overhead."
1478403,15517,517,Combinatorial Testing for an Automotive Hybrid Electric Vehicle Control System: A Case Study,2014,"Embedded electrical systems for passenger vehicles are highly complex distributed systems with varying system boundaries. The surge towards further electrification of vehicles demands the deployment of high voltage systems that provide propulsion through an electric motor as part of a hybrid electric or pure electric drive train. This demands additional care and robust deployment to ensure the safety of the end user and the environment around them. Exhaustive testing is not feasible for large systems and the use of formal approaches can be restrictive. In the presented work a combinatorial test approach is applied to a real Hybrid Electric Vehicle control system as part of a hardware-in-the-loop test system. 2-way, 3-way and mixed strength up to 4-way testing is carried out. The concept of CAN main and local is devised to intercept CAN messages and replace them with the generated combinatorial tests using the HIL simulator's own processor to assure real-time testing. Early results indicate that the approach is effective in exposing incidents in system behavior not normally found during traditional functional testing."
2082001,15517,122,Efficient search for inputs causing high floating-point errors,2014,"Tools for floating-point error estimation are fundamental to program understanding and optimization. In this paper, we focus on tools for determining the input settings to a floating point routine that maximizes its result error. Such tools can help support activities such as precision allocation, performance optimization, and auto-tuning. We benchmark current abstraction-based precision analysis methods, and show that they often do not work at scale, or generate highly pessimistic error estimates, often caused by non-linear operators or complex input constraints that define the set of legal inputs. We show that while concrete-testing-based error estimation methods based on maintaining shadow values at higher precision can search out higher error-inducing inputs, suit able heuristic search guidance is key to finding higher errors. We develop a heuristic search algorithm called Binary Guided Random Testing (BGRT). In 45 of the 48 total benchmarks, including many real-world routines, BGRT returns higher guaranteed errors. We also evaluate BGRT against two other heuristic search methods called ILS and PSO, obtaining better results."
1237113,15517,517,Automatic and Incremental Product Optimization for Software Product Lines,2014,"Software Product Lines (SPLs) have gained popularity in industry as they foster the reuse of artifacts, such as code, and reduce product development effort. Although some SPLs ensure that only valid products are configurable, those products are not necessarily optimal. For instance, they may include code that is not necessary for providing the desired functionality -- often because of erroneous traceability between features and code. Such unnecessary code may be disallowed in safety critical domains, it may lead to losses in runtime performance, or it may lead to errors during later SPL evolution. In this paper, we present an approach for automatic and incremental product optimization. Our approach leverages product functionality tests to ensure that configured products do not include unnecessary artifacts -- an automatic re-optimization of products after SPL evolution is performed incrementally. The evaluation results show that such a re-optimization takes only milliseconds."
709248,15517,339,Towards reducing the attack surface of software backdoors,2013,"Backdoors in software systems probably exist since the very first access control mechanisms were implemented and they are a well-known security problem. Despite a wave of public discoveries of such backdoors over the last few years, this threat has only rarely been tackled so far.   In this paper, we present an approach to reduce the attack surface for this kind of attacks and we strive for an automated identification and elimination of backdoors in binary applications. We limit our focus on the examination of server applications within a client-server model. At the core, we apply variations of the delta debugging technique and introduce several novel heuristics for the identification of those regions in binary application that backdoors are typically installed in (i.e., authentication and command processing functions). We demonstrate the practical feasibility of our approach on several real-world backdoors found in modified versions of the popular software tools ProFTPD and OpenSSH. Furthermore, we evaluate our implementation not only on common instruction set architectures such as x86-64, but also on commercial off-the-shelf embedded devices powered by a MIPS32 processor."
2443551,15517,374,Mining Malware Specifications through Static Reachability Analysis,2013,"Abstract. The number of malicious software (malware) is growing out of control. Syntactic signature based detection cannot cope with such growth and manual construction of malware signature databases needs to be replaced by computer learning based approaches. Currently, a single modern signature capturing the semantics of a malicious behavior can be used to replace an arbitrarily large number of old-fashioned syntactical signatures. However teaching computers to learn such behaviors is a challenge. Existing work relies on dynamic analysis to extract malicious behaviors, but such technique does not guarantee the coverage of all behaviors. To sidestep this limitation we show how to learn malware signatures using static reachability analysis. The idea is to model binary programs using pushdown systems (that can be used to model the stack operations occurring during the binary code execution), use reachability analysis to extract behaviors in the form of trees, and use subtrees that are common among the trees extracted from a training set of malware files as signatures. To detect malware we propose to use a tree automaton to compactly store malicious behavior trees and check if any of the subtrees extracted from the file under analysis is malicious. Experimental data shows that our approach can be used to learn signatures from a training set of malware files and use them to detect a test set of malware that is 5 times the size of the training set."
1852194,15517,517,TripleT: Improving Test Responsiveness for High Performance Embedded Systems,2011,"Timed testing, i.e. a method of testing where timing plays a crucial role in the test verdict of the test cases, is an important quality assurance strategy in the development of embedded systems. Tool support is essential for timed testing, as timed test cases cannot be executed manually with the required precision. In case studies, we found that the existing timed testing tools do not fulfill all requirements needed for real-world-timed testing: they lack both features and responsiveness. Therefore, we have started the implementation of a timed testing tool called TripleT. TripleT is based on the notion of timed ioco as conformance relation, and uses as basis algorithms already present in TorX. However, we have improved in the responsiveness and execution time of our test cases - two major points when it comes to timeliness in timed testing. This has been achieved by using look-ahead and parallelizing the test case execution of TripleT. In the paper we will report on our approach and the results achieved."
1995637,15517,517,Improved Bug Reporting and Reproduction through Non-intrusive GUI Usage Monitoring and Automated Replaying,2011,"Most software systems are operated using a Graphical User Interface (GUI). Therefore, bugs are often triggered by user interaction with the software's GUI. Hence, accurate and reliable GUI usage information is an important tool for bug fixing, as the reproduction of a bug is the first important step towards fixing it. To support bug reproduction, a generic, easy to integrate, non-intrusive GUI usage monitoring mechanism is introduced in this paper. As supplement for the monitoring, a method for automatically replaying the monitored usage logs is provided. The feasibility of both is demonstrated through proof-of-concept implementations. A case-study shows that the monitoring mechanism can be integrated into large-scale software products without significant effort and that the logs are replayable. Additionally, a usage-based end-to-end GUI testing approach is outlined, in which the monitoring and replaying play major roles."
1243258,15517,339,Chucky: exposing missing checks in source code for vulnerability discovery,2013,"Uncovering security vulnerabilities in software is a key for operating secure systems. Unfortunately, only some security flaws can be detected automatically and the vast majority of vulnerabilities is still identified by tedious auditing of source code. In this paper, we strive to improve this situation by accelerating the process of manual auditing. We introduce Chucky, a method to expose missing checks in source code. Many vulnerabilities result from insufficient input validation and thus omitted or false checks provide valuable clues for finding security flaws. Our method proceeds by statically tainting source code and identifying anomalous or missing conditions linked to security-critical objects.In an empirical evaluation with five popular open-source projects, Chucky is able to accurately identify artificial and real missing checks, which ultimately enables us to uncover 12 previously unknown vulnerabilities in two of the projects (Pidgin and LibTIFF)."
1399255,15517,517,Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,2014,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e1, e2) is counted the same as if it occurs in the order (e2, e1). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies."
1752317,15517,517,Search-Based Propagation of Regression Faults in Automated Regression Testing,2013,"Over the lifetime of software programs, developers make changes by adding, removing, enhancing functionality or by refactoring code. These changes can sometimes result in undesired side effects in the original functionality of the software, better known as regression faults. To detect these, developers either have to rely on an existing set of test cases, or have to create new tests that exercise the changes. However, simply executing the changed code does not guarantee that a regression fault manifests in a state change, or that this state change propagates to an observable output where it could be detected by a test case. To address this propagation aspect, we present EVOSUITER, an extension of the EVOSUITE unit test generation tool. Our approach generates tests that propagate regression faults to an observable difference using a search-based approach, and captures this observable difference with test assertions. We illustrate on an example program that EVOSUITER can be effective in revealing regression errors in cases where alternative approaches may fail, and motivate further research in this direction."
1583004,15517,517,Challenges of Testing for Critical Interactive Systems,2013,"Interactive systems cover all systems that represent a bridge to enable the user interaction over an interface. The advance of technologies such as ubiquitous computing brings new interaction designs. The new interaction styles have illustrated the shift from design graphical user interfaces to design human-computer interactions (HCI). However, several approaches still focus on testing GUI instead of testing HCI. The goal of this thesis is to investigate the use of new concepts to automate the interactive systems testing by addressing the issues of critical interactive systems."
2188155,15517,517,A Unified Approach for Localizing Non-deadlock Concurrency Bugs,2012,"This paper presents UNICORN, a new automated dynamic pattern-detection-based technique that finds and ranks problematic memory access patterns for non-deadlock concurrency bugs. UNICORN monitors pairs of memory accesses, combines the pairs into problematic patterns, and ranks the patterns by their suspiciousness scores. UNICORN detects significant classes of bug types, including order violations and both single-variable and multi-variable atomicity violations, which have been shown to be the most important classes of non-deadlock concurrency bugs. The paper also describes implementations of UNICORN in Java and C++, along with empirical evaluation using these implementations. The evaluation shows that UNICORN can effectively compute and rank the patterns that represent concurrency bugs, and perform computation and ranking with reasonable efficiency."
2319425,15517,517,Analogies and Differences between Mutation Operators for WS-BPEL 2.0 and Other Languages,2011,"Applying mutation testing to a program written in a certain language requires that a set of mutation operators is defined for that language. The mutation operators need to adequately cover the features of that language in order to be effective. In this work, we evaluate qualitatively the operators defined for the Web Services Business Process Execution Language 2.0 (WS-BPEL) and study the differences and similarities between WS-BPEL and other languages. We review the existing operators for several structured and object-oriented general-purpose programming languages, and for several domain-specific languages. Results confirm that WS-BPEL is very different from other languages, as half of the mutation operators for this language are equivalent to those of other languages. Our study concludes that the set of WS-BPEL mutation operators can be improved."
1905922,15517,517,Using Property-Based Oracles when Testing Embedded System Applications,2011,"Embedded systems are becoming increasingly ubiquitous, controlling a wide variety of popular and safety-critical devices. Effective testing techniques could improve the dependability of these systems. In prior work we presented an approach for testing embedded systems, focusing on embedded system applications and the tasks that comprise them. In this work we focus on a second but equally important aspect of testing embedded systems, namely, the need to provide observability of system behavior sufficient to allow engineers to detect failures. We present several property-based oracles that can be instantiated in embedded systems through program analysis and instrumentation, and can detect failures for which simple output-based oracles are inadequate. An empirical study of our approach shows that it can be effective."
1257488,15517,517,The Seed is Strong: Seeding Strategies in Search-Based Software Testing,2012,"Search-based techniques have been shown useful for the task of generating tests, for example in the case of object-oriented software. But, as for any meta-heuristic search, the efficiency is heavily dependent on many different factors, seeding is one such factor that may strongly influence this efficiency. In this paper, we evaluate new and typical strategies to seed the initial population as well as to seed values introduced during the search when generating tests for object-oriented code. We report the results of a large empirical analysis carried out on 20 Java projects (for a total of 1,752 public classes). Our experiments show with strong statistical confidence that, even for a testing tool that is already able to achieve high coverage, the use of appropriate seeding strategies can further improve performance."
1816703,15517,517,Compression Strategies for Passive Testing,2011,"Testing is one of the most widely used techniques to increase the confidence on the correctness of complex software systems. In this paper we extend our previous work on passive testing with invariants, this technique checks the logs, collected from the system under test, in order to detect faults. This new proposal is focused on how the collected logs can be compressed without loosing information and how the invariants must be adapted with respect to the selected compression strategy, in a correct way. We show the soundness of this new methodology."
2391401,15517,517,Experimental Comparison of Test Case Generation Methods for Finite State Machines,2012,"Testing from finite state machines has been widely investigated due to its well-founded and sound theory as well as its practical application in different areas, e.g., Web-based systems and protocol testing. There has been a recurrent interest in developing methods capable of generating test suites that detect all faults in a given fault domain. However, the proposal of new methods motivates the comparison with traditional methods. In this context, we conducted a set of experiments that compares W, HSI, H, SPY, and P methods. The results have shown that H, SPY, and P methods produce smaller test suites than traditional methods (W, HSI). Although the P method presented the shortest test suite in most cases, its reduction is smaller compared with H and SPY. We have also observed that the reduction ratio in partial machines is smaller than that in complete machines."
1815116,15517,122,Race directed scheduling of concurrent programs,2014,"Detection of data races in Java programs remains a difficult problem. The best static techniques produce many false positives, and also the best dynamic techniques leave room for improvement. We present a new technique called race directed scheduling that for a given race candidate searches for an input and a schedule that lead to the race. The search iterates a combination of concolic execution and schedule improvement, and turns out to find useful inputs and schedules efficiently. We use an existing technique to produce a manageable number of race candidates. Our experiments on 23 Java programs found 72 real races that were missed by the best existing dynamic techniques. Among those 72 races, 31 races were found with schedules that have between 1 million and 108 million events, which suggests that they are rare and hard-to-find races."
2329172,15517,23827,Guaranteeing correct evolution of software product lines: setting up the problem,2011,"The research question that we posed ourselves and which has led to this paper is: how can we guarantee the correct functioning of products of an SPL when core components evolve? This exploratory paper merely proposes an overview of a novel approach that, by extending and adapting assume-guarantee reasoning to evolving SPLs, guarantees the resilience against changes in the environment of products of an SPL. The idea is to selectively model check and test assume-guarantee properties on those SPL components affected by the changes."
1896054,15517,23827,First workshop on developing tools as plug-ins (TOPI 2011),2011,"Our knowledge as to how to solve software engineering problems is increasingly being encapsulated in tools. These tools are at their strongest when they operate in a pre-existing development environment that can provide integration with existing elements such as compilers, debuggers, profilers and visualizers. The first Workshop on Developing Tools as Plug-ins is a new forum in which to addresses research, ongoing work, ideas, concepts, and critical questions related to the engineering of software tools and plug-ins."
1268859,15517,517,Viewpoint-Based Test Requirement Analaysis Modeling,2014,"As there is no perfect software requirement specification practically, software testing needs test requirement analysis called TRA. TRA is one of the most important activities. In TRA, test requirement model is constructed and mainly consists of test conditions. In this research we assume that TRA engineers may implicitly omit necessary test conditions if they construct test requirement models not based on potential test conditions, but based on actual test conditions. This paper proposes a concept of test viewpoints as potential test conditions and a notation of test requirement modeling to reduce omission of test conditions in TRA. We also show an experiment that a test requirement model based on test viewpoint can reduce omission of test conditions."
1246399,15517,8806,WSCCT: a tool for WS-BPEL compositions conformance testing,2013,"WS-BPEL is considered as the emerging standard for Web service compositions. As more and more compositions are modeled using WS-BPEL, ensuring good quality of WS-BPEL codes becomes crucial. This paper shows a novel tool, WSCCT, implemented for conformance testing of WS-BPEL compositions. It is based on Timed Automata as model for online testing of WS-BPEL implementations. The tool is being extended with graphical user interfaces. Besides, we illustrate how to use our proposed tool for conformance testing of a travel agency scenario which is implemented in BPEL."
877268,15517,507,Automatic Verification of Database-Centric Systems,2014,"Automatic Verification of Database-Centric Systems Alin Deutsch Richard Hull Victor Vianu UC San Diego IBM Yorktown Research Center UC San Diego & INRIA Saclay deutsch@cs.ucsd.edu vianu@cs.ucsd.edu hull@us.ibm.com INTRODUCTION area of business process management, concomitantly with an evolution from the traditional process-centric approach towards data awareness. A notable expo- nent of this class is the business artifact model pio- neered in [63, 51], deployed by IBM in professional services. Business artifacts (or simply “artifacts”) model key business-relevant entities, which are up- dated by a set of services that implement business process tasks. A collection of artifacts and services is called an artifact system. This modeling approach has been successfully deployed in practice [7, 6, 21, 27, 69], and has been adopted in the OMG standard for Case Management [9]. Tools such as the above automatically generate the database-centric application code from the high- level specification. This not only allows fast proto- typing and improves programmer productivity but, as a side effect, provides new opportunities for au- tomatic verification. Indeed, the high-level specifi- cation is a natural target for verification, as it ad- dresses the most likely source of errors (the applica- tion’s specification, as opposed to the less likely er- rors in the automatic generator’s implementation). The theoretical and practical results obtained so far concerning the verification of such systems are quite encouraging. They suggest that, unlike arbi- trary software systems, significant classes of data- driven systems may be amenable to automatic veri- fication. This relies on a novel marriage of database and model checking techniques, and is relevant to both the database and the computer-aided verifica- tion communities. In this article, we describe several models and re- sults on automatic verification of database-driven systems, focusing on temporal properties of their underlying workflows. To streamline the presenta- tion, we focus on verification of business artifacts, and use it as a vehicle to introduce the main con- cepts and results. We then summarize some of the work pertaining to other applications such as data- driven web services. Software systems centered around a database are pervasive in numerous applications. They are en- countered in areas as diverse as electronic commerce, e-government, scientific applications, enterprise in- formation systems, and business process manage- ment. Such systems are often very complex and prone to costly bugs, whence the need for verifica- tion of critical properties. Classical software verification techniques that can be applied to such systems include model check- ing and theorem proving. However, both have se- rious limitations. Indeed, model checking usually requires performing finite-state abstraction on the data, resulting in loss of semantics for both the sys- tem and properties being verified. Theorem proving is incomplete, requiring expert user feedback. Recently, an alternative approach to verification of database-centric systems has taken shape, at the confluence of the database and computer-aided ver- ification areas. It aims to identify restricted but sufficiently expressive classes of database-driven ap- plications and properties for which sound and com- plete verification can be performed in a fully auto- matic way. This approach leverages another trend in database-driven applications: the emergence of high-level specification tools for database-centered systems, such as interactive web applications and data-driven business processes. We review next a few representative examples. A commercially successful high-level specification tool for web applications is Web Ratio [1], an out- growth of the earlier academic prototype WebML [20, 17]. Web Ratio allows to specify a Web ap- plication using an interactive variant of the E-R model augmented with a workflow formalism. Non- interactive variants of Web page specifications had already been proposed in Strudel [39], Araneus [58] and Weave [40], targeting the automatic generation of Web sites from an underlying database. High- level specification tools have also emerged in the"
726948,15517,23865,Connecting technology with real-world problems - from copy-paste detection to detecting known bugs (keynote abstract),2011,"In my talk, I will share with you our experience in applying and deploying our source code mining technology in industry. In particular, the most valuable lesson we have learned is that sometimes there is a bigger problem in the real world that can really benefit from our technology but unfortunately we do not know about it until we closely work with industry. In 2004, motivated from some previous research work that pointed out copy-pasting as a major reason for majority of the bugs in device driver code, my students and I applied data mining technology (specifically frequent subsequence mining algorithms) in identifying copy-pasted code and also detecting forget-to-change bugs introduced during copy-pasting. The benefit of using data mining is that it is highly scalable (20 minutes for 4-5 millions lines of code), and can tolerate changes such as statement insertion/deletion/modification as well as variable name changes. When we released our tool called CP-Miner to the open source community, it attracted some inquiries from industry. These inquiries have motivated us to start a company to commercialize our tools. During the commercialization process, our customers have taught us that our technology can be applied to solve a major headache faced by many embedded system vendors such as storage companies, network devices, telecommunications, handhelds, electronics, etc. These companies typically have to maintain tens and even hundreds of active branches of similar software, one for slightly different devices or with different customization. These branches are 60-70% similar and are being developed in parallel after split (and are usually never merged back together). So when developers decide to fix a bug or security hole in one branch, it is usually a big challenge for them to check what other branches a similar fix should be also applied. So many costly incidents have happened in the field due to some known bugs (a bug that has already been diagnosed and fixed in some other branches). Since these branches have diverged over the years, many code are similar but are not exactly the same. Also, file names can change, etc. Therefore, it is hard to rely on source control systems such as ClearCase, Subversion to keep track their differences. Being pushed by many customers who suffer this pain, we applied our technology to this problem and build a tool called PatchMiner. Currently PatchMiner has been deployed and widely used in several large companies (more information can be found at http://www.patterninsight.com/). It is a very interesting journey. To me personally, I learned that sometimes what I (as an academic) feel as a solution to a major problem may be only a nice-to-have in the real world; and it really requires close interaction with industry to understand their painpoints."
179107,15517,9438,Against the Odds: Managing the Unmanagable in a Time of Crisis,2014,"Information technology systems at the Greek Ministry of Finance could be the ideal tools for fighting widespread tax evasion, bureaucratic inefficiency, waste, and corruption. Yet making this hap- pen requires battling against protracted procurement processes and im- plementation schedules, ineffective operations, and rigid management structures. This experience report details some unconventional measures, tools, and techniques that were adopted to sidestep the barriers in a time of crisis. The measures involved meritocracy, it utilization, and manage- ment by objectives. Sadly, this report is also a story (still being written) on the limits of such approaches. On balance, it demonstrates that in any large organization there are ample opportunities to bring about change, even against considerable odds. It seemed like a good idea at the time. In 2009, Greece's new government, instead of staffing the ministries' Secretary General positions through the, commonly, opaque political appointments (18, p. 79)(19, p. 157), it provided a form to submit online applications. As a professor of software engineering interested in public service, I decided to apply for the position of the Secretary General for Infor- mation Systems at the Ministry of Finance, a position that was in effect the Ministry's cio post. I reasoned that by serving in this position I could apply in practice what I taught to my students; in this case how to provide high quality eGovernment services in an efficient manner. After a couple of interviews and a few weeks of waiting time I started serving, for what was to be a two year term, at the General Secretariat for Information Systems (gsis). Gsis develops, runs, and supports the information systems used at the Greek Ministry of Finance. The most important of these at the end of 2009 were taxis, the software used in all of 300 Greece's tax offices, taxisnet, a web application that taxpayers use for electronic filing, and, icisnet, which supports the customs agencies. Another significant body of work at the time involved the keying-in of tax filing forms, the calculation of tax returns, and the printing of tax assessment forms and payment notices (about 15 million documents per year). In addition gsis supported diverse tax compliance actions, mainly by running electronic tax compliance checks. Finally, gsis, contributed input for fiscal policy making in the form of statistical data and ad-hoc calculations. M. Jarke et al. (Eds.): CAiSE 2014, LNCS 8484, pp. 24-41, 2014."
99906,15517,22232,Compositional Invariant Generation for Timed Systems,2014,"In this paper we address the state space explosion problem inherent to model-checking timed systems with a large number of compo- nents. The main challenge is to obtain pertinent global timing constraints from the timings in the components alone. To this end, we make use of auxiliary clocks to automatically generate new invariants which capture the constraints induced by the synchronisations between components. The method has been implemented as an extension of the D-Finder tool and successfully experimented on several benchmarks. Compositional methods in verification have been developed to cope with state space explosion. Generally based on divide et impera principles, these methods attempt to break monolithic verification problems into smaller sub-problems by exploiting either the structure of the system or the property or both. Composi- tional reasoning can be used in different manners e.g., for deductive verification, assume-guarantee, contract-based verification, compositional generation, etc. The development of compositional verification for timed systems remains how- ever challenging. State-of-the-art tools (7,13,25,18) for the verification of such systems are mostly based on symbolic state space exploration, using efficient data structures and particularly involved exploration techniques. In the timed context, the use of compositional reasoning is inherently difficult due to the synchronous model of time. Time progress is an action that synchronises contin- uously all the components of the system. Getting rid of the time synchronisation is necessary for analysing independently different parts of the system (or of the property) but becomes problematic when attempting to re-compose the partial verification results. Nonetheless, compositional verification is actively investi- gated and several approaches have been recently developed and employed in timed interfaces (2) and contract-based assume-guarantee reasoning (15,22). In this paper, we propose a different approach for exploiting compositionality for analysis of timed systems using invariants. In contrast to exact reachability analysis, invariants are symbolic approximations of the set of reachable states of the system. We show that rather precise invariants can be computed composi- tionally, from the separate analysis of the components in the system and from � Work partially supported by the European Integrated Projects 257414 ASCENS, 288175 CERTAINTY, and STREP 318772 D-MILS."
152859,15517,23827,Formal proofs of code generation and verification tools,2014,"Tool-assisted verification of critical software has great po- tential but is limited by two risks: unsoundness of the verification tools, and miscompilation when generating executable code from the sources that were verified. A radical solution to these two risks is the deductive verification of compilers and verification tools themselves. In this invited talk, I describe two ongoing projects along this line: CompCert, a veri- fied C compiler, and Verasco, a verified static analyzer based on abstract interpretation. Abstract of Invited Talk Tool-assisted formal verification of software is making inroads in the critical soft- ware industry. While full correctness proofs for whole applications can rarely be achieved (6,12), tools based on static analysis and model checking can already establish important safety and security properties (memory safety, absence of arithmetic overflow, unreachability of some failure states) for large code bases (1). Likewise, deductive program verifiers based on Hoare logic or separation logic can verify full correctness for crucial algorithms and data structures and their implementations (11). In the context of critical software that must be qualified against demanding regulations (such as DO-178 in avionics or Common Crite- ria in security), such tool-assisted verifications provide independent evidence, complementing that obtained by conventional verification based on testing and reviews. The trust we can put in the results of verification tools is limited by two risks. The first is unsoundness of the tool: by design or by mistake in its implementa- tion, the tool can fail to account for all possible executions of the software under verification, reporting no alarms while an incorrect execution can occur. The second risk is miscompilation of the code that was formally verified. With a few exceptions (3), most verif ication tools operate over source code (C, Java, . ..) or models (Simulink or Scade block diagrams). A bug in the compilers or code gen- erators used to produce the executable machine code can result in an incorrect executable being produced from correct source code (13). Both unsoundness and miscompilation risks are known in the critical software industry and accounted for in DO-178 and other regulations (7). It is extremely difficult, however, to verify an optimizing compiler or sophisticated static an- alyzer using conventional testing. Formal verification of compilers, static ana- lyzers, and related tools provides a radical, mathematically-grounded answer to"
2696646,15517,22232,CPAchecker with Adjustable Predicate Analysis (Competition Contribution),2012,"CPAchecker is a freely available software-verification framework, built on the concepts of Configurable Program Analysis (CPA). CPAchecker integrates most of the state-of-the-art technologies for software model checking, such as counterexample-guided abstraction refinement (CEGAR), lazy predicate abstraction, interpolation-based refinement, and large-block encoding. The CPA for predicate analysis with adjustable-block encoding (ABE) is very promising in many categories, and thus, we submit a CPAchecker configuration that uses this analysis approach to the competition. 1 Verification Approach Predicate analysis is a common approach to software verification, and tools like Blast and SLAM showed that it can be used effectively for software verifica- tion of medium sized programs. CPAchecker (2) constructs —like Blast —a n abstract reachability graph (ARG) as a central data structure, by continuous successor computations along the edges of the control-flow automaton (CFA) of the program. The nodes of the ARG, representing sets of reachable program states, store relevant information like control-flow location, call stack, and, most importantly, the formulas that represent the abstract data states. When single-block encoding (as implemented in Blast) is used, abstractions are computed for every single edge in the CFA. The major drawback of this approach is the large number of successor computations, each requiring expensive calls to a theorem prover. Furthermore, boolean abstraction is prohibitive for such a large number of successor computations, and only the more imprecise cartesian abstraction can be used. Therefore, CPAchecker implements an approach called adjustable-block en- coding (3), which completely separates the process of computing successors from the process of computing a predicate abstraction for a formula. The post opera- tions in this approach (purely syntactically) assemble formulas for the strongest postcondition. Then, at certain points that can be chosen arbitrarily, the pro- cedure applies an (expensive) computation of the predicate abstraction for a given abstract state. This method reduces the number of theorem-prover calls by effectively combining program blocks of arbitrary size into a single formula before computing an abstraction. Because the model checker now delegates much larger problems to the SMT solver (the formulas will contain a disjunction for each control-flow join point inside a block), this technique is able to leverage"
1870297,15517,23620,Programming languages for programmable networks,2012,"Today's computer networks perform a bewildering array of tasks, from routing and access control, to traffic monitoring and load balancing. To support wireless users accessing services hosted in the cloud, enterprise and data-center networks are under increasing pressure to support client mobility, virtual-machine migration, resource isolation between cloud services, and energy-efficient operation. Yet, network administrators must configure the network through closed and proprietary interfaces to heterogeneous devices, such as routers, switches, firewalls, load balancers, network address translators, and intrusion detection systems. Not surprisingly, configuring these complex networks is expensive and error-prone, and innovation in network management proceeds at a snail's pace.   During the past several years, the networking industry and research community have pushed for greater openness in networking software, and a clearer separation between networking devices and the software that controls them. This broad trend is known as Software Defined Networking (SDN). A hallmark of SDN is having an open interface for controller software running on a commodity computer to install packet-processing rules in the underlying switches. In particular, the OpenFlow protocol (see www.openflow.org) has significant momentum. Many commercial switches support OpenFlow, and a number of campus, data-center, and backbone networks have deployed the new technology.   With the emergence of open interfaces to network devices, the time is ripe to rethink the design of network software, to put networking on a stronger foundation and foster innovation in networked services. The programming languages community can play a vital role in this transformation, by creating languages, compilers, run-time systems, and testing and verification techniques that raise the level of abstraction for programming the network. In this talk, we give an overview of Software Defined Networking, and survey the early programming-languages research in this area. We also outline exciting opportunities for interdisciplinary research at the intersection of programming languages and computer networks."
894674,15517,11058,LeakChaser: helping programmers narrow down causes of memory leaks,2011,"In large programs written in managed languages such as Java and C#, holding unnecessary references often results in memory leaks and bloat, degrading significantly their run-time performance and scalability. Despite the existence of many leak detectors for such languages, these detectors often target low-level objects; as a result, their reports contain many false warnings and lack sufficient semantic information to help diagnose problems. This paper introduces a specification-based technique called LeakChaser that can not only capture precisely the unnecessary references leading to leaks, but also explain, with high-level semantics, why these references become unnecessary.   At the heart of LeakChaser is a three-tier approach that uses varying levels of abstraction to assist programmers with different skill levels and code familiarity to find leaks. At the highest tier of the approach, the programmer only needs to specify the boundaries of coarse-grained activities, referred to as transactions. The tool automatically infers liveness properties of these transactions, by monitoring the execution, in order to find unnecessary references. Diagnosis at this tier can be performed by any programmer after inspecting the APIs and basic modules of a program, without understanding of the detailed implementation of these APIs. At the middle tier, the programmer can introduce application-specific semantic information by specifying properties for the transactions. At the lowest tier of the approach is a liveness checker that does not rely on higher-level semantic information, but rather allows a programmer to assert lifetime relationships for pairs of objects. This task could only be performed by skillful programmers who have a clear understanding of data structures and algorithms in the program.   We have implemented LeakChaser in Jikes RVM and used it to help us diagnose several real-world leaks. The implementation incurs a reasonable overhead for debugging and tuning. Our case studies indicate that the implementation is powerful in guiding programmers with varying code familiarity to find the root causes of several memory leaks---even someone who had not studied a leaking program can quickly find the cause after using LeakChaser's iterative process that infers and checks properties with different levels of semantic information."
1034362,15517,23620,Optimizing data structures in high-level programs: new directions for extensible compilers based on staging,2013,"High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts.   Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops.   We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments."
1665799,15517,20524,The WTE+ framework: automated construction and runtime adaptation of service mashups,2013,"An emerging software engineering methodology is the combination of functionality and content from existing software components into service mashups, creating greater value than the sum of the individual participating building blocks. For businesses, using catalogues of reusable services means agile development of new applications using open communication standards including the Simple Object Access Protocol (SOAP) for transmitting data, and the Web Services Description Language (WSDL) for defining services. The result is faster adaptation to the changing business environment creating value at reduced development time and cost while increasing revenues.#R##N##R##N#This manuscript presents the developed service management framework and evaluation results of the WTE+ (Web2.0/Telco2.0/Enterprise2.0 with semantics) research project. Main goal is supporting application developers with the automatic construction and runtime reconfiguration of custom-made service mashups at a minimum performance cost without the need for constant IT intervention. Semantically enriched services are automatically combined into custom-made service mashups by the designed planning algorithms. These planning techniques are optimized with late binding and runtime adaptation to changing user-context taking fully into account the quality of service parameters of the available components which are provided at design time and updated at runtime. In addition, recovery mechanisms are provided in case of failed services and/or resources. During mashup execution, unavailable services are dynamically replaced by equivalent ones or alternative service mashups keeping in mind the current execution state.#R##N##R##N#The developed planning algorithms are put through extensive performance and scalability experiments for a typical e-commerce scenario, in which e-shop services such as product payment and delivery are on-the-fly composed to an e-shop application. The results show that an automatic construction of a new application out of existing services can take up between 5 to 43 seconds for 500 services while runtime adaptation takes up to 5 seconds on average depending on the availability of equivalent services."
2086031,15517,9436,Towards Feature-Oriented Requirements Validation for Automotive Systems,2014,"With the growing complexity of embedded real-time systems, requirements validation becomes an ever-more critical activity for developing such systems. Studies have revealed that most of the anomalies, discovered in the development of complex systems, belong to requirement and specification phases. To ease the situation, many efforts have been investigated into the area. Model-based techniques, enabling formal semantics and requirements traceability, are emerging as promising solutions to cost-effective requirements validation. In these techniques, the functional behaviors derived from lower-level requirements are specified in terms of analyzable models at a certain level of abstraction. Further, upper-level requirements are formalized into verifiable queries and/or formulas. Meanwhile, trace links between requirements at various levels of abstraction as well as between requirements and subsequent artifacts (such as verifiable queries and/or formulas, and analyzable models) are built, through which the queries and/or formulas can be fed into the corresponding models. However, such model-based techniques suffer from some limitations, such as how to support semi- or fully-automatic trace links creation between diverse development artifacts, how to ease the demand of heavy mathematics background knowledge to specify queries and/or formulas, and how to analyze models without encountering the state explosion problem. In this thesis, we cover two aspects centering around requirements validation to ease the aforementioned limitations, which are mainly about requirements traceability and model-based requirements validation. In particular, the technical contributions are four-fold: 1) we have introduced an improved VSM-based requirements traceability creation/recovery approach using a novel context analysis and, 2) we have proposed a lightweight model-based approach to requirements validation by using the Timed Abstract State Machine (TASM) language with newly defined Observer and Event constructs and, 3) we have combined our model-based approach with a restricted use case modeling approach for feature-oriented requirements validation and, 4) we have improved the Observer construct of TASM via proposing a new observer specification logic to facilitate the observer specification, as well as defining the corresponding observer execution process. Finally, we have demonstrated the applicability of our contributions in real world usage through various applications."
23454,15517,9438,Goal-Oriented Security Requirements Analysis for a System Used in Several Different Activities,2013,"Because an information system is used in different activities simul- taneously today, we have to analyze usages of the system in the existing activi- ties and to-be usages in an intended activity together. Especially, security aspects should be carefully analyzed because existing activities are not always secure. We propose a security requirements analysis method for resolving this problem. To take both existing and intended activities into account together, we integrate them on the basis of the unification of common actors. To explore possible at- tacks under integrated activities, we enumerate achievable attacks on the basis of the possible means in each actor with the help of security knowledge. To avoid or mitigate the attacks and to achieve fundamental goals, we disable some means or narrow down the means to be monitored with the help of propositional logic formulae. Through case studies on insurance business, we illustrated our idea. When an information system is intended to be introduced into some activity such as business or amusements, we have to analyze the activity and define what kinds of func- tionalities and qualities including security needs are required for the system. In many cases, such a system or a part of it has already used in other activities. For example, we are using PC's, Web browsers, emails, social network services (SNS) and free software for our daily activities. Some business person cannot perform his/her business without own smart phones, and he/she also uses SNS for fun via the smart phones. We thus have to take such existing activities into account when an information system is intended to be introduced into an activity. Especially, we have to take them into account carefully for analyzing security requirements because not all existing activities are secure. In this paper, we propose a method for analyzing security requirements for an infor- mation system used in several different activities. When actors and their dependencies in each activity are specified, we can clarify the common parts among different activ- ities. Through such common parts, good or bad impacts are conveyed. We thus use i* notation (1,2) for specifying each activity. In i*, the needs of actors are represented as goals. Although some goals are fundamental for some actor and others are just subcon- tracts, there is no explicit distinction between fundamental goals and others in i*. We"
577761,15517,23684,Directed nowhere dense classes of graphs,2012,"Many natural computational problems on graphs such as finding dominating or independent sets of a certain size are well known to be intractable, both in the classical sense as well as in the framework of parameterized complexity. Much work therefore has focussed on exhibiting restricted classes of graphs on which these problems become tractable. While in the case of undirected graphs, there is a rich structure theory which can be used to develop tractable algorithms for these problems on large classes of undirected graphs, such a theory is much less developed for directed graphs.#R##N##R##N#Many attempts to identify structure properties of directed graphs tailored towards algorithmic applications have focussed on a directed analogue of undirected tree-width. These attempts have proved to be successful in the development of algorithms for linkage problems but none of the existing width-measures allow for tractable solutions to important problems such as dominating sets and many other related problems.#R##N##R##N#In this paper we take a radically different approach to identifying classes of directed graphs where domination and other problems become tractable. In particular, whereas most existing approaches treat the class of acyclic graphs as simple in their respective width measure, we will specifically study classes of digraphs which do not contain all acyclic digraphs. It is this new approach that make the algorithmic results reported herein possible.#R##N##R##N#More specifically, we introduce the concept of shallow directed minors and based on this a new classification of classes of directed graphs which is diametric to existing directed graph decompositions and directed width measures proposed in the literature.#R##N##R##N#We then study in depth one type of classes of directed graphs which we call nowhere crownful. The classes are very general as they include, on the one hand, all classes of directed graphs whose underlying undirected class is nowhere dense, such as planar, bounded-genus, and H-minor-free graphs; and on the other hand, also contain classes of high edge density whose underlying class is not nowhere dense. Yet we are able to show that problems such as directed dominating set and many others become fixed-parameter tractable on nowhere crownful classes of directed graphs. This is of particular interest as these problems are not tractable on any existing digraph measure for sparse classes.#R##N##R##N#The algorithmic results are established via proving a structural equivalence of nowhere crownful classes and classes of graphs which are directed uniformly quasi-wide. While this result is inspired by [Nesetril and Ossona de Mendez 2008], their proof method does not extend to the directed case and a different and much more involved proof is needed, turning it into a particularly significant part of our contribution."
787745,15517,20524,Proving MCAPI executions are correct using SMT,2013,"Asynchronous message passing is an important paradigm in writing applications for embedded heterogeneous multicore systems. The Multicore Association (MCA), an industry consortium promoting multicore technology, is working to standardize message passing into a single API, MCAPI, for bare metal implementation and portability across platforms. Correctness in such an API is difficult to reason about manually, and testing against reference solutions is equally difficult as reference solutions implement an unknown set of allowed behaviors, and programmers have no way to directly control API internals to expose or reproduce errors. This paper provides a way to encode an MCAPI execution as a Satisfiability Modulo Theories (SMT) problem, which if satisfiable, yields a feasible execution schedule on the same trace, such that it resolves non-determinism in the MCAPI runtime in a way that it now fails user provided assertions. The paper proves the problem is NP-complete. The encoding is useful for test, debug, and verification of MCAPI program execution. The novelty in the encoding is the direct use of match pairs (potential send and receive couplings). Match-pair encoding for MCAPI executions, when compared to other encoding strategies, is simpler to reason about, results in significantly fewer terms in the SMT problem, and captures feasible behaviors that are ignored in previously published techniques. Further, to our knowledge, this is the first SMT encoding that is able to run in infinite-buffer semantics, meaning the runtime has unlimited internal buffering as opposed to no internal buffering. Results demonstrate that the SMT encoding, restricted to zero-buffer semantics, uses fewer clauses when compared to another zero-buffer technique, and it runs faster and uses less memory. As a result the encoding scales well for programs with high levels of non-determinism in how sends and receives may potentially match."
2291344,15517,8385,Scalable and incremental software bug detection,2013,"An important, but often neglected, goal of static analysis for detecting bugs is the ability to show defects to the programmer quickly. Unfortunately, existing static analysis tools scale very poorly, or are shallow and cannot find complex interprocedural defects. Previous attempts at reducing the analysis time by adding more resources (CPU, memory) or by splitting the analysis into multiple sub-analyses based on defect detection capabilities resulted in limited/negligible improvements.     We present a technique for parallel and incremental static analysis using top-down, bottom-up and global specification inference based around the concept of a work unit, a self-contained atom of analysis input, that deterministically maps to its output. A work unit contains both abstract and concrete syntax to analyze, a supporting fragment of the class hierarchy, summarized interprocedural behavior, and defect reporting information, factored to ensure a high level of reuse when analyzing successive versions incrementally. Work units are created and consumed by an analysis master process that coordinates the multiple analysis passes, the flow of information among them, and incrementalizes the computation. Meanwhile, multiple analysis worker processes use abstract interpretation to compute work unit results. Process management and interprocess communication is done by a general-purpose computation distributor layer.     We have implemented our approach and our experimental results show that using eight processor cores, we can perform complete analysis of code bases with millions of lines of code in a few hours, and even faster after incremental changes to that code. The analysis is thorough and accurate: it usually reports about one crash-causing defect per thousand lines of code, with a false positive rate of 10--20%"
1651067,15517,23827,Using MOOCs to reinvigorate software engineering education (keynote),2014,"The spectacular failure of the Affordable Care Act website (Obamacare) has focused public attention on software engineering. Yet experienced practitioners mostly sighed and shrugged, because the historical record shows that only 10 % of large (> 10 M) software projects using conventional methodologies such as Waterfall are successful. In contrast, Amazon and others successfully build comparably large and complex sites with hundreds of integrated subsystems by using modern agile methods and service-oriented architecture.     This contrast is one reason Industry has complained that academia ignores vital software topics, leaving students unprepared upon graduation. In too many courses, well-meaning instructors teach traditional approaches to software development that are neither supported by tools that students can readily use, nor appropriate for projects whose scope matches a college course. Students respond by continuing to build software more or less the way they always have, which is boring for students, frustrating for instructors, and disappointing for industry.     This talk explains how the confluence of cloud computing, software as a service (SaaS), and Massive Open Online Courses (MOOCs) have not only revolutionized the future of software, but changed it in a way that makes it easier and more rewarding to teach. UC Berkeley's revised Software Engineering course allows students to both enhance a legacy application and to develop a new app that matches requirements of non-technical customers, using the same tools and techniques that professionals use. By experiencing the whole software lifecycle repeatedly within a single college course, students learn to use and appreciate the skills that industry has long encouraged. The course is now popular with students, rewarding for faculty, and praised by industry."
2193601,15517,23836,Communication Optimizations for Distributed-Memory X10 Programs,2011,"X10 is a new object-oriented PGAS (Partitioned Global Address Space) programming language with support for distributed asynchronous dynamic parallelism that goes beyond past SPMD message-passing models such as MPI and SPMD PGAS models such as UPC and Co-Array Fortran. The concurrency constructs in X10 make it possible to express complex computation and communication structures with higher productivity than other distributed-memory programming models. However, this productivity often comes at the cost of high performance overhead when the language is used in its full generality. This paper introduces high-level compiler optimizations and transformations to reduce communication and synchronization overheads in distributed-memory implementations of X10 programs. Specifically, we focus on locality optimizations such as scalar replacement and task localization, combined with supporting transformations such as loop distribution, scalar expansion, loop tiling, and loop splitting. We have completed a prototype implementation of these high-level optimizations, and performed a performance evaluation that shows significant improvements in performance, scalability, communication volume and number of tasks. We evaluated the communication optimizations on three platforms: a 128-node Blue Gene/P cluster, a 32-node Nehalem cluster, and a 16-node Power7 cluster. On the Blue Gene/P cluster, we observed a maximum performance improvement of 31.46x relative to the unoptimized case (for the MolDyn benchmark). On the Nehalem cluster, we observed a maximum performance improvement of 3.01x (for the NQueens benchmark) and on the Power7 cluster, we observed a maximum performance improvement of 2.73x (for the MolDyn benchmark). In addition, there was no case in which the optimized code was slower than the unoptimized case. We also believe that the optimizations presented in this paper will be necessary for any high-productivity PGAS language based on modern object-oriented principles, that is designed for execution on future Extreme Scale systems that place a high premium on locality improvement for performance and energy efficiency."
2153477,15517,11058,A security policy oracle: detecting security holes using multiple API implementations,2011,"Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct.   In this paper, we exploit the fact that many modern APIs have  multiple, independent  implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API  must  enforce the same policy, or at least one of them is wrong!   Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering deep bugs in them."
1869199,15517,8385,Proving programs robust,2011,"We present a program analysis for verifying quantitative robustness properties of programs, stated generally as: If the inputs of a program are perturbed by an arbitrary amount epsilon, then its outputs change at most by (K . epsilon), where K can depend on the size of the input but not its value. Robustness properties generalize the analytic notion of continuity---e.g., while the function e x  is continuous, it is not robust. Our problem is to verify the robustness of a function P that is coded as an imperative program, and can use diverse data types and features such as branches and loops.   Our approach to the problem soundly decomposes it into two subproblems: (a) verifying that the smallest possible perturbations to the inputs of P do not change the corresponding outputs significantly, even if control now flows along a different control path; and (b) verifying the robustness of the computation along each control-flow path of P. To solve the former subproblem, we build on an existing method for verifying that a program encodes a continuous function [5]. The latter is solved using a static analysis that bounds the magnitude of the slope of any function computed by a control flow path of P. The outcome is a sound program analysis for robustness that uses proof obligations which do not refer to epsilon-changes and can often be fully automated using off-the-shelf SMT-solvers.   We identify three application domains for our analysis. First, our analysis can be used to guarantee the predictable execution of embedded control software, whose inputs come from physical sources and can suffer from error and uncertainty. A guarantee of robustness ensures that the system does not react disproportionately to such uncertainty. Second, our analysis is directly applicable to approximate computation, and can be used to provide foundations for a recently-proposed program approximation scheme called {loop perforation}. A third application is in database privacy: proofs of robustness of queries are essential to  differential privacy , the most popular notion of privacy for statistical databases."
848226,15517,20524,Finding relevant answers in software forums,2011,"Online software forums provide a huge amount of valuable content. Developers and users often ask questions and receive answers from such forums. The availability of a vast amount of thread discussions in forums provides ample opportunities for knowledge acquisition and summarization. For a given search query, current search engines use traditional information retrieval approach to extract webpages containing relevant keywords. However, in software forums, often there are many threads containing similar keywords where each thread could contain a lot of posts as many as 1,000 or more. Manually finding relevant answers from these long threads is a painstaking task to the users. Finding relevant answers is particularly hard in software forums as: complexities of software systems cause a huge variety of issues often expressed in similar technical jargons, and software forum users are often expert internet users who often posts answers in multiple venues creating many duplicate posts, often without satisfying answers, in the world wide web. To address this problem, this paper provides a semantic search engine framework to process software threads and recover relevant answers according to user queries. Different from standard information retrieval engine, our framework infer semantic tags of posts in the software forum threads and utilize these tags to recover relevant answer posts. In our case study, we analyze 6,068 posts from three software forums. In terms of accuracy of our inferred tags, we could achieve on average an overall precision, recall and F-measure of 67%, 71%, and 69% respectively. To empirically study the benefit of our overall framework, we also conduct a user-assisted study which shows that as compared to a standard information retrieval approach, our proposed framework could increase mean average precision from 17% to 71% in retrieving relevant answers to various queries and achieve a Normalized Discounted Cumulative Gain (nDCG) @1 score of 91.2% and nDCG@2 score of 71.6%."
2154000,15517,23620,Gradual typing embedded securely in JavaScript,2014,"JavaScript's flexible semantics makes writing correct code hard and writing secure code extremely difficult. To address the former problem, various forms of gradual typing have been proposed, such as Closure and TypeScript. However, supporting all common programming idioms is not easy; for example, TypeScript deliberately gives up type soundness for programming convenience. In this paper, we propose a gradual type system and implementation techniques that provide important safety and security guarantees.   We present TS# , a gradual type system and source-to-source compiler for JavaScript. In contrast to prior gradual type systems, TS# features full runtime reflection over three kinds of types: (1) simple types for higher-order functions, recursive datatypes and dictionary-based extensible records; (2) the type any, for dynamically type-safe TS# expressions; and (3) the type un, for untrusted, potentially malicious JavaScript contexts in which TS# is embedded. After type-checking, the compiler instruments the program with various checks to ensure the type safety of TS# despite its interactions with arbitrary JavaScript contexts, which are free to use eval, stack walks, prototype customizations, and other offensive features. The proof of our main theorem employs a form of type-preserving compilation, wherein we prove all the runtime invariants of the translation of TS# to JavaScript by showing that translated programs are well-typed in JS# , a previously proposed dependently typed language for proving functional correctness of JavaScript programs.   We describe a prototype compiler, a secure runtime, and sample applications for TS#. Our examples illustrate how web security patterns that developers currently program in JavaScript (with much difficulty and still with dubious results) can instead be programmed naturally in TS#, retaining a flavor of idiomatic JavaScript, while providing strong safety guarantees by virtue of typing."
2209826,15517,11058,Type-directed automatic incrementalization,2012,"Application data often changes slowly or incrementally over time. Since incremental changes to input often result in only small changes in output, it is often feasible to respond to such changes asymptotically more efficiently than by re-running the whole computation. Traditionally, realizing such asymptotic efficiency improvements requires designing problem-specific algorithms known as dynamic or incremental algorithms, which are often significantly more complicated than conventional algorithms to design, analyze, implement, and use. A long-standing open problem is to develop techniques that automatically transform conventional programs so that they correctly and efficiently respond to incremental changes.   In this paper, we describe a significant step towards solving the problem of automatic incrementalization: a programming language and a compiler that can, given a few type annotations describing what can change over time, compile a conventional program that assumes its data to be static (unchanging over time) to an incremental program. Based on recent advances in self-adjusting computation, including a theoretical proposal for translating purely functional programs to self-adjusting programs, we develop techniques for translating conventional Standard ML programs to self-adjusting programs. By extending the Standard ML language, we design a fully featured programming language with higher-order features, a module system, and a powerful type system, and implement a compiler for this language. The resulting programming language, LML, enables translating conventional programs decorated with simple type annotations into incremental programs that can respond to changes in their data correctly and efficiently.   We evaluate the effectiveness of our approach by considering a range of benchmarks involving lists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes existing code with only trivial amounts of annotation. The resulting programs are often asymptotically more efficient, leading to orders of magnitude speedups in practice."
1101640,15517,11058,Parallelizing top-down interprocedural analyses,2012,"Modularity is a central theme in any scalable program analysis. The core idea in a modular analysis is to build summaries at procedure boundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. There are two ways to perform a modular program analysis: (1) top-down and (2) bottomup. A bottom-up analysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most general calling context and builds its summary. In contrast, a top-down analysis starts from the root of the call graph, and proceeds downward, analyzing each procedure in its calling context. Top-down analyses have several applications in verification and software model checking. However, traditionally, bottom-up analyses have been easier to scale and parallelize than top-down analyses.   In this paper, we propose a generic framework, BOLT, which uses MapReduce style parallelism to scale top-down analyses. In particular, we consider top-down analyses that are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural analysis happens in the context of a reachability query. A query  Q  over a procedure  P  results in query tree that consists of sub-queries over the procedures called by  P . The key insight in BOLT is that the query tree can be explored in parallel using MapReduce style parallelism -- the map stage can be used to run a set of enabled queries in parallel, and the reduce stage can be used to manage inter-dependencies between queries. Iterating the map and reduce stages alternately, we can exploit the parallelism inherent in top-down analyses. Another unique feature of BOLT is that it is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including may analyses, mustanalyses, and may-must-analyses can be parallelized using BOLT.   We have implemented the BOLT framework and instantiated the intraprocedural parameter with a may-must-analysis. We have run BOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our results demonstrate an average speedup of 3.71x and a maximum speedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential analysis fails, BOLT is able to successfully complete its analysis."
1819893,15517,23497,Verifying systems rules using rule-directed symbolic execution,2013,"Systems code must obey many rules, such as opened files must be closed. One approach to verifying rules is static analysis, but this technique cannot infer precise runtime effects of code, often emitting many false positives. An alternative is symbolic execution, a technique that verifies program paths over all inputs up to a bounded size. However, when applied to verify rules, existing symbolic execution systems often blindly explore many redundant program paths while missing relevant ones that may contain bugs.   Our key insight is that only a small portion of paths are relevant to rules, and the rest (majority) of paths are irrelevant and do not need to be verified. Based on this insight, we create WOODPECKER, a new symbolic execution system for effectively checking rules on systems programs. It provides a set of builtin checkers for common rules, and an interface for users to easily check new rules. It directs symbolic execution toward the program paths relevant to a checked rule, and soundly prunes redundant paths, exponentially speeding up symbolic execution. It is designed to be heuristic-agnostic, enabling users to leverage existing powerful search heuristics.   Evaluation on 136 systems programs totaling 545K lines of code, including some of the most widely used programs, shows that, with a time limit of typically just one hour for each verification run, WOODPECKER effectively verifies 28.7% of the program and rule combinations over bounded input, whereas an existing symbolic execution system KLEE verifies only 8.5%. For the remaining combinations, WOODPECKER verifies 4.6 times as many relevant paths as KLEE. With a longer time limit, WOODPECKER verifies much more paths than KLEE,  e.g.,  17 times as many with a fourhour limit. WOODPECKER detects 113 rule violations, including 10 serious data loss errors with 2 most serious ones already confirmed by the corresponding developers."
897729,15517,9856,Down to the bare metal: using processor features for binary analysis,2012,"A detailed understanding of the behavior of exploits and malicious software is necessary to obtain a comprehensive overview of vulnerabilities in operating systems or client applications, and to develop protection techniques and tools. To this end, a lot of research has been done in the last few years on binary analysis techniques to efficiently and precisely analyze code. Most of the common analysis frameworks are based on software emulators since such tools offer a fine-grained control over the execution of a given program. Naturally, this leads to an arms race where the attackers are constantly searching for new methods to detect such analysis frameworks in order to successfully evade analysis.   In this paper, we focus on two aspects. As a first contribution, we introduce several novel mechanisms by which an attacker can  delude  an emulator. In contrast to existing detection approaches that perform a dedicated test on the environment and combine the test with an  explicit  conditional branch, our detection mechanisms introduce code sequences that have an  implicitly  different behavior on a native machine when compared to an emulator. Such differences in behavior are caused by the side-effects of the particular operations and imperfections in the emulation process that cannot be mitigated easily. Motivated by these findings, we introduce a novel approach to generate execution traces. We propose to utilize the processor itself to generate such traces. Mores precisely, we propose to use a hardware feature called  branch tracing  available on commodity x86 processors in which the log of all branches taken during code execution is generated directly by the processor. Effectively, the logging is thus performed at the lowest level possible. We evaluate the practical viability of this approach."
2141795,15517,8385,Asynchronous programs with prioritized task-buffers,2012,"We consider the algorithmic analysis of asynchronous software systems as a means for building reliable software. A key challenge in designing such analyses is identifying a concurrency model which does not extraneously introduce behaviors infeasible in the actual system, does not extraneously exclude actual behaviors, and isolates the challenging features for analyses to focus on.   Guided by real-world asynchronous software, we propose a concurrency model which enriches the existing serial task-buffer asynchrony model [29] with task-priorities and multiple task-buffers. Our model allows non-serial execution: tasks with higher priority preempt lower-priority tasks, and tasks drawn from distinct buffers freely interleave with one another. Modeling these features allows analysis algorithms to detect otherwise uncaught programming errors in asynchronous programs due to inter-buffer interleaving and task-interruption, while correctly ignoring false errors due to infeasible out-of-priority-order executions.   Besides more precisely capturing real-world systems, our concurrency model inspires the design of a novel analysis algorithm. Given parameters  K  1 ,  K  2  e N that restrict inter-buffer task interleaving and intra-buffer task reordering, we give a code-to-code translation to sequential programs, which can then be analyzed by off-the-shelf program analysis tools. For any given parameter values, the resulting sequential program encodes a subset of possible behaviors, and in the limit as both parameters approach infinity, the sequential program encodes all behaviors. We demonstrate the viability of our technique by experimenting with a prototype implementation. Our prototype is competitive with state-of-the-art concurrent program verification tools, and is able to correctly identify errors in simplified Windows device driver code, while ignoring infeasible executions."
1681876,15517,9766,Simulation of Built-in PHP Features for Precise Static Code Analysis,2014,"The World Wide Web grew rapidly during the last decades and is used by millions of people every day for online shopping, banking, networking, and other activities. Many of these websites are developed with PHP, the most popular scripting language on the Web. However, PHP code is prone to different types of critical security vulnerabilities that can lead to data leakage, server compromise, or attacks against an application's users. This problem can be addressed by analyzing the source code of the application for security vulnerabilities before the application is deployed on a web server. In this paper, we present a novel approach for the precise static analysis of PHP code to detect security vulnerabilities in web applications. As dismissed by previous work in this area, a comprehensive configuration and simulation of over 900 PHP built-in features allows us to precisely model the highly dynamic PHP language. By performing an intra- and inter-procedural data flow analysis and by creating block and function summaries, we are able to efficiently perform a backward-directed taint analysis for 20 different types of vulnerabilities. Furthermore, string analysis enables us to validate sanitization in a context-sensitive manner. Our method is the first to perform fine-grained analysis of the interaction between different types of sanitization, encoding, sources, sinks, markup contexts, and PHP settings. We implemented a prototype of our approach in a tool called RIPS. Our evaluation shows that RIPS is capable of finding severe vulnerabilities in popular real-world applications: we reported 73 previously unknown vulnerabilities in five well-known PHP applications such as phpBB, osCommerce, and the conference management software HotCRP."
1960820,15517,11058,Automatic compilation of MATLAB programs for synergistic execution on heterogeneous processors,2011,"MATLAB is an array language, initially popular for rapid prototyping, but is now being increasingly used to develop production code for numerical and scientific applications. Typical MATLAB programs have abundant data parallelism. These programs also have control flow dominated scalar regions that have an impact on the program's execution time. Today's computer systems have tremendous computing power in the form of traditional CPU cores and throughput oriented accelerators such as graphics processing units(GPUs). Thus, an approach that maps the control flow dominated regions to the CPU and the data parallel regions to the GPU can significantly improve program performance.   In this paper, we present the design and implementation of MEGHA, a compiler that automatically compiles MATLAB programs to enable synergistic execution on heterogeneous processors. Our solution is fully automated and does not require programmer input for identifying data parallel regions. We propose a set of compiler optimizations tailored for MATLAB. Our compiler identifies data parallel regions of the program and composes them into kernels. The problem of combining statements into kernels is formulated as a constrained graph clustering problem. Heuristics are presented to map identified kernels to either the CPU or GPU so that kernel execution on the CPU and the GPU happens synergistically and the amount of data transfer needed is minimized. In order to ensure required data movement for dependencies across basic blocks, we propose a data flow analysis and edge splitting strategy. Thus our compiler automatically handles composition of kernels, mapping of kernels to CPU and GPU, scheduling and insertion of required data transfer. The proposed compiler was implemented and experimental evaluation using a set of MATLAB benchmarks shows that our approach achieves a geometric mean speedup of 19.8X for data parallel benchmarks over native execution of MATLAB."
2072420,15517,11058,Test-driven repair of data races in structured parallel programs,2014,"A common workflow for developing parallel software is as follows: 1) start with a sequential program, 2) identify subcomputations that should be converted to parallel tasks, 3) insert synchronization to achieve the same semantics as the sequential program, and repeat steps 2) and 3) as needed to improve performance. Though this is not the only approach to developing parallel software, it is sufficiently common to warrant special attention as parallel programming becomes ubiquitous. This paper focuses on automating step 3), which is usually the hardest step for developers who lack expertise in parallel programming.   Past solutions to the problem of repairing parallel programs have used static-only or dynamic-only approaches, both of which incur significant limitations in practice. Static approaches can guarantee soundness in many cases but are limited in precision when analyzing medium or large-scale software with accesses to pointer-based data structures in multiple procedures. Dynamic approaches are more precise, but their proposed repairs are limited to a single input and are not reflected back in the original source program. In this paper, we introduce a hybrid static+dynamic test-driven approach to repairing data races in structured parallel programs. Our approach includes a novel coupling between static and dynamic analyses. First, we execute the program on a concrete test input and determine the set of data races for this input dynamically. Next, we compute a set of finish placements that prevent these races and also respects the static scoping rules of the program while maximizing parallelism. Empirical results on standard benchmarks and student homework submissions from a parallel computing course establish the effectiveness of our approach with respect to compile-time overhead, precision, and performance of the repaired code."
378424,15517,23827,Engineering and Software Engineering,2011,"The phrase 'software engineering' has many meanings. One central meaning is the reliable development of dependable computer-based systems, especially those for critical applications. This is not a solved problem. Failures in software development have played a large part in many fatalities and in huge economic losses. While some of these failures may be attributable to programming errors in the narrowest sense—a program's failure to satisfy a given formal specification—there is good reason to think that most of them have other roots. These roots are located in the problem of software engineering rather than in the problem of program correctness. The famous 1968 conference was motivated by the belief that software development should be based on the types of theoretical foundations and practical disciplines that are traditional in the established branches of engineering. Yet after forty years of currency the phrase 'software engineering' still denotes no more than a vague and largely unfulfilled aspiration. Two major causes of this disappointment are immediately clear. First, too many areas of software development are inadequately specialised, and consequently have not developed the repertoires of normal designs that are the indispensable basis of reliable engineering success. Second, the relationship between structural design and formal analytical techniques for software has rarely been one of fruitful synergy: too often it has defined a boundary between competing dogmas, at which mutual distrust and incomprehension deprive both sides of advantages that should be within their grasp. This paper discusses these causes and their effects. Whether the common practice of software development will eventually satisfy the broad aspiration of 1968 is hard to predict; but an understanding of past failure is surely a prerequisite of future success."
2451047,15517,10973,Model Checking for Successor-Invariant First-Order Logic on Minor-Closed Graph Classes,2013,"Model checking problems for first- and monadic second-order logic on graphs have received considerable attention in the past, not the least due to their connections to problems in algorithmic graph structure theory. While the model checking problem for these logics on general graphs is computationally intractable, it becomes tractable on important classes of graphs such as those of bounded tree-width, planar graphs or more generally, classes of graphs excluding a fixed minor. It is well known that allowing an order relation or successor function can greatly increase the expressive power of the respective logics. This remains true even in cases where we require the formulas to be order- or successor-invariant, that is, while they can use an order relation, their truth in a given graph must not depend on the particular ordering or successor function chosen. Naturally, the question arises whether this increase in expressive power comes at a cost in terms of tractability on specific classes of graphs. In LICS 2012, Engel Mann et al. studied this problem and showed that order-invariant monadic second-order logic (MSO) remains tractable on the same classes of graphs than MSO without an ordering. That is, adding order-invariance to MSO essentially comes at no extra cost in terms of model checking complexity. For successor-invariant first-order logic something similar should be true. However, they only managed to show that successor-invariant first-order logic is tractable on the class of planar graphs which is very far from the best tractability results currently known for first-order logic. In this paper we significantly improve the latter result and show that successor-invariant first-order logic is tractable on any class of graphs excluding a fixed minor. This is much closer to the best results known for FO without an ordering. The proof relies on the construction of k-walks in suitable super graphs of the input graphs, i.e., walks which visit every vertex at least once and at most k times, for some k depending on the excluded minor H. The super graphs may in general contain H minors, but they still exclude some possible larger minorH0, so by results of Flum and Grohe [20] model checking on these graphs is still fixed-parameter tractable."
2528108,15517,23827,Ten years of automated code analysis at Microsoft (invited industrial talk),2012,"Automated code analysis is technology aimed at locating, describing and repairing areas of weakness in code. Code weaknesses range from security vulnerabilities, logic errors, concurrency violations, to improper resource usage, violations of architectures or coding guidelines. Common to all code analysis techniques is that they build abstractions of code and then check those abstractions for properties of interest. For instance a type checker computes how types are used, abstract interpreters and symbolic evaluators check how values flow, model checkers analyze how state evolves. Building modern program analysis tools thus requires a multi-pronged approach to find a variety of weaknesses. In this talk I will discuss and compare several program analysis tools, which MSR build during the last ten years. They include theorem provers, program verifiers, bug finders, malware scanners, and test case generators. I will describe the need for their development, their innovation, and application. Many of these tools had considerable impact on Microsoft's development practices, as well as on the research community. Some of them are being shipped in products such as the Static Driver Verifier or as part of Visual Studio. Performing program analysis as part of quality assurance is meanwhile standard practice in many software development companies. However several challenges have not yet been resolved. Thus, I will conclude with a set of open challenges in program analysis which hopefully triggers new aspiring directions in our joint quest of delivering predictable software that is free from defect and vulnerabilities."
1405440,15517,9704,An adaptive genetic programming approach to QoS-aware web services composition,2013,"Web services are software entities that can be deployed, discovered and invoked in the distributed environment of the Internet through a set of standards such as Simple Object Access Protocol (SOAP), Web Services Description Language (WSDL) and Universal Description, Discovery and Integration (UDDI). However, atomic web service can only provide simple functionality. A range of web services are required to be incorporated into one composite service in order to offer value-added and complicated functionality when no existing web service can be found to satisfy users' request. In service-oriented architecture (SOA), web services composition has become an efficient solution to support business-to-business and enterprise application integration (EAI). In addition to functional properties (i.e., inputs and outputs), web services have non-functional properties called quality of service (QoS) that encompasses a number of parameters such as execution cost, response time and availability. Nowadays with the rapid increase in the number of available web services, a great number of services provide overlapping or identical functionality but vary in QoS attribute values. Due to the huge search space of the composition problem, a genetic programming (GP) approach is proposed in this paper, which aims to produce the desired outputs based on available inputs, as well as ensure that the composite service has the optimal QoS value. Furthermore, an adaptive method is applied to the standard form of GP in order to avoid low rate of convergence and premature convergence. A series of experiments have been conducted to evaluate the proposed approach, and the results show that the adaptive genetic programming approach (AGP) has a good performance in finding a valid solution within low search time and is superior to the traditional approaches"
1730039,15517,11058,Surgical precision JIT compilers,2014,"Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable.   In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables smart libraries to supply domain-specific compiler optimizations or safety checks.   We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction.   In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode."
2503027,15517,9772,Silverline: toward data confidentiality in storage-intensive cloud applications,2011,"By offering high availability and elastic access to resources, third-party cloud infrastructures such as Amazon EC2 are revolutionizing the way today's businesses operate. Unfortunately, taking advantage of their benefits requires businesses to accept a number of serious risks to data security. Factors such as software bugs, operator errors and external attacks can all compromise the confidentiality of sensitive application data on external clouds, by making them vulnerable to unauthorized access by malicious parties.   In this paper, we study and seek to improve the confidentiality of application data stored on third-party computing clouds. We propose to identify and encrypt all  functionally encryptable  data, sensitive data that can be encrypted without limiting the functionality of the application on the cloud. Such data would be stored on the cloud only in an encrypted form, accessible only to users with the correct keys, thus protecting its confidentiality against unintentional errors and attacks alike. We describe  Silverline , a set of tools that automatically 1) identify all functionally encryptable data in a cloud application, 2) assign encryption keys to specific data subsets to minimize key management complexity while ensuring robustness to key compromise, and 3) provide transparent data access at the user device while preventing key compromise even from malicious clouds. Through experiments with real applications, we find that many web applications are dominated by  storage and data sharing  components that do not require interpreting raw data. Thus, Silverline can protect the vast majority of data on these applications, simplify key management, and protect against key compromise. Together, our techniques provide a substantial first step towards simplifying the complex process of incorporating data confidentiality into these storage-intensive cloud applications."
2556595,15517,20754,Verified Security for Browser Extensions,2011,"Popup blocking, form filling, and many other features of modern web browsers were first introduced as third-party extensions. New extensions continue to enrich browsers in unanticipated ways. However, powerful extensions require capabilities, such as cross-domain network access and local storage, which, if used improperly, pose a security risk. Several browsers try to limit extension capabilities, but an empirical survey we conducted shows that many extensions are over-privileged under existing mechanisms. This paper presents \ibex, a new framework for authoring, analyzing, verifying, and deploying secure browser extensions. Our approach is based on using type-safe, high-level languages to program extensions against an API providing access to a variety of browser features. We propose using Data log to specify fine-grained access control and dataflow policies to limit the ways in which an extension can use this API, thus restricting its privilege over security-sensitive web content and browser resources. We formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance. Additionally, we provide visualization tools to assist with policy analysis, and compilers to translate extension source code to either. NET byte code or JavaScript, facilitating cross-browser deployment of extensions. We evaluate our work by implementing and verifying~\NumExt extensions with a diverse set of features and security policies. We deploy our extensions in Internet Explorer, Chrome, Fire fox, and a new experimental HTML5 platform called C3. In so doing, we demonstrate the versatility and effectiveness of our approach."
2413569,15517,23827,Overcoming the challenges in cost estimation for distributed software projects,2012,"We describe how we studied, in-situ, the operational processes of three large high process maturity distributed software development companies and discovered three common problems they faced with respect to early stage project cost estimation. We found that project managers faced significant challenges to accurately estimate project costs because the standard metrics-based estimation tools they used (a) did not effectively incorporate diverse distributed project configurations and characteristics, (b) required comprehensive data that was not fully available for all starting projects, and (c) required significant domain experience to derive accurate estimates. To address these challenges, we collaborated with practitioners at the three firms and developed a new learning-oriented and semi-automated early-stage cost estimation solution that was specifically designed for globally distributed software projects. The key idea of our solution was to augment the existing metrics-driven estimation methods with a case repository that stratified past incidents related to project effort estimation issues from the historical project databases at the firms into several generalizable categories. This repository allowed project managers to quickly and effectively “benchmark” their new projects to all past projects across the firms, and thereby learn from them. We deployed our solution at each of our three research sites for real-world field-testing over a period of six months. Project managers of 219 new large globally distributed projects used both our method to estimate the cost of their projects as well as the established metrics-based estimation approaches they were used to. Our approach achieved significantly reduced estimation errors (of up to 60%). This resulted in more than 20% net cost savings, on average, per project — a massive total cost savings across all projects at the three firms!"
1396320,15517,8912,Manipulating semantic values in kernel data structures: Attack assessments and implications,2013,"Semantic values in kernel data structures are critical to many security applications, such as virtual machine introspection, malware analysis, and memory forensics. However, malware, or more specifically a kernel rootkit, can often directly tamper with the raw kernel data structures, known as DKOM (Direct Kernel Object Manipulation) attacks, thereby significantly thwarting security analysis. In addition to manipulating pointer fields to hide certain kernel objects, DKOM attacks may also mutate semantic values, which are data values with important semantic meanings. Prior research efforts have been made to defeat pointer manipulation attacks and thus identify hidden kernel objects. However, the space and severity of Semantic Value Manipulation (SVM) attacks have not received sufficient understanding. In this paper, we take a first step to systematically assess this attack space. To this end, we devise a new fuzz testing technique, namely - duplicate-value directed semantic field fuzzing, and implement a prototype called MOSS. Using MOSS, we evaluate two widely used operating systems: Windows XP and Ubuntu 10.04. Our experimental results show that the space of SVM attacks is vast for both OSes. Our proof-of-concept kernel rootkit further demonstrates that it can successfully evade all the security tools tested in our experiments, including recently proposed robust signature schemes. Moreover, our duplicate value analysis implies the challenges in defeating SVM attacks, such as an intuitive cross checking approach on duplicate values can only provide marginal detection improvement. Our study motivates revisiting of existing security solutions and calls for more effective defense against kernel threats."
1119593,15517,23620,An abstract interpretation framework for termination,2012,"Proof, verification and analysis methods for termination all rely on two induction principles: (1) a variant function or induction on data ensuring progress towards the end and (2) some form of induction on the program structure. The abstract interpretation design principle is first illustrated for the design of new forward and backward proof, verification and analysis methods for safety. The safety collecting semantics defining the strongest safety property of programs is first expressed in a constructive fixpoint form. Safety proof and checking/verification methods then immediately follow by fixpoint induction. Static analysis of abstract safety properties such as invariance are constructively designed by fixpoint abstraction (or approximation) to (automatically) infer safety properties. So far, no such clear design principle did exist for termination so that the existing approaches are scattered and largely not comparable with each other.   For (1), we show that this design principle applies equally well to potential and definite termination. The trace-based termination collecting semantics is given a fixpoint definition. Its abstraction yields a fixpoint definition of the best variant function. By further abstraction of this best variant function, we derive the Floyd/Turing termination proof method as well as new static analysis methods to effectively compute approximations of this best variant function.   For (2), we introduce a generalization of the syntactic notion of struc- tural induction (as found in Hoare logic) into a semantic structural induction based on the new semantic concept of inductive trace cover covering execution traces by segments, a new basis for formulating program properties. Its abstractions allow for generalized recursive proof, verification and static analysis methods by induction on both program structure, control, and data. Examples of particular instances include Floyd's handling of loop cutpoints as well as nested loops, Burstall's intermittent assertion total correctness proof method, and Podelski-Rybalchenko transition invariants."
2026888,15517,11058,kb -anonymity: a model for anonymized behaviour-preserving test and debugging data,2011,"It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.   For the purpose of releasing private data for testing and debugging, this paper proposes the  kb -anonymity model, which combines the  k -anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like  k -anonymity,  kb -anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike  k -anonymity,  kb -anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype."
687578,15517,20592,Fast and precise sanitizer analysis with BEK,2011,"Web applications often use special string-manipulating sanitizers on untrusted user data, but it is difficult to reason manually about the behavior of these functions, leading to errors. For example, the Internet Explorer cross-site scripting filter turned out to transform some web pages without JavaScript into web pages with valid Java-Script, enabling attacks. In other cases, sanitizers may fail to commute, rendering one order of application safe and the other dangerous.#R##N##R##N#BEK is a language and system for writing sanitizers that enables precise analysis of sanitizer behavior, including checking idempotence, commutativity, and equivalence. For example, BEK can determine if a target string, such as an entry on the XSS Cheat Sheet, is a valid output of a sanitizer. If so, our analysis synthesizes an input string that yields that target. Our language is expressive enough to capture real web sanitizers used in ASP.NET, the Internet Explorer XSS Filter, and the Google AutoEscape framework, which we demonstrate by porting these sanitizers to BEK.#R##N##R##N#Our analyses use a novel symbolic finite automata representation to leverage fast satisfiability modulo theories (SMT) solvers and are quick in practice, taking fewer than two seconds to check the commutativity of the entire set of Internet Exporer XSS filters, between 36 and 39 seconds to check implementations of HTMLEncode against target strings from the XSS Cheat Sheet, and less than ten seconds to check equivalence between all pairs of a set of implementations of HTMLEncode. Programs written in BEK can be compiled to traditional languages such as JavaScript and C#, making it possible for web developers to write sanitizers supported by deep analysis, yet deploy the analyzed code directly to real applications."
114123,15517,23827,Rapid Development of Executable Ontology for Financial Instruments and Trading Strategies,2011,"Abstract. In this paper we employ Rapid Ontology Development approach (ROD) with constant evaluation of steps in the process of ontology construction for development of Financial Instruments and Trading Strategies (FITS) ontology. We show that ontology development process does not conclude with successful definition of schematic part of ontology but we continue with post development activities where additional axiomatic information and instances with dynamic imports from various sources are defined. The result is executable ontology as part of Semantic Web application that uses data from several semi structured sources. The overall process of construction is suitable for users without extensive technical and programming skills and those users are rather experts in the problem domain. Keywords: ontology, semantic web, financial instruments, trading strategies, rapid ontology development. 1 Introduction Semantic Web technologies are being adopted less than expected and are mainly limited to academic environment, while we are still waiting for greater adoption in industry. The reasons for this situation can be found in technologies itself and also in the development process, because existence of verified approaches is a good indicator of maturity. There are various technologies available that consider different aspects of Semantic Web, from languages for capturing the knowledge, persisting data, inferring new knowledge to querying for knowledge etc. Regarding the development process, there is also a great variety of methodologies for ontology development, as it will be further discussed in section 2, but simplicity of using approaches for ontology construction is another issue. Current approaches in ontology development are technically very demanding and require long learning curve and are therefore inappropriate for developers with little technical skills and knowledge. Besides simplification of the development process ontology completeness is also a very important aspect. In building ontology, majority of approaches focus on defining common understanding of a problem domain as a schematic model of the problem and conclude the development after few successful iterations. Post development"
2312601,15517,8868,Finding your way in the testing jungle: a learning approach to web security testing,2013,"Black-box security testing of web applications is a hard problem. The main complication lies in the black-box assumption: The testing tool has limited insight into the workings of server-side defenses. This has traditionally led commercial as well as research vulnerability scanners toward heuristic approaches, such as testing each input point (e.g. HTTP parameter) with a short, predefined list of effective test payloads to balance between coverage and performance.     We take a fresh approach to the problem of security testing, casting it into a learning setting. In our approach, the testing algorithm has available a comprehensive database of test payloads, such that if the web application's defenses are broken, then with near certainty one of the candidate payloads is able to demonstrate the vulnerability. The question then becomes how to efficiently search through the payload space to find a good candidate. In our solution, the learning algorithm infers from a failed test---by analyzing the website's response---which other payloads are also likely to fail, thereby pruning substantial portions of the search space.     We have realized our approach in XSS Analyzer, an industry-level cross-site scripting (XSS) scanner featuring 500,000,000 test payloads. Our evaluation on 15,552 benchmarks shows solid results: XSS Analyzer achieves >99% coverage relative to brute-force traversal over all payloads, while trying only 10 payloads on average per input point. XSS Analyzer also outperforms several competing algorithms, including a mature commercial algorithm---featured in IBM Security AppScan Standard V8.5---by a far margin. XSS Analyzer has recently been integrated into the latest version of AppScan (V8.6) instead of that algorithm."
1169138,15517,23827,State-based monitoring and goal-driven project steering: field study of the SEMAT essence framework,2014,"At Carnegie Mellon University in Silicon Valley, the graduate master program ends with a practicum project during which students serve as software engineering consultants for an industry client. In this context, students are challenged to demonstrate their ability to work on self-managing and self-organizing teams. This paper presents a field study of the Software Engineering Method and Theory (SEMAT) Essence framework. The objective is to evaluate the effectiveness of the Essence’s novel state-based monitoring and goal-driven steering approach provided by the Essence kernel alphas and their states. The researchers conducted the study on seven graduate master student teams applying the approach throughout their practicum projects. The research methodology involves weekly observation and recording of each team’s state progression and collecting students’ reflection on the application of the approach. The main result validates that the approach provides student teams with a holistic, lightweight, non-prescriptive and method-agnostic way to monitor progress and steer projects, as well as an effective structure for team reflection and risk management. The paper also validates that the Essence kernel provides an effective mechanism for monitoring and steering work common to most student software projects. This includes the work done during project initiation as well as the work done at the project or release level. Support for technical work should come from additional practices added on top of the kernel, or by extending or altering the kernel definition. The conclusion is that the approach enables students to learn to steer projects effectively by addressing the various dimensions of software engineering. Hence the approach could be leveraged in software engineering education."
1064108,15517,517,System Level Combinatorial Testing in Practice -- The Concurrent Maintenance Case Study,2014,"Combinatorial test design (CTD) is an effective test design technique that reveals faults resulting from parameter interactions in a system. CTD requires a test space definition in the form of a set of parameters, their respective values, and restrictions on the value combinations. Though CTD is considered an industry best practice, there is only a small body of work on the practical application of CTD to industrial systems, and some key elements of the CTD application process are under-explored. Specifically, little consideration has been given to the process for identifying the parameters of the test space and their interrelations, how to validate the test space definition remains an open question, the application of CTD in reported work concentrates mostly on function or interface level testing and is hardly expanded to other levels of testing, and there is a significant lack of evaluation of the degree to which CTD helped improve the quality of the system under test. In this work, we analyze the continuous application of CTD in system test of two large industrial systems: IBM® POWER7® and IBM® System z®. For POWER7, CTD was used to design test cases for server concurrent maintenance. The application of CTD was in direct response to inconsistent reliability of those features on the prior POWER6® servers, and resulted in noteworthy quality improvements on POWER7. Success with POWER7 led to application of the methods to System z Enhanced Driver Maintenance testing, also featured in this work. To the best of our knowledge, this is the first comprehensive analysis of CTD usage in system test of industrial products, and the first analysis of long term use of CTD. We describe the methodology that we followed to define the combinatorial test space, while answering some unique challenges rising from the use of CTD to design functional test cases at system level rather than at interface or function level. We also describe our methodology for evaluating the test space definition and continuously improving it over time. In addition, we describe advanced CTD features that we found helpful for achieving an effective yet affordable test plan. Finally, we quantitatively and qualitatively evaluate the overall effectiveness of CTD usage, and show that it resulted in significantly improved server concurrent maintenance features."
1821968,15517,11058,"Natural proofs for structure, data, and separation",2013,"We propose natural proofs for reasoning with programs that manipulate data-structures against specifications that describe the structure of the heap, the data stored within it, and separation and framing of sub-structures. Natural proofs are a subclass of proofs that are amenable to completely automated reasoning, that provide sound but incomplete procedures, and that capture common reasoning tactics in program verification. We develop a dialect of separation logic over heaps, called Dryad, with recursive definitions that avoids explicit quantification. We develop ways to reason with heaplets using classical logic over the theory of sets, and develop natural proofs for reasoning using proof tactics involving disciplined unfoldings and formula abstractions. Natural proofs are encoded into decidable theories of first-order logic so as to be discharged using SMT solvers.   We also implement the technique and show that a large class of more than 100 correct programs that manipulate data-structures are amenable to full functional correctness using the proposed natural proof method. These programs are drawn from a variety of sources including standard data-structures, the Schorr-Waite algorithm for garbage collection, a large number of low-level C routines from the Glib library and OpenBSD library, the Linux kernel, and routines from a secure verified OS-browser project. Our work is the first that we know of that can handle such a wide range of full functional verification properties of heaps automatically, given pre/post and loop invariant annotations. We believe that this work paves the way for deductive verification technology to be used by programmers who do not (and need not) understand the internals of the underlying logic solvers, significantly increasing their applicability in building reliable systems."
1786400,15517,122,Resilient X10: efficient failure-aware programming,2014,"Scale-out programs run on multiple processes in a cluster. In scale-out systems, processes can fail. Computations using traditional libraries such as MPI fail when any component process fails. The advent of Map Reduce, Resilient Data Sets and MillWheel has shown dramatic improvements in productivity are possible when a high-level programming framework handles scale-out and resilience automatically.   We are concerned with the development of general-purpose languages that support resilient programming. In this paper we show how the X10 language and implementation can be extended to support resilience. In Resilient X10, places may fail asynchronously, causing loss of the data and tasks at the failed place. Failure is exposed through exceptions. We identify a {\em Happens Before Invariance Principle} and require the runtime to automatically repair the global control structure of the program to maintain this principle. We show this reduces much of the burden of resilient programming. The programmer is only responsible for continuing execution with fewer computational resources and the loss of part of the heap, and can do so while taking advantage of domain knowledge.   We build a complete implementation of the language, capable of executing benchmark applications on hundreds of nodes. We describe the algorithms required to make the language runtime resilient. We then give three applications, each with a different approach to fault tolerance (replay, decimation, and domain-level checkpointing). These can be executed at scale and survive node failure. We show that for these programs the overhead of resilience is a small fraction of overall runtime by comparing to equivalent non-resilient X10 programs. On one program we show end-to-end performance of Resilient X10 is ~100x faster than Hadoop."
2052707,15517,8385,Regression tests to expose change interaction errors,2013,"Changes often introduce program errors, and hence recent software testing literature has focused on generating tests which stress changes. In this paper, we argue that changes cannot be treated as isolated program artifacts which are stressed via testing. Instead, it is the complex dependency across multiple changes which introduce subtle errors. Furthermore, the complex dependence structures, that need to be exercised to expose such errors, ensure that they remain undiscovered even in well tested and deployed software. We motivate our work based on empirical evidence from a well tested and stable project - Linux GNU Coreutils - where we found that one third of the regressions take more than two (2) years to be fixed, and that two thirds of such long-standing regressions are introduced due to change interactions for the utilities we investigated.     To combat change interaction errors, we first define a notion of change interaction where several program changes are found to affect the result of a program statement via program dependencies. Based on this notion, we propose a change sequence graph (CSG) to summarize the control-flow and dependencies across changes. The CSG is then used as a guide during program path exploration via symbolic execution - thereby efficiently producing test cases which witness change interaction errors. Our experimental infra-structure was deployed on various utilities of GNU Coreutils, which have been distributed with Linux for almost twenty years. Apart from finding five (5) previously unknown errors in the utilities, we found that only one in five generated test cases exercises a sequence that is critical to exposing a change-interaction error, while being an order of magnitude more likely to expose an error. On the other hand, stressing changes in isolation only exposed half of the change interaction errors. These results demonstrate the importance and difficulty of change dependence-aware regression testing."
1339825,15517,8868,Using binary decision diagrams for combinatorial test design,2011,"Combinatorial test design (CTD) is an effective test planning technique that reveals faulty feature interaction in a given system. The test space is modeled by a set of parameters, their respective values, and restrictions on the value combinations. A subset of the test space is then automatically constructed so that it covers all valid value combinations of every  t  parameters, where  t  is a user input. Various combinatorial testing tools exist, implementing different approaches to finding a set of tests that satisfies  t -wise coverage. However, little consideration has been given to the process of defining the test space for CTD, which is usually a manual, labor-intensive, and error-prone effort. Potential errors include missing parameters and their values, wrong identification of parameters and of valid value combinations, and errors in the definition of restrictions that cause them not to capture the intended combinations. From our experience, lack of support for the test space definition process is one of the main obstacles in applying CTD to a wide range of testing domains.   In this work, we present a Cartesian product based methodology and technology that assist in defining a complete and consistent test space for CTD. We then show how using binary decision diagrams (BDDs) to represent and build the test space dramatically increases the scalability of our approach, making it applicable to large and complex real-life test design tasks, for which explicit representation of the test space is infeasible.   Finally, we show how BDDs can be used also to solve the CTD problem itself. We present a new and highly effective BDD-based approach for solving CTD, which finds a set of tests that satisfies  t -wise coverage by subset selection. Our approach supports also advanced requirements such as requirements on the distribution of values in the selected tests. We apply our algorithm to real-life testing problems of varying complexity, and show its superior performance."
1833350,15517,8422,"Upper bounds for newton's method on monotone polynomial systems, and p-time model checking of probabilistic one-counter automata",2013,"A central computational problem for analyzing and model checking various classes of infinite-state recursive probabilistic systems (including quasi-birth-death processes, multi-type branching processes, stochastic context-free grammars, probabilistic pushdown automata and recursive Markov chains) is the computation of termination probabilities, and computing these probabilities in turn boils down to computing the least fixed point (LFP) solution of a corresponding monotone polynomial system (MPS) of equations, denoted x=P(x).#R##N##R##N#It was shown by Etessami and Yannakakis [11] that a decomposed variant of Newton's method converges monotonically to the LFP solution for any MPS that has a non-negative solution. Subsequently, Esparza, Kiefer, and Luttenberger [7] obtained upper bounds on the convergence rate of Newton's method for certain classes of MPSs. More recently, better upper bounds have been obtained for special classes of MPSs ([10, 9]).#R##N##R##N#However, prior to this paper, for arbitrary (not necessarily strongly-connected) MPSs, no upper bounds at all were known on the convergence rate of Newton's method as a function of the encoding size |P| of the input MPS, x=P(x).#R##N##R##N#In this paper we provide worst-case upper bounds, as a function of both the input encoding size |P|, and e>0, on the number of iterations required for decomposed Newton's method (even with rounding) to converge to within additive error e>0 of q*, for an arbitrary MPS with LFP solution q*. Our upper bounds are essentially optimal in terms of several important parameters of the problem.#R##N##R##N#Using our upper bounds, and building on prior work, we obtain the first P-time algorithm (in the standard Turing model of computation) for quantitative model checking, to within arbitrary desired precision, of discrete-time QBDs and (equivalently) probabilistic 1-counter automata, with respect to any (fixed) ω-regular or LTL property."
698936,15517,11058,End-to-end verification of stack-space bounds for C programs,2014,"Verified compilers guarantee the preservation of semantic properties and thus enable formal verification of programs at the source level. However, important quantitative properties such as memory and time usage still have to be verified at the machine level where interactive proofs tend to be more tedious and automation is more challenging.   This article describes a framework that enables the formal verification of stack-space bounds of compiled machine code at the C level. It consists of a verified CompCert-based compiler that preserves quantitative properties, a verified quantitative program logic for interactive stack-bound development, and a verified stack analyzer that automatically derives stack bounds during compilation.   The framework is based on event traces that record function calls and returns. The source language is CompCert Clight and the target language is x86 assembly. The compiler is implemented in the Coq Proof Assistant and it is proved that crucial properties of event traces are preserved during compilation. A novel quantitative Hoare logic is developed to verify stack-space bounds at the CompCert Clight level. The quantitative logic is implemented in Coq and proved sound with respect to event traces generated by the small-step semantics of CompCert Clight. Stack-space bounds can be proved at the source level without taking into account low-level details that depend on the implementation of the compiler. The compiler fills in these low-level details during compilation and generates a concrete stack-space bound that applies to the produced machine code. The verified stack analyzer is guaranteed to automatically derive bounds for code with non-recursive functions. It generates a derivation in the quantitative logic to ensure soundness as well as interoperability with interactively developed stack bounds.   In an experimental evaluation, the developed framework is used to obtain verified stack-space bounds for micro benchmarks as well as real system code. The examples include the verified operating-system kernel CertiKOS, parts of the MiBench embedded benchmark suite, and programs from the CompCert benchmarks. The derived bounds are close to the measured stack-space usage of executions of the compiled programs on a Linux x86 system."
2504837,15517,20524,Automated unit test generation for classes with environment dependencies,2014,"Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated.   In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80%/+90%. Furthermore, our techniques reduce the number of unstable tests by more than 50%."
2420077,15517,11058,TRANSIT: specifying protocols with concolic snippets,2013,"With the maturing of technology for model checking and constraint solving, there is an emerging opportunity to develop programming tools that can transform the way systems are specified. In this paper, we propose a new way to program distributed protocols using concolic snippets. Concolic snippets are sample execution fragments that contain both concrete and symbolic values. The proposed approach allows the programmer to describe the desired system partially using the traditional model of communicating extended finite-state-machines (EFSM), along with high-level invariants and concrete execution fragments. Our synthesis engine completes an EFSM skeleton by inferring guards and updates from the given fragments which is then automatically analyzed using a model checker with respect to the desired invariants. The counterexamples produced by the model checker can then be used by the programmer to add new concrete execution fragments that describe the correct behavior in the specific scenario corresponding to the counterexample.   We describe TRANSIT, a language and prototype implementation of the proposed specification methodology for distributed protocols. Experimental evaluations of TRANSIT to specify cache coherence protocols show that (1) the algorithm for expression inference from concolic snippets can synthesize expressions of size 15 involving typical operators over commonly occurring types, (2) for a classical directory-based protocol, TRANSIT automatically generates, in a few seconds, a complete implementation from a specification consisting of the EFSM structure and a few concrete examples for every transition, and (3) a published partial description of the SGI Origin cache coherence protocol maps directly to symbolic examples and leads to a complete implementation in a few iterations, with the programmer correcting counterexamples resulting from underspecified transitions by adding concrete examples in each iteration."
1374575,15517,8868,Adaptive random testing: an illusion of effectiveness?,2011,"Adaptive Random Testing (ART) has been proposed as an enhancement to random testing, based on assumptions on how failing test cases are distributed in the input domain. The main assumption is that failing test cases are usually grouped into contiguous regions. Many papers have been published in which ART has been described as an effective alternative to random testing when using the average number of test case executions needed to find a failure (F-measure). But all the work in the literature is based either on simulations or case studies with unreasonably high failure rates. In this paper, we report on the largest empirical analysis of ART in the literature, in which 3727 mutated programs and nearly ten trillion test cases were used. Results show that ART is highly inefficient even on trivial problems when accounting for distance calculations among test cases, to an extent that probably prevents its practical use in most situations. For example, on the infamous Triangle Classification program, random testing finds failures in few milliseconds whereas ART execution time is prohibitive. Even when assuming a small, fixed size test set and looking at the probability of failure (P-measure), ART only fares slightly better than random testing, which is not sufficient to make it applicable in realistic conditions. We provide precise explanations of this phenomenon based on rigorous empirical analyses. For the simpler case of single-dimension input domains, we also perform formal analyses to support our claim that ART is of little use in most situations, unless drastic enhancements are developed. Such analyses help us explain some of the empirical results and identify the components of ART that need to be improved to make it a viable option in practice."
1581315,15517,8868,Hybrid security analysis of web JavaScript code via dynamic partial evaluation,2014,"This paper addresses the problem of detecting JavaScript security vulnerabilities in the client side of Web applications. Such vulnerabilities are becoming a source of growing concern due to the rapid migration of server-side business logic to the client side, combined with new JavaScript-backed Web technologies, such as AJAX and HTML5. Detection of client-side vulnerabilities is challenging given the dynamic and event-driven nature of JavaScript. We present a hybrid form of JavaScript analysis, which augments static analysis with (semi-)concrete information by applying partial evaluation to JavaScript functions according to dynamic data recorded by the Web crawler. The dynamic component rewrites the program per the enclosing HTML environment, and the static component then explores all possible behaviors of the partially evaluated program (while treating user-controlled aspects of the environment conservatively).     We have implemented this hybrid architecture as the JSA analysis tool, which is integrated into the IBM AppScan Standard Edition product. We formalize the static analysis and prove useful properties over it. We also tested the system across a set of 170,000 Web pages, comparing it with purely static and dynamic alternatives. The results provide conclusive evidence in favor of our hybrid approach. Only 10% of the reports by JSA are false alarms compared to 63% of the alarms flagged by its purely static counterpart, while not a single true warning is lost. This represents a reduction of 94% in false alarms. Compared with a commercial testing algorithm, JSA detects vulnerabilities in >4x more Web sites with only 4 false alarms."
2614375,15517,22232,Context-Bounded Model Checking with ESBMC 1.17 (Competition Contribution),2012,"ESBMC is a context-bounded symbolic model checker for single- and multi-threaded ANSI-C code. It converts the verification conditions using differ- ent background theories and passes them directly to an SMT solver. 1O verview ESBMC is a context-bounded symbolic model checker that allows the verification of single- and multi-threaded C code with shared variables and locks. ESBMC supports full ANSI-C (as defined in ISO/IEC 9899:1990), and can verify programs that make use of bit-level operations, arrays, pointers, structs, unions, memory allocation and floating- point arithmetic. It can reason about arithmetic under- and overflows, pointer safety, memory leaks, array bounds violations, atomicity and order violations, local and global deadlocks, data races, and user-specified assertions. However, as with other bounded model checkers, ESBMC is in general incomplete. ESBMC uses the CBMC (2) frontend to generate the verification conditions (VCs) for a given program, but converts the VCs using different background theories and passes them to a Satisfiability Modulo Theories (SMT) solver. ESBMC natively sup- ports Z3 and Boolector but can also output the VCs using the SMTLib format. ESBMC supports the analysis of multi-threaded ANSI-C code that uses the synchro- nization primitives of the POSIX Pthread library. It traverses a reachability tree (RT) derived from the system in depth-first fashion, and calls the SMT solver whenever it reaches a leaf node. It stops when it finds a bug, or has explored all possible interleav- ings (i.e., the full RT). This combination of symbolic model checking with explicit state space exploration is similar to the ESST approach (1) for SystemC."
1419286,15517,8385,DTAM: dynamic taint analysis of multi-threaded programs for relevancy,2012,"Testing and debugging multi-threaded programs are notoriously difficult due to non-determinism not only in inputs but also in OS schedules. In practice, dynamic analysis and failure replay systems instrument the program to record events of interest in the test execution, e.g., program inputs, accesses to shared objects, synchronization operations, context switches, etc. To reduce the overhead of logging during runtime, these testing and debugging efforts have proposed tradeoffs for sampling or selective logging, at the cost of reducing coverage or performing more expensive search offline.   We propose to identify a subset of input sources and shared objects that are, in a sense,  relevant  for covering program behavior. We classify various types of relevancy in terms of how an input source or a shared object can affect control flow (e.g., a conditional branch) or dataflow (e.g., state of the shared objects) in the program. Such relevancy data can be used by testing and debugging methods to reduce their recording overhead and to guide coverage.   To conduct relevancy analysis, we propose a novel framework based on  d ynamic  t aint  a nalysis for  m ulti-threaded programs, called  DTAM . It performs thread-modular taint analysis for each thread in parallel during runtime, and then aggregates the thread-modular results offline. This approach has many advantages: (a) it is faster than conducting taint analysis for serialized multi-threaded executions, (b) it can compute results for alternate thread interleavings by generalizing the observed execution, and (c) it provides a knob to tradeoff precision with coverage, depending on how thread-modular results are aggregated to account for alternate interleavings. We have implemented DTAM and performed an experimental evaluation on publicly available benchmarks for relevancy analysis. Our experiments show that most shared accesses and conditional branches are dependent on some program input sources. Interestingly in our test runs, on average, only about 25% input sources and 3% shared objects affect other shared accesses through conditional branches. Thus, it is important to identify such relevant input sources and shared objects for testing and debugging."
739046,15517,8385,Modeling the HTML DOM and browser API in static analysis of JavaScript web applications,2011,"Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API.   One application of such a static analysis is to detect type-related and dataflow-related programming errors. We report on experiments with a range of modern web applications, including Chrome Experiments and IE Test Drive applications, to measure the precision and performance of the technique. The experiments indicate that the analysis is able to show absence of errors related to missing object properties and to identify dead and unreachable code. By measuring the precision of the types inferred for object properties, the analysis is precise enough to show that most expressions have unique types. By also producing precise call graphs, the analysis additionally shows that most invocations in the programs are monomorphic. We furthermore study the usefulness of the analysis to detect spelling errors in the code. Despite the encouraging results, not all problems are solved and some of the experiments indicate a potential for improvement, which allows us to identify central remaining challenges and outline directions for future work."
1902131,15517,23827,Sound empirical evidence in software testing,2012,"Several promising techniques have been proposed to automate different tasks in software testing, such as test data generation for object-oriented software. However, reported studies in the literature only show the feasibility of the proposed techniques, because the choice of the employed artifacts in the case studies (e.g., software applications) is usually done in a non-systematic way. The chosen case study might be biased, and so it might not be a valid representative of the addressed type of software (e.g., internet applications and embedded systems). The common trend seems to be to accept this fact and get over it by simply discussing it in a threats to validity section. In this paper, we evaluate search-based software testing (in particular the EvoSuite tool) when applied to test data generation for open source projects. To achieve sound empirical results, we randomly selected 100 Java projects from SourceForge, which is the most popular open source repository (more than 300,000 projects with more than two million registered users). The resulting case study not only is very large (8,784 public classes for a total of 291,639 bytecode level branches), but more importantly it is statistically sound and representative for open source projects. Results show that while high coverage on commonly used types of classes is achievable, in practice environmental dependencies prohibit such high coverage, which clearly points out essential future research directions. To support this future research, our SF100 case study can serve as a much needed corpus of classes for test generation."
2048028,15517,11330,Unified on-chip memory allocation for SIMT architecture,2014,"The popularity of general purpose Graphic Processing Unit (GPU) is largely attributed to the tremendous concurrency enabled by its underlying architecture -- single instruction multiple thread (SIMT) architecture. It keeps the context of a significant number of threads in registers to enable fast ``context switches when the processor is stalled due to execution dependence, memory requests and etc. The SIMT architecture has a large register file evenly partitioned among all concurrent threads. Per-thread register usage determines the number of concurrent threads, which strongly affects the whole program performance. Existing register allocation techniques, extensively studied in the past several decades, are oblivious to the register contention due to the concurrent execution of many threads. They are prone to making optimization decisions that benefit single thread but degrade the whole application performance.   Is it possible for compilers to make register allocation decisions that can maximize the whole GPU application performance? We tackle this important question from two different aspects in this paper. We first propose an unified on-chip memory allocation framework that uses scratch-pad memory to help: (1) alleviate single-thread register pressure; (2) increase whole application throughput. Secondly, we propose a characterization model for the SIMT execution model in order to achieve a desired on-chip memory partition given the register pressure of a program. Overall, we discovered that it is possible to automatically determine an on-chip memory resource allocation that maximizes concurrency while ensuring good single-thread performance at compile-time. We evaluated our techniques on a representative set of GPU benchmarks with non-trivial register pressure. We are able to achieve up to 1.70 times speedup over the baseline of the traditional register allocation scheme that maximizes single thread performance."
968559,15517,11375,A compiler-level intermediate representation based binary analysis and rewriting system,2013,"This paper presents component techniques essential for converting executables to a high-level intermediate representation (IR) of an existing compiler. The compiler IR is then employed for three distinct applications: binary rewriting using the compiler's binary back-end, vulnerability detection using source-level symbolic execution, and source-code recovery using the compiler's C backend. Our techniques enable complex high-level transformations not possible in existing binary systems, address a major challenge of input-derived memory addresses in symbolic execution and are the first to enable recovery of a fully functional source-code.   We present techniques to segment the flat address space in an executable containing undifferentiated blocks of memory. We demonstrate the inadequacy of existing variable identification methods for their promotion to symbols and present our methods for symbol promotion. We also present methods to convert the physically addressed stack in an executable (with a stack pointer) to an abstract stack (without a stack pointer). Our methods do not use symbolic, relocation, or debug information since these are usually absent in deployed executables.   We have integrated our techniques with a prototype x86 binary framework called SecondWrite that uses LLVM as IR. The robustness of the framework is demonstrated by handling executables totaling more than a million lines of source-code, produced by two different compilers (gcc and Microsoft Visual Studio compiler), three languages (C, C++, and Fortran), two operating systems (Windows and Linux) and a real world program (Apache server)."
1691088,15517,23827,Process mining software repositories from student projects in an undergraduate software engineering course,2014,"An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations."
1793134,15517,21022,"Parrot: a practical runtime for deterministic, stable, and reliable threads",2013,"Multithreaded programs are hard to get right. A key reason is that the contract between developers and runtimes grants exponentially many schedules to the runtimes. We present Parrot, a simple, practical runtime with a new contract to developers. By default, it orders thread synchronizations in the well-defined round-robin order, vastly reducing schedules to provide determinism (more precisely, deterministic synchronizations) and  stability  (i.e., robustness against input or code perturbations, a more useful property than determinism). When default schedules are slow, it allows developers to write intuitive  performance hints  in their code to switch or add schedules for speed. We believe this meet in the middle contract eases writing correct, efficient programs.   We further present an ecosystem formed by integrating Parrot with a model checker called dbug. This ecosystem is more effective than either system alone: dbug checks the schedules that matter to Parrot, and Parrot greatly increases the coverage of dbug.   Results on a diverse set of 108 programs, roughly 10× more than any prior evaluation, show that Parrot is easy to use (averaging 1.2 lines of hints per program); achieves low overhead (6.9% for 55 real-world programs and 12.7% for all 108 programs), 10× better than two prior systems; scales well to the maximum allowed cores on a 24-core server and to different scales/types of workloads; and increases Dbug's coverage by 10 6 --10 19734  for 56 programs. Parrot's source code, entire benchmark suite, and raw results are available at github.com/columbia/smt-mc."
46138,15517,22113,Automated grading of DFA constructions,2013,"One challenge in making online education more effective is to develop automatic grading software that can provide meaningful feedback. This paper provides a solution to automatic grading of the standard computation-theory problem that asks a student to construct a deterministic finite automaton (DFA) from the given description of its language. We focus on how to assign partial grades for incorrect answers. Each student's answer is compared to the correct DFA using a hybrid of three techniques devised to capture different classes of errors. First, in an attempt to catch syntactic mistakes, we compute the edit distance between the two DFA descriptions. Second, we consider the entropy of the symmetric difference of the languages of the two DFAs, and compute a score that estimates the fraction of the number of strings on which the student answer is wrong. Our third technique is aimed at capturing mistakes in reading of the problem description. For this purpose, we consider a description language MOSEL, which adds syntactic sugar to the classical Monadic Second Order Logic, and allows defining regular languages in a concise and natural way. We provide algorithms, along with optimizations, for transforming MOSEL descriptions into DFAs and vice-versa. These allow us to compute the syntactic edit distance of the incorrect answer from the correct one in terms of their logical representations. We report an experimental study that evaluates hundreds of answers submitted by (real) students by comparing grades/feedback computed by our tool with human graders. Our conclusion is that the tool is able to assign partial grades in a meaningful way, and should be preferred over the human graders for both scalability and consistency."
1118070,15517,23827,The dimensions of software engineering success,2014,"Software engineering research and practice are hampered by the lack of a well-understood, top-level dependent variable. Recent initiatives on General Theory of Software Engineering suggest a multifaceted variable – Software Engineering Success. However, its exact dimensions are unknown. This paper investigates the dimensions (not causes) of software engineering success. An interdisciplinary sample of 191 design professionals (68 in the software industry) were interviewed concerning their perceptions of success. Non-software designers (e.g. architects) were included to increase the breadth of ideas and facilitate comparative analysis. Transcripts were subjected to supervised, semi-automated semantic content analysis, including a software developer vs. other professionals comparison. Findings suggest that participants view their work as time-constrained projects with explicit clients and other stakeholders. Success depends on stakeholder impacts – financial, social, physical and emotional – and is understood through feedback. Concern with meeting explicit requirements is peculiar to software engineering and design is not equated with aesthetics in many other fields. Software engineering success is a complex multifaceted variable, which cannot sufficiently be explained by traditional dimensions including user satisfaction, profitability or meeting requirements, budgets and schedules. A proto-theory of success is proposed, which models success as the net impact on a particular stakeholder at a particular time. Stakeholder impacts are driven by project efficiency, artifact quality and market performance. Success is not additive, e.g., ‘low’ success for clients does not average with ‘high’ success for developers to make ‘moderate’ success overall; rather, a project may be simultaneously successful and unsuccessful from different perspectives."
1802855,15517,20649,Automatic Verification of Floating Point Units,2014,"Floating Point Units (FPUs) pose a singular challenge for traditional verification methods, such as coverage driven simulation, given the large and complex data paths and intricate control structures which renders those methods incomplete and error prone. Formal verification (FV) has been successfully leveraged to achieve the high level of quality desired of these critical logics. Typically, FV-based approaches to verify FPUs rely on introducing higher level abstractions to allow reasoning. This however has to be done manually, and quickly becomes tedious for optimized bit level implementations on board high performance microprocessors. Automated formal methods working directly on the bit level and providing a full end-to-end check exist but are limited to single instructions (issued in an empty pipeline), hence lack in checking control aspects related to inter-instruction interactions, or pipeline control. In this paper we present an approach based on equivalence checking to overcome the single instruction limitation for automated bit level proofs in the formal verification of FPUs. The sequential execution of instructions is modeled by two instances of the design-under-test. One of the instances acts as a reference model for the other. This allows for large numbers of internal equivalences to be leveraged by equivalence checking techniques. We show that this method is capable of proving instruction sequences for industrial FPU designs. Together with a proof of correctness of individual instructions it guarantees correctness of the FPU design as a whole. In our experience this is a one of a kind approach to perform automated end-to-end verification of FPUs."
2050495,15517,23497,Automated repair of binary and assembly programs for cooperating embedded devices,2013,"We present a method for automatically repairing arbitrary software defects in embedded systems, which have limited memory, disk and CPU capacities, but exist in great numbers. We extend evolutionary computation (EC) algorithms that search for valid repairs at the source code level to assembly and ELF format binaries, compensating for limited system resources with several algorithmic innovations. Our method does not require access to the source code or build toolchain of the software under repair, does not require program instrumentation, specialized execution environments, or virtual machines, or prior knowledge of the bug type.   We repair defects in ARM and x86 assembly as well as ELF binaries, observing decreases of 86% in memory and 95% in disk requirements, with 62% decrease in repair time, compared to similar source-level techniques. These advances allow repairs previously possible only with C source code to be applied to any ARM or x86 assembly or ELF executable. Efficiency gains are achieved by introducing stochastic fault localization, with much lower overhead than comparable deterministic methods, and low-level program representations.   When distributed over multiple devices, our algorithm finds repairs faster than predicted by naive parallelism. Four devices using our approach are five times more efficient than a single device because of our collaboration model. The algorithm is implemented on Nokia N900 smartphones, with inter-phone communication fitting in 900 bytes sent in 7 SMS text messages per device per repair on average."
1488829,15517,23865,"Fantasy, farms, and freemium: what game data mining teaches us about retention, conversion, and virality (keynote abstract)",2011,"In December of 2010, the new game CityVille achieved 6 million daily active users in just 8 days. Clearly the success of CityVille owes something to the fun gameplay experience it provides. That said, it was far from the best game released in 2010. Why did it grow so fast? In this talk the key factors behind the dramatic success of social network games are explained. Social network games build word-of-mouth player acquisition directly into the gameplay experience via friend invitations and game mechanics that require contributions by friends to succeed. Software analytics (mined data about player sessions) yield detailed models of factors that affect player retention and engagement. Player engagement is directly related to conversion, shifting a free player into a paying player, the critical move in a freemium business model. Analytics also permit tracking of player virality, the degree to which one player invites other players into the game. Social network games offer multiple lessons for software engineers in general, and software mining researchers in particular. Since software is in competition for people's attention along with a wide range of other media and software, it is important to design software for high engagement and retention. Retention engineering requires constant attention to mined user experience data, and this data is easiest to acquire with web-based software. Building user acquisition directly into software provides powerful benefits, especially when it is integrated deeply into the experience delivered by the software. Since retention engineering and viral user acquisition are much easier with web-based software, the trend of software applications migrating to the web will accelerate."
2161807,15517,8385,Finding incorrect compositions of atomicity,2013,"In object-oriented code, atomicity is ideally isolated in a library which encapsulates shared program state and provides atomic APIs for access. The library provides a convenient way for programmers to reason about the needed synchronization. However, as the library exports a limited set of APIs, it cannot satisfy every unplanned atomicity demand; therefore, clients may have to compose invocations of the library APIs to obtain new atomic functionality. This process is error-prone due to the complexity of reasoning required, hence tool support for uncovering incorrect compositions (i.e., atomic compositions that are implemented incorrectly) would be very helpful. A key difficulty is how to determine the intended atomic compositions, which are rarely documented. Existing inference techniques cannot be used to infer the atomic compositions because they cannot recognize the library and the client, which requires understanding the related program state. Even if extended to support the library/client, they lead to many false positives or false negatives because they miss the key program logic which reflects programmers’ coding paradigms for atomic compositions.     We define a new inference technique which identifies intended atomic compositions using two key symptoms based on program dependence. We then check dynamically whether these atomic compositions are implemented incorrectly as non-atomic. Evaluation on thirteen large applications shows that our approach finds around 50 previously unknown incorrect compositions. Further study on Tomcat shows that almost half (5 out of 12) of discovered incorrect compositions are confirmed as bugs by the developers. Given that Tomcat is heavily used in 250,000 sites including Linkedin.com and Ebay.com, we believe finding multiple new bugs in it automatically with relatively few false positives supports our heuristics for determining intended atomicity."
1436923,15517,517,Security Model Evolution of PHP Web Applications,2011,"Web sites are often a mixture of static sites and programs that integrate relational databases as a back-end. As they evolve to meet ever-changing user needs, new versions of programs, interactions and functionalities may be added and existing ones may be removed or modified. Web sites require configuration and programming attention to assure security, confidentiality, and trust of the published information. During evolution of Web software, from one version to the next one, security properties may change and possible changes may include new flaws or corrections. Changes to security properties, including access control privileges, can be monitored by observing and analyzing changes between security models extracted from different versions of an application. This paper defines Property Satisfaction Profiles (PSP) as the satisfaction values of properties computed on the extracted models. This paper presents also an investigation of the evolution of the changes in the PSP computed on security models of different versions of a Web application. Model extraction and PSP computation can be performed in linear time on one version. Comparison between two versions is also linear and practical performance is fast. This paper reports results about experiments performed on 31 versions of phpBB, that is a publicly available bulletin board written in PHP. Version 1.0.0 (9547 LOC) to version 2.0.22 (40663 LOC) have been considered as a case study. Results show that the proposed approach can be used to observe and monitor the evolution of PSP in successive versions of the same software package. Suggestions for further research are also presented."
2515856,15517,23497,ConSeq: detecting concurrency bugs through sequential errors,2011,"Concurrency bugs are caused by non-deterministic interleavings between shared memory accesses. Their effects propagate through data and control dependences until they cause software to crash, hang, produce incorrect output, etc. The lifecycle of a bug thus consists of three phases: (1) triggering, (2) propagation, and (3) failure.   Traditional techniques for detecting concurrency bugs mostly focus on phase (1)--i.e., on finding certain structural patterns of interleavings that are common triggers of concurrency bugs, such as data races. This paper explores a consequence-oriented approach to improving the accuracy and coverage of state-space search and bug detection. The proposed approach first statically identifies potential failure sites in a program binary (i.e., it first considers a phase (3) issue). It then uses static slicing to identify critical read instructions that are highly likely to affect potential failure sites through control and data dependences (phase (2)). Finally, it monitors a single (correct) execution of a concurrent program and identifies suspicious interleavings that could cause an incorrect state to arise at a critical read and then lead to a software failure (phase (1)).   ConSeq's backwards approach, (3)!(2)!(1), provides advantages in bug-detection coverage and accuracy but is challenging to carry out. ConSeq makes it feasible by exploiting the empirical observationthat phases (2) and (3) usually are short and occur within one thread. Our evaluation on large, real-world C/C++ applications shows that ConSeq detects more bugs than traditional approaches and has a much lower false-positive rate."
1711438,15517,8385,ELI-ALPS: the ultrafast challenges in Hungary,2011,"The ELI -- Extreme Light Infrastructure -- or as it is commonly referred to: the SUPERLASER will be one of the large research facilities of the European Union. ELI will be built with a joint international effort to form an integrated infrastructure comprised of three branches. The ELI Beamline Facility (Prague, Czech Republic) will mainly focus on particle acceleration and X-ray generation, while the ELI Nuclear Physics Facility (Magurele, Romania) will be dealing with laser-based nuclear physics as well as high field physics. In the talk we introduce the ELI Attosecond Light Pulse Source (ELI-ALPS) to be built in Szeged, Hungary.   The primary mission of the ELI-ALPS Research Infrastructure is to provide the international scientific community with a broad range of ultrafast light sources, especially with coherent XUV and X-ray radiation, including single attosecond pulses. Thanks to this combination of parameters never achieved before, energetic attosecond X-ray pulses of ELI-ALPS will enable recording freeze-frame images of the dynamical electronic-structural behaviour of complex atomic, molecular and condensed matter systems, with attosecond-picometer resolution. The secondary purpose is to contribute to the scientific and technological development towards generating 200 PW pulses, being the ultimate goal of the ELI project. ELI-ALPS will be operated also as a user facility and hence serve basic and applied research in physical, chemical, material and biomedical sciences as well as industrial applications.   The Facility will be built by the end of 2015 from a budget exceeding 240M EUR. The building and the IT infrastructure, from high speed internal networking, remote controlled system alignment, targetry and data aquisition through laser and radiation safety tools until security systems, will challenge the state of the art of similar research facilities."
2385795,15517,23827,Categorizing bugs with social networks: a case study on four open source software communities,2013,"Efficient bug triaging procedures are an important precondition for successful collaborative software engineering projects. Triaging bugs can become a laborious task particularly in open source software (OSS) projects with a large base of comparably inexperienced part-time contributors. In this paper, we propose an efficient and practical method to identify valid bug reports which a) refer to an actual software bug, b) are not duplicates and c) contain enough information to be processed right away. Our classification is based on nine measures to quantify the social embeddedness of bug reporters in the collaboration network. We demonstrate its applicability in a case study, using a comprehensive data set of more than 700, 000 bug reports obtained from the Bugzilla installation of four major OSS communities, for a period of more than ten years. For those projects that exhibit the lowest fraction of valid bug reports, we find that the bug reporters' position in the collaboration network is a strong indicator for the quality of bug reports. Based on this finding, we develop an automated classification scheme that can easily be integrated into bug tracking platforms and analyze its performance in the considered OSS communities. A support vector machine (SVM) to identify valid bug reports based on the nine measures yields a precision of up to 90.3% with an associated recall of 38.9%. With this, we significantly improve the results obtained in previous case studies for an automated early identification of bugs that are eventually fixed. Furthermore, our study highlights the potential of using quantitative measures of social organization in collaborative software engineering. It also opens a broad perspective for the integration of social awareness in the design of support infrastructures."
1887804,15517,23749,Using Model Checking to Analyze the System Behavior of the LHC Production Grid,2012,"DIRAC (Distributed Infrastructure with Remote Agent Control) is the grid solution designed to support production activities as well as user data analysis for the Large Hadron Collider beauty experiment. It consists of cooperating distributed services and a plethora of light-weight agents delivering the workload to the grid resources. Services accept requests from agents and running jobs, while agents actively fulfill specific goals. Services maintain database back-ends to store dynamic state information of entities such as jobs, queues, or requests for data transfer. Agents continuously check for changes in the service states, and react to these accordingly. The logic of each agent is rather simple, the main source of complexity lies in their cooperation. These agents run concurrently, and communicate using the services' databases as a shared memory for synchronizing the state transitions. Despite the effort invested in making DIRAC reliable, entities occasionally get into inconsistent states. Tracing and fixing such behaviors is difficult, given the inherent parallelism among the distributed components and the size of the implementation. In this paper we present an analysis of DIRAC with mCRL2, process algebra with data. We have reverse engineered two critical and related DIRAC subsystems, and subsequently modeled their behavior with the mCRL2 toolset. This enabled us to easily locate race conditions and live locks which were confirmed to occur in the real system. We further formalized and verified several behavioral properties of the two modeled subsystems."
1118461,15517,8868,eXpress: guided path exploration for efficient regression test generation,2011,"Software programs evolve throughout their lifetime undergoing various changes. While making these changes, software developers may introduce regression faults. It is desirable to detect these faults as quickly as possible to reduce the cost involved in fixing them. One existing solution is continuous testing, which runs an existing test suite to quickly find regression faults as soon as code changes are saved. However, the effectiveness of continuous testing depends on the capability of the existing test suite for finding behavioral differences across versions.   To address the issue, we propose an approach, called eXpress, that conducts efficient regression test generation based on a path-exploration-based test generation (PBTG) technique, such as dynamic symbolic execution. eXpress prunes various irrelevant paths with respect to detecting behavioral differences to optimize the search strategy of a PBTG technique. As a result, the PBTG technique focuses its efforts on regression test generation. In addition, eXpress leverages the existing test suite (if available) for the original version to efficiently execute the changed code regions of the program and infect program states. Experimental results on 67 versions (in total) of four programs (two from the subject infrastructure repository and two from real-world open source projects) show that, using eXpress, a state-of-the-art PBTG technique, called Pex, requires about 36% less amount of time (on average) to detect behavioral differences than without using eXpress. In addition, Pex using eXpress detects four behavioral differences that could not be detected without using eXpress (within a time bound). Furthermore, Pex requires 67% less amount of time to find behavioral differences by exploiting an existing test suite than exploration without using the test suite."
2227516,15517,23497,S2E: a platform for in-vivo multi-path analysis of software systems,2011,"This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.   S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.   Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API."
1685136,15517,339,Diglossia: detecting code injection attacks with precision and efficiency,2013,"Code injection attacks continue to plague applications that incorporate user input into executable programs. For example, SQL injection vulnerabilities rank fourth among all bugs reported in CVE, yet all previously proposed methods for detecting SQL injection attacks suffer from false positives and false negatives.   This paper describes the design and implementation of DIGLOSSIA, a new tool that precisely and efficiently detects code injection attacks on server-side Web applications generating SQL and NoSQL queries. The main problems in detecting injected code are (1) recognizing code in the generated query, and (2) determining which parts of the query are tainted by user input. To recognize code, DIGLOSSIA relies on the precise definition due to Ray and Ligatti. To identify tainted characters, DIGLOSSIA dynamically maps all application-generated characters to shadow characters that do not occur in user input and computes shadow values for all input-dependent strings. Any original characters in a shadow value are thus exactly the taint from user input.   Our key technical innovation is dual parsing. To detect injected code in a generated query, DIGLOSSIA parses the query in tandem with its shadow and checks that (1) the two parse trees are syntactically isomorphic, and (2) all code in the shadow query is in shadow characters and, therefore, originated from the application itself, as opposed to user input.   We demonstrate that DIGLOSSIA accurately detects both SQL and NoSQL code injection attacks while avoiding the false positives and false negatives of prior methods. By recasting the problem of detecting injected code as a string propagation and parsing problem, we gain substantial improvements in efficiency and precision over prior work. Our approach does not require any changes to the databases, Web servers, or Web browsers, adds virtually unnoticeable performance overhead, and is deployable today."
1374768,15517,23620,Tracing compilation by abstract interpretation,2014,"Tracing just-in-time compilation is a popular compilation schema for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python, and PHP. It relies on two key ideas. First, it monitors the execution of the program to detect so-called hot paths, i.e., the most frequently executed paths. Then, it uses some store information available at runtime to optimize hot paths. The result is a residual program where the optimized hot paths are guarded by sufficient conditions ensuring the equivalence of the optimized path and the original program. The residual program is persistently mutated during its execution, e.g., to add new optimized paths or to merge existing paths. Tracing compilation is thus fundamentally different than traditional static compilation. Nevertheless, despite the remarkable practical success of tracing compilation, very little is known about its theoretical foundations.   We formalize tracing compilation of programs using abstract interpretation. The monitoring (viz., hot path detection) phase corresponds to an abstraction of the trace semantics that captures the most frequent occurrences of sequences of program points together with an abstraction of their corresponding stores, e.g., a type environment. The optimization (viz., residual program generation) phase corresponds to a transform of the original program that preserves its trace semantics up to a given observation as modeled by some abstraction. We provide a generic framework to express dynamic optimizations and to prove them correct. We instantiate it to prove the correctness of dynamic type specialization. We show that our framework is more general than a recent model of tracing compilation introduced in POPL~2011 by Guo and Palsberg (based on operational bisimulations). In our model we can naturally express hot path reentrance and common optimizations like dead-store elimination, which are either excluded or unsound in Guo and Palsberg's framework."
1994612,15517,23593,Improving high level synthesis optimization opportunity through polyhedral transformations,2013,"High level synthesis (HLS) is an important enabling technology for the adoption of hardware accelerator technologies. It promises the performance and energy efficiency of hardware designs with a lower barrier to entry in design expertise, and shorter design time. State-of-the-art high level synthesis now includes a wide variety of powerful optimizations that implement efficient hardware. These optimizations can implement some of the most important features generally performed in manual designs including parallel hardware units, pipelining of execution both within a hardware unit and between units, and fine-grained data communication. We may generally classify the optimizations as those that optimize hardware implementation within a code block (intra-block) and those that optimize communication and pipelining between code blocks (inter-block). However, both optimizations are in practice difficult to apply. Real-world applications contain data-dependent blocks of code and communicate through complex data access patterns. Existing high level synthesis tools cannot apply these powerful optimizations unless the code is inherently compatible, severely limiting the optimization opportunity. In this paper we present an integrated framework to model and enable both intra- and inter-block optimizations. This integrated technique substantially improves the opportunity to use the powerful HLS optimizations that implement parallelism, pipelining, and fine-grained communication. Our polyhedral model-based technique systematically defines a set of data access patterns, identifies effective data access patterns, and performs the loop transformations to enable the intra- and inter-block optimizations. Our framework automatically explores transformation options, performs code transformations, and inserts the appropriate HLS directives to implement the HLS optimizations. Furthermore, our framework can automatically generate the optimized communication blocks for fine-grained communication between hardware blocks. Experimental evaluation demonstrates that we can achieve an average of 6.04X speedup over the high level synthesis solution without our transformations to enable intra- and inter-block optimizations."
1694764,15517,11058,Æminium: a permission based concurrent-by-default programming language approach,2014,"The aim of AEMINIUM is to study the implications of having a concurrent-by-default programming language. This includes language design, runtime system, performance and software engineering considerations.   We conduct our study through the design of the concurrent-by-default AEMINIUM programming language. AEMINIUM leverages the permission flow of object and group permissions through the program to validate the program's correctness and to automatically infer a possible parallelization strategy via a dataflow graph. AEMINIUM supports not only fork-join parallelism but more general dataflow patterns of parallelism.   In this paper we present a formal system, called μAEMINIUM, modeling the core concepts of AEMINIUM. μAEMINIUM's static type system is based on Featherweight Java with AEMINIUM-specific extensions. Besides checking for correctness AEMINIUM's type system it also uses the permission flow to compute a potential parallel execution strategy for the program. μAEMINIUM's dynamic semantics use a concurrent-by-default evaluation approach. Along with the formal system we present its soundness proof.   We provide a full description of the implementation along with the description of various optimization techniques we used. We implemented AEMINIUM as an extension of the Plaid programming language, which has first-class support for permissions built-in. The AEMINIUM implementation and all case studies are publicly available under the General Public License.   We use various case studies to evaluate AEMINIUM's applicability and to demonstrate that AEMINIUM parallelized code has performance improvements compared to its sequential counterpart. We chose to use case studies from common domains or problems that are known to benefit from parallelization, to show that AEMINIUM is powerful enough to encode them. We demonstrate through a webserver application, which evaluates AEMINIUM's impact on latency-bound applications, that AEMINIUM can achieve a 70% performance improvement over the sequential counterpart. In another case study we chose to implement a dictionary function to evaluate AEMINIUM's capabilities to express essential data structures. Our evaluation demonstrates that AEMINIUM can be used to express parallelism in such data-structures and that the performance benefits scale with the amount of annotation effort which is put into the implementation. We chose an integral computationally example to evaluate pure functional programming and computational intensive use cases. Our experiments show that AEMINIUM is capable of extracting parallelism from functional code and achieving performance improvements up to the limits of Plaid's inherent performance bounds.   Overall, we hope that the work helps to advance concurrent programming in modern programming environments."
657269,15517,9438,Correlating Services with Business Objectives in the ServAlign Framework,2013,"We present a novel approach to modeling business objectives (strategies) and a novel notion of alignment between strategy and service models leading to the successfully deployed ServAlign tool that supports automated alignment analysis. The central role of the notion of business objectives in the theory and prac- tice of management has long been recognized. A framework, methodology and supporting toolkit for strategic service alignment underpins services manage- ment in a variety of ways. The ability to assess alignment between the set of service offerings of an organization and its strategic objectives is critical as a correctness check for its operations. Alignment checking can help reveal un- realized strategies (i.e., strategies for which no operational support exists within the organization). It can also reveal redundant services (i.e., services that do not support any strategic intent). Alignment can provide the basis for service design by using the strategic landscape of an organization to guide the design of its services. The re-alignment of services to an altered strategic landscape pro- vides the machinery for organizational response to dynamic business contexts. The alignment of strategies and business functionality has received considerable recent attention (1,2,3,4,5) but much remains to be done on 3 counts: (1) Offer- ing a richer vocabulary for describing business objectives (2) Offering a formal definition of correlation (or alignment) between service designs and business ob- jectives and (3) Developing automated support for the analysis of correlation or alignment. TheServAlign framework addresses these challenges. The tool has been implemented and evaluated successfully in industry contexts (tool and eval- uation details, plus comparison with related work, ommitted here due to space constraints, can be found at www.uow.edu.au/∼aditya/servalign)."
2346135,15517,11058,Natural proofs for data structure manipulation in C using separation logic,2014,"The natural proof technique for heap verification developed by Qiu et al. [32] provides a platform for powerful sound reasoning for specifications written in a dialect of separation logic called Dryad. Natural proofs are proof tactics that enable automated reasoning exploiting recursion, mimicking common patterns found in human proofs. However, these proofs are known to work only for a simple toy language [32].   In this work, we develop a framework called VCDryad that extends the Vcc framework [9] to provide an automated deductive framework against separation logic specifications for C programs based on natural proofs. We develop several new techniques to build this framework, including (a) a novel tool architecture that allows encoding natural proofs at a higher level in order to use the existing Vcc framework (including its intricate memory model, the underlying type-checker, and the SMT-based verification infrastructure), and (b) a synthesis of ghost-code annotations that captures natural proof tactics, in essence forcing Vcc to find natural proofs using primarily decidable theories.   We evaluate our tool extensively, on more than 150 programs, ranging from code manipulating standard data structures, well-known open source library routines (Glib, OpenBSD), Linux kernel routines, customized OS data structures, etc. We show that all these C programs can be fully automatically verified using natural proofs (given pre/post conditions and loop invariants) without  any user-provided proof tactics . VCDryad is perhaps the first deductive verification framework for heap-manipulating programs in a real language that can prove such a wide variety of programs automatically."
1711482,15517,23865,Analysis of customer satisfaction survey data,2012,"Cisco Systems, Inc., conducts a customer satisfaction survey (CSAT) each year to gauge customer sentiment regarding Cisco products, technical support, partner- and Cisco-provided technical services, order fulfillment, and a number of other aspects of the companys business. The results of the analysis of this data are used for several purposes, including ascertaining the viability of new products, determining if customer support objectives are being met, setting engineering in-process and customer experience yearly metrics goals, and assessing, indirectly, the success of engineering initiatives. Analyzing this data, which includes 110,000 yearly sets of survey responses that address over 100 product and services categories, is in many respects complicated. For example, skip logic is an integral part of the survey mechanics, and forming aggregate views of customer sentiment is statistically challenging in this data environment. In this paper, we describe several of the various analysis approaches currently used, pointing out some situations where a high level of precision is not easily achieved, and some situations in which it is possible to easily end up with erroneous results. The analysis and statistical territory covered in this paper is in parts well-known and straightforward, but other parts, which we address, are susceptible to large inaccuracies and errors. We address several of these difficulties and develop reasonable solutions for two known issues, high missing value levels and high colinearity of independent variables."
1830279,15517,23497,Production-run software failure diagnosis via hardware performance counters,2013,"Sequential and concurrency bugs are widespread in deployed software. They cause severe failures and huge financial loss during production runs. Tools that diagnose production-run failures with low overhead are needed. The state-of-the-art diagnosis techniques use software instrumentation to sample program properties at run time and use off-line statistical analysis to identify properties most correlated with failures. Although promising, these techniques suffer from high run-time overhead, which is sometimes over 100%, for concurrency-bug failure diagnosis and hence are not suitable for production-run usage.   We present PBI, a system that uses existing hardware performance counters to diagnose production-run failures caused by sequential and concurrency bugs with low overhead. PBI is designed based on several key observations. First, a few widely supported performance counter events can reflect a wide variety of common software bugs and can be monitored by hardware with almost no overhead. Second, the counter overflow interrupt supported by existing hardware and operating systems provides a natural and effective mechanism to conduct event sampling at user level. Third, the noise and non-determinism in interrupt delivery complements well with statistical processing.   We evaluate PBI using 13 real-world concurrency and sequential bugs from representative open-source server, client, and utility programs, and 10 bugs from a widely used software-testing benchmark. Quantitatively, PBI can effectively diagnose failures caused by these bugs with a small overhead that is never higher than 10%. Qualitatively, PBI does not require any change to software and presents a novel use of existing hardware performance counters."
2707859,15517,9438,An Error Correction Methodology for Time Dependent Ontologies,2011,"Department of Physics, I3N, University of AveiroAbstract. An increasing number of applications have become depen-dent upon information described in ontologies. Information may be cor-rect for a limited period of time, for example, the assertion: BarackObama is the current president of the USA will be incorrect in 2017.A presidential lifespan can be measured in years, however in a more dy-namic domain, assertions may have lifespans of: months, weeks or days.In addition, erroneous relations may be introduced into an Ontologythrough mistakes in the information source or construction methodol-ogy. Ontologies which contain a large number of errors may impair theeﬀectiveness of applications which depend on it. This paper describesan error correction methodology for ontologies automatically generatedfrom news stories. The information contained in news stories can have avery limited lifespan, consequently constructing an Ontology by an addi-tion of assertions will overtime accumulate errors. The proposed methodavoids this problem through an assignment of a lifespan to each relation.A relation’s lifespan is dependent upon: frequency of assertion, relationvolatility and domain volatility. Once a relation’s lifespan has elapsedthe relation is either deleted or archived as a temporal snapshot of thedomain. Individuals with 0 relations are also removed or archived. AnevaluationofanOntologyconstructedwiththeproposedschemerevealeda gain in the total number of relations overtime without an increase inthe number of the errors. A comparison with an Ontology constructedwith an accumulative addition of relations over an eight week periodrevealed that the proposed method reduced the error count by 81%.Keywords: Business News, Temporal Ontologies, OntologyManagement."
1687714,15517,23620,Regular expression containment: coinductive axiomatization and computational interpretation,2011,"We present a new sound and complete axiomatization of regular expression containment. It consists of the conventional axiomatization of concatenation, alternation, empty set and (the singleton set containing) the empty string as an idempotent semiring, the fixed- point rule  E * = 1 +  E  ×  E * for Kleene-star, and a general coinduction rule as the only additional rule.   Our axiomatization gives rise to a natural computational interpretation of regular expressions as simple types that represent parse trees, and of containment proofs as  coercions . This gives the axiom- atization a Curry-Howard-style constructive interpretation: Containment proofs do not only certify a language-theoretic contain- ment, but, under our computational interpretation, constructively transform a membership proof of a string in one regular expression into a membership proof of the same string in another regular expression.   We show how to encode regular expression equivalence proofs in Salomaa's, Kozen's and Grabmayer's axiomatizations into our containment system, which equips their axiomatizations with a computational interpretation and implies completeness of our axiomatization. To ensure its soundness, we require that the computational interpretation of the coinduction rule be a hereditarily total function. Hereditary totality can be considered the mother of syn- tactic side conditions: it explains their soundness, yet cannot be used as a conventional side condition in its own right since it turns out to be undecidable.   We discuss application of  regular expressions as types  to bit coding of strings and hint at other applications to the wide-spread use of regular expressions for substring matching, where classical automata-theoretic techniques are  a priori  inapplicable.   Neither regular expressions as types nor subtyping interpreted coercively are novel  per se . Somewhat surprisingly, this seems to be the first investigation of a general proof-theoretic framework for the latter in the context of the former, however."
749880,15517,20358,Codewebs: scalable homework search for massive open online programming courses,2014,"Massive open online courses (MOOCs), one of the latest internet revolutions have engendered hope that constant iterative improvement and economies of scale may cure the ``cost disease of higher education. While scalable in many ways, providing feedback for homework submissions (particularly open-ended ones) remains a challenge in the online classroom. In courses where the student-teacher ratio can be ten thousand to one or worse, it is impossible for instructors to personally give feedback to students or to understand the multitude of student approaches and pitfalls. Organizing and making sense of massive collections of homework solutions is thus a critical web problem. Despite the challenges, the dense solution space sampling in highly structured homeworks for some MOOCs suggests an elegant solution to providing quality feedback to students on a massive scale.   We outline a method for decomposing online homework submissions into a vocabulary of code phrases, and based on this vocabulary, we architect a queryable index that allows for fast searches into the massive dataset of student homework submissions. To demonstrate the utility of our homework search engine we index over a million code submissions from users worldwide in Stanford's Machine Learning MOOC and (a) semi-automatically learn shared structure amongst homework submissions and (b) generate specific feedback for student mistakes.   Codewebs is a tool that leverages the redundancy of densely sampled, highly structured homeworks in order to force-multiply teacher effort. Giving articulate, instant feedback is a crucial component of the online learning process and thus by building a homework search engine we hope to take a step towards higher quality free education."
2140687,15517,8385,Understanding myths and realities of test-suite evolution,2012,"Test suites, once created, rarely remain static. Just like the application they are testing, they evolve throughout their lifetime. Test obsolescence is probably the most known reason for test-suite evolution---test cases cease to work because of changes in the code and must be suitably repaired. Repairing existing test cases manually, however, can be extremely time consuming, especially for large test suites, which has motivated the recent development of automated test-repair techniques. We believe that, for developing effective repair techniques that are applicable in real-world scenarios, a fundamental prerequisite is a thorough understanding of how test cases evolve in practice. Without such knowledge, we risk to develop techniques that may work well for only a small number of tests or, worse, that may not work at all in most realistic cases. Unfortunately, to date there are no studies in the literature that investigate how test suites evolve. To tackle this problem, in this paper we present a technique for studying test-suite evolution, a tool that implements the technique, and an extensive empirical study in which we used our technique to study many versions of six real-world programs and their unit test suites. This is the first study of this kind, and our results reveal several interesting aspects of test-suite evolution. In particular, our findings show that test repair is just one possible reason for test-suite evolution, whereas most changes involve refactorings, deletions, and additions of test cases. Our results also show that test modifications tend to involve complex, and hard-to-automate, changes to test cases, and that existing test-repair techniques that focus exclusively on assertions may have limited practical applicability. More generally, our findings provide initial insight on how test cases are added, removed, and modified in practice, and can guide future research efforts in the area of test-suite evolution."
1314471,15517,23827,A characteristic study on failures of production distributed data-parallel programs,2013,"SCOPE is adopted by thousands of developers from tens of different product teams in Microsoft Bing for daily web-scale data processing, including index building, search ranking and advertisement display. A SCOPE job is composed of declarative SQL-like queries and imperative C# user-defined functions (UDFs), which are executed in pipeline by thousands of machines. There are tens of thousands of SCOPE jobs executed on Microsoft clusters per day, while some of them fail after a long execution time and thus waste tremendous resources. Reducing SCOPE failures would save significant resources. This paper presents a comprehensive characteristic study on 200 SCOPE failures/fixes and 50 SCOPE failures with debugging statistics from Microsoft Bing, investigating not only major failure types, failure sources, and fixes, but also current debugging practice. Our major findings include (1) most of the failures (84.5%) are caused by defects in data processing rather than defects in code logic; (2) table-level failures (22.5%) are mainly caused by programmers mistakes and frequent data schema changes while row-level failures (62%) are mainly caused by exceptional data; (3) 93.0% fixes do not change data processing logic; (4) there are 8.0% failures with root cause not at the failure-exposing stage, making current debugging practice insufficient in this case. Our study results provide valuable guidelines for future development of data-parallel programs. We believe that these guidelines are not limited to SCOPE, but can also be generalized to other similar data-parallel platforms."
338880,15517,8422,"HAMPI: a string solver for testing, analysis and vulnerability detection",2011,"Many automatic testing, analysis, and verification techniques for programs can effectively be reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. The increasing efficiency of offthe-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, and hence researchers end up implementing their own ad-hoc solvers. Thus, there is a clear need for an effective and expressive string-constraint solver that can be easily integrated into a variety of applications.#R##N##R##N#To fulfill this need, we designed and implemented Hampi, an efficient and easy-to-use string solver. Users of the Hampi string solver specify constraints using membership predicate over regular expressions, context-free grammars, and equality/dis-equality between string terms. These terms are constructed out of string constants, bounded string variables, and typical string operations such as concatenation and substring extraction. Hampi takes such a constraint as input and decides whether it is satisfiable or not. If an input constraint is satisfiable, Hampi generates a satsfying assignment for the string variables that occur in it.#R##N##R##N#We demonstrate Hampi's expressiveness and efficiency by applying it to program analysis and automated testing: We used Hampi in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications with hundreds of thousands of lines of code.We also used Hampi in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). Hampi's source code, documentation, and experimental data are available at http://people.csail.mit.edu/akiezun/hampi"
1804509,15517,9748,Cache Accurate Time Skewing in Iterative Stencil Computations,2011,"We present a time skewing algorithm that breaks the memory wall for certain iterative stencil computations. A stencil computation, even with constant weights, is a completely memory-bound algorithm. For example, for a large 3D domain of $500^3$ doubles and 100 iterations on a quad-core Xeon X5482 3.2GHz system, a hand-vectorized and parallelized naive 7-point stencil implementation achieves only 1.4 GFLOPS because the system memory bandwidth limits the performance. Although many efforts have been undertaken to improve the performance of such nested loops, for large data sets they still lag far behind synthetic benchmark performance. The state-of-art automatic locality optimizer PluTo achieves 3.7 GFLOPS for the above stencil, whereas a parallel benchmark executing the inner stencil computation directly on registers performs at 25.1 GFLOPS. In comparison, our algorithm achieves 13.0 GFLOPS (52\% of the stencil peak benchmark).We present results for 2D and 3D domains in double precision including problems with gigabyte large data sets. The results are compared against hand-optimized naive schemes, PluTo, the stencil peak benchmark and results from literature. For constant stencils of slope one we break the dependence on the low system bandwidth and achieve at least 50\% of the stencil peak, thus performing within a factor two of an ideal system with infinite bandwidth (the benchmark runs on registers without memory access). For large stencils and banded matrices the additional data transfers let the limitations of the system bandwidth come into play again, however, our algorithm still gains a large improvement over the other schemes."
1591075,15517,23620,Fissile type analysis: modular checking of almost everywhere invariants,2014,"We present a generic analysis approach to the  imperative relationship update problem , in which destructive updates temporarily violate a global invariant of interest. Such invariants can be conveniently and concisely specified with dependent refinement types, which are efficient to check flow-insensitively. Unfortunately, while traditional flow-insensitive type checking is fast, it is inapplicable when the desired invariants can be temporarily broken. To overcome this limitation, past works have directly ratcheted up the complexity of the type analysis and associated type invariants, leading to inefficient analysis and verbose specifications. In contrast, we propose a  generic lifting  of modular refinement type analyses with a symbolic analysis to efficiently and effectively check concise invariants that hold  almost everywhere . The result is an efficient, highly modular flow-insensitive type analysis to  optimistically  check the preservation of global relationship invariants that can fall back to a precise, disjunctive symbolic analysis when the optimistic assumption is violated. This technique permits programmers to temporarily break and then re-establish relationship invariants--a flexibility that is crucial for checking relationships in real-world, imperative languages. A significant challenge is selectively violating the global type consistency invariant over heap locations, which we achieve via  almost type-consistent heaps . To evaluate our approach, we have encoded the problem of verifying the safety of reflective method calls in dynamic languages as a refinement type checking problem. Our analysis is capable of validating reflective call safety at interactive speeds on commonly-used Objective-C libraries and applications."
1559252,15517,23827,Science in the cloud (keynote),2013,"Recent trends in science have made computational capabilities an essential part of scientific discovery. This is often referred to as enhanced scientific discovery, or eScience. eScience has been an integral part of high energy physics for several decades due to the complexity and volume of data produced by experiments. In the 1990s, eScience become central to biology with the sequencing of the human genome. More recently, eScience has become integral to neuroscience to understand neural circuits and human behavior.   It is my view that the demands of 21st century science will mean that eScience is largely done in the Cloud. There are several reasons for this. Foremost, many of the computing requirements of scientists are bursty, requiring massive capabilities for short periods of time. This requirement is well suited to the Cloud. Second, 21st century science will frequently require the publication of large datasets such as the Allen Institute's Brain Atlas and the world wide network of genomics data. Hosting these datasets in public clouds will be much easier than requiring individual scientists (or even universities) to build their own data hosting systems. Third, progress in science increasingly requires collaborations among many distributed groups. Thecloud can greatly facilitate these collaborations.   This talk discusses the requirements for science in the Cloud, and efforts underway to address these requirements. I will provide considerable detail about Google's Exacycle project that is donating one billion core hours to scientific discovery in molecular modeling, drug analysis, and astronomy."
1127892,15517,11330,Characterizing and improving the use of demand-fetched caches in GPUs,2012,"Initially introduced as special-purpose accelerators for games and graphics code, graphics processing units (GPUs) have emerged as widely-used high-performance parallel computing platforms. GPUs traditionally provided only software-managed local memories (or scratchpads) instead of demand-fetched caches. Increasingly, however, GPUs are being used in broader application domains where memory access patterns are both harder to analyze and harder to manage in software-controlled caches. In response, GPU vendors have included sizable demand-fetched caches in recent chip designs. Nonetheless, several problems remain. First, since these hardware caches are quite new and highly-configurable, it can be difficult to know when and how to use them; they sometimes degrade performance instead of improving it. Second, since GPU programming is quite distinct from general-purpose programming, application programmers do not yet have solid intuition about which memory reference patterns are amenable to demand-fetched caches.   In response, this paper characterizes application performance on GPUs with caches and provides a taxonomy for reasoning about different types of access patterns and locality. Based on this taxonomy, we present an algorithm which can be automated and applied at compile-time to identify an application's memory access patterns and to use that information to intelligently configure cache usage to improve application performance. Experiments on real GPU systems show that our algorithm reliably predicts when GPU caches will help or hurt performance. Compared to always passively turning caches on, our method can increase the average benefit of caches from 5.8% to 18.0% for applications that have significant performance sensitivity to caching."
1684708,15517,9856,WebJail: least-privilege integration of third-party components in web mashups,2011,"In the last decade, the Internet landscape has transformed from a mostly static world into Web 2.0, where the use of web applications and mashups has become a daily routine for many Internet users. Web mashups are web applications that combine data and functionality from several sources or components. Ideally, these components contain benign code from trusted sources. Unfortunately, the reality is very different. Web mashup components can misbehave and perform unwanted actions on behalf of the web mashup's user.   Current mashup integration techniques either impose no restrictions on the execution of a third-party component, or simply rely on the Same-Origin Policy. A least-privilege approach, in which a mashup integrator can restrict the functionality available to each component, can not be implemented using the current integration techniques, without ownership over the component's code.   We propose WebJail, a novel client-side security architecture to enable least-privilege integration of components into a web mashup, based on high-level policies that restrict the available functionality in each individual component. The policy language was synthesized from a study and categorization of sensitive operations in the upcoming HTML 5 JavaScript APIs, and full mediation is achieved via the use of deep aspects in the browser.   We have implemented a prototype of WebJail in Mozilla Firefox 4.0, and applied it successfully to mainstream platforms such as iGoogle and Facebook. In addition, microbenchmarks registered a negligible performance penalty for page load-time (7ms), and the execution overhead in case of sensitive operations (0.1ms)."
2160037,15517,339,Prospect: peripheral proxying supported embedded code testing,2014,"Embedded systems are an integral part of almost every electronic product today. From consumer electronics to industrial components in SCADA systems, their possible fields of application are manifold. While especially in industrial and critical infrastructures the security requirements are high, recent publications have shown that embedded systems do not cope well with this demand. One of the reasons is that embedded systems are being less scrutinized as embedded security analysis is considered to be more time consuming and challenging in comparison to PC systems. One of the key challenges on proprietary, resource constrained embedded devices is dynamic code analysis. The devices typically do not have the capabilities for a full-scale dynamic security evaluation. Likewise, the analyst cannot execute the software implementation inside a virtual machine due to the missing peripheral hardware that is required by the software to run. In this paper, we present PROSPECT, a system that can overcome these shortcomings and enables dynamic code analysis of embedded binary code inside arbitrary analysis environments. By transparently forwarding peripheral hardware accesses from the original host system into a virtual machine, PROSPECT allows security analysts to run the embedded software implementation without the need to know which and how embedded peripheral hardware components are accessed. We evaluated PROSPECT with respect to the performance impact and conducted a case study by doing a full-scale security audit of a widely used commercial fire alarm system in the building automation domain. Our results show that PROSPECT is both practical and usable for real-world application."
2280001,15517,23827,Combining functional and imperative programming for multicore software: an empirical study evaluating Scala and Java,2012,"Recent multi-paradigm programming languages combine functional and imperative programming styles to make software development easier. Given today's proliferation of multicore processors, parallel programmers are supposed to benefit from this combination, as many difficult problems can be expressed more easily in a functional style while others match an imperative style. Due to a lack of empirical evidence from controlled studies, however, important software engineering questions are largely unanswered. Our paper is the first to provide thorough empirical results by using Scala and Java as a vehicle in a controlled comparative study on multicore software development. Scala combines functional and imperative programming while Java focuses on imperative shared-memory programming. We study thirteen programmers who worked on three projects, including an industrial application, in both Scala and Java. In addition to the resulting 39 Scala programs and 39 Java programs, we obtain data from an industry software engineer who worked on the same project in Scala. We analyze key issues such as effort, code, language usage, performance, and programmer satisfaction. Contrary to popular belief, the functional style does not lead to bad performance. Average Scala run-times are comparable to Java, lowest run-times are sometimes better, but Java scales better on parallel hardware. We confirm with statistical significance Scala's claim that Scala code is more compact than Java code, but clearly refute other claims of Scala on lower programming effort and lower debugging effort. Our study also provides explanations for these observations and shows directions on how to improve multi-paradigm languages in the future."
801815,15517,23865,Process mining multiple repositories for software defect resolution from control and organizational perspective,2014,"Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities."
28791,15517,8422,From c to infinity and back: unbounded auto-active verification with VCC,2012,"In this tutorial I'll show how to prove deep functional properties of tricky sequential and concurrent C programs using VCC. I'll get into induction, termination, algebraic data types, infinite maps, and lemmas, all unified as ghost data and C-like code manipulating it. Once these are provided, verification is automatic, but the development process of such annotations tends to be very interactive, thus auto-active verification using C as a proof language.#R##N##R##N#VCC [1] is an industrial-strength verification environment for low-level concurrent systems code written in C. VCC takes a program (annotated with function contracts, state assertions, and type invariants) and attempts to prove the correctness of these annotations. VCC's verification methodology[3] allows global two-state invariants that restrict update of shared state and enforces simple, semantic conditions sufficient for checking those global invariants modularly. VCC works by translating C, via the Boogie intermediate verification language, to verification conditions handled by the Z3 SMT solver.#R##N##R##N#The environment includes tools for monitoring proof attempts and constructing partial counterexample executions for failed proofs and has been used to verify functional correctness of tens of thousands of lines of Microsoft's Hyper-V virtualization platform and of SYSGO's embedded real-time operating system PikeOS.#R##N##R##N#VCC is available with sources for non-commercial use at    http://vcc.codeplex.com/       , and online at    http://rise4fun.com/Vcc       . A tutorial [2] is also provided."
2501354,15517,9856,IntFlow: improving the accuracy of arithmetic error detection using information flow tracking,2014,"Integer overflow and underflow, signedness conversion, and other types of arithmetic errors in C/C++ programs are among the most common software flaws that result in exploitable vulnerabilities. Despite significant advances in automating the detection of arithmetic errors, existing tools have not seen widespread adoption mainly due to their increased number of false positives. Developers rely on wrap-around counters, bit shifts, and other language constructs for performance optimizations and code compactness, but those same constructs, along with incorrect assumptions and conditions of undefined behavior, are often the main cause of severe vulnerabilities. Accurate differentiation between legitimate and erroneous uses of arithmetic language intricacies thus remains an open problem.   As a step towards addressing this issue, we present  IntFlow , an accurate arithmetic error detection tool that combines static information flow tracking and dynamic program analysis. By associating sources of untrusted input with the identified arithmetic errors, IntFlow differentiates between non-critical, possibly developer-intended undefined arithmetic operations, and potentially exploitable arithmetic bugs. IntFlow examines a broad set of integer errors, covering almost all cases of C/C++ undefined behaviors, and achieves high error detection coverage. We evaluated IntFlow using the SPEC benchmarks and a series of real-world applications, and measured its effectiveness in detecting arithmetic error vulnerabilities and reducing false positives. IntFlow successfully detected all real-world vulnerabilities for the tested applications and achieved a reduction of 89% in false positives over standalone static code instrumentation."
1059554,15517,23827,Data science for software engineering,2013,"Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part 1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When privacy concerns block access, we show how to privatize data while still being able to mine it. When working with data of dubious quality, we show how to prune spurious information. When data or models seem too complex, we show how to simplify data mining results. When data is too scarce to support intricate models, we show methods for generating predictions. When the world changes, and old models need to be updated, we show how to handle those updates. When the effect is too complex for one model, we show how to reason across ensembles of models. Pre-requisites: This tutorial makes minimal use of maths of advanced algorithms and would be understandable by developers and technical managers."
1853321,15517,23827,Integrated impact analysis for managing software changes,2012,"The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved.     To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over stand-alone approaches."
1718882,15517,23620,A kripke logical relation between ML and assembly,2011,"There has recently been great progress in proving the correctness of compilers for increasingly realistic languages with increasingly realistic runtime systems. Most work on this problem has focused on proving the correctness of a particular compiler, leaving open the question of how to verify the correctness of assembly code that is hand-optimized or linked together from the output of multiple compilers. This has led Benton and other researchers to propose more abstract, compositional notions of when a low-level program correctly realizes a high-level one. However, the state of the art in so-called compositional compiler correctness has only considered relatively simple high-level and low-level languages.   In this paper, we propose a novel, extensional, compiler-independent notion of equivalence between high-level programs in an expressive, impure ML-like λ-calculus and low-level programs in an (only slightly) idealized assembly language. We define this equivalence by means of a biorthogonal, step-indexed, Kripke logical relation, which enables us to reason quite flexibly about assembly code that uses local state in a different manner than the high-level code it implements ( e.g.  self-modifying code). In contrast to prior work, we factor our relation in a symmetric, language-generic fashion, which helps to simplify and clarify the formal presentation, and we also show how to account for the presence of a garbage collector. Our approach relies on recent developments in Kripke logical relations for ML-like languages, in particular the idea of possible worlds as state transition systems."
938045,15517,20524,Refactoring techniques for aggressive object inlining in Java applications,2012,"Object Inlining (OI) is a known optimization in object oriented programming in which referenced objects of class B are inlined into their referencing objects of class A by making all fields and methods of class B part of class A. The optimization saves all the new operations of B type objects from class A and at the same time replaces all indirect accesses, from A to fields of B, by direct accesses. To the best of our knowledge, in-spite of the significant performance potential of the OI optimization, reported performance measurements were relatively moderate. This is because an aggressive OI optimization requires complex analysis and code transformations to overcome problems like multiple references to the inlinable object, object references that escape their object scope, etc.#R##N##R##N#To extract the full potential of OI, we propose a two-stage process. The first stage includes automatic analysis of the source code that informs the user, via comments in the IDE, about code transformations that are needed in order to enable or to maximize the potential of the OI optimization. In the second stage, the OI optimization is applied automatically on the source code as a code refactoring operation, or preferably, as part of the compilation process prior to javac run.#R##N##R##N#We show that this half-automated technique helps to extract the full potential of OI. The proposed OI refactoring process also determines the order of applying the inlinings of the objects and enables us to apply inlinings of objects created inside a method; thus enabling us to reach better performance gain.#R##N##R##N#In this work we also include an evaluation of the OI optimization effects on multithreaded applications running on multicore machines.#R##N##R##N#The comments and the OI transformation were implemented in the Eclipse JDT (Java Development Tools) plugin. The system was then applied on the SPECjbb2000 source code along with profiling data collected by the Eclipse TPTP plugin. The proposed system achieved 46% improvement in performance."
1823753,15517,11058,Look up!: your future is in the cloud,2013,"The Cloud is a wonderfully expansive phrase used to denote computation and data storage centralized in a large datacenter and elastically accessed across a network. The concept is not new; web sites and business servers have run in datacenters for a long time. These, however, were specialized applications, outside of the mainstream of desktop programs. The past few years has seen enormous change as the mainstream shifts from a single computer to mobile devices and clusters of computers. Three factors are driving this change. 1) Mobile computing, where apps run on a size- and power-constrained device and would be far less interesting without backend systems to augment computation and storage capacity. 2) Big data, which uses clusters of computers to extract valuable information from vast amounts of unstructured data. 3) Inexpensive, elastic computing, pioneered by Amazon Web Services, which enables everyone to rapidly obtain and use many servers.   As a researcher from the language and compiler community, I firmly believe this sea change is at heart a programming problem. Cloud computing is far different from the environment in which most of today's languages and tools were developed, and few programmers have mastered its complexity. New challenges include pervasive parallelism, partial failure, high and variable communication latency, and replication for reliability and throughput."
1142094,15517,23827,"Nirikshan: process mining software repositories to identify inefficiencies, imperfections, and enhance existing process capabilities",2014,"Process mining is to extract knowledge about business processes from data stored implicitly in ad-hoc way or explicitly by information systems. The aim is to discover runtime process, analyze performance and perform conformance verification, using process mining tools like ProM and Disco, for single software repository and processes spanning across multiple repositories. Application of process mining to software repositories has recently gained interest due to availability of vast data generated during software development and maintenance. Process data are embodied in repositories which can be used for analysis to improve the efficiency and capability of process, however, involves a lot of challenges which have not been addressed so far. Project team defines workflow, design process and policies for tasks like issue tracking (defect or feature enhancement), peer code review (review the submitted patch to avoid defects before they are injected) etc. to streamline and structure the activities. The reality may not be the same as defined because of imperfections so the extent of non-conformance needs to be measured. We propose a research framework `Nirikshan' to process mine the data of software repositories from multiple perspectives like process, organizational, data and time. We apply process mining on software repositories to derive runtime process map, identify and remove inefficiencies and imperfections, extend the capabilities of existing software engineering tools to make them more process aware, and understand interaction pattern between various contributors to improve the efficiency of project."
2001896,15517,20592,A large-scale analysis of the security of embedded firmwares,2014,"As embedded systems are more than ever present in our society, their security is becoming an increasingly important issue. However, based on the results of many recent analyses of individual firmware images, embedded systems acquired a reputation of being insecure. Despite these facts, we still lack a global understanding of embedded systems' security as well as the tools and techniques needed to support such general claims.#R##N##R##N#In this paper we present the first public, large-scale analysis of firmware images. In particular, we unpacked 32 thousand firmware images into 1.7 million individual files, which we then statically analyzed. We leverage this large-scale analysis to bring new insights on the security of embedded devices and to underline and detail several important challenges that need to be addressed in future research. We also show the main benefits of looking at many different devices at the same time and of linking our results with other large-scale datasets such as the ZMap's HTTPS survey.#R##N##R##N#In summary, without performing sophisticated static analysis, we discovered a total of 38 previously unknown vulnerabilities in over 693 firmware images. Moreover, by correlating similar files inside apparently unrelated firmware images, we were able to extend some of those vulnerabilities to over 123 different products. We also confirmed that some of these vulnerabilities altogether are affecting at least 140K devices accessible over the Internet. It would not have been possible to achieve these results without an analysis at such wide scale.#R##N##R##N#We believe that this project, which we plan to provide as a firmware unpacking and analysis web service, will help shed some light on the security of embedded devices."
1489288,15517,23865,Thesaurus-based automatic query expansion for interface-driven code search,2014,"Software engineers often resort to code search practices to support software maintenance and evolution tasks, in particular code reuse. An issue that affects code search is the vocabulary mismatch problem: while searching for a particular function, users have to guess the exact words that were chosen by original developers to name code entities. In this paper we present an automatic query expansion (AQE) approach that uses word relations to increase the chances of finding relevant code. The approach is applied on top of Test-Driven Code Search (TDCS), a promising code retrieval technique that uses test cases as inputs to formulate the search query, but can also be used with other techniques that handle interface definitions to produce queries (interface-driven code search). Since these techniques rely on keywords and types, the vocabulary mismatch problem is also relevant. AQE is carried out by leveraging WordNet, a type thesaurus for expanding types, and another thesaurus containing only software-related word relations. Our approach is general but was specifically designed for non-native English speakers, who are frequently unaware of the most common terms used to name functions in software. Our evaluation with 36 non-native subjects - including developers and senior Computer Science students - provides evidence that our approach can improve the chances of finding relevant functions by 41% (recall improvement of 30%, on average), without hurting precision."
2367045,15517,8385,Leveraging existing instrumentation to automatically infer invariant-constrained models,2011,"Computer systems are often difficult to debug and understand. A common way of gaining insight into system behavior is to inspect execution logs and documentation. Unfortunately, manual inspection of logs is an arduous process and documentation is often incomplete and out of sync with the implementation.   This paper presents  Synoptic , a tool that helps developers by inferring a concise and accurate system model. Unlike most related work, Synoptic does not require developer-written scenarios, specifications, negative execution examples, or other complex user input. Synoptic processes the logs most systems already produce and requires developers only to specify a set of regular expressions for parsing the logs.   Synoptic has two unique features. First, the model it produces satisfies three kinds of temporal invariants mined from the logs, improving accuracy over related approaches. Second, Synoptic uses refinement and coarsening to explore the space of models. This improves model efficiency and precision, compared to using just one approach.   In this paper, we formally prove that Synoptic always produces a model that satisfies exactly the temporal invariants mined from the log, and we argue that it does so efficiently. We empirically evaluate Synoptic through two user experience studies, one with a developer of a large, real-world system and another with 45 students in a distributed systems course. Developers used Synoptic-generated models to verify known bugs, diagnose new bugs, and increase their confidence in the correctness of their systems. None of the developers in our evaluation had a background in formal methods but were able to easily use Synoptic and detect implementation bugs in as little as a few minutes."
2392415,15517,11058,Proving acceptability properties of relaxed nondeterministic approximate programs,2012,"Approximate program transformations such as skipping tasks [29, 30], loop perforation [21, 22, 35], reduction sampling [38], multiple selectable implementations [3, 4, 16, 38], dynamic knobs [16], synchronization elimination [20, 32], approximate function memoization [11],and approximate data types [34] produce programs that can execute at a variety of points in an underlying performance versus accuracy tradeoff space. These transformed programs have the ability to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control their execution.   We call such transformed programs  relaxed  programs because they have been extended with additional nondeterminism to relax their semantics and enable greater flexibility in their execution.   We present language constructs for developing and specifying  relaxed  programs. We also present proof rules for reasoning about  properties  [28] which the program must satisfy to be acceptable. Our proof rules work with two kinds of acceptability properties:  acceptability properties  [28], which characterize desired relationships between the values of variables in the original and relaxed programs, and  unary  acceptability properties, which involve values only from a single (original or relaxed) program. The proof rules support a  staged  reasoning approach in which the majority of the reasoning effort works with the original program. Exploiting the common structure that the original and relaxed programs share, relational reasoning transfers reasoning effort from the original program to prove properties of the relaxed program.   We have formalized the dynamic semantics of our target programming language and the proof rules in Coq and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine-checked verifications of their relaxed programs."
1923025,15517,23827,Integrating software project resources using source code identifiers,2014,"Source code identifiers such as classes, methods, and fields appear in many different contexts. For instance, a developer performing a task using the android.app.Activity class could consult various project resources including the class's source file, API documentation, issue tracker, mailing list discussions, code reviews, or questions on Stack Overflow.     These information sources are logically connected by the source code elements they describe, but are generally decoupled from each other. This has historically been tolerated by developers, since there was no obvious way to easily navigate between the data sources. However, it is now common for these sources to have web-based front ends that provide a standard mechanism (the browser) for viewing and interacting with the data they contain. Augmenting these front ends with hyperlinks and search would make development easier by allowing developers to quickly navigate between disparate sources of information about the same code element.     In this paper, we propose a method of automatically linking disparate information repositories with an emphasis on high precision. We also propose a method of augmenting web-based front ends with these links to make it easier for developers to quickly gain a comprehensive view of the source code elements they are investigating. Research challenges include identifying source code tokens in the midst of natural language text and incomplete code fragments, dynamically augmenting the web views of the data repositories, and supporting novel composition of the link data to provide comprehensive views for specific source code elements."
1532579,15517,23876,Condensing class diagrams by analyzing design and network metrics using optimistic classification,2014,"A class diagram of a software system enhances our ability to understand software design. However, this diagram is often unavailable. Developers usually reconstruct the diagram by reverse engineering it from source code. Unfortunately, the resultant diagram is often very cluttered; making it difficult to learn anything valuable from it. Thus, it would be very beneficial if we are able to condense the reverse- engineered class diagram to contain only the important classes depicting the overall design of a software system. Such diagram would make program understanding much easier. A class can be important, for example, if its removal would break many connections between classes. In our work, we estimate this kind of importance by using design (e.g., number of attributes, number of dependencies, etc.) and network metrics (e.g., betweenness centrality, closeness centrality, etc.). We use these metrics as features and input their values to our optimistic classifier that will predict if a class is important or not. Different from standard classification, our newly proposed optimistic classification technique deals with data scarcity problem by optimistically assigning labels to some of the unlabeled data and use them for training a better statistical model. We have evaluated our approach to condense reverse-engineered diagrams of 9 software systems and compared our approach with the state-of-the-art work of Osman et al. Our experiments show that our approach can achieve an average Area Under the Receiver Operating Characteristic Curve (AUC) score of 0.825, which is a 9.1% improvement compared to the state-of-the-art approach."
855418,15517,8385,Mining preconditions of APIs in large-scale code corpus,2014,"Modern software relies on existing application programming interfaces (APIs) from libraries. Formal specifications for the APIs enable many software engineering tasks as well as help developers correctly use them. In this work, we mine large-scale repositories of existing open-source software to derive potential preconditions for API methods. Our key idea is that APIs’ preconditions would appear frequently in an ultra-large code corpus with a large number of API usages, while project-specific conditions will occur less frequently. First, we find all client methods invoking APIs. We then compute a control dependence relation from each call site and mine the potential conditions used to reach those call sites. We use these guard conditions as a starting point to automatically infer the preconditions for each API. We analyzed almost 120 million lines of code from SourceForge and Apache projects to infer preconditions for the standard Java Development Kit (JDK) library. The results show that our technique can achieve high accuracy with recall from 75–80% and precision from 82–84%. We also found 5 preconditions missing from human written specifications. They were all confirmed by a specification expert. In a user study, participants found 82% of the mined preconditions as a good starting point for writing specifications. Using our mining result, we also built a benchmark of more than 4,000 precondition-related bugs."
1081198,15517,9856,Leveraging semantic signatures for bug search in binary programs,2014,"Software vulnerabilities still constitute a high security risk and there is an ongoing race to patch known bugs. However, especially in closed-source software, there is no straightforward way (in contrast to source code analysis) to find buggy code parts, even if the bug was publicly disclosed.   To tackle this problem, we propose a method called Tree Edit Distance Based Equational Matching (TEDEM) to automatically identify binary code regions that are similar to code regions containing a reference bug. We aim to find bugs both in the same binary as the reference bug and in completely unrelated binaries (even compiled for different operating systems). Our method even works on proprietary software systems, which lack source code and symbols.   The analysis task is split into two phases. In a preprocessing phase, we condense the semantics of a given binary executable by symbolic simplification to make our approach robust against syntactic changes across different binaries. Second, we use  tree edit distances  as a basic block-centric metric for code similarity. This allows us to find instances of the same bug in different binaries and even spotting its variants (a concept called  vulnerability extrapolation ). To demonstrate the practical feasibility of the proposed method, we implemented a prototype of TEDEM that can find real-world security bugs across binaries and even across OS boundaries, such as in MS Word and the popular messengers Pidgin (Linux) and Adium (Mac OS)."
2515140,15517,23497,STABILIZER: statistically sound performance evaluation,2013,"Researchers and software developers require effective performance evaluation. Researchers must evaluate optimizations or measure overhead. Software developers use automatic performance regression tests to discover when changes improve or degrade performance. The standard methodology is to compare execution times before and after applying changes.   Unfortunately, modern architectural features make this approach unsound. Statistically sound evaluation requires multiple samples to test whether one can or cannot (with high confidence) reject the null hypothesis that results are the same before and after. However, caches and branch predictors make performance dependent on machine-specific parameters and the exact layout of code, stack frames, and heap objects. A single binary constitutes just one sample from the space of program layouts, regardless of the number of runs. Since compiler optimizations and code changes also alter layout, it is currently impossible to distinguish the impact of an optimization from that of its layout effects.   This paper presents Stabilizer, a system that enables the use of the powerful statistical techniques required for sound performance evaluation on modern architectures. Stabilizer forces executions to sample the space of memory configurations by repeatedly re-randomizing layouts of code, stack, and heap objects at runtime. Stabilizer thus makes it possible to control for layout effects. Re-randomization also ensures that layout effects follow a Gaussian distribution, enabling the use of statistical tests like ANOVA. We demonstrate Stabilizer's efficiency ("
1401996,15517,20524,Potential biases in bug localization: do they matter?,2014,"Issue tracking systems are valuable resources during software maintenance activities and contain information about the issues faced during the development of a project as well as after its release. Many projects receive many reports of bugs and it is challenging for developers to manually debug and fix them. To mitigate this problem, past studies have proposed information retrieval (IR)-based bug localization techniques, which takes as input a textual description of a bug stored in an issue tracking system, and returns a list of potentially buggy source code files.   These studies often evaluate their effectiveness on issue reports marked as bugs in issue tracking systems, using as ground truth the set of files that are modified in commits that fix each bug. However, there are a number of potential biases that can impact the validity of the results reported in these studies. First, issue reports marked as bugs might not be reports of bugs due to error in the reporting and classification process. Many issue reports are about documentation update, request for improvement, refactoring, code cleanups, etc. Second, bug reports might already explicitly specify the buggy program files and for these reports bug localization techniques are not needed. Third, files that get modified in commits that fix the bugs might not contain the bug.   This study investigates the extent these potential biases affect the results of a bug localization technique and whether bug localization researchers need to consider these potential biases when evaluating their solutions. In this paper, we analyse issue reports from three different projects: HTTPClient, Jackrabbit, and Lucene-Java to examine the impact of above three biases on bug localization. Our results show that one of these biases significantly and substantially impacts bug localization results, while the other two biases have negligible or minor impact."
2243083,15517,9748,Adaptive Runtime Selection for GPU,2013,"It is often hard to predict the performance of a statically generated code. Hardware availability, hardware specification and problem size may change from one execution context to another. The main contribution of this work is an entirely automatic method aiming to predict execution times of semantically equivalent versions of affine loop nests on GPUs, then, to run the best performing one on GPU or CPU. To make accurate predictions, our framework relies on three consecutive stages: a static code generation, an offline profiling and an online prediction. Different versions are statically generated by PPCG, a source-to-source polyhedral compiler, able to generate CUDA code from static control loops written in C. The code versions differ by their block sizes, tiling and parallel schedule. The profiling code carries out the required measurements on the target machine: throughput between host and device memory, and execution time of the kernels with various parameters. At runtime, we rely on those results to calculate a predicted execution time on GPU. This is followed by a fastest wins algorithm, that runs instances of the target code concurrently on CPU and GPU, the first completed kills the other one. We validate this proposal on the polyhedral benchmark suite, showing that the predictions are accurate and that the runtime selection is effective on two different architectures."
1633925,15517,517,Using Projections to Debug Large Combinatorial Models,2013,"Combinatorial test design (CTD) is an effective test planning technique that reveals faults resulting from parameters interactions in a system. The test space is manually modeled by a set of parameters, their respective values, and restrictions on the value combinations - referred to as a CTD model. Each possible combination of values in the cross product of the parameters, that is not excluded by restrictions, represents a valid test. A subset of the test space is then automatically constructed so that it covers all valid value combinations of every t parameters, where t is usually a user input. In many real-life testing problems, the relationships between the different test parameters are complex. Thus, precisely capturing them by restrictions in the CTD model might be a very challenging and time consuming task. Since the test space is of exponential size in the number of parameters, it is impossible to exhaustively review all potential tests. In this paper, we present technology that supports the modeling process by enabling repeated reviews of projections of the test space on a subset of the parameters, while indicating how the value combinations under review are affected by the restrictions. In addition, we generate explanations as to why the restrictions exclude specific value combinations of the subsets of parameters under review. These explanations can be used to identify modeling mistakes, as well as to increase the understanding of the test space. Furthermore, we identify specific excluded combinations that may require special attention, and list them for review together with their corresponding exclusion explanation. To enable the review of subsets of the exponential test space, indicate their status, and identify excluded combinations for review, we use a compact representation of the test space that is based on Binary Decision Diagrams. For the generation of explanations we use satisfiability solvers. We evaluate the proposed technology on real-life CTD models and demonstrate its effectiveness."
2285603,15517,23620,Consistency analysis of decision-making programs,2014,"Applications in many areas of computing make discrete decisions under  uncertainty , for reasons such as limited numerical precision in calculations and errors in sensor-derived inputs. As a result, individual decisions made by such programs may be nondeterministic, and lead to contradictory decisions at different points of an execution. This means that an otherwise correct program may execute along paths, that it would not follow under its ideal semantics, violating essential program invariants on the way. A program is said to be  consistent  if it does not suffer from this problem despite uncertainty in decisions.   In this paper, we present a sound, automatic program analysis for verifying that a program is consistent in this sense. Our analysis proves that each decision made along a program execution is consistent with the decisions made earlier in the execution. The proof is done by generating an invariant that abstracts the set of all decisions made along executions that end at a program location  l , then verifying, using a fixpoint constraint-solver, that no contradiction can be derived when these decisions are combined with new decisions made at  l .   We evaluate our analysis on a collection of programs implementing algorithms in  computational geometry . Consistency is known to be a critical, frequently-violated, and thoroughly studied correctness property in geometry, but ours is the first attempt at automated verification of consistency of geometric algorithms. Our benchmark suite consists of implementations of convex hull computation, triangulation, and point location algorithms. On almost all examples that are not consistent (with two exceptions), our analysis is able to verify consistency within a few minutes."
591369,15517,20349,A NICE way to test openflow applications,2012,"The emergence of OpenFlow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single controller program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently distributed and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present efficient, systematic techniques for testing unmodified controller programs. Our NICE tool applies model checking to explore the state space of the entire system--the controller, the switches, and the hosts. Scalability is the main challenge, given the diversity of data packets, the large system state, and the many possible event orderings. To address this, we propose a novel way to augment model checking with symbolic execution of event handlers (to identify representative packets that exercise code paths on the controller). We also present a simplified OpenFlow switch model (to reduce the state space), and effective strategies for generating event interleavings likely to uncover bugs. Our prototype tests Python applications on the popular NOX platform. In testing three real applications--a MAC-learning switch, in-network server load balancing, and energy-efficient traffic engineering--we uncover eleven bugs."
1764623,15517,20754,Rozzle: De-cloaking Internet Malware,2012,"JavaScript-based malware attacks have increased in recent years and currently represent a signicant threat to the use of desktop computers, smartphones, and tablets. While static and runtime methods for malware detection have been proposed in the literature, both on the client side, for just-in-time in-browser detection, as well as offline, crawler-based malware discovery, these approaches encounter the same fundamental limitation. Web-based malware tends to be environment-specific, targeting a particular browser, often attacking specic versions of installed plugins. This targeting occurs because the malware exploits vulnerabilities in specific plugins and fails otherwise. As a result, a fundamental limitation for detecting a piece of malware is that malware is triggered infrequently, only showing itself when the right environment is present. We observe that, using fingerprinting techniques that capture and exploit unique properties of browser configurations, almost all existing malware can be made virtually impssible for malware scanners to detect. This paper proposes Rozzle, a JavaScript multi-execution virtual machine, as a way to explore multiple execution paths within a single execution so that environment-specific malware will reveal itself. Using large-scale experiments, we show that Rozzle increases the detection rate for offline runtime detection by almost seven times. In addition, Rozzle triples the effectiveness of online runtime detection. We show that Rozzle incurs virtually no runtime overhead and allows us to replace multiple VMs running different browser configurations with a single Rozzle-enabled browser, reducing the hardware requirements, network bandwidth, and power consumption."
719773,15517,23827,Using psycho-physiological measures to assess task difficulty in software development,2014,"Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem --- detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code.     In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99% precision and 64.58% recall, and for a new task with 84.38% precision and 69.79% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools."
2428210,15517,23497,2ndStrike: toward manifesting hidden concurrency typestate bugs,2011,"Concurrency bugs are becoming increasingly prevalent in the multi-core era. Recently, much research has focused on data races and atomicity violation bugs, which are related to low-level memory accesses. However, a large number of concurrency typestate bugs such as invalid reads to a closed file from a different thread are under-studied. These concurrency typestate bugs are important yet challenging to study since they are mostly relevant to high-level program semantics.   This paper presents 2ndStrike, a method to manifest hidden concurrency typestate bugs in software testing. Given a state machine describing correct program behavior on certain object typestates, 2ndStrike profiles runtime events related to the typestates and thread synchronization. Based on the profiling results, 2ndStrike then identifies bug candidates, each of which is a pair of runtime events that would cause typestate violation if the event order is reversed. Finally, 2ndStrike re-executes the program with controlled thread interleaving to manifest bug candidates.   We have implemented a prototype of 2ndStrike on Linux and have illustrated our idea using three types of concurrency typestate bugs, including invalid file operation, invalid pointer dereference, and invalid lock operation. We have evaluated 2ndStrike with six real world bugs (including one previously unknown bug) from three open-source server and desktop programs (i.e., MySQL, Mozilla, pbzip2). Our experimental results show that 2ndStrike can effectively and efficiently manifest all six software bugs, most of which are difficult or impossible to manifest using stress testing or active testing techniques that are based on data race/atomicity violation. Additionally, 2ndStrike reports no false positives, provides detailed bug reports for each manifested bug, and can consistently reproduce the bug after manifesting it once."
1916197,15517,23620,A parametric segmentation functor for fully automatic and scalable array content analysis,2011,"We introduce FunArray, a parametric segmentation abstract domain functor for the fully automatic and scalable analysis of array content properties. The functor enables a natural, painless and efficient lifting of existing abstract domains for scalar variables to the analysis of uniform compound data-structures such as arrays and collections. The analysis automatically and semantically divides arrays into consecutive non-overlapping possibly empty segments. Segments are delimited by sets of bound expressions and abstracted uniformly. All symbolic expressions appearing in a bound set are equal in the concrete. The FunArray can be naturally combined via reduced product with any existing analysis for scalar variables. The analysis is presented as a general framework parameterized by the choices of bound expressions, segment abstractions and the reduction operator. Once the functor has been instantiated with fixed parameters, the analysis is fully automatic.   We first prototyped FunArray in Arrayal to adjust and experiment with the abstractions and the algorithms to obtain the appropriate precision/ratio cost. Then we implemented it into Clousot, an abstract interpretation-based static contract checker for .NET. We empirically validated the precision and the performance of the analysis by running it on the main libraries of .NET and on its own code. We were able to infer thousands of non-trivial invariants and verify the implementation with a modest overhead (circa 1%). To the best of our knowledge this is the first analysis of this kind applied to such a large code base, and proven to scale."
1544376,15517,8385,Data debugging with continuous testing,2013,"Today, systems rely as heavily on data as on the software that manipulates those data. Errors in these systems are incredibly costly, annually resulting in multi-billion dollar losses, and, on multiple occasions, in death. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: discovering system errors caused by well-formed but incorrect data. In this paper, we propose continuous data testing: using otherwise-idle CPU cycles to run test queries, in the background, as a user or database administrator modifies a database. This technique notifies the user or administrator about a data bug as quickly as possible after that bug is introduced, leading to at least three benefits: (1) The bug is discovered quickly and can be fixed before it is likely to cause a problem. (2) The bug is discovered while the relevant change is fresh in the user's or administrator's mind, increasing the chance that the underlying cause of the bug, as opposed to only the discovered side-effect, is fixed. (3) When poor documentation or company policies contribute to bugs, discovering the bug quickly is likely to identify these contributing factors, facilitating updating documentation and policies to prevent similar bugs in the future. We describe the problem space and potential benefits of continuous data testing, our vision for the technique, challenges we encountered, and our prototype implementation for PostgreSQL. The prototype's low overhead shows promise that continuous data testing can address the important problem of data debugging."
1662659,15517,20524,Symbolic state validation through runtime data,2014,"Real world programs are typically built on top of many library functions. Symbolic analysis of these programs generally requires precise models of these functions? Application Programming Interfaces (APIs), which are mostly unavailable because these models are costly to construct. A variant approach of symbolic analysis is to over-approximate the return values of those APIs that have not been modeled. However, such approximation can induce many unreachable symbolic states, which are expensive to validate manually. In this paper, we propose a static approach to automatically validating the reported anomalous symbolic states. The validation makes use of the available runtime data of the un-modeled APIs collected from previous program executions. We show that the symbolic state validation problem can be cast as a MAX-SAT problem and solved by existing constraint solvers.   Our approach is motivated by two observations. We may bind the symbolic parameters in un-modeled APIs based on observations made in former executions by other programs. The binding enables us to use the corresponding observed concrete return values of APIs to validate the symbolic states arising from the over-approximated return values of the un-modeled APIs. Second, some symbolic constraints can be accurately evaluated despite the imprecision of the over-approximated symbolic values.   Our technique found 80 unreported bugs when it was applied to 10 popular programs with a total of 1.5 million lines of code. All of them can be confirmed by test cases. Our technique presents a promising way to apply the big data paradigm to software engineering. It provides a mechanism to validate the symbolic states of a project by leveraging the many concrete input-output values of APIs collected from other projects."
623120,15517,22232,IC3 Modulo Theories via Implicit Predicate Abstraction,2013,"We present a novel approach for generalizing the IC3 algorithm for invariant checking from finite-state to infinite-state transition systems, expressed over some background theories. The procedure is based on a tight integration of IC3 with Implicit (predicate) Abstraction, a technique that expresses abstract tran- sitions without computing explicitly the abstract system and is incremental with respect to the addition of predicates. In this scenario, IC3 operates only at the Boolean level of the abstract state space, discovering inductive clauses over the abstraction predicates. Theory reasoning is confined within the underlying SMT solver, and applied transparently when performing satisfiability checks. When the current abstraction allows for a spurious counterexample, it is refined by discov- ering and adding a sufficient set of new predicates. Importantly, this can be done in a completely incremental manner, without discarding the clauses found in the previous search. The proposed approach has two key advantages. First, unlike current SMT generalizations of IC3, it allows to handle a wide range of background theories without relying on ad-hoc extensions, such as quantifier elimination or theory- specific clause generalization procedures, which might not always be available, and can moreover be inefficient. Second, compared to a direct exploration of the concrete transition system, the use of abstraction gives a significant performance improvement, as our experiments demonstrate."
882552,15517,8385,Risky files: an approach to focus quality improvement effort,2013,"As the development of software products frequently transitions among globally distributed teams, the knowledge about the source code, design decisions, original requirements, and the history of troublesome areas gets lost. A new team faces tremendous challenges to regain that knowledge. In numerous projects we observed that only 1% of project files are involved in more than 60% of the customer reported defects (CFDs), thus focusing quality improvement on such files can greatly reduce the risk of poor product quality. We describe a mostly automated approach that annotates the source code at the file and module level with the historic information from multiple version control, issue tracking, and an organization's directory systems. Risk factors (e.g, past changes and authors who left the project) are identified via a regression model and the riskiest areas undergo a structured evaluation by experts. The results are presented via a web-based tool and project experts are then trained how to use the tool in conjunction with a checklist to determine risk remediation actions for each risky file. We have deployed the approach in seven projects in Avaya and are continuing deployment to the remaining projects as we are evaluating the results of earlier deployments. The approach is particularly helpful to focus quality improvement effort for new releases of deployed products in a resource-constrained environment."
1550847,15517,23865,"Think locally, act globally: improving defect and effort prediction models",2012,"Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds."
630157,15517,20349,Model checking a networked system without the network,2011,"Current approaches to model checking distributed systems reduce the problem to that of model checking centralized systems: global states involving all nodes and communication links are systematically explored. The frequent changes in the network element of the global states lead however to a rapid state explosion and make it impossible to model check any non-trivial distributed system. We explore in this paper an alternative: a local approach where the network is ignored, a priori: only the local nodes' states are explored and in a separate manner. The set of valid system states is a subset of all combinations of the node local states and checking validity of such a combination is only performed a posteriori, in case of a possible bug. This approach drastically reduces the number of transitions executed by the model checker. It takes for example the classic global approach several minutes to explore the interleaving of messages in the celebrated Paxos distributed protocol even considering only three nodes and a single proposal. Our local approach explores the entire system state in a few seconds. Our local approach does clearly not eliminate the state exponential explosion problem. Yet, it postpones its manifestations till some deeper levels. This is already good enough for online testing tools that restart the model checker periodically from the current live state of a running system. We show for instance how this approach enables us to find two bugs in variants of Paxos."
2145624,15517,23876,Part-of-speech tagging of program identifiers for improved text-based software engineering tools,2013,"To aid program comprehension, programmers choose identifiers for methods, classes, fields and other program elements primarily by following naming conventions in software. These software “naming conventions” follow systematic patterns which can convey deep natural language clues that can be leveraged by software engineering tools. For example, they can be used to increase the accuracy of software search tools, improve the ability of program navigation tools to recommend related methods, and raise the accuracy of other program analyses. After splitting multi-word names into their component words, the next step to extracting accurate natural language information is tagging each word with its part of speech (POS) and then chunking the name into natural language phrases. State-of-theart approaches, most of which rely on “traditional POS taggers” trained on natural language documents, do not capture the syntactic structure of program elements. In this paper, we present a POS tagger and syntactic chunker for source code names that takes into account programmers' naming conventions to understand the regular, systematic ways a program element is named. We studied the naming conventions used in Object Oriented Programming and identified different grammatical constructions that characterize a large number of program identifiers. This study then informed the design of our POS tagger and chunker. Our evaluation results show a significant improvement in accuracy(11%-20%) of POS tagging of identifiers, over the current approaches. With this improved accuracy, both automated software engineering tools and developers will be able to better capture and understand the information available in code."
1642961,15517,23827,Motivating and orienting novice students to value introductory software engineering,2013,"Students with little professional software development experience typically have low intrinsic motivation and beyond achieving a good grade, low extrinsic motivation to study and appreciate the value of software engineering curricula. Unlike other subjects, introductory software engineering instructors exert a great deal of effort justifying and motivating their course topics. Since 2006 the University of Hawaii has been developing a series of “early awareness” engagements within an introductory Systems Analysis and Design course designed to foster intrinsic and extrinsic motivation and orient students to value learning software engineering. Measuring motivation and perceived value is difficult, but there are key indicators to determine the impact of our improvements such as higher course evaluations, greater class attendance, and increased positive feedback from students and employers. Our results show that these key indicators have improved since introducing early engagements. Furthermore, students like this approach, value the course more, do better quality work, and evaluate the course more positively. Longitudinal follow-ups indicate greater interest and success in pursuing software engineering related careers. This paper shares the details of our early awareness engagements, how they are applied in the classroom, some of our experiences in using them, and evidence that they have a positive impact. Our goal is to provide specific and practical means that instructors can use immediately to improve the perceived value of software engineering for novice students prior to or while they study it."
1262512,15517,11058,"Harmonizing classes, functions, tuples, and type parameters in virgil iii",2013,"Languages are becoming increasingly multi-paradigm. Subtype polymorphism in statically-typed object-oriented languages is being supplemented with parametric polymorphism in the form of generics. Features like first-class functions and lambdas are appearing everywhere. Yet existing languages like Java, C#, C++, D, and Scala seem to accrete ever more complexity when they reach beyond their original paradigm into another; inevitably older features have some rough edges that lead to nonuniformity and pitfalls. Given a fresh start, a new language designer is faced with a daunting array of potential features. Where to start? What is important to get right first, and what can be added later? What features must work together, and what features are orthogonal? We report on our experience with Virgil III, a practical language with a careful balance of classes, functions, tuples and type parameters. Virgil intentionally lacks many advanced features, yet we find its core feature set enables new species of design patterns that bridge multiple paradigms and emulate features not directly supported such as interfaces, abstract data types, ad hoc polymorphism, and variant types. Surprisingly, we find variance for function types and tuple types often replaces the need for other kinds of type variance when libraries are designed in a more functional style."
2595098,15517,20876,"Static analysis of variability in system software: the 90,000 #ifdefs issue",2014,"System software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. The Linux v3.2 kernel, for instance, provides more than 12,000 configurable features, which control the configuration-dependent inclusion of 31,000 source files with 89,000 #ifdef blocks.#R##N##R##N#Tools for static analyses can greatly assist with ensuring the quality of code-bases of this size. Unfortunately, static configurability limits the success of automated software testing and bug hunting. For proper type checking, the tools need to be invoked on a concrete configuration, so programmers have to manually derive many configurations to ensure that the configuration-conditional parts of their code are checked. This tedious and error-prone process leaves many easy to find bugs undetected.#R##N##R##N#We propose an approach and tooling to systematically increase the configuration coverage (CC) in compile-time configurable system software. Our VAMPYR tool derives the required configurations and can be combined with existing static checkers to improve their results. With GCC as static checker, we thereby have found hundreds of issues in Linux v3.2, BUSYBOX, and L4/FIASCO, many of which went unnoticed for several years and have to be classified as serious bugs. Our resulting patches were accepted by the respective upstream developers."
1146511,15517,8385,On the efficiency of automated testing,2014,"The aim of automated program testing is to gain confidence about a program's correctness by sampling its input space. The sampling process can be either systematic or random. For every systematic testing technique the sampling is informed by the analysis of some program artefacts, like the specification, the source code (e.g., to achieve coverage), or even faulty versions of the program (e.g., mutation testing). This analysis incurs some cost. In contrast, random testing is unsystematic and does not sustain any analysis cost. In this paper, we investigate the theoretical efficiency of systematic versus random testing. First, we mathematically model the most effective systematic testing technique S_0 in which every sampled test input strictly increases the degree of confidence and is subject to the analysis cost c. Note that the efficiency of S_0 depends on c. Specifically, if we increase c, we also increase the time it takes S_0 to establish the same degree of confidence. So, there exists a maximum analysis cost beyond which R is generally more efficient than S_0. Given that we require the confidence that the program works correctly for x% of its input, we prove an upper bound on c of S_0, beyond which R is more efficient on the average. We also show that this bound depends asymptotically only on x. For instance, let R take 10ms time to sample one test input; to establish that the program works correctly for 90% of its input, S_0 must take less than 41ms to sample one test input. Otherwise, R is expected to establish the 90%-degree of confidence earlier. We prove similar bounds on the cost if the software tester is interested in revealing as many errors as possible in a given time span."
2083618,15517,23620,Meta-theory à la carte,2013,"Formalizing meta-theory, or proofs about programming languages, in a proof assistant has many well-known benefits. Unfortunately, the considerable effort involved in mechanizing proofs has prevented it from becoming standard practice. This cost can be amortized by reusing as much of existing mechanized formalizations as possible when building a new language or extending an existing one. One important challenge in achieving reuse is that the inductive definitions and proofs used in these formalizations are closed to extension. This forces language designers to cut and paste existing definitions and proofs in an ad-hoc manner and to expend considerable effort to patch up the results.   The key contribution of this paper is the development of an induction technique for extensible Church encodings using a novel reinterpretation of the universal property of folds. These encodings provide the foundation for a framework, formalized in Coq, which uses type classes to automate the composition of proofs from modular components. This framework enables a more structured approach to the reuse of meta-theory formalizations through the composition of modular inductive definitions and proofs.   Several interesting language features, including binders and general recursion, illustrate the capabilities of our framework. We reuse these features to build fully mechanized definitions and proofs for a number of languages, including a version of mini-ML. Bounded induction enables proofs of properties for non-inductive semantic functions, and mediating type classes enable proof adaptation for more feature-rich languages."
2013118,15517,23827,A requirements-based approach for the design of adaptive systems,2012,"Complexity is now one of the major challenges for the IT industry [1]. Systems might become too complex to be managed by humans and, thus, will have to be self-managed: Self-configure themselves for operation, self-protect from attacks, self-heal from errors and self-tune for optimal performance [2]. (Self-)Adaptive systems evaluate their own behavior and change it when the evaluation indicates that it is not accomplishing the software's purpose or when better functionality and performance are possible [3]. To that end, we need to monitor the behavior of the running system and compare it to an explicit formulation of requirements and domain assumptions [4]. Feedback loops (e.g., the MAPE loop [2]) constitute an architectural solution for this and, as proposed by past research [5], should be a first class citizen in the design of such systems. We advocate that adaptive systems should be designed this way from as early as Requirements Engineering and that reasoning over requirements is fundamental for run-time adaptation. We therefore propose an approach for the design of adaptive systems based on requirements and inspired in control theory [6]. Our proposal is goal-oriented and targets softwareintensive socio-technical systems [7], in an attempt to integrate control-loop approaches with decentralized agents inspired approaches [8]. Our final objective is a set of extensions to state-of-the-art goal-oriented modeling languages that allow practitioners to clearly specify the requirements of adaptive systems and a run-time framework that helps developers implement such requirements. In this 2-page abstract paper, we summarize this approach."
2473119,15517,23827,Active refinement of clone anomaly reports,2012,"Software clones have been widely studied in the recent literature and shown useful for finding bugs because inconsistent changes among clones in a clone group may indicate potential bugs. However, many inconsistent clone groups are not real bugs (true positives). The excessive number of false positives could easily impede broad adoption of clone-based bug detection approaches. In this work, we aim to improve the usability of clone-based bug detection tools by increasing the rate of true positives found when a developer analyzes anomaly reports. Our idea is to control the number of anomaly reports a user can see at a time and actively incorporate incremental user feedback to continually refine the anomaly reports. Our system first presents top few anomaly reports from the list of reports generated by a tool in its default ordering. Users then either accept or reject each of the reports. Based on the feedback, our system automatically and iteratively refines a classification model for anomalies and re-sorts the rest of the reports. Our goal is to present the true positives to the users earlier than the default ordering. The rationale of the idea is based on our observation that false positives among the inconsistent clone groups could share common features (in terms of code structure, programming patterns, etc.), and these features can be learned from the incremental user feedback. We evaluate our refinement process on three sets of clone-based anomaly reports from three large real programs: the Linux Kernel (C), Eclipse, and ArgoUML (Java), extracted by a clone-based anomaly detection tool. The results show that compared to the original ordering of bug reports, we can improve the rate of true positives found (i.e., true positives are found faster) by 11%, 87%, and 86% for Linux kernel, Eclipse, and ArgoUML, respectively."
656651,15517,11058,Reasoning about nondeterminism in programs,2013,"Branching-time temporal logics (e.g. CTL, CTL*, modal mu-calculus) allow us to ask sophisticated questions about the nondeterminism that appears in systems. Applications of this type of reasoning include planning, games, security analysis, disproving, precondition synthesis, environment synthesis, etc. Unfortunately, existing automatic branching-time verification tools have limitations that have traditionally restricted their applicability (e.g. push-down systems only, universal path quantifiers only, etc).   In this paper we introduce an automation strategy that lifts many of these previous restrictions. Our method works reliably for properties with non-trivial mixtures of universal and existential modal operators. Furthermore, our approach is designed to support (possibly infinite-state) programs.   The basis of our approach is the observation that existential reasoning can be reduced to universal reasoning if the system's state-space is appropriately restricted. This restriction on the state-space must meet a constraint derived from recent work on proving non-termination. The observation leads to a new route for implementation based on existing tools. To demonstrate the practical viability of our approach, we report on the results applying our preliminary implementation to a set of benchmarks drawn from the Windows operating system, the PostgreSQL database server, SoftUpdates patching system, as well as other hand-crafted examples."
1953735,15517,23620,Dynamic inference of static types for ruby,2011,"There have been several efforts to bring static type inference to object-oriented dynamic languages such as Ruby, Python, and Perl. In our experience, however, such type inference systems are extremely difficult to develop, because dynamic languages are typically complex, poorly specified, and include features, such as eval and reflection, that are hard to analyze.   In this paper, we introduce  constraint-based dynamic type inference , a technique that infers static types based on dynamic program executions. In our approach, we wrap each run-time value to associate it with a type variable, and the wrapper generates constraints on this type variable when the wrapped value is used. This technique avoids many of the often overly conservative approximations of static tools, as constraints are generated based on how values are used during actual program runs. Using wrappers is also easy to implement, since we need only write a constraint resolution algorithm and a transformation to introduce the wrappers. The best part is that we can eat our cake, too: our algorithm will infer sound types as long as it observes every path through each method body---note that the number of such paths may be dramatically smaller than the number of paths through the program as a whole.   We have developed Rubydust, an implementation of our algorithm for Ruby. Rubydust takes advantage of Ruby's dynamic features to implement wrappers as a language library. We applied Rubydust to a number of small programs and found it to be both easy to use and useful: Rubydust discovered 1 real type error, and all other inferred types were correct and readable."
1131466,15517,23827,How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms,2013,"Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results.     Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is ableto identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search."
1526509,15517,23865,Mining usage data and development artifacts,2012,"Software repository mining techniques generally focus on analyzing, unifying, and querying different kinds of development artifacts, such as source code, version control meta-data, defect tracking data, and electronic communication. In this work, we demonstrate how adding real-world usage data enables addressing broader questions of how software systems are actually used in practice, and by inference how development characteristics ultimately affect deployment, adoption, and usage. In particular, we explore how usage data that has been extracted from web server logs can be unified with product release history to study questions that concern both users' detailed dynamic behaviour as well as broad adoption trends across different deployment environments. To validate our approach, we performed a study of two open source web browsers: Firefox and Chrome. We found that while Chrome is being adopted at a consistent rate across platforms, Linux users have an order of magnitude higher rate of Firefox adoption. Also, Firefox adoption has been concentrated mainly in North America, while Chrome users appear to be more evenly distributed across the globe. Finally, we detected no evidence in age-specific differences in navigation behaviour among Chrome and Firefox users; however, we hypothesize that younger users are more likely to have more up-to-date versions than more mature users."
2334175,15517,23827,Development of auxiliary functions: should you be agile? an empirical assessment of pair programming and test-first programming,2012,"A considerable part of software systems is comprised of functions that support the main modules, such as array or string manipulation and basic math computation. These auxiliary functions are usually considered less complex, and thus tend to receive less attention from developers. However, failures in these functions might propagate to more critical modules, thereby affecting the system's overall reliability. Given the complementary role of auxiliary functions, a question that arises is whether agile practices, such as pair programming and test-first programming, can improve their correctness without affecting time-to-market. This paper presents an empirical assessment comparing the application of these agile practices with more traditional approaches. Our study comprises independent experiments of pair versus solo programming, and test-first versus test-last programming. The first study involved 85 novice programmers who applied both traditional and agile approaches in the development of six auxiliary functions within three different domains. Our results suggest that the agile practices might bring benefits in this context. In particular, pair programmers delivered correct implementations much more often, and test-first programming encouraged the production of larger and higher coverage test sets. On the downside, the main experiment showed that both practices significantly increase total development time. A replication of the test-first experiment with professional developers shows similar results."
1701155,15517,8385,Cachetor: detecting cacheable data to remove bloat,2013,"Modern object-oriented software commonly suffers from runtime bloat that significantly affects its performance and scalability. Studies have shown that one important pattern of bloat is the work repeatedly done to compute the same data values. Very often the cost of computation is very high and it is thus beneficial to memoize the invariant data values for later use. While this is a common practice in real-world development, manually finding invariant data values is a daunting task during development and tuning. To help the developers quickly find such optimization opportunities for performance improvement, we propose a novel run-time profiling tool, called Cachetor, which uses a combination of dynamic dependence profiling and value profiling to identify and report operations that keep generating identical data values. The major challenge in the design of Cachetor is that both dependence and value profiling are extremely expensive techniques that cannot scale to large, real-world applications for which optimizations are important. To overcome this challenge, we propose a series of novel abstractions that are applied to run-time instruction instances during profiling, yielding significantly improved analysis time and scalability. We have implemented Cachetor in Jikes Research Virtual Machine and evaluated it on a set of 14 large Java applications. Our experimental results suggest that Cachetor is effective in exposing caching opportunities and substantial performance gains can be achieved by modifying a program to cache the reported data."
2076197,15517,8385,Sample size vs. bias in defect prediction,2013,"Most empirical disciplines promote the reuse and sharing of datasets, as it leads to greater possibility of replication. While this is increasingly the case in Empirical Software Engineering, some of the most popular bug-fix datasets are now known to be biased. This raises two significant concerns: first, that sample bias may lead to underperforming prediction models, and second, that the external validity of the studies based on biased datasets may be suspect. This issue has raised considerable consternation in the ESE literature in recent years. However, there is a confounding factor of these datasets that has not been examined carefully: size. Biased datasets are sampling only some of the data that could be sampled, and doing so in a biased fashion; but biased samples could be smaller, or larger. Smaller data sets in general provide less reliable bases for estimating models, and thus could lead to inferior model performance. In this setting, we ask the question, what affects performance more, bias, or size? We conduct a detailed, large-scale meta-analysis, using simulated datasets sampled with bias from a high-quality dataset which is relatively free of bias. Our results suggest that size always matters just as much bias direction, and in fact much more than bias direction when considering information-retrieval measures such as AUCROC and F-score. This indicates that at least for prediction models, even when dealing with sampling bias, simply finding larger samples can sometimes be sufficient. Our analysis also exposes the complexity of the bias issue, and raises further issues to be explored in the future."
1925884,15517,9748,Automatic Parallelization of Tiled Loop Nests with Enhanced Fine-Grained Parallelism on GPUs,2012,"Automatically parallelizing loop nests into CUDA kernels must exploit the full potential of GPUs to obtain high performance. One state-of-the-art approach makes use of the polyhedral model to extract parallelism from a loop nest by applying a sequence of affine transformations to the loop nest. However, how to automate this process to exploit both intra and inter-SM parallelism for GPUs remains a challenging problem. Presently, compilers may generate code significantly slower than hand-optimized code for certain applications. This paper describes a compiler framework for tiling and parallelizing loop nests with uniform dependences into CUDA code. We aim to improve two levels of wave front parallelism. We find tiling hyper planes by embedding parallelism enhancing constraints in the polyhedral model to maximize intra-tile, i.e., intra-SM parallelism. This improves the load balance among the SPs in an SM executing a wave front of loop iterations within a tile. We eliminate parallelism-hindering false dependences to maximize inter-tile, i.e., inter-SM parallelism. This improves the load balance among the SMs executing a wave front of tiles. Our approach has been implemented in PLUTO and validated using eight benchmarks on two different NVIDIA GPUs (C1060 and C2050). Compared to PLUTO, our approach achieves 2 -- 5.5X speedups across the benchmarks. Compared to highly hand-optimized 1-D Jacobi (3 points), 2-D Jacobi (5 points), 3-D Jacobi (7 points) and 3-D Jacobi (27 points), our speedups, 1.17X, 1.41X, 0.97X and 0.87X with an average of 1.10X on C1060 and 1.24X, 1.20X, 0.86X and 0.95X with an average of 1.06X on C2050, are competitive."
1435179,15517,9856,VAMO: towards a fully automated malware clustering validity analysis,2012,"Malware clustering is commonly applied by malware analysts to cope with the increasingly growing number of distinct malware variants collected every day from the Internet. While malware clustering systems can be useful for a variety of applications, assessing the quality of their results is intrinsically hard. In fact, clustering can be viewed as an  unsupervised learning  process over a dataset for which the complete ground truth is usually not available. Previous studies propose to evaluate malware clustering results by leveraging the labels assigned to the malware samples by multiple anti-virus scanners (AVs). However, the methods proposed thus far require a (semi-)manual adjustment and mapping between labels generated by different AVs, and are limited to selecting a  reference sub-set  of samples for which an agreement regarding their labels can be reached across a majority of AVs. This approach may bias the reference set towards easy to cluster malware samples, thus potentially resulting in an overoptimistic estimate of the accuracy of the malware clustering results.   In this paper we propose VAMO, a system that provides a  fully automated  quantitative analysis of the validity of malware clustering results. Unlike previous work, VAMO  does not seek a majority voting-based consensus  across different AV labels, and does not discard the malware samples for which such a consensus cannot be reached. Rather, VAMO explicitly deals with the inconsistencies typical of multiple AV labels to build a more representative reference set, compared to majority voting-based approaches. Furthermore, VAMO avoids the need of a (semi-)manual mapping between AV labels from different scanners that was required in previous work. Through an extensive evaluation in a controlled setting and a real-world application, we show that VAMO outperforms majority voting-based approaches, and provides a better way for malware analysts to automatically assess the quality of their malware clustering results."
935129,15517,23827,Does scale really matter? ultra-large-scale systems seven years after the study (keynote),2013,"In 2006, Ultra-Large-Scale Systems: The Software Challenge of the Future (ISBN 0-9786956-0-7) documented the results of a year-long study on ultra-large, complex, distributed systems. Ultra-large-scale (ULS) systems are socio-technical ecosystems of ultra-large size on one or many dimensions number of lines of code; number of people employing the system for different purposes; amount of data stored, accessed, manipulated, and refined; number of connections and interdependencies among software components; number of hardware elements to which they interface. The characteristics of such systems require changes in traditional software development and management practices, which in turn require a new multi-disciplinary perspective and research. A carefully prescribed research agenda was suggested. What has happened since the study results were published? This talk shares a perspective on the post study reality --- a perspective based on research motivated by the study and direct experiences with ULS systems. Linda Northrop is director of the Research, Technology, and Systems Solution Program at the Software Engineering Institute (SEI) where she leads the work in architecture-centric engineering, software product lines, cyber-physical systems, advanced mobile systems, and ultra-large-scale systems. Linda is coauthor of the book Software Product Lines: Practices and Patterns and led the research group on ultra-large-scale systems that resulted in the book, Ultra-Large-Scale Systems: The Software Challenge of the Future. Before joining the SEI, she was associated with both the United States Air Force Academy and the State University of New York as professor of computer science, and with both Eastman Kodak and IBM as a software engineer. She is an SEI Fellow and an ACM Distinguished Member."
1049802,15517,8868,Generating analyses for detecting faults in path segments,2011,"Although static bug detectors are extensively applied, there is a cost in using them. One challenge is that static analysis often reports a large number of false positives but little diagnostic information. Also, individual bug detectors need to be built in response to new types of faults, and tuning a static tool for precision and scalability is time-consuming. This paper presents a novel frame-work that automatically generates scalable, interprocedural, path-sensitive analyses to detect user-specified faults. The framework consists of a specification technique that expresses faults and information needed for their detection, a scalable, path-sensitive algorithm, and a generator that unifies the two. The analysis produced identifies not only faults but also the path segments where the root causes of a fault are located. The generality of the framework is accomplished for both  data-  and  control-centric  faults. We implemented our framework and generated fault detectors for identifying buffer overflows, integer violations, null-pointer dereferences and memory leaks. We experimentally demonstrate that the generated analyses scales to large deployed software, and its detection capability is comparable to tools that target a specific type of fault. In our experiments, we identify a total of 146 faults of the four types. While the length of path segments for the majority of faults is 1--4 procedures, we are able to detect faults deeply embedded in the code across 35 procedures."
997200,15517,8806,A hybrid bug triage algorithm for developer recommendation,2013,"With a great number of software applications that have been developed, software maintenance has become an important and challenging task, particularly due to the increasing scale of software projects. Even if developers can create and update bug reports in bug repositories to support software maintenance, a large software project receives a large number of bug reports each day. For reducing the workload of developers, many researchers and software engineers have begun recommending appropriate developers to fix bugs. This process is called bug triage and is a hot research topic for software maintenance. In this paper, we propose a hybrid bug triage algorithm, combining a probability model and an experience model to rank all candidate developers for fixing a new bug. For this study, we adopted the smoothed Unigram Model (UM) instead of the traditional Vector Space Model (VSM) to search similar bug reports. In the probability model, we used a social network to analyze the probability of fixing a new bug for a candidate developer. We first proposed to add a new feature (the number of re-opened bugs) in order to get the fixing probability. In the experience model, we considered the number of fixed bugs and fixing cost for each candidate developer as the estimate factor. In addition, we introduced a new concept, activity factor, to better model developers' experience. We performed the experiments on two large-scale, open source projects. The results show that our method can effectively recommend the best developer for fixing bugs."
1739563,15517,9836,A data layout optimization framework for NUCA-based multicores,2011,"Future multicore architectures are likely to include a large number of cores connected using an on-chip network with Non-uniform Cache Access (NUCA). In such architectures, whether a data request is satisfied from a local cache or a remote cache can make an important difference. To exploit this NUCA property, prior research explored both architectural enhancements as well as compiler-based code optimization strategies. In this work, we take an alternate view, and explore data layout optimizations to improve locality of data accesses in a NUCA-based system. Our proposed approach includes three steps: array tiling, computation-to-core mapping, and layout customization. The first of these tries to identify the affinity between data and computation taking into account parallelization information, with the goal of minimizing remote accesses. The second step maps computations (and their associated data) to cores with the goal of minimizing average distance-to-data, and the last step further customizes the memory layout taking into account the data placement policy adopted by the underlying architecture. We evaluated the success of this three-step approach in enhancing on-chip cache behavior using all application programs from the SPECOMP suite on a full-system simulator. Our results show that the proposed approach improves on average data access latency and execution time by 24.7% and 18.4%, respectively, in the case of static NUCA, and 18.1% and 12.7%, respectively, in the case of dynamic NUCA."
446328,15517,11321,Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically,2013,"In a recent pioneering approach LDA was used to discover cross cutting concerns (CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed in (Williamson et al., 2010) for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models (STM). STM uses a generalized stick breaking process (GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics."
2436926,15517,23620,High-level separation logic for low-level code,2013,"Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs.   The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures.   We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection."
1472113,15517,11330,A new approach for performance analysis of openMP programs,2013,"The number of hardware threads is growing with each new generation of multicore chips; thus, one must effectively use threads to fully exploit emerging processors. OpenMP is a popular directive-based programming model that helps programmers exploit thread-level parallelism. In this paper, we describe the design and implementation of a novel performance tool for OpenMP. Our tool distinguishes itself from existing OpenMP performance tools in two principal ways. First, we develop a measurement methodology that attributes blame for work and inefficiency back to program contexts. We show how to integrate prior work on measurement methodologies that employ directed and undirected blame shifting and extend the approach to support dynamic thread-level parallelism in both time-shared and dedicated environments. Second, we develop a novel deferred context resolution method that supports online attribution of performance metrics to full calling contexts within an OpenMP program execution. This approach enables us to collect compact call path profiles for OpenMP program executions without the need for traces. Support for our approach is an integral part of an emerging standard performance tool application programming interface for OpenMP. We demonstrate the effectiveness of our approach by applying our tool to analyze four well-known application benchmarks that cover the spectrum of OpenMP features. In case studies with these benchmarks, insights from our tool helped us significantly improve the performance of these codes."
2229300,15517,23827,WitchDoctor: IDE support for real-time auto-completion of refactorings,2012,"Integrated Development Environments (IDEs) have come to perform a wide variety of tasks on behalf of the programmer, refactoring being a classic example. These operations have undeniable benefits, yet their large (and growing) number poses a cognitive scalability problem. Our main contribution is WitchDoctor -- a system that can detect, on the fly, when a programmer is hand-coding a refactoring. The system can then complete the refactoring in the background and propose it to the user long before the user can complete it. This implies a number of technical challenges. The algorithm must be 1) highly efficient, 2) handle unparseable programs, 3) tolerate the variety of ways programmers may perform a given refactoring, 4) use the IDE's proven and familiar refactoring engine to perform the refactoring, even though the the refactoring has already begun, and 5) support the wide range of refactorings present in modern IDEs. Our techniques for overcoming these challenges are the technical contributions of this paper. We evaluate WitchDoctor's design and implementation by simulating over 5,000 refactoring operations across three open-source projects. The simulated user is faster and more efficient than an average human user, yet WitchDoctor can detect more than 90% of refactoring operations as they are being performed -- and can complete over a third of refactorings before the simulated user does. All the while, WitchDoctor remains robust in the face of non-parseable programs and unpredictable refactoring scenarios. We also show that WitchDoctor is efficient enough to perform computation on a keystroke-by-keystroke basis, adding an average overhead of only 15 milliseconds per keystroke."
2385844,15517,11058,"Probabilistic, modular and scalable inference of typestate specifications",2011,"Static analysis tools aim to find bugs in software that correspond to violations of specifications. Unfortunately, for large and complex software, these specifications are usually either unavailable or sophisticated, and hard to write.   This paper presents ANEK, a tool and accompanying methodology for inferring specifications useful for modular typestate checking of programs. In particular, these specifications consist of pre and postconditions along with aliasing annotations known as access permissions. A novel feature of ANEK is that it can generate program specifications even when the code under analysis gives rise to conflicting constraints, a situation that typically occurs when there are bugs. The design of ANEK also makes it easy to add heuristic constraints that encode intuitions gleaned from several years of experience writing such specifications, and this allows it to infer specifications that are better in a subjective sense. The ANEK algorithm is based on a modular analysis that makes it fast and scalable, while producing reliable specifications. All of these features are enabled by its underlying probabilistic analysis that produces specifications that are very  likely .   Our implementation of ANEK infers access permissions specifications used by the PLURAL [5] modular typestate checker for Java programs. We have run ANEK on a number of Java benchmark programs, including one large open-source program(approximately 38K lines of code), to infer specifications that were then checked using PLURAL. The results for the large benchmark show that ANEK can quickly infer specifications that are both accurate and qualitatively similar to those written by hand, and at 5% of the time taken to manually discover and hand-code the specifications."
2432539,15517,11058,Optimal inference of fields in row-polymorphic records,2014,"Flexible records are a powerful concept in type systems that form the basis of, for instance, objects in dynamically typed languages. One caveat of using flexible records is that a program may try to access a record field that does not exist. We present a type inference algorithm that checks for these runtime errors. The novelty of our algorithm is that it satisfies a clear notion of completeness: The inferred types are optimal in the sense that type annotations cannot increase the set of typeable programs. Under certain assumptions, our algorithm guarantees the following stronger property: it rejects a program if and only if it contains a path from an empty record to a field access on which the field has not been added. We derive this optimal algorithm by abstracting a semantics to types. The derived inference rules use a novel combination of type terms and Boolean functions that retains the simplicity of unification-based type inference but adds the ability of Boolean functions to express implications, thereby addressing the challenge of combining implications and types. By following our derivation method, we show how various operations such as record concatenation and branching if a field exists lead to Boolean satisfiability problems of different complexity. Analogously, we show that more expressive type systems give rise to SMT problems. On the practical side, we present an implementation of the select and update operations and give practical evidence that these are sufficient in real-world applications."
1841149,15517,23620,Abstract acceleration of general linear loops,2014,"We present abstract acceleration techniques for computing loop invariants for numerical programs with linear assignments and conditionals. Whereas abstract interpretation techniques typically over-approximate the set of reachable states iteratively, abstract acceleration captures the effect of the loop with a single, non-iterative transfer function applied to the initial states at the loop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without restrictions. Its novelty lies in the use of the  Jordan normal form  decomposition of the loop body to derive symbolic expressions for the entries of the matrix modeling the effect of η ≥ Ο iterations of the loop. The entries of such a matrix depend on η through complex polynomial, exponential and trigonometric functions. Therefore, we introduces an  abstract domain for matrices  that captures the linear inequality relations between these complex expressions. This results in an abstract matrix for describing the fixpoint semantics of the loop.   Our approach integrates smoothly into standard abstract interpreters and can handle programs with nested loops and loops containing conditional branches. We evaluate it over small but complex loops that are commonly found in control software, comparing it with other tools for computing linear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and oscillatory behaviors that present challenges to existing approaches. Our approach finds non-trivial invariants to prove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches in terms of precision while exhibiting good performance."
2223622,15517,8385,A large scale study of programming languages and code quality in github,2014,"What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages."
2398830,15517,517,Search-Based Test Input Generation for String Data Types Using the Results of Web Queries,2012,"Generating realistic, branch-covering string inputs is a challenging problem, due to the diverse and complex types of real-world data that are naturally encodable as strings, for example resource locators, dates of different localised formats, international banking codes, and national identity numbers. This paper presents an approach in which examples of inputs are sought from the Internet by reformulating program identifiers into web queries. The resultant URLs are downloaded, split into tokens, and used to augment and seed a search-based test data generation technique. The use of the Internet as part of test input generation has two key advantages. Firstly, web pages are a rich source of valid inputs for various types of string data that may be used to improve test coverage. Secondly, the web pages tend to contain realistic, human-readable values, which are invaluable when test cases need manual confirmation due to the lack of an automated oracle. An empirical evaluation of the approach is presented, involving string input validation code from 10 open source projects. Well-formed, valid string inputs were retrieved from the web for 96% of the different string types analysed. Using the approach, coverage was improved for 75% of the Java classes studied by an average increase of 14%."
1084530,15517,8868,"Hybrid learning: interface generation through static, dynamic, and symbolic analysis",2013,"This paper addresses the problem of efficient generation of component interfaces through learning. Given a white-box component C with specified unsafe states, an interface captures safe orderings of invocations of C's public methods. In previous work we presented Psyco, an interface generation framework that combines automata learning with symbolic component analysis: learning drives the framework in exploring different combinations of method invocations, and symbolic analysis computes method guards corresponding to constraints on the method parameters for safe execution. In this work we propose solutions to the two main bottlenecks of Psyco. The explosion of method sequences that learning generates to validate its computed interfaces is reduced through partial order reduction resulting from a static analysis of the component. To avoid scalability issues associated with symbolic analysis, we propose novel algorithms that are primarily based on dynamic, concrete component execution, while resorting to symbolic analysis on a limited, as needed, basis. Dynamic execution enables the introduction of a concept of state matching, based on which our proposed approach detects, in some cases, that it has exhausted the exploration of all component behaviors. On the other hand, symbolic analysis is enhanced with symbolic summaries. Our new approach, X-Psyco, has been implemented in the Java PathFinder (JPF) software model checking platform. We demonstrated the effectiveness of X-Psyco on a number of realistic software components by generating more complete and precise interfaces than was previously possible."
1509559,15517,20524,Design and evaluation of the ModelHealth toolchain for continuity of care web services,2013,"Motivation: Systems interoperability is a key challenge in providing continuity of care to all patients. The challenge is addressed with information standards and new approaches to systems integration based on service-oriented architectures. Model-driven development promise utilities that are suitable for software service development in the healthcare domain, but development tools are still immature and their industry uptake is low. The knowledge about how model-driven development tools can become more useful to the healthcare software developers should be strengthened. Approach: This paper presents the ModelHealth toolchain that was created in four design/assess cycles, involving 28 students and 41 professional developers in the period 2007---2010. The toolchain provides design assistance for creating software services based on concepts from the CEN-13940 standard for continuity of care, which facilitates development of interoperable software services. Results: The CEN-13940 standard was successfully incorporated into the ModelHealth Toolchain assisting developers in creating software service design models that adhered to the standard. The developers expressed that improved understanding of the target system, documentation generation, and artifact traceability were essential utilities of the model-driven approach. Conclusion: The paper concludes healthcare domain knowledge can be successfully incorporated in a model-driven development toolchain, providing valuable input to the healthcare software service design process. A set of recommendations on how to incorporate domain specific concepts into model-driven development tools is provided. To our knowledge, no other scientific publications have reported from healthcare specific model-driven tool design and evaluations. Our recommendations extend and nuance existing knowledge on model-driven development tooling in general."
1429889,15517,8385,Learning to rank relevant files for bug reports using domain knowledge,2014,"When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects."
750143,15517,23827,Assisting developers of big data analytics applications when deploying on hadoop clouds,2013,"Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures."
2317947,15517,21022,Practical software model checking via dynamic interface reduction,2011,"Implementation-level software model checking explores the state space of a system implementation directly to find potential software defects without requiring any specification or modeling. Despite early successes, the effectiveness of this approach remains severely constrained due to poor scalability caused by state-space explosion. DeMeter makes software model checking more practical with the following contributions: (i) proposing  dynamic interface reduction , a new state-space reduction technique, (ii) introducing a framework that enables dynamic interface reduction in an existing model checker with a reasonable amount of effort, and (iii) providing the framework with a distributed runtime engine that supports parallel distributed model checking.   We have integrated DeMeter into two existing model checkers, MaceMC and MoDist, each involving changes of around 1,000 lines of code. Compared to the original MaceMC and MoDist model checkers, our experiments have shown state-space reduction from a factor of five to up to five orders of magnitude in representative distributed applications such as Paxos, Berkeley DB, Chord, and Pastry. As a result, when applied to a deployed Paxos implementation, which has been running in production data centers for years to manage tens of thousands of machines, DeMeter manages to explore  completely  a logically meaningful state space that covers both phases of the Paxos protocol, offering higher assurance of software reliability that was not possible before."
1757605,15517,8385,How do fixes become bugs,2011,"Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation.   This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8%--24.4% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process."
1185741,15517,8868,EnforceMOP: a runtime property enforcement system for multithreaded programs,2013,"Multithreaded programs are hard to develop and test. In order for programs to avoid unexpected concurrent behaviors at runtime, for example data-races, synchronization mechanisms are typically used to enforce a safe subset of thread interleavings. Also, to test multithreaded programs, devel- opers need to enforce the precise thread schedules that they want to test. These tasks are nontrivial and error prone.     This paper presents EnforceMOP, a framework for specifying and enforcing complex properties in multithreaded Java programs. A property is enforced at runtime by blocking the threads whose next actions would violate it. This way the remaining threads, whose execution is safe, can make global progress until the system eventually reaches a global state in which the blocked threads can be safely unblocked and allowed to execute. Users of EnforceMOP can specify the properties to be enforced using the expressive MOP multi-formalism notation, and can provide code to be executed at deadlock (when no thread is safe to continue). EnforceMOP was used in two different kinds of applications. First, to enforce general properties in multithreaded programs, as a formal, semantic alternative to the existing rigid and sometimes expensive syntactic synchronization mechanisms. Second, to enforce testing desirable schedules in unit testing of multithreaded programs, as an alternative to the existing limited and often adhoc techniques. Results show that EnforceMOP is able to effectively express and enforce complex properties and schedules in both scenarios."
1759764,15517,11058,Logical inference techniques for loop parallelization,2012,"This paper presents a fully automatic approach to loop parallelization that integrates the use of static and run-time analysis and thus overcomes many known difficulties such as nonlinear and indirect array indexing and complex control flow. Our hybrid analysis framework validates the parallelization transformation by verifying the independence of the loop's memory references. To this end it represents array references using the USR (uniform set representation) language and expresses the independence condition as an equation,  S =0, where  S  is a set expression representing array indexes. Using a language instead of an array-abstraction representation for  S  results in a smaller number of conservative approximations but exhibits a potentially-high runtime cost. To alleviate this cost we introduce a language translation  F  from the USR set-expression language to an equally rich language of predicates ( F ( S ) ==>  S  = 0). Loop parallelization is then validated using a novel logic inference algorithm that factorizes the obtained complex predicates (F( S )) into a sequence of sufficient independence conditions that are evaluated first statically and, when needed, dynamically, in increasing order of their estimated complexities. We evaluate our automated solution on 26 benchmarks from PERFECT-Club and SPEC suites and show that our approach is effective in parallelizing large, complex loops and obtains much better full program speedups than the Intel and IBM Fortran compilers."
1498204,15517,23620,Self-certification: bootstrapping certified typecheckers in F* with Coq,2012,"Well-established dependently-typed languages like Agda and Coq provide reliable ways to build and check formal proofs. Several other dependently-typed languages such as Aura, ATS, Cayenne, Epigram, F*, F7, Fine, Guru, PCML5, and Ur also explore reliable ways to develop and verify programs. All these languages shine in their own regard, but their implementations do not themselves enjoy the degree of safety provided by machine-checked verification. We propose a general technique called self-certification that allows a typechecker for a suitably expressive language to be certified for correctness. We have implemented this technique for F*, a dependently typed language on the .NET platform. Self-certification involves implementing a typechecker for F* in F*, while using all the conveniences F* provides for the compiler-writer (e.g., partiality, effects, implicit conversions, proof automation, libraries). This typechecker is given a specification (in~F*) strong enough to ensure that it computes valid typing derivations. We obtain a typing derivation for the core typechecker by running it on itself, and we export it to Coq as a type-derivation certificate. By typechecking this derivation (in Coq) and applying the F* metatheory (also mechanized in Coq), we conclude that our type checker is correct. Once certified in this manner, the F* typechecker is emancipated from Coq.   Self-certification leads to an efficient certification scheme---we no longer depend on verifying certificates in Coq---as well as a more broadly applicable one. For instance, the self-certified F* checker is suitable for use in adversarial settings where Coq is not intended for use, such as run-time certification of mobile code."
2132116,15517,23827,Teaching software process modeling,2013,"Most university curricula consider software processes to be on the fringes of software engineering (SE). Students are told there exists a plethora of software processes ranging from RUP over V-shaped processes to agile methods. Furthermore, the usual students' programming tasks are of a size that either one student or a small group of students can manage the work. Comprehensive processes being essential for large companies in terms of reflecting the organization structure, coordinating teams, or interfaces to business processes such as contracting or sales, are complex and hard to teach in a lecture, and, therefore, often out of scope. We experienced tutorials on using Java or C#, or on developing applications for the iPhone to gather more attention by students, simply speaking, as these are more fun for them. So, why should students spend their time in software processes? From our experiences and the discussions with a variety of industrial partners, we learned that students often face trouble when taking their first “real” jobs, even if the company is organized in a lean or agile shape. Therefore, we propose to include software processes more explicitly into the SE curricula. We designed and implemented a course at Master's level in which students learn why software processes are necessary, and how they can be analyzed, designed, implemented, and continuously improved. In this paper, we present our course's structure, its goals, and corresponding teaching methods. We evaluate the course and further discuss our experiences so that lecturers and researchers can directly use our lessons learned in their own curricula."
2031532,15517,10973,Temporal Specifications with Accumulative Values,2011,"There is recently a significant effort to add quantitative objectives to formal verification and synthesis. We introduce and investigate the extension of temporal logics with quantitative atomic assertions, aiming for a general and flexible framework for quantitative-oriented specifications. In the heart of quantitative objectives lies the accumulation of values along a computation. It is either the accumulated summation, as with the energy objectives, or the accumulated average, as with the mean-payoff objectives. We investigate the extension of temporal logics with the {\em prefix-accumulation assertions\/} $\Sum(v) \geq c$ and $\Avg(v) \geq c$, where $v$ is a numeric variable of the system, $c$ is a constant rational number, and $\Sum(v)$ and $\Avg(v)$ denote the accumulated sum and average of the values of $v$ from the beginning of the computation up to the current point of time. We also allow the {\em path-accumulation assertions\/} $\LimInfAvg(v)\geq c$ and $\LimSupAvg(v)\geq c$, referring to the average value along an entire computation. We study the border of decidability for extensions of various temporal logics. In particular, we show that extending the fragment of CTL that has only the EX, EF, AX, and AG temporal modalities by prefix-accumulation assertions and extending LTL with path-accumulation assertions, result in temporal logics whose model-checking problem is decidable. The extended logics allow to significantly extend the currently known energy and mean-payoff objectives. Moreover, the prefix-accumulation assertions may be refined with ``controlled-accumulation'', allowing, for example, to specify constraints on the average waiting time between a request and a grant. On the negative side, we show that the fragment we point to is, in a sense, the maximal logic whose extension with prefix-accumulation assertions permits a decidable model-checking procedure. Extending a temporal logic that has the EG or EU modalities, and in particular CTL and LTL, makes the problem undecidable."
1349012,15517,23620,A type system for borrowing permissions,2012,"In object-oriented programming, unique permissions to object references are useful for checking correctness properties such as consistency of typestate and noninterference of concurrency. To be usable, unique permissions must be  borrowed  --- for example, one must be able to read a unique reference out of a field, use it for something, and put it back. While one can null out the field and later reassign it, this paradigm is ungainly and requires unnecessary writes, potentially hurting cache performance. Therefore, in practice borrowing must occur in the type system, without requiring memory updates. Previous systems support borrowing with external alias analysis and/or explicit programmer management of  fractional permissions . While these approaches are powerful, they are also awkward and difficult for programmers to understand. We present an integrated language and type system with unique, immutable, and shared permissions, together with new  local permissions  that say that a reference may not be stored to the heap. Our system also includes  change permissions  such as unique>>unique and unique>>none that describe how permissions flow in and out of method formal parameters. Together, these features support common patterns of borrowing, including borrowing multiple local permissions from a unique reference and recovering the unique reference when the local permissions go out of scope, without any explicit management of fractions in the source language. All accounting of fractional permissions is done by the type system under the hood. We present the syntax and static and dynamic semantics of a formal core language and state soundness results. We also illustrate the utility and practicality of our design by using it to express several realistic examples."
2105431,15517,8235,Integrating code search into the development session,2011,"To support rapid and efficient software development, we propose to demonstrate our tool, integrating code search into software development process. For example, a developer, right during writing a module, can find a code piece sharing the same syntactic structure from a large code corpus representing the wisdom of other developers in the same team (or in the universe of open-source code). While there exist commercial code search engines on the code universe, they treat software as text (thus oblivious of syntactic structure), and fail at finding semantically related code. Meanwhile, existing tools, searching for syntactic clones, do not focus on efficiency, focusing on “post-mortem” usage scenario of detecting clones “after” the code development is completed. In clear contrast, we focus on optimizing efficiency for syntactic code search and making this search “interactive” for large-scale corpus, to complement the existing two lines of research. From our demonstration, we will show how such interactive search supports rapid software development, as similarly claimed lately in SE and HCI communities [1], [2]. As an enabling technology, we design efficient index building and traversal techniques, optimized for code corpus and code search workload. Our tool can identify relevant code in the corpus of 1.7 million code pieces in a sub-second response time, without compromising any accuracy obtained by a state-of-the-art tool, as we report our extensive evaluation results in [3]."
2385437,15517,8868,Combined static and dynamic automated test generation,2011,"In an object-oriented program, a unit test often consists of a sequence of method calls that create and mutate objects, then use them as arguments to a method under test. It is challenging to automatically generate sequences that are  legal  and  behaviorally-diverse , that is, reaching as many different program states as possible.   This paper proposes a combined static and dynamic automated test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests.   Our Palus tool implements this testing approach. We compared its effectiveness with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on several popular open-source Java programs. Tests generated by Palus achieved higher structural coverage and found more bugs.   Palus is also internally used in Google. It has found 22 previously-unknown bugs in four well-tested Google products."
645713,15517,517,A Call Graph Mining and Matching Based Defect Localization Technique,2013,"Locating defects in the source code of a software system is one of the most challenging tasks in software debugging. Defect localization tools aim to assist developers in finding the location of defects. Both static and dynamic analysis approaches are used. In the case of dynamic approaches, two different scenarios apply. The first is one in which we have multiple (different) executions that exhibit the faulty behavior. The second is one in which we have just a single faulty execution. This is the focus of this paper. In this paper, we present a novel technique for localization of structure-affecting defects (i.e., defects that make an execution diverge from the expected path, thus creating unexpected dynamic call graphs), using tree mining and tree matching techniques. Given a target system and a failing test case, the proposed technique finds methods in the source code of the system which are likely to have caused the failure (i.e., defective) or lie on the same path in the dynamic call tree representation of the failing execution as the defective methods, and thus can be used as a starting point to find the defective method(s). The proposed defect localization technique is implemented as a prototype and evaluated using four subject programs of various sizes, developed in Java or C. Our experiments show comparable results to similar defect localization tools, but unlike most of its counterparts, our technique does not require the availability of multiple failing executions to localize the defects. We believe that this is a major advantage, since it is often the case that we have only a single failing execution to work with."
1665974,15517,23827,RPC automation: making legacy code relevant,2013,"Due to the well-known issues with Remote Procedure Calls (RPC), the rather simple idea of modifying legacy applications---that have low spatial locality to the data they need to process---to execute all of their procedures via RPC is not a feasible option. A more realistic and feasible alternative is to provide a self-management mechanism that can dynamically monitor and alter the execution of an existing application by selectively modifying certain procedures to execute remotely when it is necessary to improve spatial locality. In this paper we describe the motivations behind such a self-management mechanism, and outline an initial design. In addition, we introduce our vision for the required profiling component of these applications. As such, we introduce the Automated Legacy system Remote Procedure Call mechanism (ALRPC). It automatically converts existing monolithic  C  applications into a distributed system semi-automatically. Thus automation is a key criterion for successfully competing with existing remote procedure tools for legacy applications and with newer solutions such as SOAP and REST [12], [21]. ALRPC is the core component to convert monolithic applications into distributable self-adaptive RPC systems. The empirical results collected from our initial experiments show that our mechanism's level of automation outperforms existing industry strength tools and improves development time. At the same time our mechanism is able to correctly function with a significant code base and shows acceptable performance in initial tests."
2302658,15517,517,Do System Test Cases Grow Old,2014,"Companies increasingly use either manual or automated system testing to ensure the quality of their software products. As a system evolves and is extended with new features the test suite also typically grows as new test cases are added. To ensure software quality throughout this process the test suite is continously executed, often on a daily basis. It seems likely that newly added tests would be more likely to fail than older tests but this has not been investigated in any detail on large-scale, industrial software systems. Also it is not clear which methods should be used to conduct such an analysis. This paper proposes three main concepts that can be used to investigate aging effects in the use and failure behaviour of system test cases: test case activation curves, test case hazard curves, and test case half-life. To evaluate these concepts and the type of analysis they enable we apply them on an industrial software system containing more than one million lines of code. The data sets comes from a total of 1,620 system test cases executed a total of more than half a million times over a time period of two and a half years. For the investigated system we find that system test cases stay active as they age but really do grow old, they go through an infant mortality phase with higher failure rates which then decline over time. The test case half-life is between 5 to 12 months for the two studied data sets."
777276,15517,23620,Pick your contexts well: understanding object-sensitivity,2011,"Object-sensitivity has emerged as an excellent context abstraction for points-to analysis in object-oriented languages. Despite its practical success, however, object-sensitivity is poorly understood. For instance, for a context depth of 2 or higher, past scalable implementations deviate significantly from the original definition of an object-sensitive analysis. The reason is that the analysis has many degrees of freedom, relating to which context elements are picked at every method call and object creation. We offer a clean model for the analysis design space, and discuss a formal and informal understanding of object-sensitivity and of how to create good object-sensitive analyses. The results are surprising in their extent. We find that past implementations have made a sub-optimal choice of contexts, to the severe detriment of precision and performance. We define a full-object-sensitive analysis that results in significantly higher precision, and often performance, for the exact same context depth. We also introduce type-sensitivity as an explicit approximation of object-sensitivity that preserves high context quality at substantially reduced cost. A type-sensitive points-to analysis makes an unconventional use of types as context: the context types are not dynamic types of objects involved in the analysis, but instead upper bounds on the dynamic types of their allocator objects. Our results expose the influence of context choice on the quality of points-to analysis and demonstrate type-sensitivity to be an idea with major impact: It decisively advances the state-of-the-art with a spectrum of analyses that simultaneously enjoy speed (several times faster than an analogous object-sensitive analysis), scalability (comparable to analyses with much less context-sensitivity), and precision (comparable to the best object-sensitive analysis with the same context depth)."
942959,15517,8385,Techniques for improving regression testing in continuous integration development environments,2014,"In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective. In a subsequent post-submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information -- two requirements for conducting testing cost-effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost-effectiveness improvements in the continuous integration process."
2011287,15517,23620,Fault tolerance via idempotence,2013,"Building distributed services and applications is challenging due to the pitfalls of distribution such as process and communication failures. A natural solution to these problems is to detect potential failures, and retry the failed computation and/or resend messages. Ensuring correctness in such an environment requires distributed services and applications to be idempotent.   In this paper, we study the inter-related aspects of process failures, duplicate messages, and idempotence. We first introduce a simple core language (based on lambda calculus inspired by modern distributed computing platforms. This language formalizes the notions of a service, duplicate requests, process failures, data partitioning, and local atomic transactions that are restricted to a single store.   We then formalize a desired (generic) correctness criterion for applications written in this language, consisting of idempotence (which captures the desired safety properties) and failure-freedom (which captures the desired progress properties).   We then propose language support in the form of a monad that automatically ensures failfree idempotence. A key characteristic of our implementation is that it is decentralized and does not require distributed coordination. We show that the language support can be enriched with other useful constructs, such as compensations, while retaining the coordination-free decentralized nature of the implementation.   We have implemented the idempotence monad (and its variants) in F# and C# and used our implementation to build realistic applications on Windows Azure. We find that the monad has low runtime overheads and leads to more declarative applications."
1512611,15517,8868,Are automated debugging techniques actually helping programmers,2011,"Debugging is notoriously difficult and extremely time consuming. Researchers have therefore invested a considerable amount of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to demonstrate their practical effectiveness. One common limitation of existing approaches, for instance, is their reliance on a set of strong assumptions on how developers behave when debugging ( e.g. , the fact that examining a faulty statement in isolation is enough for a developer to understand and fix the corresponding bug). In more general terms, most existing techniques just focus on selecting subsets of potentially faulty statements and ranking them according to some criterion. By doing so, they ignore the fact that understanding the root cause of a failure typically involves complex activities, such as navigating program dependencies and rerunning the program with different inputs. The overall goal of this research is to investigate how developers use and benefit from automated debugging tools through a set of human studies. As a first step in this direction, we perform a preliminary study on a set of developers by providing them with an automated debugging tool and two tasks to be performed with and without the tool. Our results provide initial evidence that several assumptions made by automated debugging techniques do not hold in practice. Through an analysis of the results, we also provide insights on potential directions for future work in the area of automated debugging."
1417231,15517,22288,Maitland: Lighter-Weight VM Introspection to Support Cyber-security in the Cloud,2012,"Despite defensive advances, malicious software (malware) remains an ever present cyber-security threat. Cloud environments are far from malware immune, in that: i) they innately support the execution of remotely supplied code, and ii) escaping their virtual machine (VM) confines has proven relatively easy to achieve in practice. The growing interest in clouds by industries and governments is also creating a core need to be able to formally address cloud security and privacy issues. VM introspection provides one of the core cyber-security tools for analyzing the run-time behaviors of code. Traditionally, introspection approaches have required close integration with the underlying hypervisors and substantial re-engineering when OS updates and patches are applied. Such heavy-weight introspection techniques, therefore, are too invasive to fit well within modern commercial clouds. Instead, lighter-weight introspection techniques are required that provide the same levels of within-VM observability but without the tight hypervisor and OS patch-level integration. This work introduces Maitland as a prototype proof-of-concept implementation a lighter-weight introspection tool, which exploits paravirtualization to meet these end-goals. The work assesses Maitland's performance, highlights its use to perform packer-independent malware detection, and assesses whether, with further optimizations, Maitland could provide a viable approach for introspection in commercial clouds."
2973802,15517,9748,Hydra: Efficient Detection of Multiple Concurrency Bugs on Fused CPU-GPU Architecture,2014,"Detecting concurrency bugs, such as data race, atomicity violation and order violation, is a cumbersome task for programmers. This situation is further being exacerbated due to the increasing number of cores in a single machine and the prevalence of threaded programming models. Unfortunately, many existing software-based approaches usually incur high runtime overhead or accuracy loss, while most hardware-based proposals usually focus on a specific type of bugs and thus are inflexible to detect a variety of concurrency bugs. In this paper, we propose Hydra, an approach that leverages massive parallelism and programmability of fused GPU architecture to simultaneously detect multiple types of concurrency bugs, including data race, atomicity violation and order violation. Hydra instruments and collects program behavior on CPU and transfers the traces to GPU for bug detection through on-chip interconnect. Furthermore, to achieve high speed, Hydra exploits bloom filter to filter out unnecessary detection traces. Hydra incurs small hardware complexity and requires no changes to internal critical-path processor components such as cache and its coherence protocol, and is with about 1.1% hardware overhead under a 32-core configuration. Experimental results show that Hydra only introduces about 0.35% overhead on average for detecting one type of bugs and 0.92% overhead for simultaneously detecting multiple bugs, yet with the similar detectability of a heavyweight software bug detector (e.g., Helgrind)."
1553424,15517,23827,Mitigating the obsolescence of specification models of service-based systems,2013,"Service-based systems (SBS) must be able to adapt their architectural configurations during runtime in order to keep satisfied their specification models. These models are the result of design time derivation of requirements into precise and verifiable specifications by using the knowledge about the current service offerings. Unfortunately, the design time knowledge may be no longer valid during runtime. Then, non- functional constraints may have different numerical meanings at different time even for the same observers. Thus, specification models become obsolete affecting the SBS’ capability of detecting requirement violations during runtime and therefore they trigger reconfigurations when appropriated. In order to mitigate the obsolescence of specification models, we propose to specify and verify them using the computing with words (CWW) methodology. First, non-functional properties (NFPs) of functionally-equivalent services are modeled as linguistic variables, whose domains are concepts or linguistic values instead of precise numbers. Second, architects specify at design time their requirements as linguistic decision models (LDMs) using these concepts. Third, during runtime, the CWW engine monitors the requirements satisfaction by the current chosen architectural configuration. And fourth, each time a global concept drift is detected in the NFPs of the services market, the numerical meanings are updated. Our initial results are encouraging, where our approach mitigates effectively and efficiently the obsolescence of the specification models used by SBS to drive their reconfigurations."
1257529,15517,23827,Situational awareness: personalizing issue tracking systems,2013,"Issue tracking systems play a central role in ongoing software development; they are used by developers to support collaborative bug fixing and the implementation of new features, but they are also used by other stakeholders including managers, QA, and end-users for tasks such as project management, communication and discussion, code reviews, and history tracking. Most such systems are designed around the central metaphor of the issue (bug, defect, ticket, feature, etc.), yet increasingly this model seems ill fitted to the practical needs of growing software projects; for example, our analysis of interviews with 20 Mozilla developers who use Bugzilla heavily revealed that developers face challenges maintaining a global understanding of the issues they are involved with, and that they desire improved support for situational awareness that is difficult to achieve with current issue management systems. In this paper we motivate the need for personalized issue tracking that is centered around the information needs of individual developers together with improved logistical support for the tasks they perform. We also describe an initial approach to implement such a system — extending Bugzilla — that enhances a developer's situational awareness of their working context by providing views that are tailored to specific tasks they frequently perform; we are actively improving this prototype with input from Mozilla developers."
2496412,15517,20524,Towards more accurate retrieval of duplicate bug reports,2011,"In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F - an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10 -- 27% relative improvement in recall rate@k and 17 -- 23% relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate@k of 37 -- 71% and mean average precision of 47%."
2503661,15517,23827,Precise identification of problems for structural test generation,2011,"An important goal of software testing is to achieve at least high structural coverage. To reduce the manual efforts of producing such high-covering test inputs, testers or developers can employ tools built based on automated structural test-generation approaches. Although these tools can easily achieve high structural coverage for simple programs, when they are applied on complex programs in practice, these tools face various problems, such as (1) the external-method-call problem (EMCP), where tools cannot deal with method calls to external libraries; (2) the object-creation problem (OCP), where tools fails to generate method-call sequences to produce desirable object states. Since these tools currently could not be powerful enough to deal with these problems in testing complex programs in practice, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. To reduce the efforts of developers in providing guidance to tools, in this paper, we propose a novel approach, called Covana, which precisely identifies and reports problems that prevent the tools from achieving high structural coverage primarily by determining whether branch statements containing notcovered branches have data dependencies on problem candidates. We provide two techniques to instantiate Covana to identify EMCPs and OCPs. Finally, we conduct evaluations on two open source projects to show the effectiveness of Covana in identifying EMCPs and OCPs."
2100871,15517,9856,Self-healing multitier architectures using cascading rescue points,2012,"Software bugs and vulnerabilities cause serious problems to both home users and the Internet infrastructure, limiting the availability of Internet services, causing loss of data, and reducing system integrity. Software self-healing using rescue points (RPs) is a known mechanism for recovering from unforeseen errors. However, applying it on multitier architectures can be problematic because certain actions, like transmitting data over the network, cannot be undone. We propose  cascading rescue points  (CRPs) to address the state inconsistency issues that can arise when using traditional RPs to recover from errors in interconnected applications. With CRPs, when an application executing within a RP transmits data, the remote peer is notified to also perform a checkpoint, so the communicating entities checkpoint in a coordinated, but loosely coupled way. Notifications are also sent when RPs successfully complete execution, and when recovery is initiated, so that the appropriate action is performed by remote parties. We developed a tool that implements CRPs by dynamically instrumenting binaries and transparently injecting notifications in the already established TCP channels between applications. We tested our tool with various applications, including the MySQL and Apache servers, and show that it allows them to successfully recover from errors, while incurring moderate overhead between 4.54% and 71.56%."
2048333,15517,23827,Predicting individual performance in student project teams,2011,"Due to the critical role of communication in project teams, capturing and analyzing developer design notes and conversations for use as performance predictors is becoming increasing important as software development processes become more asynchronous. Current prediction methods require human Subject Matter Experts (SME) to laboriously examine and rank user content along various categories such as participation and the information they express. SEREBRO is an integrated courseware tool that captures social and development artifacts automatically and provides real time rewards, in the form of badges and titles, indicating a user's progress towards predefined goals using a variety of automated assessment measures. The tool allows for instructor visualization, involvement, and feedback in the ongoing projects and provides avenues for the instructor to adapt or adjust project scope or individual role assignments based on past or current individual performance levels. This paper evaluates and compares the use of two automated SEREBRO measures with SME content-based analysis and work product grades as predictors of individual performance. Data is collected from undergraduate software engineering teams using SEREBRO, whose automated measures of content and contribution perform as well as SME ratings and grades to suggest individual performance can be predicted in real-time."
1424072,15517,11058,Input-sensitive profiling,2012,"In this paper we present a profiling methodology and toolkit for helping developers discover hidden asymptotic inefficiencies in the code. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produce performance plots and derive trend functions by statistical curve fitting or bounding techniques. A key feature of our method is the ability to automatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several case studies, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and characterize the workload and behavior of individual routines in the context of real applications. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimental evaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to other prominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for most algorithmically-critical routines."
1840325,15517,23876,Towards more accurate content categorization of API discussions,2014,"Nowadays, software developers often discuss the usage of various APIs in online forums. Automatically assigning pre-defined semantic categorizes to API discussions in these forums could help manage the data in online forums, and assist developers to search for useful information. We refer to this process as content categorization of API discussions. To solve this problem, Hou and Mo proposed the usage of naive Bayes multinomial, which is an effective classification algorithm.     In this paper, we propose a Cache-bAsed compoSitE algorithm, short formed as CASE, to automatically categorize API discussions. Considering that the content of an API discussion contains both textual description and source code, CASE has 3 components that analyze an API discussion in 3 different ways: text, code, and original. In the text component, CASE only considers the textual description; in the code component, CASE only considers the source code; in the original component, CASE considers the original content of an API discussion which might include textual description and source code. Next, for each component, since different terms (i.e., words) have different affinities to different categories, CASE caches a subset of terms which have the highest affinity scores to each category, and builds a classifier based on the cached terms. Finally, CASE combines all the 3 classifiers to achieve a better accuracy score. We evaluate the performance of CASE on 3 datasets which contain a total of 1,035 API discussions. The experiment results show that CASE achieves accuracy scores of 0.69, 0.77, and 0.96 for the 3 datasets respectively, which outperforms the state-of-the-art method proposed by Hou and Mo by 11%, 10%, and 2%, respectively."
745151,15517,8385,Grail: context-aware fixing of concurrency bugs,2014,"Writing efficient synchronization for multithreaded programs is notoriously hard. The resulting code often contains subtle concurrency bugs. Even worse, many bug fixes introduce new bugs. A classic example, seen widely in practice, is deadlocks resulting from fixing of an atomicity violation. These complexities have motivated the development of automated fixing techniques. Current techniques generate fixes that are typically conservative, giving up on available parallelism. Moreover, some of the techniques cannot guarantee the correctness of a fix, and may introduce deadlocks similarly to manual fix, whereas techniques that ensure correctness do so at the expense of even greater performance loss. We present Grail, a novel fixing algorithm that departs from previous techniques by simultaneously providing both correctness and optimality guarantees. Grail synthesizes bug-free yet optimal lock-based synchronization. To achieve this, Grail builds an analysis model of the buggy code that is both contextual, distinguishing different aliasing contexts to ensure efficiency, and global, accounting for the entire synchronization behavior of the involved threads to ensure correctness. Evaluation of Grail on 12 bugs from popular codebases confirms its practical advantages, especially compared with existing techniques: Grail patches are, in general, >=40% more efficient than the patches produced by other techniques, and incur only 2% overhead."
1267907,15517,9836,ATDetector: improving the accuracy of a commercial data race detector by identifying address transfer,2011,"In order to take advantage of multi-core hardware, more and more applications are becoming multi-threaded. Unfortunately concurrent programs are prone to bugs, such as data races. Recently much work has been devoted to detecting data races in multi-threaded programs. Most tools, however, require the accurate knowledge of synchronizations in the program, and may otherwise suffer from false positives in race detection, limiting their usability. To address this problem, some tools such as Intel ®  Inspector provide mechanisms for suppressing false positives and/or annotating synchronizations not automatically recognized by the tools. However, they require users' input or even changes of the source code. We took a different approach to address this problem. More specifically, we first used a state-of-the-art commercial data race detector, namely Intel ®  Inspector on 17 applications of various types including 5 servers, 5 client/desktop applications, and 7 scientific ones, without utilizing any suppression or annotation mechanisms provided by the product that need users' input. We examined a total of 1420 false data races and identified two major root causes including address transfer, where one thread passes memory address to another thread. We found more than 62% false data races were caused by address transfer. Based on this observation, we designed and implemented an algorithm that automatically identify address transfer and use the information to prune the false data races. Our evaluation with 8 real-world applications shows that it can effectively prune all false data races caused by unrecognized address transfers, without eliminating any true data race that was originally reported."
1731612,15517,11330,Scalable analysis of multicore data reuse and sharing,2014,"The performance and energy efficiency of multicore systems are increasingly dominated by the costs of communication. As hardware parallelism grows, developers require more powerful tools to assess the data sharing and reuse properties of their algorithms. The reuse distance is an effective metric to study the temporal locality of programs and model private and shared caches. But the application of this method is challenging. First, generating memory traces is very expensive in storage and very intrusive on execution, possibly distorting the parallel schedule. And second, the algorithm is computationally very expensive, limiting the length, memory size and parallelism of analyzable programs.   This paper introduces a novel coarse-grained reuse distance method, called Kernel Reuse Distance (KRD), which addresses these challenges. KRD enables a quick assessment of data locality by studying the reuse characteristics of the kernels' inputs and outputs. We analyze the performance of the initial prototype implementation and show two use cases comparing different parallel implementations. On a 24-core system, analyzing a trace from a matrix multiplication representing 24 threads, 1.37 terabytes of streamed data and 800 million distinct accesses, the parallel KRD implementation is able to compute the coherence-aware kernel reuse distance histogram for one socket (six cores) in 11.1 seconds."
1828198,15517,11058,Mostly-automated verification of low-level programs in computational separation logic,2011,"Several recent projects have shown the feasibility of verifying low-level systems software. Verifications based on automated theorem-proving have omitted reasoning about  first-class code pointers , which is critical for tasks like certifying implementations of threads and processes. Conversely, verifications that deal with first-class code pointers have featured long, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostly-automated proofs about programs with the full range of features needed to implement, e.g., language runtime systems.   The heart of our approach is in mostly-automated discharge of verification conditions inspired by separation logic. Our take on separation logic is  computational , in the sense that function specifications are usually written in terms of  reference implementations in a purely functional language . Logical quantifiers are the most challenging feature for most automated verifiers; by relying on functional programs (written in the expressive language of the Coq proof assistant), we are able to avoid quantifiers almost entirely. This leads to some dramatic improvements compared to both past work in classical verification, which we compare against with implementations of data structures like binary search trees and hash tables; and past work in verified programming with code pointers, which we compare against with examples like function memoization and a cooperative threading library."
1198327,15517,517,"Model-Based Testing of Video Conferencing Systems: Challenges, Lessons Learnt, and Results",2014,"Model-Based Testing (MBT) is a well-established and intense field of research in academia. It has attracted attention of many industries as it can be seen from many industrial experiences of MBT reported in the literature and availability of commercial and open source tools in recent years. The thorough and methodical approach of MBT facilitates automated testing with the intention of improving the quality of software systems. Every industrial application of MBT faces varied challenges depending on the application domain, the current testing practices and tools, and the type of testing. Reporting such challenges, their solutions, and lessons learnt provides a body of knowledge, which can direct practitioners of MBT for their future applications of MBT. With such aim in our mind, we present results from an MBT project that is being carried out for testing embedded video conferencing systems developed by Cisco Systems, Inc. Norway for the last several years. We present challenges faced while conducting MBT, our solutions, some of the key results, and lessons learnt from our experience. Our experience showed that search algorithms provide an efficient solution for test case selection and test data generation. In addition, aspect-oriented modeling provides a scalable modeling solution for non-functional testing. Finally, we learned that model transformation offers an elegant solution for developing a model-based test case generation tool. All of our results are based on a large number of rigorous empirical evaluations."
1396343,15517,11058,Maximal sound predictive race detection with control flow abstraction,2014,"Despite the numerous static and dynamic program analysis techniques in the literature, data races remain one of the most common bugs in modern concurrent software. Further, the techniques that do exist either have limited detection capability or are unsound, meaning that they report false positives. We present a sound race detection technique that achieves a provably higher detection capability than existing sound techniques. A key insight of our technique is the inclusion of abstracted control flow information into the execution model, which increases the space of the causal model permitted by classical happens-before or causally-precedes based detectors. By encoding the control flow and a minimal set of feasibility constraints as a group of first-order logic formulae, we formulate race detection as a constraint solving problem. Moreover, we formally prove that our formulation achieves the maximal possible detection capability for any sound dynamic race detector with respect to the same input trace under the sequential consistency memory model. We demonstrate via extensive experimentation that our technique detects more races than the other state-of-the-art sound race detection techniques, and that it is scalable to executions of real world concurrent applications with tens of millions of critical events. These experiments also revealed several previously unknown races in real systems (e.g., Eclipse) that have been confirmed or fixed by the developers. Our tool is also adopted by Eclipse developers."
1228055,15517,20524,Symbolic modular deadlock analysis,2011,"Methods in object-oriented concurrent libraries often encapsulate internal synchronization details. As a result of information hiding, clients calling the library methods may cause thread safety violations by invoking methods in an unsafe manner. This is frequently a cause of deadlocks. Given a concurrent library, we present a technique for inferring interface contracts that specify permissible concurrent method calls and patterns of aliasing among method arguments. In this work, we focus on deriving contracts that guarantee deadlock-free execution for the methods in the library. The contracts also help client developers by documenting required assumptions about the library methods. Alternatively, the contracts can be statically enforced in the client code to detect potential deadlocks in the client. Our technique combines static analysis with a symbolic encoding scheme for tracking lock dependencies, allowing us to synthesize contracts using a SMT solver. Additionally, we investigate extensions of our technique to reason about deadlocks in libraries that employ signaling primitives such as wait-notify for cooperative synchronization. Our prototype tool analyzes over a million lines of code for some widely-used Java libraries within an hour, thus demonstrating its scalability and efficiency. Furthermore, the contracts inferred by our approach have been able to pinpoint real deadlocks in clients, i.e. deadlocks that have been a part of bug-reports filed by users and developers of client code."
1825077,15517,517,EFindBugs: Effective Error Ranking for FindBugs,2011,"Static analysis tools have been widely used to detect potential defects without executing programs. It helps programmers raise the awareness about subtle correctness issues in the early stage. However, static defect detection tools face the high false positive rate problem. Therefore, programmers have to spend a considerable amount of time on screening out real bugs from a large number of reported warnings, which is time-consuming and inefficient. To alleviate the above problem during the report inspection process, we present EFindBugs to employ an effective two-stage error ranking strategy that suppresses the false positives and ranks the true error reports on top, so that real bugs existing in the programs could be more easily found and fixed by the programmers. In the first stage, EFindBugs initializes the ranking by assigning predefined defect likelihood for each bug pattern and sorting the error reports by the defect likelihood in descending order. In the second stage, EFindbugs optimizes the initial ranking self-adaptively through the feedback from users. This optimization process is executed automatically and based on the correlations among error reports with the same bug pattern. Our experiment on three widely-used Java projects (AspectJ, Tomcat, and Axis) shows that our ranking strategy outperforms the original ranking in Find Bugs in terms of precision, recall and F1-score."
974888,15517,8868,Orthogonal exploration of the search space in evolutionary test case generation,2013,"The effectiveness of evolutionary test case generation based on Genetic Algorithms (GAs) can be seriously impacted by genetic drift, a phenomenon that inhibits the ability of such algorithms to effectively diversify the search and look for alternative potential solutions. In such cases, the search becomes dominated by a small set of similar individuals that lead GAs to converge to a sub-optimal solution and to stagnate, without reaching the desired objective. This problem is particularly common for hard-to-cover program branches, associated with an extremely large solution space. In this paper, we propose an approach to solve this problem by integrating a mechanism for orthogonal exploration of the search space into standard GA. The diversity in the population is enriched by adding individuals in orthogonal directions, hence providing a more effective exploration of the solution space. To the best of our knowledge, no prior work has addressed explicitly the issue of evolution direction based diversification in the context of evolutionary testing. Results achieved on 17 Java classes indicate that the proposed enhancements make GA much more effective and efficient in automating the testing process. In particular, effectiveness (coverage) was significantly improved in 47% of the subjects and efficiency (search budget consumed) was improved in 85% of the subjects on which effectiveness remains the same."
2314737,15517,23827,Value-based program characterization and its application to software plagiarism detection,2011,"Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various  automated  code transformation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (1) most of them cannot handle advanced obfuscation techniques; (2) the methods based on source code analysis are less practical since the source code of suspicious programs is typically not available until strong evidences are collected; and (3) those depending on the features of specific operating systems or programming languages have limited applicability.   Based on an observation that some critical runtime values are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by  SandMark , plagiarisms heavily obfuscated by  KlassMaster , programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo."
1099819,15517,8385,Solving complex path conditions through heuristic search on induced polytopes,2014,"Test input generators using symbolic and concolic execution must solve path conditions to systematically explore a program and generate high coverage tests. However, path conditions may contain complicated arithmetic constraints that are infeasible to solve: a solver may be unavailable, solving may be computationally intractable, or the constraints may be undecidable. Existing test generators either simplify such constraints with concrete values to make them decidable, or rely on strong but incomplete constraint solvers. Unfortunately, simplification yields coarse approximations whose solutions rarely satisfy the original constraint. Moreover, constraint solvers cannot handle calls to native library methods. We show how a simple combination of linear constraint solving and heuristic search can overcome these limitations. We call this technique Concolic Walk. On a corpus of 11 programs, an instance of our Concolic Walk algorithm using tabu search generates tests with two- to three-times higher coverage than simplification-based tools while being up to five-times as efficient. Furthermore, our algorithm improves the coverage of two state-of-the-art test generators by 21% and 32%. Other concolic and symbolic testing tools could integrate our algorithm to solve complex path conditions without having to sacrifice any of their own capabilities, leading to higher overall coverage."
1826692,15517,23620,Symbolic finite state transducers: algorithms and applications,2012,"Finite automata and finite transducers are used in a wide range of applications in software engineering, from regular expressions to specification languages. We extend these classic objects with symbolic alphabets represented as parametric theories. Admitting potentially infinite alphabets makes this representation strictly more general and succinct than classical finite transducers and automata over strings. Despite this, the main operations, including composition, checking that a transducer is single-valued, and equivalence checking for single-valued symbolic finite transducers are effective given a decision procedure for the background theory. We provide novel algorithms for these operations and extend composition to symbolic transducers augmented with registers. Our base algorithms are unusual in that they are nonconstructive, therefore, we also supply a separate model generation algorithm that can quickly find counterexamples in the case two symbolic finite transducers are not equivalent. The algorithms give rise to a complete decidable algebra of symbolic transducers. Unlike previous work, we do not need any syntactic restriction of the formulas on the transitions, only a decision procedure. In practice we leverage recent advances in satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on four case studies, covering a wide range of applications. Our techniques can synthesize string pre-images in excess of 8,000 bytes in roughly a minute, and we find that our new encodings significantly outperform previous techniques in succinctness and speed of analysis."
1455024,15517,23827,A study of enabling factors for rapid fielding: combined practices to balance speed and stability,2013,"Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practices - Agile, architecture, or other - in creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge."
912716,15517,23827,An automated approach to detect violations with high confidence in incremental code using a learning system,2014,"Static analysis (SA) tools are often used to analyze a software system to identify violation of good programming practices (such as not validating arguments to public methods, use of magic numbers etc.) and potential defects (such as misused APIs, race conditions, deadlocks etc.). Most widely used SA tools perform shallow data flow analysis with the results containing considerable number of False Positives (FPs) and False Negatives (FNs). Moreover it is difficult to run these tools only on newly added or modified piece of code. In order to determine which violations are new we need to perform tedious process of post processing the SA tool results. The proposed system takes into consideration the above mentioned issues of SA and provides a lightweight approach to detection of coding violations statically and proactively, with high degree of confidence using a learning system. It also identifies the violations with a quality perspective using the predefined mapping of violations to quality attributes. We successfully implemented a prototype of the system and studied its use across some of the projects in Siemens, Corporate Technology, Development Center, Asia Australia (CT DC AA). Experimental results showed significant reduction in time required in result analysis and also in FPs and FNs reported."
2478591,15517,23827,"Ownership, experience and defects: a fine-grained study of authorship",2011,"Recent research indicates that people factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a file might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a file might lead to more defects. Prior research considered this phenomenon at the level of modules or files, and thus does not tease apart and study the effect of contributions of different developers to each module or file. We exploit a modern version control system to  examine this issue at a fine-grained level . Using version history, we examine contributions to code fragments that are actually repaired to fix bugs. Are these code fragments  implicated  in bugs the result of contributions from many? or from one? Does experience matter? What type of experience? We find that implicated code is more strongly associated with a single developer's contribution; our findings also indicate that an author's specialized experience in the target file is more important than general experience. Our findings suggest that quality control efforts could be profitably targeted at changes made by single developers with limited prior experience on that file."
2670215,15517,8868,Declarative mocking,2013,"Test-driven methodologies encourage testing early and often. Mock objects support this approach by allowing a component to be tested before all depended-upon components are available. Today mock objects typically reflect little to none of an object's intended functionality, which makes it difficult and error-prone for developers to test rich properties of their code. This paper presents declarative mocking, which enables the creation of expressive and reliable mock objects with relatively little effort. In our approach, developers write method specifications in a high-level logical language for the API being mocked, and a constraint solver dynamically executes these specifications when the methods are invoked. In addition to mocking functionality, this approach seamlessly allows data and other aspects of the environment to be easily mocked. We have implemented the approach as an extension to an existing tool for executable specifications in Java called PBnJ. We have performed an exploratory study of declarative mocking on several existing Java applications, in order to understand the power of the approach and to categorize its potential benefits and limitations. We also performed an experiment to port the unit tests of several open-source applications from a widely used mocking library to PBnJ. We found that more than half of these unit tests can be enhanced, in terms of the strength of properties and coverage, by exploiting executable specifications, with relatively little additional developer effort."
2441840,15517,23620,Symbolic optimization with SMT solvers,2014,"The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities.   In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for  objective functions  in the theory of linear real arithmetic (LRA). Given a formula φ and an objective function  t , SYMBA finds a satisfying assignment of φthat maximizes the value of  t . SYMBA utilizes efficient SMT solvers as black boxes. As a result, it is easy to implement and it directly benefits from future advances in SMT solvers. Moreover, SYMBA can optimize a set of objective functions, reusing information between them to speed up the analysis. We have implemented SYMBA and evaluated it on a large number of optimization benchmarks drawn from program analysis tasks. Our results indicate the power and efficiency of SYMBA in comparison with competing approaches, and highlight the importance of its multi-objective-function feature."
1103747,15517,20524,Improving the accuracy of oracle verdicts through automated model steering,2014,"The oracle - a judge of the correctness of the system under test (SUT) - is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as oracles. These models, however, typically represent an  idealized  system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the oracle.   We propose an automated  steering  framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This  model steering  is limited by a set of constraints (defining acceptable differences in behavior) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavior differences, while preventing future mismatches, by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, development costs."
844919,15517,8868,Identifying optimal trade-offs between CPU time usage and temporal constraints using search,2014,"Integration of software from different sources is a critical activity in many embedded systems across most industry sectors. Software integrators are responsible for producing reliable systems that fulfil various functional and performance requirements. In many situations, these requirements inversely impact one another. In particular, embedded system integrators often need to make compromises regarding some of the functional system properties to optimize the use of various resources, such as CPU time. In this paper, motivated by challenges faced by industry, we introduce a multi-objective decision support approach to help balance the minimization of CPU time usage and the satisfaction of temporal constraints in automotive systems. We develop a multi-objective, search-based optimization algorithm, specifically designed to work for large search spaces, to identify optimal trade-off solutions fulfilling these two objectives. We evaluated our algorithm by applying it to a large automotive system. Our results show that our algorithm can find solutions that are very close to the estimated ideal optimal values, and further, it finds significantly better solutions than a random strategy while being faster. Finally, our approach efficiently identifies a large number of diverse solutions, helping domain experts and other stakeholders negotiate the solutions to reach an agreement."
1770524,15517,9436,Requirements negotiation for multilayer system components,2011,"Current software systems are hybrid in nature. They are built by integrating third party Off-The-Shelf (OTS) components with preexisting legacy and bespoke custom-made software. In such systems, components are usually arranged into layers (e.g., hardware platform, operating systems and database layers, among others) to allow for their operation and interaction. Although several methods have been proposed to support OTS component selection, the truth is that in many cases the process is driven by political and other non-technical aspects, considering components as independent and isolated. Because of this, relevant stakeholder's requirements and concerns, as well as the implications that the selection of a particular component may bring to the system architecture, are simply ignored. In the worst case this may lead to the selection of unsuited or inappropriate components and eventually to miscarried projects, but more often to situations in which projects froze due to lack of stakeholders' agreement in relation to the newly created architectural scenario and some of its emerging requirements. In this paper we address these issues and present a proposal that uses software quality models as framework to support the negotiation of both initial and emerging requirements and the reconciliation of stakeholders' concerns. The approach considers components at different layers of system's architecture. The advantages of the proposal are illustrated with a real case conducted in a banking organization."
1029852,15517,20524,"Adaptable, model-driven security engineering for SaaS cloud-based applications",2014,"Software-as-a-service (SaaS) multi-tenancy in cloud-based applications helps service providers to save cost, improve resource utilization, and reduce service customization and maintenance time. This is achieved by sharing of resources and service instances among multiple tenants of the cloud-hosted application. However, supporting multi-tenancy adds more complexity to SaaS applications required capabilities. Security is one of these key requirements that must be addressed when engineering multi-tenant SaaS applications. The sharing of resources among tenants--i.e. multi-tenancy--increases tenants' concerns about the security of their cloud-hosted assets. Compounding this, existing traditional security engineering approaches do not fit well with the multi-tenancy application model where tenants and their security requirements often emerge after the applications and services were first developed. The resultant applications do not usually support diverse security capabilities based on different tenants' needs, some of which may change at run-time i.e. after cloud application deployment. We introduce a novel model-driven security engineering approach for multi-tenant, cloud-hosted SaaS applications. Our approach is based on externalizing security from the underlying SaaS application, allowing both application/service and security to evolve at runtime. Multiple security sets can be enforced on the same application instance based on different tenants' security requirements. We use abstract models to capture service provider and multiple tenants' security requirements and then generate security integration and configurations at runtime. We use dependency injection and dynamic weaving via Aspect-Oriented Programming (AOP) to integrate security within critical application/service entities at runtime. We explain our approach, architecture and implementation details, discuss a usage example, and present an evaluation of our approach on a set of open source web applications."
1608638,15517,8806,Comparative stability of cloned and non-cloned code: an empirical study,2012,"Code cloning is a controversial software engineering practice due to contradictory claims regarding its effect on software maintenance. Code stability is a recently introduced measurement technique that has been used to determine the impact of code cloning by quantifying the changeability of a code region. Although most of the existing stability analysis studies agree that cloned code is more stable than non-cloned code, the studies have two major flaws: (i) each study only considered a single stability measurement (e.g., lines of code changed, frequency of change, age of change); and, (ii) only a small number of subject systems were analyzed and these were of limited variety.   In this paper, we present a comprehensive empirical study on code stability using three different stability measuring methods. We use a recently introduced hybrid clone detection tool, NiCAD, to detect the clones and analyze their stability in four dimensions: by clone type, by measuring method, by programming language, and by system size and age. Our four-dimensional investigation on 12 diverse subject systems written in three programming languages considering three clone types reveals that: (i) Type-1 and Type-2 clones are unstable, but Type-3 clones are not; (ii) clones in Java and C systems are not as stable as clones in C# systems; (iii) a system's development strategy might play a key role in defining its comparative code stability scenario; and, (iv) cloned and non-cloned regions of a subject system do not follow a consistent change pattern."
1722042,15517,10973,Fibred Data Types,2013,"Data types are undergoing a major leap forward in their sophistication driven by a conjunction of i) theoretical advances in the foundations of data types, and ii) requirements of programmers for ever more control of the data structures they work with. In this paper we develop a theory of indexed data types where, crucially, the indices are generated inductively at the same time as the data. In order to avoid commitment to any specific notion of indexing we take an axiomatic approach to such data types using fibrations - thus giving us a theory of what we call fibred data types. The genesis of these fibred data types can be traced within the literature, most notably to Dybjer and Setzer's introduction of the concept of induction-recursion. This paper, while drawing heavily on their seminal work for inspiration, gives a categorical reformulation of Dybjer and Setzer's original work which leads to a large number of extensions of induction-recursion. Concretely, the paper provides i) conceptual clarity as to what induction-recursion fundamentally is about, ii) greater expressiveness in allowing not just the inductive-recursive definition of families of sets, or even indexed families of sets, but rather the inductive-recursive definition of a whole host of other structures, iii) a semantics for induction-recursion based not on the specific model of families, but rather an axiomatic model based upon fibrations which therefore encompasses diverse structures (domain theoretic, realisability, games etc) arising in the semantics of programming languages, and iv) technical justification as to why these fibred data types exist using large cardinals from set theory."
922174,15517,9436,Requirements development and management of embedded real-time systems,2014,"It is well recognized that most of the anomalies, discovered in the development of embedded real-time systems, belong to requirement and specification phases. To ease the situation, many efforts have been investigated into the area. For requirements development, especially requirements validation and verification, model-driven architecture techniques can be considered as a cost-efficient solution. In order to utilize such advantages, the design of the proposed system is often specified in terms of analyzable models at the certain level of abstraction. Further, different levels of requirements are translated into verifiable queries and fed into the models to be either validated or verified. For requirements management, requirements traceability provides critical support for performing change impact analysis, risk analysis, regression testing, etc. In this thesis, we cover several topics about requirements validation, requirements verification, and requirements traceability. In particular, the technical contributions are three-fold: 1) we propose an approach to requirements validation by using the extended Timed Abstract State Machine (TASM) language with newly defined TASM constructs and, 2) we present a simulation-based method which is powered up by statistical techniques to conduct requirements verification, working with industrial applications and, 3) we introduce an improved VSM-based requirements traceability recovery approach using a novel context analysis. Further, we have demonstrated the applicability of our contributions in real world usage through various case studies."
729594,15517,11058,P: safe asynchronous event-driven programming,2013,"We describe the design and implementation of P, a domain-specific language to write asynchronous event driven code. P allows the programmer to specify the system as a collection of interacting state machines, which communicate with each other using events. P unifies modeling and programming into one activity for the programmer. Not only can a P program be compiled into executable code, but it can also be tested using model checking techniques. P allows the programmer to specify the environment, used to close the system during testing, as nondeterministic ghost machines. Ghost machines are erased during compilation to executable code; a type system ensures that the erasure is semantics preserving.   The P language is designed so that a P program can be checked for responsiveness---the ability to handle every event in a timely manner. By default, a machine needs to handle every event that arrives in every state. But handling every event in every state is impractical. The language provides a notion of deferred events where the programmer can annotate when she wants to delay processing an event. The default safety checker looks for presence of unhandled events. The language also provides default liveness checks that an event cannot be potentially deferred forever.   P was used to implement and verify the core of the USB device driver stack that ships with Microsoft Windows 8. The resulting driver is more reliable and performs better than its prior incarnation (which did not use P); we have more confidence in the robustness of its design due to the language abstractions and verification provided by P."
1929887,15517,20524,Re-founding software engineering – SEMAT at the age of three (keynote abstract),2012,"Software engineering is gravely hampered by immature practices. Specific problems include: The prevalence of fads more typical of the fashion industry than an engineering discipline; a huge number of methods and method variants, with differences little understood and artificially magnified; the lack of credible experimental evaluation and validation; and the split between industry practice and academic research. At the root of the problems we lack a sound, widely accepted theoretical basis. A prime example of such a basis is Maxwell&#x2019;s equations in electrical engineering. It is difficult to fathom what electrical engineering would be today without those four concise equations. They are a great example to the statement &#x201C;There is nothing so practical as a good theory&#x201D;. In software engineering we have nothing similar, and there is widespread doubt whether it is needed. This talk will argue for the need of a basic theory in software engineering, a theory identifying its pure essence, its common ground or its kernel. The Semat (Software Engineering Methods and Theory) community addresses this huge challenge. It supports a process to refound software engineering based on a kernel of widely-agreed elements, extensible for specific uses, addressing both technology and people issues. This kernel represents the essence of software engineering. This talk promises to make you see the light in the tunnel."
702541,15517,517,Kepler -- Raising Browser Security Awareness,2013,"Web browser security is lacking and browsers are often unable to detect what is unwanted traffic, like contacting tracking sites. Without our direct knowledge, we are exposed to different kinds of security risks during web browsing. Browsing the web causes our browser to render a web page from HTML, CSS and JavaScript but many things that we cannot see happens during this. The web page may have saved data on our computer, tracked sites we have visited, sent us unwanted advertisements and maybe even executed a malicious script. This triggers the need for users to be able to determine themselves which risk they are willing to take. To improve browsing security, third party developers are developing browser extensions. By giving the users detailed information about web requests, we raise awareness and help users make decisions themselves about which websites are secure. To this end, a prototype for displaying this information was developed. The prototype is a Google Chrome extension which gathers and displays detailed information about web sites' activities in real-time. With the extension we are able to inspect every request made by the browser in detail. We can also filter the data according to country. The data is presented in a human readable form using geolocation. With the help of this prototype, users' awareness of web browsing security is raised in an informative and interesting way."
2330250,15517,339,SCRIPTGARD: automatic context-sensitive sanitization for large-scale legacy web applications,2011,"We empirically analyzed sanitizer use in a shipping web ap- plication with over 400,000 lines of code and over 23,244 methods, the largest empirical analysis of sanitizer use of which we are aware. Our analysis reveals two novel classes of errors: context-mismatched sanitization and inconsistent multiple sanitization. Both of these arise not because sanitizers are incorrectly implemented, but rather because they are not placed in code correctly. Much of the work on crosssite scripting detection to date has focused on finding missing sanitizers in programs of average size. In large legacy applications, other sanitization issues leading to cross-site scripting emerge. To address these errors, we propose ScriptGard, a system for ASP.NET applications which can detect and repair the incorrect placement of sanitizers. ScriptGard serves both as a testing aid to developers as well as a runtime mitigation technique. While mitigations for cross site scripting attacks have seen intense prior research, we consider both server and browser context, none of them achieve the same degree of precision, and many other mitigation techniques require major changes to server side code or to browsers. Our approach, in contrast, can be incrementally retrofitted to legacy systems with no changes to the source code and no browser changes. With our optimizations, when used for mitigation, ScriptGard incurs virtually no statistically significant overhead."
1210600,15517,20524,BOOM: Experiences in language and tool design for distributed systems (keynote),2013,"With the rapid expansion of cloud infrastructure and mobile devices, distributed systems have quickly emerged as a dominant computing platform. Distributed systems bring significant complexity to programming, due to platform issues including asynchrony, concurrency, and partial failure. Meanwhile, scalable distributed infrastructure—notably “NoSQL” systems—have put additional burdens on programmers by sacrificing traditional infrastructure contracts like linearizable or transactional I/O in favor of high availability. A growing segment of the developer community needs to deal with these issues today, and for the most part developers are still using languages and tools designed for sequential computation on tightly coupled architectures. This has led to software that is increasingly hard to test and hard to trust. Over the past 5 years, the BOOM project at Berkeley has focused on making it easier to write correct and maintainable code for distributed systems. Our work has taken a number of forms, including the development of the Bloom programming language for distributed systems, tools for testing and checking distributed programs, and the CALM Theorem, which connects programmer level concerns of determinism to system-level concerns about the need for distributed coordination. This talk will reflect on this work, and highlight opportunities for improved collaboration between the software engineering and distributed systems research communities."
2318724,15517,11058,Automating formal proofs for reactive systems,2014,"Implementing systems in proof assistants like Coq and proving their correctness in full formal detail has consistently demonstrated promise for making extremely strong guarantees about critical software, ranging from compilers and operating systems to databases and web browsers. Unfortunately, these verifications demand such heroic manual proof effort, even for a  single  system, that the approach has not been widely adopted.   We demonstrate a technique to eliminate the manual proof burden for verifying many properties within an entire  class  of applications, in our case reactive systems, while only expending effort comparable to the manual verification of a single system. A crucial insight of our approach is simultaneously designing both (1) a domain-specific language (DSL) for expressing reactive systems and their correctness properties and (2) proof automation which exploits the constrained language of both programs and properties to enable fully automatic, pushbutton verification. We apply this insight in a deeply embedded Coq DSL, dubbed Reflex, and illustrate Reflex's expressiveness by implementing and automatically verifying realistic systems including a modern web browser, an SSH server, and a web server. Using Reflex radically reduced the proof burden: in previous, similar versions of our benchmarks written in Coq by experts, proofs accounted for over 80% of the code base; our versions require no manual proofs."
1427228,15517,23836,Improving High-Performance Sparse Libraries Using Compiler-Assisted Specialization: A PETSc Case Study,2012,"Scientific libraries are written in a general way in anticipation of a variety of use cases that reduce optimization opportunities. Significant performance gains can be achieved by specializing library code to its execution context: the application in which it is invoked, the input data set used, the architectural platform and its backend compiler. Such specialization is not typically done because it is time consuming, leads to nonportable code and requires performance-tuning expertise that application scientists may not have. Tool support for library specialization in the above context could potentially reduce the extensive understanding required while significantly improving performance, code reuse and portability. In this work, we study the performance gains achieved by specializing the single processor sparse linear algebra functions in PETSc (Portable, Extensible Toolkit for Scientific Computation) in the context of three scalable scientific applications on the Hopper Cray XE6 Supercomputer at NERSC. We use CHiLL (Compos able High-Level Loop Transformation Framework) to apply source level transformations tailored to the special needs of sparse computations and automatically generate highly optimized PETSc functions. We demonstrate significant performance improvements of more than 1.8X on the library functions and overall gains of 9 to 24% on three scalable applications that use PETSc's sparse matrix capabilities."
1921050,15517,11058,Monadic abstract interpreters,2013,"Recent developments in the systematic construction of abstract interpreters hinted at the possibility of a broad unification of concepts in static analysis. We deliver that unification by showing context-sensitivity, polyvariance, flow-sensitivity, reachability-pruning, heap-cloning and cardinality-bounding to be independent of any particular semantics. Monads become the unifying agent between these concepts and between semantics. For instance, by plugging the same context-insensitivity monad into a monadically-parameterized semantics for Java or for the lambda calculus, it yields the expected context-insensitive analysis.   To achieve this unification, we develop a systematic method for transforming a concrete semantics into a monadically-parameterized abstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover a spectrum of machines---from the original concrete semantics to a monovariant, flow- and context-insensitive static analysis with a singly-threaded heap and weak updates.   The monadic parameterization also suggests an abstraction over the ubiquitous monotone fixed-point computation found in static analysis. This abstraction makes it straightforward to instrument an analysis with high-level strategies for improving precision and performance, such as abstract garbage collection and widening.   While the paper itself runs the development for continuation-passing style, our generic implementation replays it for direct-style lambda-calculus and Featherweight Java to support generality."
1174548,15517,20649,Cloud platforms and embedded computing: the operating systems of the future,2013,"The discussion on how to effectively program embedded systems has often in the past revolved around issues like the ideal instruction set architecture (ISA) or the best operating system. Much of this has been motivated by the inherently resource-constrained nature of embedded devices that mandates efficiency as the primary design principle.   In this paper, we advocate a change in the way we see and treat embedded systems. Not only have embedded systems become much more powerful and resources more affordable, we also see a trend towards making embedded devices more consumable, programmable, and customizable by end users. In fact, we see a strong similarity with recent developments in cloud computing.   We outline several challenges and opportunities in turning a language runtime system like the Java Virtual Machine into a cloud platform. We focus in particular on support for running multiple tenants concurrently within the platform. Multi-tenant support is essential for efficient resource utilization in cloud environments but can also improve application performance and overall user experience in embedded environments. We believe that today's modern language runtimes, with extensions to support multi-tenancy, can form the basis for a single continuous platform for emerging embedded applications backed by cloud-based service infrastructures."
940491,15517,23865,Understanding software evolution: the maisqual ant data set,2014,"Software engineering is a maturing discipline which has seen many drastic advances in the last years. However, some studies still point to the lack of rigorous and mathematically grounded methods to raise the field to a new emerging science, with proper and reproducible foundations to build upon. Indeed, mathematicians and statisticians do not necessarily have software engineering knowledge, while software engineers and practitioners do not necessarily have a mathematical background.     The Maisqual research project intends to fill the gap between both fields by proposing a controlled and peer-reviewed data set series ready to use and study. These data sets feature metrics from different repositories, from source code to mail activity and configuration management meta data. Metrics are described and commented, and all the steps followed for their extraction and treatment are described with contextual information about the data and its meaning.     This article introduces the Apache Ant weekly data set, featuring 636 extracts of the project over 12 years at different levels of artefacts – application, files, functions. By associating community and process related information to code extracts, this data set unveils interesting perspectives on the evolution of one of the great success stories of open source."
1974595,15517,20524,SymCrash: selective recording for reproducing crashes,2014,"Software often crashes despite tremendous effort on software quality assurance. Once developers receive a crash report, they need to reproduce the crash in order to understand the problem and locate the fault. However, limited information from crash reports often makes crash reproduction difficult. Many capture-and-replay techniques have been proposed to automatically capture program execution data from the failing code, and help developers replay the crash scenarios based on the captured data. However, such techniques often suffer from heavy overhead and introduce privacy concerns. Recently, methods such as BugRedux were proposed to generate test input that leads to crash through symbolic execution. However, such methods have inherent limitations because they rely on conventional symbolic execution techniques. In this paper, we propose a dynamic symbolic execution method called SymCon, which addresses the limitation of conventional symbolic execution by selecting functions that are hard to be resolved by a constraint solver and using their concrete runtime values to replace the symbols. We then propose SymCrash, a selective recording approach that only instruments and monitors the hard-to-solve functions. SymCrash can generate test input for crashes through SymCon. We have applied our approach to successfully reproduce 13 failures of 6 real-world programs. Our results confirm that the proposed approach is suitable for reproducing crashes, in terms of effectiveness, overhead, and privacy. It also outperforms the related methods."
2206396,15517,23865,Software bertillonage: finding the provenance of an entity,2011,"Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components -- such as external libraries or cloned source code -- is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of fingerprints. As an example, we have developed a fast, simple, and approximate technique called anchored signature matching for identifying library version information within a given Java application. This technique involves a type of structured signature matching performed against a database of candidates drawn from the Maven2 repository, a 150GB collection of open source Java libraries. An exploratory case study using a proprietary e-commerce Java application illustrates that the approach is both feasible and effective."
830795,15517,11330,Scalable fine-grained call path tracing,2011,"Applications must scale well to make efficient use of even medium-scale parallel systems. Because scaling problems are often difficult to diagnose, there is a critical need for scalable tools that guide scientists to the root causes of performance bottlenecks.   Although tracing is a powerful performance-analysis technique, tools that employ it can quickly become bottlenecks themselves. Moreover, to obtain actionable performance feedback for modular parallel software systems, it is often necessary to collect and present fine-grained context-sensitive data --- the very thing scalable tools avoid. While existing tracing tools can collect calling contexts, they do so only in a coarse-grained fashion; and no prior tool scalably presents both context- and time-sensitive data.   This paper describes how to collect, analyze and present fine-grained call path traces for parallel programs. To scale our measurements, we use asynchronous sampling, whose granularity is controlled by a sampling frequency, and a compact representation.   To present traces at multiple levels of abstraction and at arbitrary resolutions, we use sampling to render complementary slices of calling-context-sensitive trace data. Because our techniques are general, they can be used on applications that use different parallel programming models (MPI, OpenMP, PGAS). This work is implemented in HPCToolkit."
955174,15517,8806,On the improvement of a fault classification scheme with implications for white-box testing,2012,"Different testing techniques can be more or less effective on different fault types; therefore, testing methods that are most likely to detect the most common fault types should be preferred. However, to enable such a selection of testing methods, a suitable and effective fault classification scheme is essential. Much software testing research proposes techniques to generate test cases and evaluates these techniques based on some hypothesized fault classification scheme. However, there is a lack in the justification of whether such 'hypothesized faults' are realistic and how often these faults occur. Recently, Pan  et al . analyzed the syntactic changes in the source code, made in fixing faults, of 7 open source software projects implemented in Java. Based on their experience, they proposed a fault classification scheme with relative frequencies of each fault type. As always, readers may question whether the resulting fault classification is reasonable and appropriate. Hence, we applied their method to Checkstyle, another open source Java program, as the subject of our study, hoping to validate the appropriateness of their fault classification scheme, with particular application for selecting testing methods. While we generally found their classification scheme reasonable, we also noted that some faults could be classified in multiple ways. We also found that the frequencies of fault categories in Checkstyle were significantly different to the seven systems studied by Pan  et al ., which had all shown to have quite similar frequencies. We identified several potential improvements to their classification, permitting the classification of a larger proportion of faults. We have identified new implications of the combined results for white-box testing, and proposed follow-up studies to examine these issues in more detail."
42129,15517,23827,A Formal Model for Constraint-Based Deployment Calculation and Analysis for Fault-Tolerant Systems,2014,"In many embedded systems like in the automotive domain, safety-critical features are increasingly realized by software. Some of these features are often required to behave fail-operational, meaning that they must stay alive even in the presence of random hardware failures. We propose a new fault-tolerant SW/HW architecture for electric ve- hicles with inherent safety capabilities that enable fail-operational fea- tures. In this paper, we introduce a constraint-based approach to calculate valid deployments of mixed-critical software components to the execution nodes. To avoid harm, faulty execution nodes have to be isolated from the remaining system. We treat the isolations of execution nodes and the re- quired changes to the deployment to keep those software components alive that realize fail-operational features. The affected software components have to be resumed on intact execution nodes. However, the remaining system resources may become insufficient to execute the full set of soft- ware components after an isolation of an execution node. Hence, some components might have to be deactivated, meaning that features might get lost. Our approach allows to formally analyze which subset of features can still be provided after one or more isolations. We present an arith- metic system model with formal constraints of the deployment-problem that can be solved by a SMT-Solver. We evaluate our approach by show- ing an example problem and its solution."
1142871,15517,517,Automated WAIT for Cloud-Based Application Testing,2014,"Cloud computing is causing a paradigm shift in the provision and use of software. This has changed the way of obtaining, managing and delivering computing services and solutions. Similarly, it has brought new challenges to software testing. A particular area of concern is the performance of cloud-based applications. This is because the increased complexity of the applications has exposed new areas of potential failure points, complicating all performance-related activities. This situation makes the performance testing of cloud environments very challenging. Similarly, the identification of performance issues and the diagnosis of their root causes are time-consuming and complex, usually require multiple tools and heavily rely on expertise. To simplify these tasks, hence increasing the productivity and reducing the dependency on human experts, this paper presents a lightweight approach to automate the usage of expert tools in the performance testing of cloud-based applications. In this paper, we use a tool named Whole-system Analysis of Idle Time to demonstrate how our research work solves this problem. The validation involved two experiments, which assessed the overhead of the approach and the time savings that it can bring to the analysis of performance issues. The results proved the benefits of the approach by achieving a significant decrease in the time invested in performance analysis while introducing a low overhead in the tested system."
2449729,15517,23865,A linked data platform for mining software repositories,2012,"The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web."
1181515,15517,11375,"Efficiently, effectively detecting mobile app bugs with AppDoctor",2014,"Mobile apps bring unprecedented levels of convenience, yet they are often buggy, and their bugs offset the convenience the apps bring. A key reason for buggy apps is that they must handle a vast variety of system and user actions such as being randomly killed by the OS to save resources, but app developers, facing tough competitions, lack time to thoroughly test these actions. AppDoctor is a system for efficiently and effectively testing apps against many system and user actions, and helping developers diagnose the resultant bug reports. It quickly screens for potential bugs using  approximate execution , which runs much faster than real execution and exposes bugs but may cause false positives. From the reports, AppDoctor automatically verifies most bugs and prunes most false positives, greatly saving manual inspection effort. It uses  action slicing  to further speed up bug diagnosis. We implement AppDoctor in Android. It operates as a cloud of physical devices or emulators to scale up testing. Evaluation on 53 out of 100 most popular apps in Google Play and 11 of the most popular open-source apps shows that, AppDoctor effectively detects 72 bugs---including two bugs in the Android framework that affect all apps---with quick checking sessions, speeds up testing by 13.3 times, and vastly reduces diagnosis effort."
1050494,15517,11166,An Incremental Tensor Factorization Approach for Web Service Recommendation,2014,"With the development of Service-Oriented technologies, the amount of Web services grows rapidly. QoS-Aware Web service recommendation can help service users to design more efficient service-oriented systems. However, existing methods assume the QoS information for service users are all known and accurate, but in real case, there are always many missing QoS values in history records, which increase the difficulty of the missing QoS value prediction. By considering the user-service-time three dimension context information, we study a Temporal QoS-Aware Web Service Recommendation Framework which aims to recommend best candidates to service user's requirements and meanwhile improve the QoS prediction accuracy. We propose to envision such QoS value data as a tensor which is known as multi-way array, and formalize this problem as a tensor factorization model and propose a Tucker decomposition (TD)algorithm which is able to deal with the triadic relations of user-service-time model. However, one major challenge is that how to deal with the dynamic incoming service QoS value data streams. Thus, we introduce the incremental tensor factorization (ITF)method which is (a) scalable, (b) space efficient (it does not need to store the past data). Extensive experiments are conductedbasedc on our real-world QoS dataset collected on Planet-Lab, comprised of service invocation response-time values from 408 users on 5,473 Web services at 240 time periods. Comprehensive empirical studies demonstrate that our approach is faster and more accuracy than other approaches."
2361271,15517,23865,An empirical study of supplementary bug fixes,2012,"A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects---those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches.   Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22% to 33%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations---they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems."
2005911,15517,23827,BALLERINA: automatic generation and clustering of efficient random unit tests for multithreaded code,2012,"Testing multithreaded code is hard and expensive. Each multithreaded unit test creates two or more threads, each executing one or more methods on shared objects of the class under test. Such unit tests can be generated at random, but basic generation produces tests that are either slow or do not trigger concurrency bugs. Worse, such tests have many false alarms, which require human effort to filter out. We present BALLERINA, a novel technique for automatic generation of efficient multithreaded random tests that effectively trigger concurrency bugs. BALLERINA makes tests efficient by having only two threads, each executing a single, randomly selected method. BALLERINA increases chances that such a simple parallel code finds bugs by appending it to more complex, randomly generated sequential code. We also propose a clustering technique to reduce the manual effort in inspecting failures of automatically generated multithreaded tests. We evaluate BALLERINA on 14 real-world bugs from 6 popular codebases: Groovy, Java JDK, jFreeChart, Log4j, Lucene, and Pool. The experiments show that tests generated by BALLERINA can find bugs on average 2X-10X faster than various configurations of basic random generation, and our clustering technique reduces the number of inspected failures on average 4X-8X. Using BALLERINA, we found three previously unknown bugs in Apache Pool and Log4j, one of which was already confirmed and fixed."
1193605,15517,11058,How to combine widening and narrowing for non-monotonic systems of equations,2013,"Non-trivial analysis problems require complete lattices with infinite ascending and descending chains. In order to compute reasonably precise post-fixpoints of the resulting systems of equations, Cousot and Cousot have suggested accelerated fixpoint iteration by means of widening and narrowing.   The strict separation into phases, however, may unnecessarily give up precision that cannot be recovered later. While widening is also applicable if equations are non-monotonic, this is no longer the case for narrowing. A narrowing iteration to improve a given post-fixpoint, additionally, must assume that all right-hand sides are monotonic. The latter assumption, though, is not met in presence of widening. It is also not met by equation systems corresponding to context-sensitive interprocedural analysis, possibly combining context-sensitive analysis of local information with flow-insensitive analysis of globals.   As a remedy, we present a novel operator that combines a given widening operator with a given narrowing operator. We present adapted versions of round-robin as well as of worklist iteration, local, and side-effecting solving algorithms for the combined operator and prove that the resulting solvers always return sound results and are guaranteed to terminate for monotonic systems whenever only finitely many unknowns (constraint variables) are encountered."
2449087,15517,20524,Counterexample-guided abstraction refinement for linear programs with arrays,2014,"Predicate abstraction refinement is one of the leading approaches to software verification. The key idea is to abstract the input program into a Boolean Program (i.e. a program whose variables range over the Boolean values only and model the truth values of predicates corresponding to properties of the program state), and refinement searches for new predicates in order to build a new, more refined abstraction. Thus Boolean programs are commonly employed as a simple, yet useful abstraction. However, the effectiveness of predicate abstraction refinement on programs that involve a tight interplay between data-flow and control-flow is still to be ascertained. We present a novel counterexample guided abstraction refinement procedure for Linear Programs with arrays, a fragment of the C programming language where variables and array elements range over a numeric domain and expressions involve linear combinations of variables and array elements. In our procedure the input program is abstracted w.r.t. a family of sets of array indices, the abstraction is a Linear Program (without arrays), and refinement searches for new array indices. We use Linear Programs as the target of the abstraction (instead of Boolean programs) as they allow to express complex correlations between data and control. Thus, unlike the approaches based on predicate abstraction, our approach treats arrays precisely. This is an important feature as arrays are ubiquitous in programming. We provide a precise account of the abstraction, Model Checking, and refinement processes, discuss their implementation in the EUREKA tool, and present a detailed analysis of the experimental results confirming the effectiveness of our approach on a number of programs of interest."
2513331,15517,8868,Demand-driven context-sensitive alias analysis for Java,2011,"Software tools for program understanding, transformation, verification, and testing often require an efficient yet highly-precise alias analysis. Typically this is done by computing points-to information, from which alias queries can be answered. This paper presents a novel context-sensitive, demand-driven alias analysis for Java that achieves efficiency by answering alias queries  directly , instead of relying on an underlying points-to analysis. The analysis is formulated as a context-free-language (CFL) reachability problem over a language that models calling context sensitivity, and over another language that models field sensitivity (i.e., flow of reference values through fields of heap objects).   To improve analysis scalability, we propose to compute  procedural reachability summaries  online, during the CFL-reachability computation. This cannot be done indiscriminately, as the benefits of using the summary information do not necessarily outweigh the cost of computing it. Our approach selects for summarization only a subset of heavily-used methods (i.e., methods having a large number of incoming edges in the static call graph). We have performed a variety of studies on the proposed analysis. The experimental results show that, within the same time budget, the precision of the analysis is higher than that of a state-of-the-art highly-precise points-to analysis. In addition, the use of method summaries can lead to significant improvements in analysis performance."
1010203,15517,8806,FortressCheck: automatic testing for generic properties,2011,"QuickCheck is a random testing library designed for the purely functional programming language Haskell. Its main features include a descriptive yet embedded domain-specific testing language, a variety of test generators including a generator for functions, and a set of operations for monitoring generated inputs. QuickCheck is limited to ad-hoc testing, compared to more systematic testing methods such as full coverage testing. However, experiences showed that well-factored functions and properties make the QuickCheck approach as effective as systematic testing while maintaining its conciseness. QuickCheck and its variants are now available in dozens of programming languages.   We present a version of QuickCheck for the Fortress programming language in this paper. Fortress is an object-oriented language with extensive support for functional programming, with the strong emphasis on high-performance computing, parallelism by default, and growability of the language. While the main features of QuickCheck are straight-forward to implement, we are extending them to support unique features of Fortress and to support seamless integration to Fortress. We observed that the prevalent uses of implicit parallelism in Fortress call for testing parallel language constructs especially those using side effects. Also, because Fortress provides both subtype polymorphism and parametric polymorphism unlike Haskell, testing both polymorphic properties becomes interesting. We propose FortressCheck to test implicit parallelism and to test parametric polymorphism via reflection, by generating first-class type objects and using QuickCheck's own implication checking as a safety mechanism."
1247320,15517,23620,Streaming transducers for algorithmic verification of single-pass list-processing programs,2011,"We introduce  streaming data string transducers  that map input data strings to output data strings in a single left-to-right pass in linear time. Data strings are (unbounded) sequences of data values, tagged with symbols from a finite set, over a potentially infinite data domain that supports only the operations of equality and ordering. The transducer uses a finite set of states, a finite set of variables ranging over the data domain, and a finite set of variables ranging over data strings. At every step, it can make decisions based on the next input symbol, updating its state, remembering the input data value in its data variables, and updating data-string variables by concatenating data-string variables and new symbols formed from data variables, while avoiding duplication. We establish PSPACE bounds for the problems of checking functional equivalence of two streaming transducers, and of checking whether a streaming transducer satisfies pre/post verification conditions specified by streaming acceptors over input/output data-strings.   We identify a class of imperative and a class of functional programs, manipulating lists of data items, which can be effectively translated to streaming data-string transducers. The imperative programs dynamically modify a singly-linked heap by changing next-pointers of heap-nodes and by adding new nodes. The main restriction specifies how the next-pointers can be used for traversal. We also identify an expressively equivalent fragment of functional programs that traverse a list using syntactically restricted recursive calls. Our results lead to algorithms for assertion checking and for checking functional equivalence of two programs, written possibly in different programming styles, for commonly used routines such as insert, delete, and reverse."
1431220,15517,23620,Static and user-extensible proof checking,2012,"Despite recent successes, large-scale proof development within proof assistants remains an arcane art that is extremely time-consuming. We argue that this can be attributed to two profound shortcomings in the architecture of modern proof assistants. The first is that proofs need to include a large amount of minute detail; this is due to the rigidity of the proof checking process, which cannot be extended with domain-specific knowledge. In order to avoid these details, we rely on developing and using tactics, specialized procedures that produce proofs. Unfortunately, tactics are both hard to write and hard to use, revealing the second shortcoming of modern proof assistants. This is because there is no static knowledge about their expected use and behavior. As has recently been demonstrated, languages that allow type-safe manipulation of proofs, like Beluga, Delphin and VeriML, can be used to partly mitigate this second issue, by assigning rich types to tactics. Still, the architectural issues remain. In this paper, we build on this existing work, and demonstrate two novel ideas: an extensible conversion rule and support for static proof scripts. Together, these ideas enable us to support both user-extensible proof checking, and sophisticated static checking of tactics, leading to a new point in the design space of future proof assistants. Both ideas are based on the interplay between a light-weight staging construct and the rich type information available."
1706035,15517,23827,Comparing test quality measures for assessing student-written tests,2014,"Many educators now include software testing activities in programming assignments, so there is a growing demand for appropriate methods of assessing the quality of student-written software tests. While tests can be hand-graded, some educators also use objective performance metrics to assess software tests. The most common measures used at present are code coverage measures—tracking how much of the student’s code (in terms of statements, branches, or some combination) is exercised by the corresponding software tests. Code coverage has limitations, however, and sometimes it overestimates the true quality of the tests. Some researchers have suggested that mutation analysis may provide a better indication of test quality, while some educators have experimented with simply running every student’s test suite against every other student’s program—an “all-pairs” strategy that gives a bit more insight into the quality of the tests. However, it is still unknown which one of these measures is more accurate, in terms of most closely predicting the true bug revealing capability of a given test suite. This paper directly compares all three methods of measuring test quality in terms of how well they predict the observed bug revealing capabilities of student-written tests when run against a naturally occurring collection of student-produced defects. Experimental results show that all-pairs testing—running each student’s tests against every other student’s solution—is the most effective predictor of the underlying bug revealing capability of a test suite. Further, no strong correlation was found between bug revealing capability and either code coverage or mutation analysis scores."
1949201,15517,23827,Performance regression testing target prioritization via performance risk analysis,2014,"As software evolves, problematic changes can significantly degrade software performance, i.e., introducing performance regression. Performance regression testing is an effective way to reveal such issues in early stages. Yet because of its high overhead, this activity is usually performed infrequently. Consequently, when performance regression issue is spotted at a certain point, multiple commits might have been merged since last testing. Developers have to spend extra time and efforts narrowing down which commit caused the problem. Existing efforts try to improve performance regression testing efficiency through test case reduction or prioritization.     In this paper, we propose a new lightweight and white-box approach, performance risk analysis (PRA), to improve performance regression testing efficiency via testing target prioritization. The analysis statically evaluates a given source code commit's risk in introducing performance regression. Performance regression testing can leverage the analysis result to test commits with high risks first while delaying or skipping testing on low-risk commits.     To validate this idea's feasibility, we conduct a study on 100 real-world performance regression issues from three widely used, open-source software. Guided by insights from the study, we design PRA and build a tool, PerfScope. Evaluation on the examined problematic commits shows our tool can successfully alarm 91% of them. Moreover, on 600 randomly picked new commits from six large-scale software, with our tool, developers just need to test only 14-22% of the 600 commits and will still be able to alert 87-95% of the commits with performance regression."
1364868,15517,23735,ReFrESH: A self-adaptation framework to support fault tolerance in field mobile robots,2014,"Mobile robots are being employed far more often in extreme environments, such as urban search and rescue, with greater levels of autonomy; yet recent studies on field robotics show that numerous failure modes affect the reliability of the robot in meeting mission objectives. Therefore, fault tolerance is increasingly important for field robots operating in unpredictable environments to ensure safety and effectiveness of the system. This paper demonstrates a self-adaptation frame- work, ReFrESH, that contains mechanisms for fault detection and fault mitigation. The goal of ReFrESH is to provide diagnosable and maintainable infrastructure support, built into a real-time operating system, to manage task performance in the presence of unexpected uncertainties. ReFrESH augments the port-based object framework by attaching evaluation and estimation mechanisms to each functional component so that the robot can easily detect and locate faults. In conjunction, a task level decision mechanism interacts with the fault detection elements in order to generate and choose an optimal approach to mitigating faults. Moreover, to increase flexibility of the fault tolerance, ReFrESH provides self-adaptation support for both software and hardware functionality. To our knowledge, this is the first framework to support both software and hardware self- adaptation. A demonstrative application of ReFrESH illustrates its applicability through a target tracking task deployed on a mobile robot system."
2073625,15517,23827,Automatic parameter recommendation for practical API usage,2012,"Programmers extensively use application programming interfaces (APIs) to leverage existing libraries and frameworks. However, correctly and efficiently choosing and using APIs from unfamiliar libraries and frameworks is still a non-trivial task. Programmers often need to ruminate on API documentations (that are often incomplete) or inspect code examples (that are often absent) to learn API usage patterns. Recently, various techniques have been proposed to alleviate this problem by creating API summarizations, mining code examples, or showing common API call sequences. However, few techniques focus on recommending API parameters.     In this paper, we propose an automated technique, called Precise, to address this problem. Differing from common code completion systems, Precise mines existing code bases, uses an abstract usage instance representation for each API usage example, and then builds a parameter usage database. Upon a request, Precise queries the database for abstract usage instances in similar contexts and generates parameter candidates by concretizing the instances adaptively.     The experimental results show that our technique is more general and applicable than existing code completion systems, specially, 64% of the parameter recommendations are useful and 53% of the recommendations are exactly the same as the actual parameters needed. We have also performed a user study to show our technique is useful in practice."
2353370,15517,10973,The Semantics of Parsing with Semantic Actions,2012,"The recovery of structure from flat sequences of input data is a problem that almost all programs need to solve. Computer Science has developed a wide array of declarative languages for describing the structure of languages, usually based on the context-free grammar formalism, and there exist parser generators that produce efficient parsers for these descriptions. However, when faced with a problem involving parsing, most programmers opt for ad-hoc hand-coded solutions, or use parser combinator libraries to construct parsing functions. This paper develops a hybrid approach, treating grammars as collections of active right-hand sides, indexed by a set of non-terminals. Active right-hand sides are built using the standard monadic parser combinators and allow the consumed input to affect the language being parsed, thus allowing for the precise description of the realistic languages that arise in programming. We carefully investigate the semantics of grammars with active right-hand sides, not just from the point of view of language acceptance but also in terms of the generation of parse results. Ambiguous grammars may generate exponentially, or even infinitely, many parse results and these must be efficiently represented using Shared Packed Parse Forests (SPPFs). A particular feature of our approach is the use of Reynolds-style parametricity to ensure that the language that grammars describe cannot be affected by the representation of parse results."
2212274,15517,23827,"Linking strategy, governance, and performance in software engineering",2014,"Applying IT in an appropriate and timely way, in harmony with strategies, goals and needs, is important for business performance success. However, because ensuring that IT will support the achievement of business strategies entails a sequence of key decisions in a variety of related IT areas, knowledge that is relevant for effective decision-making tends to be dispersed among multiple business and IT stakeholders, who represent an organisation's diverse and conflicting interests. Consequently, inappropriately assigning decision rights for critical IT decisions increases the risk of misalignment. We study the context in which decision rights are managed as part of governance in software development for business process support so that organisations can achieve high-level alignment between software systems and business processes during software development. In this paper we establish the key components of a system for governance design that links software development governance arrangements, business strategy and performance. We position business process executives and managers as roles responsible for software development governance. Their responsibilities include defining the software system's strategic aims, providing the leadership to put them into effect, and satisfying themselves that an appropriate governance structure is, and remains in place during software development. We also position business process and software developer stakeholders as roles responsible for implementing software systems that help realise strategic aims. We propose four key software development decisions: functional and non-functional requirements, and system architecture and deliverables."
1606427,15517,23827,"Automated testing of GUI applications: models, tools, and controlling flakiness",2013,"System testing of applications with graphical user interfaces (GUIs) such as web browsers, desktop, or mobile apps, is more complex than testing from the command line. Specialized tools are needed to generate and run test cases, models are needed to quantify behavioral coverage, and changes in the environment, such as the operating system, virtual machine or system load, as well as starting states of the executions, impact the repeatability of the outcome of tests making tests appear flaky. In this tutorial, we present an overview of the state of the art in GUI testing, consisting of both lectures and demonstrations on various platforms (desktop, web and mobile applications), using an open source testing tool, GUITAR. We show how to setup a system under test, how to extract models without source code, and how to then use those models to generate and replay test cases. We then present a lecture on the various factors that may cause flakiness in the execution of GUI-centric software, and hence impact the results of analyses and experiments based on such software. We end with a demonstration of a community resource for sharing GUI testing artifacts aimed at controlling these factors. This tutorial targets both researchers who develop techniques for testing GUI software, and practitioners from industry who want to learn more about model-based GUI testing or who run and rerun GUI tests and often find their runs are flaky."
1239593,15517,8868,Automatic partial loop summarization in dynamic test generation,2011,"Whitebox fuzzing extends dynamic test generation based on symbolic execution and constraint solving from unit testing to whole-application security testing. Unfortunately, input-dependent loops may cause an explosion in the number of constraints to be solved and in the number of execution paths to be explored. In practice, whitebox fuzzers arbitrarily bound the number of constraints and paths due to input-dependent loops, at the risk of missing code and bugs.   In this work, we investigate the use of simple loop-guard pattern-matching rules to automatically guess an input constraint defining the number of iterations of input-dependent loops during dynamic symbolic execution. We discover the loop structure of the program on the fly, detect  induction variables , which are variables modified by a constant value during loop iterations, and infer simple partial loop invariants relating the value of such variables. Whenever a guess is confirmed later during the current dynamic symbolic execution, we then inject new constraints representing pre and post loop conditions, effectively summarizing sets of executions of that loop. These pre and post conditions are derived from partial loop invariants synthesized dynamically using pattern-matching rules on the loop guards and induction variables, without requiring any static analysis, theorem proving, or input-format specification. This technique has been implemented in the whitebox fuzzer SAGE, scales to large programs with many nested loops, and we present results of experiments with a Windows 7 image parser."
1106144,15517,23865,MIC check: a correlation tactic for ESE data,2012,"Empirical software engineering researchers are concerned with understanding the relationships between outcomes of interest,  e.g . defects, and process and product measures. The use of correlations to uncover strong relationships is a natural precursor to multivariate modeling. Unfortunately, correlation coefficients can be difficult and/or misleading to interpret. For example, a strong correlation occurs between variables that stand in a polynomial relationship; this may lead one mistakenly, and eventually misleadingly, to model a polynomially related variable in a linear regression. Likewise, a non-monotonic functional, or even non-functional relationship might be entirely missed by a correlation coefficient. Outliers can influence standard correlation measures, tied values can unduly influence even robust non-parametric rank correlation, measures, and smaller sample sizes can cause instability in correlation measures. A new bivariate measure of association, Maximal Information Coefficient (MIC) [1], promises to simultaneously discover if two variables have:  a)  any association,  b)  a functional relationship, and  c)  a non-linear relationship. The MIC is a very useful complement to standard and rank correlation measures. It separately characterizes the  existence  of a relationship and  its precise nature ; thus, it enables more informed choices in modeling non-functional and non-linear relationships,  and  a more nuanced indicator of potential problems with the values reported by standard and rank correlation measures. We illustrate the use of MIC using a variety of software engineering metrics. We study and explain the distributional properties of MIC and related measures in software engineering data, and illustrate the value of these measures for the empirical software engineering researcher."
1796992,15517,23827,UDesignIt: towards social media for community-driven design,2012,"Online social networks are now common place in day-to-day lives. They are also increasingly used to drive social action initiatives, either led by government or communities themselves (e.g., SeeClickFix, LoveLewisham.org, mumsnet). However, such initiatives are mainly used for crowd sourcing community views or coordinating activities. With the changing global economic and political landscape, there is an ever pressing need to engage citizens on a large-scale, not only in consultations about systems that affect them, but also involve them directly in the design of these very systems. In this paper we present the UDesignIt platform that combines social media technologies with software engineering concepts to empower communities to discuss and extract high-level design features. It combines natural language processing, feature modelling and visual overlays in the form of ``image clouds'' to enable communities and software engineers alike to unlock the knowledge contained in the unstructured and unfiltered content of social media where people discuss social problems and their solutions. By automatically extracting key themes and presenting them in a structured and organised manner in near real-time, the approach drives a shift towards large-scale engagement of community stakeholders for system design."
778242,15517,20524,Fusion fault localizers,2014,"Many spectrum-based fault localization techniques have been proposed to measure how likely each program element is the root cause of a program failure. For various bugs, the best technique to localize the bugs may differ due to the characteristics of the buggy programs and their program spectra. In this paper, we leverage the diversity of existing spectrum-based fault localization techniques to better localize bugs using data fusion methods. Our proposed approach consists of three steps: score normalization, technique selection, and data fusion. We investigate two score normalization methods, two technique selection methods, and five data fusion methods resulting in twenty variants of  Fusion Localizer . Our approach is bug specific in which the set of techniques to be fused are adaptively selected for each buggy program based on its spectra. Also, it requires no training data, i.e., execution traces of the past buggy programs.   We evaluate our approach on a common benchmark dataset and a dataset consisting of real bugs from three medium to large programs. Our evaluation demonstrates that our approach can significantly improve the effectiveness of existing state-of-the-art fault localization techniques. Compared to these state-of-the-art techniques, the best variants of  Fusion Localizer  can statistically significantly reduce the amount of code to be inspected to find all bugs. Our best variants can increase the proportion of bugs localized when developers only inspect the top 10% most suspicious program elements by more than 10% and increase the number of bugs that can be successfully localized when developers only inspect up to 10 program blocks by more than 20%."
759526,15517,11058,Optimizing database-backed applications with query synthesis,2013,"Object-relational mapping libraries are a popular way for applications to interact with databases because they provide transparent access to the database using the same language as the application. Unfortunately, using such frameworks often leads to poor performance, as modularity concerns encourage developers to implement relational operations in application code. Such application code does not take advantage of the optimized relational implementations that database systems provide, such as efficient implementations of joins or push down of selection predicates. In this paper we present QBS, a system that automatically transforms fragments of application logic into SQL queries. QBS differs from traditional compiler optimizations as it relies on synthesis technology to generate invariants and postconditions for a code fragment. The postconditions and invariants are expressed using a new theory of ordered relations that allows us to reason precisely about both the contents and order of the records produced complex code fragments that compute joins and aggregates. The theory is close in expressiveness to SQL, so the synthesized postconditions can be readily translated to SQL queries. Using 75 code fragments automatically extracted from over 120k lines of open-source code written using the Java Hibernate ORM, we demonstrate that our approach can convert a variety of imperative constructs into relational specifications and significantly improve application performance asymptotically by orders of magnitude."
2076680,15517,23827,Graph-based analysis and prediction for software evolution,2012,"We exploit recent advances in analysis of graph topology to better understand software evolution, and to construct predictors that facilitate software development and maintenance. Managing an evolving, collaborative software system is a complex and expensive process, which still cannot ensure software reliability. Emerging techniques in graph mining have revolutionized the modeling of many complex systems and processes. We show how we can use a graph-based characterization of a software system to capture its evolution and facilitate development, by helping us estimate bug severity, prioritize refactoring efforts, and predict defect-prone releases. Our work consists of three main thrusts. First, we construct graphs that capture software structure at two different levels: (a) the product, i.e., source code and module level, and (b) the process, i.e., developer collaboration level. We identify a set of graph metrics that capture interesting properties of these graphs. Second, we study the evolution of eleven open source programs, including Firefox, Eclipse, MySQL, over the lifespan of the programs, typically a decade or more. Third, we show how our graph metrics can be used to construct predictors for bug severity, high-maintenance software parts, and failure-prone releases. Our work strongly suggests that using graph topology analysis concepts can open many actionable avenues in software engineering research and practice."
1142969,15517,23620,The power of parameterization in coinductive proof,2013,"Coinduction is one of the most basic concepts in computer science. It is therefore surprising that the commonly-known lattice-theoretic accounts of the principles underlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning (i.e. breaking proofs into separate pieces that can be developed in isolation), and they do not support incremental reasoning (i.e. developing proofs interactively by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary).   In this paper, we show how to support coinductive proofs that are both compositional and incremental, using a dead simple construction we call the parameterized greatest fixed point. The basic idea is to parameterize the greatest fixed point of interest over the accumulated knowledge of the proof so far. While this idea has been proposed before, by Winskel in 1989 and by Moss in 2001, neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof.   In addition to presenting the lattice-theoretic foundations of parameterized coinduction, demonstrating its utility on representative examples, and studying its composition with up-to techniques, we also explore its mechanization in proof assistants like Coq and Isabelle. Unlike traditional approaches to mechanizing coinduction (e.g. Coq's cofix), which employ syntactic guardedness checking, parameterized coinduction offers a semantic account of guardedness. This leads to faster and more robust proof development, as we demonstrate using our new Coq library, Paco."
243008,15517,20592,Blanket execution: dynamic similarity testing for program binaries and components,2014,"Matching function binaries--the process of identifying similar functions among binary executables--is a challenge that underlies many security applications such as malware analysis and patch-based exploit generation. Recent work tries to establish semantic similarity based on static analysis methods. Unfortunately, these methods do not perform well if the compared binaries are produced by different compiler toolchains or optimization levels. In this work, we propose blanket execution, a novel dynamic equivalence testing primitive that achieves complete coverage by overriding the intended program logic. Blanket execution collects the side effects of functions during execution under a controlled randomized environment. Two functions are deemed similar, if their corresponding side effects, as observed under the same environment, are similar too.#R##N##R##N#We implement our blanket execution technique in a system called BLEX. We evaluate BLEX rigorously against the state of the art binary comparison tool BinDiff. When comparing optimized and un-optimized executables from the popular GNU coreutils package, BLEX outperforms BinDiff by up to 3.5 times in correctly identifying similar functions. BLEX also outperforms BinDiff if the binaries have been compiled by different compilers. Using the functionality in BLEX, we have also built a binary search engine that identifies similar functions across optimization boundaries. Averaged over all indexed functions, our search engine ranks the correct matches among the top ten results 77% of the time."
2148823,15517,122,Scalable data race detection for partitioned global address space programs,2013,"Contemporary and future programming languages for HPC promote hybrid parallelism and shared memory abstractions using a global address space. In this programming style, data races occur easily and are notoriously hard to find. Previous work on data race detection for shared memory programs reports 10X-100X slowdowns for non-scientific programs. Previous work on distributed memory programs instruments only communication operations. In this paper we present the first complete implementation of data race detection at scale for UPC programs. Our implementation tracks local and global memory references in the program and it uses two techniques to reduce the overhead: 1) hierarchical function and instruction level sampling; and 2) exploiting the runtime persistence of aliasing and locality specific to Partitioned Global Address Space applications. The results indicate that both techniques are required in practice: well optimized instruction sampling introduces overheads as high as 6500% (65X slowdown), while each technique in separation is able to reduce it to 1000% (10X slowdown). When applying the optimizations in conjunction our tool finds all previously known data races in our benchmark programs with at most 50% overhead. Furthermore, while previous results illustrate the benefits of function level sampling, our experiences show that this technique does not work for scientific programs: instruction sampling or a hybrid approach is required."
1839996,15517,9896,How social Q&A sites are changing knowledge sharing in open source software communities,2014,"Historically, mailing lists have been the preferred means for coordinating development and user support activities. With the emergence and popularity growth of social Q&A sites such as the StackExchange network (e.g., StackOverflow), this is beginning to change. Such sites offer different socio-technical incentives to their participants than mailing lists do, e.g., rich web environments to store and manage content collaboratively, or a place to showcase their knowledge and expertise more vividly to peers or potential recruiters. A key difference between StackExchange and mailing lists is gamification, i.e., StackExchange participants compete to obtain reputation points and badges. In this paper, we use a case study of R (a widely-used tool for data analysis) to investigate how mailing list participation has evolved since the launch of StackExchange. Our main contribution is the assembly of a joint data set from the two sources, in which participants in both the texttt{r-help} mailing list and StackExchange are identifiable. This permits their activities to be linked across the two resources and also over time. With this data set we found that user support activities show a strong shift away from texttt{r-help}. In particular, mailing list experts are migrating to StackExchange, where their behaviour is different. First, participants active both on texttt{r-help} and on StackExchange are more active than those who focus exclusively on only one of the two. Second, they provide faster answers on StackExchange than on texttt{r-help}, suggesting they are motivated by the emph{gamified} environment. To our knowledge, our study is the first to directly chart the changes in behaviour of specific contributors as they migrate into gamified environments, and has important implications for knowledge management in software engineering."
2386891,15517,23876,Professional status and expertise for UML class diagram comprehension: An empirical study,2012,"Professional experience is one of the most important criteria for almost any job offer in software engineering. Professional experience refers both to professional status (practitioner vs. student) and expertise (expert vs. novice). We perform an experiment with 21 subjects including both practitioners and students, and experts and novices. We seek to understand the relation between the speed and accuracy of the subjects and their status and expertise in performing maintenance tasks on UML class diagrams. We also study the impact of the formulation of the maintenance task. We use an eye-tracking system to gather the fixations of the subjects when performing the task. We measure the subjects' comprehension using their accuracy, the time spent, the search effort, the overall effort, and the question comprehension effort. We found that (1) practitioners are more accurate than students while students spend around 35 percent less time than practitioners, (2) experts are more accurate than novices while novices spending around 33 percent less time than experts, (3) expertise is the most important factor for accuracy and speed, (4) experienced students are more accurate and spend around 37 percent less time than experienced practitioners, and (5) when the description of the task is precise, the novice students can be accurate. We conclude that it is an illusion for project managers to focus on status only when recruiting a software engineer. Our result is the starting point to consider the differences between status and expertise when studying software engineers' productivity. Thus, it can help project managers to recruit productive engineers and motivated students to acquire the experience and ability in the projects."
2325125,15517,20358,Inferring dependency constraints on parameters for web services,2013,"Recently many popular websites such as Twitter and Flickr expose their data through web service APIs, enabling third-party organizations to develop client applications that provide function-alities beyond what the original websites offer. These client appli-cations should follow certain constraints in order to correctly in-teract with the web services. One common type of such constraints is Dependency Constraints on Parameters. Given a web service operation  O  and its parameters Pi, Pj, these constraints describe the requirement on one parameter Pi that is dependent on the conditions of some other parameter(s) Pj. For example, when requesting the Twitter operation GET statuses/user_timeline, a user_id parameter must be provided if a screen_name parameter is not provided. Violations of such constraints can cause fatal errors or incorrect results in the client applications. However, these con-straints are often not formally specified and thus not available for automatic verification of client applications. To address this issue, we propose a novel approach, called INDICATOR, to automatically infer dependency constraints on parameters for web services, via a hybrid analysis of heterogeneous web service artifacts, including the service documentation, the service SDKs, and the web services themselves. To evaluate our approach, we applied INDICATOR to infer dependency constraints for four popular web services. The results showed that INDICATOR effectively infers constraints with an average precision of 94.4% and recall of 95.5%."
848881,15517,11330,Scaling data race detection for partitioned global address space programs,2013,"Contemporary and future programming languages for HPC promote hybrid parallelism and shared memory abstractions using a global address space. In this programming style, data races occur easily and are notoriously hard to find. Existing state-of-the-art data race detectors exhibit 10X-100X performance degradation and do not handle hybrid parallelism. In this paper we present the first complete implementation of data race detection at scale for UPC programs. Our implementation tracks local and global memory references in the program and it uses two techniques to reduce the overhead: 1) hierarchical function and instruction level sampling; and 2) exploiting the runtime persistence of aliasing and locality specific to Partitioned Global Address Space applications. The results indicate that both techniques are required in practice: well optimized instruction sampling introduces overheads as high as 6500% (65X slowdown), while each technique in separation is able to reduce it only to 1000% (10X slowdown). When applying the optimizations in conjunction our tool finds all previously known data races in our benchmark programs with at most 50% overhead when running on 2048 cores. Furthermore, while previous results illustrate the benefits of function level sampling, our experiences show that this technique does not work for scientific programs: instruction sampling or a hybrid approach is required."
1505606,15517,9080,Minimizing test suites in software product lines using weight-based genetic algorithms,2013,"Test minimization techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. In the context of software product line, we can save effort and cost in the selection and minimization of test cases for testing a specific product by modeling the product line. However, minimizing the test suite for a product requires addressing two potential issues: 1) the minimized test suite may not cover all test requirements compared with the original suite; 2) the minimized test suite may have less fault revealing capability than the original suite. In this paper, we apply weight-based Genetic Algorithms (GAs) to minimize the test suite for testing a product, while preserving fault detection capability and testing coverage of the original test suite. The challenge behind is to define an appropriate fitness function, which is able to preserve the coverage of complex testing criteria (e.g., Combinatorial Interaction Testing criterion). Based on the defined fitness function, we have empirically evaluated three different weight-based GAs on an industrial case study provided by Cisco Systems, Inc. Norway. We also presented our results of applying the three weight-based GAs on five existing case studies from the literature. Based on these case studies, we conclude that among the three weight-based GAs, Random-Weighted GA (RWGA) achieved significantly better performance than the other ones."
2166221,15517,23827,Automated detection of client-state manipulation vulnerabilities,2012,"Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this paper, we show that such client-state manipulation vulnerabilities are amenable to tool supported detection. We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities. Moreover, the inferred information can be applied to configure a security filter that automatically guards against attacks. Experiments on a collection of open source web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities."
1530543,15517,23827,High-quality specification of self-adaptive software systems,2013,"Today's software systems have to cope with changing environments while at the same time facing high non-functional requirements such as flexibility and dependability. Recently, these non-functional requirements are addressed using self-adaptivity features, that is, the system monitors its environment and adjusts its structure or behavior in reaction to changes.   In classical model-driven software engineering approaches, self-adaptivity introduces additional complexity since self-adaptation features are distributed in a cross-cutting manner at various different locations in the models, resulting in a tightly interwoven model landscape that is hard to understand and maintain. A particular solution to cope with this problem is the separation of concerns (SoC) to focus on the specific concern of self-adaptivity and allow in-depth analyses. Applying SoC requires suitable development processes, languages, and techniques, e.g., for quality assurance, to be available.   In this paper, we present a method for the specification of self-adaptive software systems using a UML based concern-specific modeling language called Adapt Case Modeling Language (ACML) that allows the separated and explicit specification of self-adaptivity concerns. Based on formal semantics we show how to apply quality assurance techniques to the modeled self-adaptive system, which enable the provisioning of hard guarantees concerning self-adaptivity characteristics such as adaptation rule set stability and deadlock freedom. Further, we show how the language and techniques integrate with existing software development processes."
2064122,15517,23827,Automating test automation,2012,"Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively."
1295964,15517,23865,Discovering complete API rules with mutation testing,2012,"Specifications are important for many activities during software construction and maintenance process such as testing, verification, debugging and repairing. Despite their importance, specifications are often missing, informal or incomplete because they are difficult to write manually. Many techniques have been proposed to automatically mine specifications describing method call sequence from execution traces or source code using  frequent pattern mining . Unfortunately, a sizeable number of such interesting specifications discovered by frequent pattern mining may not capture the correct use patterns of method calls. Consequently, when used in software testing or verification, these mined specifications lead to many false positive defects, which in turn consume much effort for manual investigation.   We present a novel framework for automatically discovering legitimate specifications from execution traces using a  mutation testing  based approach. Such an approach gives a semantics bearing to the legitimacy of the discovered specifications. We introduce the notion of maximal precision and completeness as the desired forms of discovered specifications, and describe in detail suppression techniques that aid efficient discovery. Preliminary evaluation of this approach on several open source software projects shows that specifications discovered through our approach, compared with those discovered through frequent pattern mining, are much more precise and complete. When used in finding bugs, our specifications also locate defects with significantly fewer false positives and more true positives."
2363860,15517,20592,Automatic mediation of privacy-sensitive resource access in smartphone applications,2013,"Mobile app development best practices suggest that developers obtain opt-in consent from users prior to accessing potentially sensitive information on the phone. We study challenges that mobile application developers have with meeting such requirements, and highlight the promise of using new automated, static analysis-based solutions that identify and insert missing prompts in order to guard otherwise unprotected resource accesses. We find evidence that third-party libraries, incorporated by developers across the mobile industry, may access privacy-sensitive resources without seeking consent or even against the user's choice. Based on insights from real examples, we develop the theoretical underpinning of the problem of mediating resource accesses in mobile applications. We design and implement a graph-theoretic algorithm to place mediation prompts that protect every resource access, while avoiding repetitive prompting and prompting in background tasks or third-party libraries.#R##N##R##N#We demonstrate the viability of our approach by analyzing 100 apps, averaging 7.3 MB in size and consisting of dozens of DLLs. Our approach scales well: once an app is represented in the form of a graph, the remaining static analysis takes under a second on average. Overall, our strategy succeeds in about 95% of all unique cases."
1100023,15517,11375,"Feature consistency in compile-time-configurable system software: facing the linux 10,000 feature problem",2011,"Much system software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. A good example is the Linux kernel, which provides more than 10,000 configurable features, growing rapidly.   From the maintenance point of view, compile-time configurability imposes big challenges. The configuration model (the selectable features and their constraints as presented to the user) and the configurability that is actually implemented in the code have to be kept in sync, which, if performed manually, is a tedious and error-prone task. In the case of Linux, this has led to numerous defects in the source code, many of which are actual bugs.   We suggest an approach to automatically check for configurability-related implementation defects in large-scale configurable system software. The configurability is extracted from its various implementation sources and examined for inconsistencies, which manifest in seemingly conditional code that is in fact unconditional. We evaluate our approach with the latest version of Linux, for which our tool detects 1,776 configurability defects, which manifest as dead/superfluous source code and bugs. Our findings have led to numerous source-code improvements and bug fixes in Linux: 123 patches (49 merged) fix 364 defects, 147 of which have been confirmed by the corresponding Linux developers and 20 as fixing a new bug."
1879922,15517,11058,Isolating and understanding concurrency errors using reconstructed execution fragments,2011,"In this paper we propose Recon, a new general approach to concurrency debugging. Recon goes beyond just detecting bugs, it also presents to the programmer short fragments of buggy execution schedules that illustrate how and why bugs happened. These fragments, called  reconstructions , are inferred from inter-thread communication surrounding the root cause of a bug and significantly simplify the process of  understanding  bugs.   The key idea in Recon is to monitor executions and build graphs that encode inter-thread communication with enough context information to build reconstructions. Recon leverages reconstructions built from multiple application executions and uses machine learning to identify which ones illustrate the root cause of a bug. Recon's approach is  general  because it does not rely on heuristics specific to any type of bug, application, or programming model. Therefore, it is able to deal with single- and multiple-variable concurrency bugs regardless of their type ( e.g. , atomicity violation, ordering, etc). To make graph collection efficient, Recon employs selective monitoring and allows metadata information to be imprecise without compromising accuracy. With these optimizations, Recon's graph collection imposes overheads typically between 5x and 20x for both C/C++ and Java programs, with overheads as low as 13% in our experiments. We evaluate Recon with buggy applications, and show it produces reconstructions that include all code points involved in bugs' causes, and presents them in an accurate order. We include a case study of understanding and fixing a previously unresolved bug to showcase Recon's effectiveness."
1545529,15517,23865,New features for duplicate bug detection,2014,"Issue tracking software of large software projects receive a large volume of issue reports each day. Each of these issues is typically triaged by hand, a time consuming and error prone task. Additionally, issue reporters lack the necessary understanding to know whether their issue has previously been reported. This leads to issue trackers containing a lot of duplicate reports, adding complexity to the triaging task.     Duplicate bug report detection is designed to aid developers by automatically grouping bug reports concerning identical issues. Previous work by Alipour et al. has shown that the textual, categorical, and contextual information of an issue report are effective measures in duplicate bug report detection. In our work, we extend previous work by introducing a range of metrics based on the topic distribution of the issue reports, relying only on data taken directly from bug reports. In particular, we introduce a novel metric that measures the first shared topic between two topic-document distributions. This paper details the evaluation of this group of pair-based metrics with a range of machine learning classifiers, using the same issues used by Alipour et al. We demonstrate that the proposed metrics show a significant improvement over previous work, and conclude that the simple metrics we propose should be considered in future studies on bug report deduplication, as well as for more general natural language processing applications."
2110770,15517,23620,Abstraction and invariance for algebraically indexed types,2013,"Reynolds' relational parametricity provides a powerful way to reason about programs in terms of invariance under changes of data representation. A dazzling array of applications of Reynolds' theory exists, exploiting invariance to yield free theorems, non-inhabitation results, and encodings of algebraic datatypes. Outside computer science, invariance is a common theme running through many areas of mathematics and physics. For example, the area of a triangle is unaltered by rotation or flipping. If we scale a triangle, then we scale its area, maintaining an invariant relationship between the two. The transformations under which properties are invariant are often organised into groups, with the algebraic structure reflecting the composability and invertibility of transformations.   In this paper, we investigate programming languages whose types are indexed by algebraic structures such as groups of geometric transformations. Other examples include types indexed by principals--for information flow security--and types indexed by distances--for analysis of analytic uniform continuity properties. Following Reynolds, we prove a general Abstraction Theorem that covers all these instances. Consequences of our Abstraction Theorem include free theorems expressing invariance properties of programs, type isomorphisms based on invariance properties, and non-definability results indicating when certain algebraically indexed types are uninhabited or only inhabited by trivial programs. We have fully formalised our framework and most examples in Coq."
2617800,15517,8422,AVATAR: The Architecture for First-Order Theorem Provers,2014,"This paper describes a new architecture for first-order resolution and superposition theorem provers called AVATAR (Advanced Vampire Architecture for Theories and Resolution). Its original motivation comes from a problem well-studied in the past  dealing with problems having clauses containing propositional variables and other clauses that can be split into components with disjoint sets of variables. Such clauses are common for problems coming from applications, for example in program verification and program analysis, where many ground literals occur in the problems and even more are generated during the proof-search.#R##N##R##N#This problem was previously studied by adding various versions of splitting. The addition of splitting resulted in some improvements in performance of theorem provers. However, even with various versions of splitting, the performance of superposition theorem provers is nowhere near SMT solvers on variable-free problems or SAT solvers on propositional problems.#R##N##R##N#This paper describes a new architecture for superposition theorem provers, where a superposition theorem prover is tightly integrated with a SAT or an SMT solver. Its implementation in our theorem prover Vampire resulted in drastic improvements over all previous implementations of splitting. Over four hundred TPTP problems previously unsolvable by any modern prover, including Vampire itself, have been proved, most of them with short runtimes. Nearly all problems solved with one of 481 variants of splitting previously implemented in Vampire can also be solved with AVATAR.#R##N##R##N#We also believe that AVATAR is an important step towards efficient reasoning with both quantifiers and theories, which is one of the key areas in modern applications of theorem provers in program analysis and verification."
1243978,15517,8806,Security mutation testing of the FileZilla FTP server,2011,"Security has become a priority for software development and many security testing techniques have been developed over the years. Benchmarks based on real-world systems, however, are in great demand for evaluating the vulnerability detection capability of these techniques. To develop such a benchmark, this paper presents an approach to security mutation analysis of FileZilla Server, a popular FTP server implementation as a case study. In the existing mutation testing research, mutants are created through syntactic changes. Such syntactic changes may not result in meaningful security vulnerabilities in security-intensive software. Our approach creates security mutants by considering the causes and consequences of vulnerabilities. The causes of vulnerabilities include design-level (e.g., incorrect policy enforcement) and implementation-level defects (such programming errors as buffer overflow and unsafe function calls). The consequences of vulnerabilities refer to various potential attacks, such as spoofing, tampering, repudiation, information disclosure, denial of service, and elevation of privilege (STRIDE). Using this approach, we have created 30 distinct mutants for FileZilla Server. They have been applied to the evaluation of two security testing methods that use attack trees and attack nets as threat models for test generation. The results show that, while these testing methods can kill most of the mutants, they have an important limitation -- they cannot detect the vulnerabilities that are not captured by the threat models."
1383996,15517,11166,Modeling High-Level Behavior Patterns for Precise Similarity Analysis of Software,2011,"The analysis of software similarity has many applications such as detecting code clones, software plagiarism, code theft, and polymorphic malware. Because often source code is unavailable and code obfuscation is used to avoid detection, there has been much research on developing effective models to capture runtime behavior to aid detection. Existing models focus on low-level information such as dependency or purely occurrence of function calls, and suffer from poor precision, poor scalability, or both. To overcome limitations of existing models, this paper introduces a precise and succinct behavior representation that characterizes high-level object-accessing patterns as regular expressions. We first distill a set of high-level patterns (the alphabet S of the regular language) based on two pieces of information: function call patterns to access objects and type state information of the objects. Then we abstract a runtime trace of a program P into a regular expression e over the pattern alphabet S to produce P's behavior signature. We show that software instances derived from the same code exhibit similar behavior signatures and develop effective algorithms to cluster and match behavior signatures. To evaluate the effectiveness of our behavior model, we have applied it to the similarity analysis of polymorphic malware. Our results on a large malware collection demonstrate that our model is both precise and succinct for effective and scalable matching and detection of polymorphic malware."
1906807,15517,23620,Sound compilation of reals,2014,"Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a finite-precision implementation that is guaranteed to meet the desired precision with respect to real numbers. Our compilation performs a number of verification steps for different candidate precisions. It generates verification conditions that treat all sources of uncertainties in a unified way and encode reasoning about finite-precision roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their non-linear nature, precise reasoning about these verification conditions remains difficult and cannot be handled using state-of-the art SMT solvers alone. We therefore propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Our implementation gives promising results on several numerical models, including dynamical systems, transcendental functions, and controller implementations."
863297,15517,8385,Jalangi: a selective record-replay and dynamic analysis framework for JavaScript,2013,"JavaScript is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of JavaScript applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavy-weight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about JavaScript, which makes it applicable to real-world JavaScript programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undefined, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation profiler in Jalangi. Our evaluation of Jalangi on the SunSpider benchmark suite and on five web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as PIN and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages."
2202968,15517,8385,Are mutants a valid substitute for real faults in software testing,2014,"A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults -- each one a simple syntactic variation -- that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite’s ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automatically-generated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations."
2374071,15517,20332,Automatically generating algebra problems,2012,"We propose computer-assisted techniques for helping with pedagogy in Algebra. In particular, given a proof problem p (of the form Left-hand-side-term = Righthand-side-term), we show how to automatically generate problems that are similar to p. We believe that such a tool can be used by teachers in making examinations where they need to test students on problems similar to what they taught in class, and by students in generating practice problems tailored to their specific needs. Our first insight is that we can generalize p syntactically to a query Q that implicitly represents a set of problems [[Q]] (which includes p). Our second insight is that we can explore the space of problems [[Q]] automatically, use classical results from polynomial identity testing to generate only those problems in [[Q]] that are correct, and then use pruning techniques to generate only unique and interesting problems. Our third insight is that with a small amount of manual tuning on the query Q, the user can interactively guide the computer to generate problems of interest to her.We present the technical details of the above mentioned steps, and also describe a tool where these steps have been implemented. We also present an empirical evaluation on a wide variety of problems from various sub-fields of algebra including polynomials, trigonometry, calculus, determinants etc. Our tool is able to generate a rich corpus of similar problems from each given problem; while some of these similar problems were already present in the textbook, several were new!."
2088805,15517,23836,Using Memory Access Traces to Map Threads and Data on Hierarchical Multi-core Platforms,2011,"In parallel programs, the tasks of a given application must cooperate in order to accomplish the required computation. However, the communication time between the tasks may be different depending on which core they are executing and how the memory hierarchy and interconnection are used. The problem is even more important in multi-core machines with NUMA characteristics, since the remote access imposes high overhead, making them more sensitive to thread and data mapping. In this context, process mapping is a technique that provides performance gains by improving the use of resources such as interconnections, main memory and cache memory. The problem of detecting the best mapping is considered NP-Hard. Furthermore, in shared memory environments, there is an additional difficulty of finding the communication pattern, which is implicit and occurs through memory accesses. This work aims to provide a method for static mapping for NUMA architectures which does not require any prior knowledge of the application. Different metrics were adopted and an heuristic method based on the Edmonds matching algorithm was used to obtain the mapping. In order to evaluate our proposal, we use the NAS Parallel Benchmarks (NPB) and two modern multi-core NUMA machines. Results show performance gains of up to 75% compared to the native scheduler and memory allocator of the operating system."
732775,15517,11058,"Introspective analysis: context-sensitivity, across the board",2014,"Context-sensitivity is the primary approach for adding more precision to a points-to analysis, while hopefully also maintaining scalability. An oft-reported problem with context-sensitive analyses, however, is that they are bi-modal: either the analysis is precise enough that it manipulates only manageable sets of data, and thus scales impressively well, or the analysis gets quickly derailed at the first sign of imprecision and becomes orders-of-magnitude more expensive than would be expected given the program's size. There is currently no approach that makes precise context-sensitive analyses (of any flavor: call-site-, object-, or type-sensitive) scale across the board at a level comparable to that of a context-insensitive analysis. To address this issue, we propose introspective analysis: a technique for uniformly scaling context-sensitive analysis by eliminating its performance-detrimental behavior, at a small precision expense. Introspective analysis consists of a common adaptivity pattern: first perform a context-insensitive analysis, then use the results to selectively refine (i.e., analyze context-sensitively) program elements that will not cause explosion in the running time or space. The technical challenge is to appropriately identify such program elements. We show that a simple but principled approach can be remarkably effective, achieving scalability (often with dramatic speedup) for benchmarks previously completely out-of-reach for deep context-sensitive analyses."
2217704,15517,8868,Variable and thread bounding for systematic testing of multithreaded programs,2013,"Previous approaches to systematic state-space exploration for testing multi-threaded programs have proposed context-bounding and depth-bounding to be effective ranking algorithms for testing multithreaded programs. This paper proposes two new metrics to rank thread schedules for systematic state-space exploration. Our metrics are based on characterization of a concurrency bug using v (the minimum number of distinct variables that need to be involved for the bug to manifest) and t (the minimum number of distinct threads among which scheduling constraints are required to manifest the bug). Our algorithm is based on the hypothesis that in practice, most concurrency bugs have low v (typically 1-2) and low t (typically 2-4) characteristics. We iteratively explore the search space of schedules in increasing orders of v and t. We show qualitatively and empirically that our algorithm finds common bugs in fewer number of execution runs, compared with previous approaches. We also show that using v and t improves the lower bounds on the probability of finding bugs through randomized algorithms. Systematic exploration of schedules requires instrumenting each variable access made by a program, which can be very expensive and severely limits the applicability of this approach. Previous work has avoided this problem by interposing only on synchronization operations (and ignoring other variable accesses). We demonstrate that by using variable bounding (v) and a static imprecise alias analysis, we can interpose on all variable accesses (and not just synchronization operations) at 10-100x less overhead than previous approaches."
2197885,15517,23827,Towards a behavioral software engineering,2014,"Throughout the history of Software Engineering (SE) it has been repeatedly found that the humans involved, i.e. the engineers and developers in addition to other stakeholders, are a key factor in determining project outcomes and success. However, the amount of research that focuses on human aspects has been limited compared to research with technology or process focus. With increasing maturity of the field, interest in agile methods and a growing dissatisfaction with the continued challenges of developing high-quality software on time, the amount of SE research putting human aspect in primary focus has increased.     In this paper we argue that a synthesized view of the emerging human-focused SE research is needed and can add value through giving focus, direction and help identify gaps. Taking cues from the addition of Behavioral Economics as an important part of the area of Economics we propose the term Behavioral Software Engineering (BSE) as an umbrella concept for research that focus on behavioral and social aspects in the work activities of software engineers. We propose that a model based on three units of analysis can give structure and point to concepts that are important for BSE. To add detail to this model we are conducting a systematic review to map out what is currently known. To exemplify the model and the area we here present the results from a subset of the identified concepts."
1655067,15517,8385,An industrial study on the risk of software changes,2012,"Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e.,  changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing . Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects."
2291141,15517,23827,Assessing programming language impact on development and maintenance: a study on c and c++,2011,"Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages."
1205755,15517,23876,Improving program comprehension by answering questions (keynote),2013,"My Natural Programming Project is working on making software development easier to learn, more effective, and less error prone. An important focus over the last few years has been to discover what are the hard-to-answer questions that developers ask while they are trying to comprehend their programs, and then to develop tools to help answer those questions. For example, when studying programmers working on everyday bugs, we found that they continuously ask “Why” and “Why Not” questions as they try to comprehend what happened. We developed the “Whyline” debugging tool, which allows programmers to directly ask these questions of their programs and get a visualization of the answers. In a small lab study, Whyline increased productivity by a factor of about two. We studied professional programmers trying to understand unfamiliar code and identified over 100 questions they identified as hard-to-answer. In particular, we saw that programmers frequently had specific questions about the feasible execution paths, so we developed a new visualization tool to directly present this information. When trying to use unfamiliar APIs, such as the Java SDK and the SAP eSOA APIs, we discovered some common patterns that make programmers up to 10 times slower in finding and understanding how to use the appropriate methods, so we developed new tools to assist them. This talk will provide an overview of our studies and resulting tools that address program comprehension issues."
837283,15517,20754,Using Frankencerts for Automated Adversarial Testing of Certificate Validation in SSL/TLS Implementations,2014,"Modern network security rests on the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. Distributed systems, mobile and desktop applications, embedded devices, and all of secure Web rely on SSL/TLS for protection against network attacks. This protection critically depends on whether SSL/TLS clients correctly validate X.509 certificates presented by servers during the SSL/TLS handshake protocol. We design, implement, and apply the first methodology for large-scale testing of certificate validation logic in SSL/TLS implementations. Our first ingredient is frankencerts, synthetic certificates that are randomly mutated from parts of real certificates and thus include unusual combinations of extensions and constraints. Our second ingredient is differential testing: if one SSL/TLS implementation accepts a certificate while another rejects the same certificate, we use the discrepancy as an oracle for finding flaws in individual implementations. Differential testing with frankencerts uncovered 208 discrepancies between popular SSL/TLS implementations such as OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL, etc. Many of them are caused by serious security vulnerabilities. For example, any server with a valid X.509 version1 certificate can act as a rogue certificate authority and issue fake certificates for any domain, enabling man-in-the-middle attacks against MatrixSSL and GnuTLS. Several implementations also accept certificate authorities created by unauthorized issuers, as well as certificates not intended for server authentication. We also found serious vulnerabilities in how users are warned about certificate validation errors. When presented with an expired, self-signed certificate, NSS, Safari, and Chrome (on Linux) report that the certificate has expired - a low-risk, often ignored error - but not that the connection is insecure against a man-in-the-middle attack. These results demonstrate that automated adversarial testing with frankencerts is a powerful methodology for discovering security flaws in SSL/TLS implementations."
2623207,15517,20332,COSTRIAGE: a cost-aware triage algorithm for bug reporting systems,2011,"'Who can fix this bug?' is an important question in bug triage to accurately assign developers to bug reports. To address this question, recent research treats it as a optimizing recommendation accuracy problem and proposes a solution that is essentially an instance of content-based recommendation (CBR). However, CBR is well-known to cause over-specialization, recommending only the types of bugs that each developer has solved before. This problem is critical in practice, as some experienced developers could be overloaded, and this would slow the bug fixing process. In this paper, we take two directions to address this problem: First, we reformulate the problem as an optimization problem of both accuracy and cost. Second, we adopt a content-boosted collaborative filtering (CBCF), combining an existing CBR with a collaborative filtering recommender (CF), which enhances the recommendation quality of either approach alone. However, unlike general recommendation scenarios, bug fix history is extremely sparse. Due to the nature of bug fixes, one bug is fixed by only one developer, which makes it challenging to pursue the above two directions. To address this challenge, we develop a topic-model to reduce the sparseness and enhance the quality of CBCF. Our experimental evaluation shows that our solution reduces the cost efficiently by 30% without seriously compromising accuracy."
1532382,15517,8385,A case study of cross-system porting in forked projects,2012,"Software forking---creating a variant product by copying and modifying an existing product---is often considered an ad hoc, low cost alternative to principled product line development. To maintain such forked products, developers often need to port an existing feature or bug-fix from one product variant to another. As a first step towards assessing whether forking is a sustainable practice, we conduct an in-depth case study of 18 years of the BSD product family history. Our study finds that maintaining forked projects involves significant effort of porting patches from other projects. Cross-system porting happens periodically and the porting rate does not necessarily decrease over time. A significant portion of active developers participate in porting changes from peer projects. Surprisingly, ported changes are less defect-prone than non-ported changes. Our work is the first to comprehensively characterize the temporal, spatial, and developer dimensions of cross-system porting in the BSD family, and our tool Repertoire is the first automated tool for detecting ported edits with high accuracy of 94% precision and 84% recall. Our study finds that the upkeep work of porting changes from peer projects is significant and currently, porting practice seems to heavily depend on developers doing their porting job on time. This result calls for new techniques to automate cross-system porting to reduce the maintenance cost of forked projects."
2299302,15517,8385,"Does adding manpower also affect quality?: an empirical, longitudinal analysis",2011,"With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics, we quantified characteristics of team expansion, including team size, expansion rate, expansion acceleration, and modularity with respect to department designations. We examined statistical correlations between our monthly team-level metrics and monthly product-level metrics. Our results indicate that increased team size and linear growth are correlated with later periods of better product quality. However, periods of accelerated team expansion are correlated with later periods of reduced software quality. Furthermore, our linear regression prediction model based on team metrics was able to predict the product's post-release failure rate within a 95% prediction interval for 38 out of 40 months. Our analysis provides insight for project managers into how the expansion of development teams can impact product quality."
2306574,15517,23827,Engineering and verifying requirements for programmable self-assembling nanomachines,2012,"We propose an extension of van Lamsweerde's goal-oriented requirements engineering to the domain of programmable DNA nanotechnology. This is a domain in which individual devices (agents) are at most a few dozen nanometers in diameter. These devices are programmed to assemble themselves from molecular components and perform their assigned tasks. The devices carry out their tasks in the probabilistic world of chemical kinetics, so they are individually error-prone. However, the number of devices deployed is roughly on the order of a nanomole (a 6 followed by fourteen 0s), and some goals are achieved when enough of these agents achieve their assigned subgoals. We show that it is useful in this setting to augment the AND/OR goal diagrams to allow goal refinements that are mediated by threshold functions, rather than ANDs or ORs. We illustrate this method by engineering requirements for a system of molecular detectors (DNA origami “pliers” that capture target molecules) invented by Kuzuya, Sakai, Yamazaki, Xu, and Komiyama (2011). We model this system in the Prism probabilistic symbolic model checker, and we use Prism to verify that requirements are satisfied, provided that the ratio of target molecules to detectors is neither too high nor too low. This gives prima facie evidence that software engineering methods can be used to make DNA nanotechnology more productive, predictable and safe."
1244543,15517,20524,Flexible feature binding in software product lines,2011,"A software product line (SPL) is a family of programs that share assets from a common code base. The programs of an SPL can be distinguished in terms of features, which represent units of program functionality that satisfy stakeholders' requirements. The features of an SPL can be bound either statically at program compile time or dynamically at run time. Both binding times are used in SPL development and have different advantages. For example, dynamic binding provides high flexibility whereas static binding supports fine-grained customizability without any impact on performance (e.g., for use on embedded systems). However, contemporary techniques for implementing SPLs force a programmer to choose the binding time already when designing an SPL and to mix different implementation techniques when multiple binding times are needed. We present an approach that integrates static and dynamic feature binding seamlessly. It allows a programmer to implement an SPL once and to decide per feature at deployment time whether it should be bound statically or dynamically. Dynamic binding usually introduces an overhead regarding resource consumption and performance. We reduce this overhead by statically merging features that are used together into dynamic binding units. A program can be configured at run time by composing binding units on demand. We use feature models to ensure that only valid feature combinations can be selected at compile and at run time. We provide a compiler and evaluate our approach on the basis of two non-trivial SPLs."
2380164,15517,23497,Continuous object access profiling and optimizations to overcome the memory wall and bloat,2012,"Future microprocessors will have more serious memory wall problems since they will include more cores and threads in each chip. Similarly, future applications will have more serious memory bloat problems since they are more often written using object-oriented languages and reusable frameworks. To overcome such problems, the language runtime environments must accurately and efficiently profile how programs access objects. We propose Barrier Profiler, a low-overhead object access profiler using a memory-protection-based approach called pointer barrierization and adaptive overhead reduction techniques. Unlike previous memory-protection-based techniques, pointer barrierization offers per-object protection by converting all of the pointers to a given object to corresponding barrier pointers that point to protected pages. Barrier Profiler achieves low overhead by not causing signals at object accesses that are unrelated to the needed profiles, based on profile feedback and a compiler analysis. Our experimental results showed Barrier Profiler provided sufficiently accurate profiles with 1.3% on average and at most 3.4% performance overhead for allocation-intensive benchmarks, while previous code-instrumentation-based techniques suffered from 9.2% on average and at most 12.6% overhead. The low overhead allows Barrier Profiler to be run continuously on production systems. Using Barrier Profiler, we implemented two new online optimizations to compress write-only character arrays and to adjust the initial sizes of mostly non-accessed arrays. They resulted in speed-ups of up to 8.6% and 36%, respectively."
2401698,15517,23827,Toddler: detecting performance problems via similar memory-access patterns,2013,"Performance bugs are programming errors that create significant performance degradation. While developers often use automated oracles for detecting functional bugs, detecting performance bugs usually requires time-consuming, manual analysis of execution profiles. The human effort for performance analysis limits the number of performance tests analyzed and enables performance bugs to easily escape to production. Unfortunately, while profilers can successfully localize slow executing code, profilers cannot be effectively used as automated oracles. This paper presents Toddler, a novel automated oracle for performance bugs, which enables testing for performance bugs to use the well established and automated process of testing for functional bugs. Toddler reports code loops whose computation has repetitive and partially similar memory-access patterns across loop iterations. Such repetitive work is likely unnecessary and can be done faster. We implement Toddler for Java and evaluate it on 9 popular Java codebases. Our experiments with 11 previously known, real-world performance bugs show that Toddler finds these bugs with a higher accuracy than the standard Java profiler. Using Toddler, we also found 42 new bugs in six Java projects: Ant, Google Core Libraries, JUnit, Apache Collections, JDK, and JFreeChart. Based on our bug reports, developers so far fixed 10 bugs and confirmed 6 more as real bugs."
1407412,15517,23827,Deploying an online software engineering education program in a globally distributed organization,2014,"A well-trained software engineering workforce is a key to success in a highly competitive environment. Changing tools and technologies, along with a rapidly changing development environment, make it incumbent on organizations to invest in training. In this paper, we describe our experience in deploying an online training program in a globally distributed organization. We write about the reasons behind ABB’s Software Development Improvement Program (SDIP), the requirements we established upfront, the people, processes and technologies we used, the promotion of SDIP, and metrics for measuring success. Finally, we share and describe results and lessons learned that could be applied to many organizations with similar issues. The goal of this paper is to provide a set of replicable best practices for initiating a software training program in a multi-national organization. The first SDIP online course was offered in June 2012. Since then, we have had more than 10,000 enrollments from employees in 54 countries. Today, our training library contains 89 e-learning, 17 webinar, video and virtual lab courses, and we have delivered more than 180 hosted webinars. Following each class, we ask students to evaluate the class. Ninety-eight percent are satisfied with the classes."
2124818,15517,517,MOST: A Multi-objective Search-Based Testing from EFSM,2011,"This paper introduces a multi-objective evolutionary approach to test case generation from extended finite state machines (EFSM), named MOST. Testing from an (E)FSM generally involves executing various transition paths, until a given coverage criterion (e.g. cover all transitions) is met. As traditional test generation methods from FSM only consider the control aspects, they can produce many infeasible paths when applied to EFSMs, due to conflicts in guard conditions along a path. In order to avoid the infeasible path generation, we propose an approach that obtains feasible paths dynamically, instead of performing static reach ability analysis as usual for FSM-based methods. Previous works have treated EFSM test case generation as a mono-objective optimization problem. Our approach takes two objectives into account that are the coverage criterion and the solution length. In this way, it is not necessary to establish in advance the test case size as earlier approaches. MOST constructs a Pareto set approximation, i.e., a group of optimal solutions, which allows the test team to select the solutions that represent a good trade-off between both objectives. The paper shows empirical studies to illustrate the benefits of the approach and comparing the results with the ones obtained in a related work."
2139294,15517,11058,Path-based inductive synthesis for program inversion,2011,"In this paper, we investigate the problem of semi-automated inversion of imperative programs, which has the potential to make it much easier and less error prone to write programs that naturally pair as inverses, such as insert/delete operations, compressors/decompressors, and so on. Viewing inversion as a subproblem of program synthesis, we propose a novel synthesis technique called Path-based inductive synthesis (PINS) and apply it to inversion. PINS starts from a program  P  and a template  T  for its inverse. PINS then iteratively refines the space of template instantiations by exploring paths in the composition of  P  and  T  with symbolic execution. PINS uses an SMT solver to intelligently guide the refinement process, based on the paths explored so far. The key idea motivating this approach is the  small path-bound hypothesis : that the behavior of a program can be summarized with a small, carefully chosen set of its program paths.   We evaluated PINS by using it to invert 14 programs such as compressors (e.g., Lempel-Ziv-Welch), encoders (e.g., UUEncode), and arithmetic operations (e.g., vector rotation). Most of these examples are difficult or impossible to invert using prior techniques, but PINS was able to invert all of them. We also found that a semi-automated technique we developed to  mine  a template from the program to be inverted worked well. In our experiments, PINS takes between one second to thirty minutes to synthesize inverses. We believe this proof-of-concept implementation demonstrates the viability of the PINS approach to program synthesis."
1774763,15517,11058,Languages as libraries,2011,"Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.    The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from native notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language.   This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation."
1158540,15517,23827,Certifiably safe software-dependent systems: challenges and directions,2014,"The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.     On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.     This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers."
1436960,15517,517,CDM-Suite: An Attributed Test Selection Tool,2013,"Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline."
998856,15517,23827,Billions and billions of constraints: whitebox fuzz testing in production,2013,"We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems."
2019366,15517,8868,Efficient mutation analysis by propagating and partitioning infected execution states,2014,"Mutation analysis evaluates a testing technique by measur- ing how well it detects seeded faults (mutants). Mutation analysis is hampered by inherent scalability problems — a test suite is executed for each of a large number of mutants. Despite numerous optimizations presented in the literature, this scalability issue remains, and this is one of the reasons why mutation analysis is hardly used in practice. Whereas most previous optimizations attempted to stati- cally reduce the number of executions or their computational overhead, this paper exploits information available only at run time to further reduce the number of executions. First, state infection conditions can reveal — with a single test execution of the unmutated program — which mutants would lead to a different state, thus avoiding unnecessary test executions. Second, determining whether an infected execution state propagates can further reduce the number of executions. Mutants that are embedded in compound expressions may infect the state locally without affecting the outcome of the compound expression. Third, those mutants that do infect the state can be partitioned based on the resulting infected state — if two mutants lead to the same infected state, only one needs to be executed as the result of the other can be inferred. We have implemented these optimizations in the Major mu- tation framework and empirically evaluated them on 14 open source programs. The optimizations reduced the mutation analysis time by 40% on average."
603156,15517,8422,Approximately bisimilar symbolic models for digital control systems,2012,"Symbolic approaches to control hybrid systems construct a discrete approximately-bisimilar abstraction of a continuous control system and apply automata-theoretic techniques to construct controllers enforcing given specifications. For the class of digital control systems (i.e., whose control signals are piecewise constant) satisfying incremental input-to-state stability (δ-ISS), existing techniques to compute discrete abstractions begin with a quantization of the state and input sets, and show that the quantized system is approximately bisimilar to the original if the sampling time is sufficiently large or if the Lyapunov functions of the system decrease fast enough. If the sampling time is not sufficiently large, the former technique fails to apply. While abstraction based on Lyapunov functions may be applicable, because of the conservative nature of Lyapunov functions in practice, the size of the discrete abstraction may be too large for subsequent analyses.#R##N##R##N#In this paper, we propose a technique to compute discrete approximately-bisimilar abstractions of δ-ISS digital control systems. Our technique quantizes the state and input sets, but is based on multiple sampling steps: instead of requiring that the sampling time is sufficiently large (which may not hold), the abstract transition system relates states multiple sampling steps apart.#R##N##R##N#We show on practical examples that the discrete state sets computed by our procedure can be several orders of magnitude smaller than existing approaches, and can compute symbolic approximate-bisimilar models even when other existing approaches do not apply or time-out. Since the size of the discrete state set is the main limiting factor in the application of symbolic control, our results enable symbolic control of larger systems than was possible before."
1730381,15517,23827,Positive transitions from the classroom to the cubicle: Creating strategies for augmenting professional development in the software engineering curriculum,2013,"Engineering ethics often receives attention in separate courses or modules, however other critical elements of professionalism are not directly addressed in a curriculum. Graduates coming to industry frequently lack proficiency in vital skills such as being adept in appropriate interpersonal communications, quickly adapting to industry protocols, and effectively acclimating to corporate hierarchy, culture and accountability. Many programs assume that professionalism is indirectly socialized through faculty modeling and classroom management. Such an assumption does not serve our students well, or the SE industry that hires them and then must put a tremendous effort into new employee orientation to create a productive work environment. This highly interactive tutorial provides practical resources for those who seek to introduce professionalism education into the SE curriculum but cannot create whole new courses devoted to the subject. Participants who will come away with concrete plans to assess, create, and integrate professional development opportunities into their academic Software Engineering curricula or industry training programs. This tutorial is a fertile opportunity for SE educators, advisors, administrators and representatives from SE undergraduate professional societies who want to respond to a vital concern for many campuses."
1069509,15517,11058,AutoSynch: an automatic-signal monitor based on predicate tagging,2013,"Most programming languages use monitors with explicit signals for synchronization in shared-memory programs. Requiring programmers to signal threads explicitly results in many concurrency bugs due to missed notifications, or notifications on wrong condition variables. In this paper, we describe an implementation of an automatic signaling monitor in Java called AutoSynch that eliminates such concurrency bugs by removing the burden of signaling from the programmer. We show that the belief that automatic signaling monitors are prohibitively expensive is wrong. For most problems, programs based on AutoSynch are almost as fast as those based on explicit signaling. For some, AutoSynch is even faster than explicit signaling because it never uses signalAll, whereas the programmers end up using signalAll with the explicit signal mechanism.   AutoSynch} achieves efficiency in synchronization based on three novel ideas. We introduce an operation called closure that enables the predicate evaluation in every thread, thereby reducing context switches during the execution of the program. Secondly, AutoSynch avoids signalAll by using a property called relay invariance that guarantees that whenever possible there is always at least one thread whose condition is true which has been signaled. Finally, AutoSynch uses a technique called predicate tagging to efficiently determine a thread that should be signaled. To evaluate the efficiency of AutoSynch, we have implemented many different well-known synchronization problems such as the producers/consumers problem, the readers/writers problems, and the dining philosophers problem. The results show that AutoSynch is almost as efficient as the explicit-signal monitor and even more efficient for some cases."
1907232,15517,23749,User Transparent Data and Task Parallel Multimedia Computing with Pyxis-DT,2012,"The research area of Multimedia Content Analysis (MMCA) considers all aspects of the automated extraction of knowledge from multimedia archives and data streams. To satisfy the increasing computational demands of emerging MMCA problems, there is an urgent need to apply High Performance Computing (HPC) techniques. However, as most MMCA researchers are not also HPC experts, in the field there is a demand~for~programming models and tools that are both efficient and easy~to~use. Today several user transparent library-based parallelization tools exist that aim to satisfy both these requirements. Such tools generally use a data parallel approach in which data structures (e.g. video frames) are scattered among the available nodes in a compute cluster. However, for certain MMCA applications a data parallel approach induces intensive communication, which significantly decreases performance. In these situations, we can benefit from applying alternative approaches. This paper presents Pyxis-DT: a user transparent parallel programming model for MMCA applications that employs both data and task parallelism. Hybrid parallel execution is obtained by run-time construction and execution of a task graph consisting of strictly defined building block operations. Each of these building block operations can be executed in data parallel fashion. Results show that for realistic MMCA applications the concurrent use of data and task parallelism can significantly improve performance compared to using either approach in isolation."
926313,15517,23865,Modern code reviews in open-source projects: which problems do they fix?,2014,"Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7–35% of review comments are discarded and that 10–22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes."
946260,15517,8868,Cross-platform feature matching for web applications,2014,"With the emergence of new computing platforms, software written for traditional platforms is being re-targeted to reach the users on these new platforms. In particular, due to the proliferation of mobile computing devices, it is common practice for companies to build mobile-specific versions of their existing web applications to provide mobile users with a better experience. Because the differences between desktop and mobile versions of a web application are not only cosmetic, but can also include substantial rewrites of key components, it is not uncommon for these different versions to provide different sets of features. Whereas some of these differences are intentional, such as the addition of location-based features on mobile devices, others are not and can negatively affect the user experience, as confirmed by numerous user reports and complaints. Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application. We implemented our technique in a tool, called FMAP, and used it to perform a preliminary empirical evaluation on nine real-world multi-platform web applications. The results of our evaluation are promising, as FMAP was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by our analysis of user reports and software fixes for these applications."
1088067,15517,8385,Automatic mining of specifications from invocation traces and method invariants,2014,"Software library documentation often describes individual methods' APIs, but not the intended protocols and method interactions. This can lead to library misuse, and restrict runtime detection of protocol violations and automated verification of software that uses the library. Specification mining, if accurate, can help mitigate these issues, which has led to significant research into new model-inference techniques that produce FSM-based models from program invariants and execution traces. However, there is currently a lack of empirical studies that, in a principled way, measure the impact of the inference strategies on model quality. To this end, we identify four such strategies and systematically study the quality of the models they produce for nine off-the-shelf libraries. We find that (1) using invariants to infer an initial model significantly improves model quality, increasing precision by 4% and recall by 41%, on average; (2) effective invariant filtering is crucial for quality and scalability of strategies that use invariants; and (3) using traces in combination with invariants greatly improves robustness to input noise. We present our empirical evaluation, implement new and extend existing model-inference techniques, and make public our implementations, ground-truth models, and experimental data. Our work can lead to higher-quality model inference, and directly improve the techniques and tools that rely on model inference."
885005,15517,20774,Parallelism in dynamic well-spaced point sets,2011,"Parallel algorithms and dynamic algorithms possess an interesting duality property: compared to sequential algorithms, parallel algorithms improve run-time while preserving work, while dynamic algorithms improve work but typically offer no parallelism. Although they are often considered separately, parallel and dynamic algorithms employ similar design techniques. They both identify parts of the computation that are independent of each other. This suggests that dynamic algorithms could be parallelized to improve work efficiency while preserving fast parallel run-time.   In this paper, we parallelize a dynamic algorithm for well-spaced point sets, an important problem related to mesh refinement in computational geometry. Our parallel dynamic algorithm computes a well-spaced superset of a dynamically changing set of points, allowing arbitrary dynamic modifications to the input set. On an EREW PRAM, our algorithm processes batches of  k  modifications such as insertions and deletions in  O ( k  log Δ) total work and in  O (log Δ) parallel time using  k  processors, where Δ is the geometric spread of the data, while ensuring that the output is always within a constant factor of the optimal size. EREW PRAM model is quite different from actual hardware such as modern multiprocessors. We therefore describe techniques for implementing our algorithm on modern multi-core computers and provide a prototype implementation. Our empirical evaluation shows that our algorithm can be practical, yielding a large degree of parallelism and good speedups."
1941776,15517,23620,Constraints as control,2012,"We present an extension of Scala that supports constraint programming over bounded and unbounded domains. The resulting language, Kaplan, provides the benefits of constraint programming while preserving the existing features of Scala. Kaplan integrates constraint and imperative programming by using constraints as an advanced control structure; the developers use the monadic 'for' construct to iterate over the solutions of constraints or branch on the existence of a solution. The constructs we introduce have simple semantics that can be understood as explicit enumeration of values, but are implemented more efficiently using symbolic reasoning. Kaplan programs can manipulate constraints at run-time, with the combined benefits of type-safe syntax trees and first-class functions. The language of constraints is a functional subset of Scala, supporting arbitrary recursive function definitions over algebraic data types, sets, maps, and integers.   Our implementation runs on a platform combining a constraint solver with a standard virtual machine. For constraint solving we use an algorithm that handles recursive function definitions through fair function unrolling and builds upon the state-of-the art SMT solver Z3. We evaluate Kaplan on examples ranging from enumeration of data structures to execution of declarative specifications. We found Kaplan promising because it is expressive, supporting a range of problem domains, while enabling full-speed execution of programs that do not rely on constraint programming."
776172,15517,23827,Build your own model checker in one month,2013,"Model checking has established as an effective method for automatic system analysis and verification. It is making its way into many domains and methodologies. Applying model checking techniques to a new domain (which probably has its own dedicated modeling language) is, however, far from trivial. Translation-based approach works by translating domain specific languages into input languages of a model checker. Because the model checker is not designed for the domain (or equivalently, the language), translation-based approach is often ad hoc. Ideally, it is desirable to have an optimized model checker for each application domain. Implementing one with reasonable efficiency, however, requires years of dedicated efforts. In this tutorial, we will briefly survey a variety of model checking techniques. Then we will show how to develop a model checker for a language combining real-time and probabilistic features using the PAT (Process Analysis Toolkit) step-by-step, and show that it could take as short as a few weeks to develop your own model checker with reasonable efficiency. The PAT system is designed to facilitate development of customized model checkers. It has an extensible and modularized architecture to support new languages (and their operational semantics), new state reduction or abstraction techniques, new model checking algorithms, etc. Since its introduction 5 years ago, PAT has attracted more than 2500 registered users (from 500+ organisations in 60 countries) and has been applied to develop model checkers for 20 different languages."
639507,15517,20524,Transferring an automated test generation tool to practice: from pex to fakes and code digger,2014,"Producing industry impacts has been an important, yet challenging task for the research community. In this paper, we report experiences on successful technology transfer of Pex and its relatives (tools derived from or associated with Pex) from Microsoft Research and lessons learned from more than eight years of research efforts by the Pex team in collaboration with academia. Moles, a tool associated with Pex, was shipped as Fakes with Visual Studio since August 2012, benefiting a huge user base of Visual Studio around the world. The number of download counts of Pex and its lightweight version called Code Digger has reached tens of thousands within one or two years. Pex4Fun (derived from Pex), an educational gaming website released since June 2010, has achieved high educational impacts, reflected by the number of clicks of the Ask Pex! button (indicating the attempts made by users to solve games in Pex4Fun) as over 1.5 million till July 2014. Evolved from Pex4Fun, the Code Hunt website has been used in a very large programming competition. In this paper, we discuss the technology background, tool overview, impacts, project timeline, and lessons learned from the project. We hope that our reported experiences can inspire more high-impact technology-transfer research from the research community."
1128078,15517,22260,Scaling Down Off-the-Shelf Data Compression: Backwards-Compatible Fine-Grain Mixing,2012,"Pu and Singaravelu presented Fine-Grain Mixing, an adaptive compression system which aimed to maximize CPU and network utilization simultaneously by splitting a network stream into a mixture of compressed and uncompressed blocks. Blocks were compressed opportunistically in a send buffer, they compressed as many blocks as they could without becoming a bottleneck. They successfully utilized all available CPU and network bandwidth even on high speed connections. In addition, they noted much greater throughput than previous adaptive compression systems. Here, we take a different view of FG-Mixing than was taken by Pu and Singaravelu and give another explanation for its high performance: that fine-grain mixing of compressed and uncompressed blocks enables off-the-shelf compressors to scale down their degree of compression linearly with decreasing CPU usage. Exploring the scaling behavior in-depth allows us to make a variety of improvements to fine-grain mixed compression: better compression ratios for a given level of CPU consumption, a wider range of data reduction and CPU cost options, and parallelized compression to take advantage of multi-core CPUs. We make full compatibility with the ubiquitous deflate decompress or (as used in many network protocols directly, or as the back-end of the gzip and Zip formats) a primary goal, rather than using a special, incompatible protocol as in the original implementation of FG-Mixing. Moreover, we show that the benefits of fine-grain mixing are retained by our compatible version."
1571620,15517,517,Overcoming Web Server Benchmarking Challenges in the Multi-core Era,2012,"Web-based services are used by many organizations to support their customers and employees. An important consideration in developing such services is ensuring the Quality of Service (QoS) that users experience is acceptable. Recent years have seen a shift toward deploying Web service son multi-core hardware. Leveraging the performance benefits of multi-core hardware is a non-trivial task. In particular, systematic Web server benchmarking techniques are needed so organizations can verify their ability to meet customer QoS objectives while effectively utilizing such hardware. However, our recent experiences suggest that the multi-core era imposes significant challenges to Web server benchmarking. In particular, due to limitations of current hardware monitoring tools, we found that a large number of experiments are needed to detect complex bottlenecks that can arise in a multi-core system due to contention for shared resources such as cache hierarchy, memory controllers and processor inter-connects. Furthermore, multiple load generator instances are needed to adequately stress multi-core hardware. This leads to practical challenges in validating and managing the test results. This paper describes the automation strategies we employed to overcome these challenges. We make our test harness available for other researchers and practitioners working on similar studies."
1888209,15517,23827,Palus: a hybrid automated test generation tool for java,2011,"In object-oriented programs, a unit test often consists of a sequence of method calls that create and mutate objects. It is challenging to automatically generate sequences that are  legal  and  behaviorally-diverse , that is, reaching as many different program states as possible.   This paper proposes a combined static and dynamic test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests.   Our Palus tool implements this approach. We compared it with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on six popular open-source Java programs. Tests generated by Palus achieved 35% higher structural coverage on average. Palus is also internally used in Google, and has found 22 new bugs in four well-tested products."
575497,15517,11223,SKI: exposing kernel concurrency bugs through systematic schedule exploration,2014,"Kernel concurrency bugs are notoriously difficult to find during testing since they are only triggered under certain instruction interleavings. Unfortunately, no tools for systematically subjecting kernel code to concurrency tests have been proposed to date. This gap in tool support may be explained by the challenge of controlling precisely which kernel interleavings are executed without modifying the kernel under test itself. Furthermore, to be practical, prohibitive runtime overheads must be avoided and tools must remain portable as the kernel evolves.#R##N##R##N#In this paper, we propose SKI, the first tool for the systematic exploration of possible interleavings of kernel code. SKI finds kernel bugs in unmodified kernels, and is thus directly applicable to different kernels. To achieve control over kernel interleavings in a portable way, SKI uses an adapted virtual machine monitor that performs an efficient analysis of the kernel execution on a virtual multiprocessor platform. This enables SKI to determine which kernel execution flows are eligible to run, and also to selectively control which flows may proceed. In addition, we detail several essential optimizations that enable SKI to scale to real-world concurrency bugs.#R##N##R##N#We reliably reproduced previously reported bugs by applying SKI to different versions of the Linux kernel and to the FreeBSD kernel. Our evaluation further shows that SKI was able to discover, in widely used and already heavily tested file systems (e.g., ext4, btrfs), several unknown bugs, some of which pose the risk of data loss."
1944111,15517,8385,An empirical analysis of the co-evolution of schema and code in database applications,2013,"Modern database applications are among the most widely used and complex software systems. They constantly evolve, responding to changes to data, database schemas, and code. It is challenging to manage these changes and ensure that everything co-evolves consistently. For example, when a database schema is modified, all the code that interacts with the database must be changed accordingly. Although database evolution and software evolution have been extensively studied in isolation, the co-evolution of schema and code has largely been unexplored.     This paper presents the first comprehensive empirical analysis of the co-evolution of database schemas and code in ten popular large open-source database applications, totaling over 160K revisions. Our major findings include: 1) Database schemas evolve frequently during the application lifecycle, exhibiting a variety of change types with similar distributions across the studied applications; 2) Overall, schema changes induce significant code-level modifications, while certain change types have more impact on code than others; and 3) Co-change analyses can be viable to automate or assist with database application evolution. We have also observed that: 1) 80% of the schema changes happened in 20-30% of the tables, while nearly 40% of the tables did not change; and 2) Referential integrity constraints and stored procedures are rarely used in our studied subjects. We believe that our study reveals new insights into how database applications evolve and useful guidelines for designing assistive tools to aid their evolution."
38186,15517,8422,Regression Test Selection for Distributed Software Histories,2014,"Regression test selection analyzes incremental changes to a codebase and chooses to run only those tests whose behavior may be affected by the latest changes in the code. By focusing on a small subset of all the tests, the testing process runs faster and can be more tightly integrated into the development process. Existing techniques for regression test selection consider two versions of the code at a time, effectively assuming a development process where changes to the code occur in a linear sequence.#R##N##R##N#Modern development processes that use distributed version-control systems are more complex. Software version histories are generally modeled as directed graphs; in addition to version changes occurring linearly, multiple versions can be related by other commands, e.g., branch, merge, rebase, cherry-pick, revert, etc. This paper describes a regression test-selection technique for software developed using modern distributed version-control systems. By modeling different branch or merge commands directly in our technique, it computes safe test sets that can be substantially smaller than applying previous techniques to a linearization of the software history.#R##N##R##N#We evaluate our technique on software histories of several large open-source projects. The results are encouraging: our technique obtained an average of 10.89× reduction in the number of tests over an existing technique while still selecting all tests whose behavior may differ."
2079607,15517,23497,On-the-fly elimination of dynamic irregularities for GPU computing,2011,"The power-efficient massively parallel Graphics Processing Units (GPUs) have become increasingly influential for general-purpose computing over the past few years. However, their efficiency is sensitive to dynamic irregular memory references and control flows in an application. Experiments have shown great performance gains when these irregularities are removed. But it remains an open question how to achieve those gains through software approaches on modern GPUs.   This paper presents a systematic exploration to tackle dynamic irregularities in both control flows and memory references. It reveals some properties of dynamic irregularities in both control flows and memory references, their interactions, and their relations with program data and threads. It describes several heuristics-based algorithms and runtime adaptation techniques for effectively removing dynamic irregularities through data reordering and job swapping. It presents a framework, G-Streamline, as a unified software solution to dynamic irregularities in GPU computing. G-Streamline has several distinctive properties. It is a pure software solution and works on the fly, requiring no hardware extensions or offline profiling. It treats both types of irregularities at the same time in a holistic fashion, maximizing the whole-program performance by resolving conflicts among optimizations. Its optimization overhead is largely transparent to GPU kernel executions, jeopardizing no basic efficiency of the GPU application. Finally, it is robust to the presence of various complexities in GPU applications. Experiments show that G-Streamline is effective in reducing dynamic irregularities in GPU computing, producing speedups between 1.07 and 2.5 for a variety of applications."
1040221,15517,20524,JFlow: Practical refactorings for flow-based parallelism,2013,"Emerging applications in the domains of recognition, mining and synthesis (RMS); image and video processing; data warehousing; and automatic financial trading admit a particular style of parallelism termed flow-based parallelism. To help developers exploit flow-based parallelism, popular parallel libraries such as Groovy's GPars, Intel's TBB Flow Graph and Microsoft's TPL Dataflow have begun introducing many new and useful constructs. However, to reap the benefits of such constructs, developers must first use them. This involves refactoring their existing sequential code to incorporate these constructs - a manual process that overwhelms even experts. To alleviate this burden, we introduce a set of novel analyses and transformations targeting flow-based parallelism. We implemented these ideas in JFlow, an interactive refactoring tool integrated into the Eclipse IDE. We used JFlow to parallelize seven applications: four from a previously known benchmark and three from a suite of large open source projects. JFlow, with minimal interaction from the developer, can successfully parallelize applications from the aforementioned domains with good performance (offering up to 3.45x speedup on a 4-core machine) and is fast enough to be used interactively as part of a developer's workflow."
1177294,15517,11058,DoubleChecker: efficient sound and precise atomicity checking,2014,"Atomicity is a key correctness property that allows programmers to reason about code regions in isolation. However, programs often fail to enforce atomicity correctly, leading to atomicity violations that are difficult to detect. Dynamic program analysis can detect atomicity violations based on an atomicity specification, but existing approaches slow programs substantially.   This paper presents DoubleChecker, a novel sound and precise atomicity checker whose key insight lies in its use of two new cooperating dynamic analyses. Its  imprecise  analysis tracks cross-thread dependences soundly but imprecisely with significantly better performance than a fully precise analysis. Its  precise  analysis is more expensive but only needs to process a subset of the execution identified as potentially involved in atomicity violations by the imprecise analysis. If DoubleChecker operates in  single-run  mode, the two analyses execute in the same program run, which guarantees soundness and precision but requires logging program accesses to pass from the imprecise to the precise analysis. In  multi-run  mode, the first program run executes only the imprecise analysis, and a second run executes both analyses. Multi-run mode trades accuracy for performance; each run of multi-run mode outperforms single-run mode, but can potentially miss violations.   We have implemented DoubleChecker and an existing state-of-the-art atomicity checker called Velodrome in a high-performance Java virtual machine. DoubleChecker's single-run mode significantly outperforms Velodrome, while still providing full soundness and precision. DoubleChecker's multi-run mode improves performance further, without significantly impacting soundness in practice. These results suggest that DoubleChecker's approach is a promising direction for improving the performance of dynamic atomicity checking over prior work."
918528,15517,20561,Effectiveness of Random Testing of Embedded Systems,2012,"Embedded systems like those used in automobiles have two peculiar attributes - they are reactive systems where each reaction is influenced by the current state of the system, and their inputs come from small domains. We hypothesise that, because inputs come from small domains, random testing is likely to cover all values in the domain and hence have an effectiveness comparable to other techniques. We also hypothesise that because of the reactive nature long sequences of interactions will be important for testing effectiveness. To test these hypotheses we conducted three experiments on three pieces of code selected from an automotive application. The first two experiments were designed to compare the effectiveness of randomly generated test cases against test cases that achieve the modified condition decision coverage (MCDC) and also evaluate the impact of length of the test cases on effectiveness. The third experiment compares the effectiveness of handwritten test cases against randomly generated test cases of similar length. Our objective is to help practitioners choose an effective technique to test their systems. Our findings from the limited experiments indicate that random test case generation is as effective as manual test generation at the system level. However, for unit testing test case generation to achieve MCDC coverage is more effective than random generation. Combining unit test cases with system level testing increases effectiveness. Our final observation is that increasing the test case length improves the effectiveness of a test suite both at the unit and system level."
660086,15517,22232,Partial model checking using networks of labelled transition systems and boolean equation systems,2012,"Partial model checking was proposed by Andersen in 1995 to verify a temporal logic formula compositionally on a composition of processes. It consists in incrementally incorporating into the formula the behavioural information taken from one process -- an operation called quotienting -- to obtain a new formula that can be verified on a smaller composition from which the incorporated process has been removed. Simplifications of the formula must be applied at each step, so as to maintain the formula at a tractable size. In this paper, we revisit partial model checking. First, we extend quotienting to the network of labelled transition systems model, which subsumes most parallel composition operators, including m among n synchronisation and parallel composition using synchronisation interfaces, available in the E-Lotos standard. Second, we reformulate quotienting in terms of a simple synchronous product between a graph representation of the formula (called formula graph) and a process, thus enabling quotienting to be implemented efficiently and easily, by reusing existing tools dedicated to graph compositions. Third, we propose simplifications of the formula as a combination of bisimulations and reductions using Boolean equation systems applied directly to the formula graph, thus enabling formula simplifications also to be implemented easily and efficiently. Finally, we describe an implementation in the CADP (Construction and Analysis of Distributed Processes) toolbox and present some experimental results in which partial model checking uses hundreds of times less memory than on-the-fly model checking."
2203052,15517,23827,University meets industry: Calling in real stakeholders,2013,"Teaching the discipline of requirements engineering (RE) is slowly establishing at universities within the software engineering curriculum. While several studies have shown that case study-based education was more efficient in RE, many teachers are still reluctant to change their teaching style, and stay with classical lectures and complementary exercises. These courses often fail to relate the different steps and stages of RE to each other and do not address crucial communication and project management issues that are common in industrial RE practice. They also miss the chance for using the classroom as a near-to-real-settings research lab, and won't show students the stakes existing in doing engineering in our society. We describe our experiences in teaching RE with a case study in two universities, achieving a triple-win: putting students in contact with real stakeholders, showing students their responsibility towards a sustainable world and doing empirical research in the classroom. We report on the course design, the evaluation, the lessons learned, and the potential success factors for such courses. We conclude that case study-based approaches to teaching RE considerably improve skills valued by industry, are feasible at a reasonable cost, and are enjoyable for the students, the teachers and the stakeholders. With this paper, we want to encourage RE educators to implement such courses in their setting."
1329406,15517,11058,Cause clue clauses: error localization using maximum satisfiability,2011,"Much effort is spent by programmers everyday in trying to reduce long, failing execution traces to the  cause  of the error. We present an algorithm for error cause localization based on a reduction to the maximal satisfiability problem (MAX-SAT), which asks what is the maximum number of clauses of a Boolean formula that can be simultaneously satisfied by an assignment. At an intuitive level, our algorithm takes as input a program and a failing test, and comprises the following three steps. First, using bounded model checking, and a bound obtained from the execution of the test, we encode the semantics of a bounded unrolling of the program as a Boolean  trace formula . Second, for a failing program execution (e.g., one that violates an assertion or a post-condition), we construct an  unsatisfiable  formula by taking the formula and additionally asserting that the input is the failing test and that the assertion condition does hold at the end. Third, using MAX-SAT, we find a maximal set of clauses in this formula that can be satisfied together, and output the complement set as a potential cause of the error.   We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as off-by-one.We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as off-by-one."
129229,15517,8422,Synthesizing programs with constraint solvers,2012,"Classical synthesis derives programs from a specification. We show an alternative approach where programs are obtained through search in a space of candidate programs. Searching for a program that meets a specification frees us from having to develop a sufficiently complete set of derivation rules, a task that is more challenging than merely describing the syntactic shape of the desired program. To make the search for a program efficient, we exploit symbolic constraint solving, lifted to synthesis from the setting of program verification.#R##N##R##N#We start by describing the interface to the synthesizer, which the programmer uses to specify the space of candidate programs as well as the desired correctness condition φ. The space is defined by a program template whose missing expressions are described with a grammar. The correctness condition is a multi-modal specification, given as a combination of assertions, input / output pairs, and traces.#R##N##R##N#Next, we describe several algorithms for solving the synthesis problem ∃P∀x φ(x,P(x)). The key idea is to reduce the problem from 2QBF to SAT by sampling the space of inputs, which eliminates the universal quantification over x.#R##N##R##N#Finally, we show how to encode the resulting SAT problem in relational logic, and how this encoding can be used to solve a range of related problems that arise in synthesis, from verification to program state repair. We will conclude with open problems on constraint-based synthesis."
1101095,15517,23620,Counter-factual typing for debugging type errors,2014,"Changing a program in response to a type error plays an important part in modern software development. However, the generation of good type error messages remains a problem for highly expressive type systems. Existing approaches often suffer from a lack of precision in locating errors and proposing remedies. Specifically, they either fail to locate the source of the type error consistently, or they report too many potential error locations. Moreover, the change suggestions offered are often incorrect. This makes the debugging process tedious and ineffective.   We present an approach to the problem of type debugging that is based on generating and filtering a comprehensive set of type-change suggestions. Specifically, we generate all (program-structure-preserving) type changes that can possibly fix the type error. These suggestions will be ranked and presented to the programmer in an iterative fashion. In some cases we also produce suggestions to change the program. In most situations, this strategy delivers the correct change suggestions quickly, and at the same time never misses any rare suggestions. The computation of the potentially huge set of type-change suggestions is efficient since it is based on a variational type inference algorithm that type checks a program with variations only once, efficiently reusing type information for shared parts.   We have evaluated our method and compared it with previous approaches. Based on a large set of examples drawn from the literature, we have found that our method outperforms other approaches and provides a viable alternative."
1643588,15517,23865,Tag recommendation in software information sites,2013,"Nowadays, software engineers use a variety of online media to search and become informed of new and interesting technologies, and to learn from and help one another. We refer to these kinds of online media which help software engineers improve their performance in software development, maintenance and test processes as software information sites. It is common to see tags in software information sites and many sites allow users to tag various objects with their own words. Users increasingly use tags to describe the most important features of their posted contents or projects. In this paper, we propose TagCombine, an automatic tag recommendation method which analyzes objects in software information sites. TagCombine has 3 different components: 1. multilabel ranking component which considers tag recommendation as a multi-label learning problem; 2. similarity based ranking component which recommends tags from similar objects; 3. tag-term based ranking component which considers the relationship between different terms and tags, and recommends tags after analyzing the terms in the objects. We evaluate TagCombine on 2 software information sites, StackOverflow and Freecode, which contain 47,668 and 39,231 text documents, respectively, and 437 and 243 tags, respectively. Experiment results show that for StackOverflow, our TagCombine achieves recall@5 and recall@10 scores of 0.5964 and 0.7239, respectively; For Freecode, it achieves recall@5 and recall@10 scores of 0.6391 and 0.7773, respectively. Moreover, averaging over StackOverflow and Freecode results, we improve TagRec proposed by Al-Kofahi et al. by 22.65% and 14.95%, and the tag recommendation method proposed by Zangerle et al. by 18.5% and 7.35% for recall@5 and recall@10 scores."
2070082,15517,23827,ProMoVer: modular verification of temporal safety properties,2011,"This paper describes ProMoVer, a tool for fully automated procedure-modular verification of Java programs equipped with method-local and global assertions that specify safety properties of sequences of method invocations. Modularity at the procedure-level is a natural instantiation of the modular verification paradigm, where correctness of global properties is relativized on the local properties of the methods rather than on their implementations, and is based here on the construction of maximal models for a program model that abstracts away from program data. This approach allows global properties to be verified in the presence of code evolution, multiple method implementations (as arising from software product lines), or even unknown method implementations (as in mobile code for open platforms). PROMOVER automates a typical verification scenario for a previously developed tool set for compositional verification of control flow safety properties, and provides appropriate pre- and post-processing. Modularity is exploited by a mechanism for proof reuse that detects and minimizes the verification tasks resulting from changes in the code and the specifications. The verification task is relatively light-weight due to support for abstraction from private methods and automatic extraction of candidate specifications from method implementations. We evaluate the tool on a number of applications from the smart card domain."
1755054,15517,8868,FLOWER: optimal test suite reduction as a network maximum flow,2014,"A trend in software testing is reducing the size of a test suite while preserving its overall quality. Given a test suite and a set of requirements covered by the suite, test suite reduction aims at selecting a subset of test cases that cover the same set of requirements. Even though this problem has received considerable attention, finding the smallest subset of test cases is still challenging and commonly-used approaches address this problem only with approximated solutions. When executing a single test case requires much manual effort (e.g., hours of preparation), finding the minimal subset is needed to reduce the testing costs. In this paper, we introduce a radically new approach to test suite reduction, called FLOWER, based on a search among network maximum flows. From a given test suite and the requirements covered by the suite, FLOWER forms a flow network (with specific constraints) that is then traversed to find its maximum flows. FLOWER leverages the Ford-Fulkerson method to compute maximum flows and Constraint Programming techniques to search among optimal flows. FLOWER is an exact method that computes a minimum-sized test suite, preserving the coverage of requirements. The experimental results show that FLOWER outperforms a non-optimized implementation of the Integer Linear Programming approach by 15-3000 times in terms of the time needed to find an optimal solution, and a simple greedy approach by 5-15% in terms of the size of reduced test suite."
1539016,15517,23827,4 th International Workshop on Software Engineering for sensor network applications (SESENA 2013),2013,"By acting as the interface between digital and physical worlds, wireless sensor networks (WSNs) represent a fundamental building block of the upcoming Internet of Things and a key enabler for Cyber-Physical and Pervasive Computing Systems. Despite the interest raised by this decade-old research topic, the development of WSN software is still carried out in a rather primitive fashion, by building software directly atop the operating system and by relying on an individual's hard-earned programming skills. WSN developers must face not only the functional application requirements but also a number of challenging, non-functional requirements and constraints resulting from scarce resources. The heterogeneity of network nodes, the unpredictable environmental influences, and the large size of the network further add to the difficulties. In the WSN community, there is a growing awareness of the need for methodologies, techniques, and abstractions that simplify development tasks and increase the confidence in the correctness and performance of the resulting software. Software engineering (SE) support is therefore sought, not only to ease the development task but also to make it more reliable, dependable, and repeatable. Nevertheless, this topic has received so far very little attention by the SE community."
1309190,15517,23827,BOAT: an experimental platform for researchers to comparatively and reproducibly evaluate bug localization techniques,2014,"Bug localization refers to the process of identifying source code files that contain defects from descriptions of these defects which are typically contained in bug reports. There have been many bug localization techniques proposed in the literature. However, often it is hard to compare these techniques since different evaluation datasets are used. At times the datasets are not made publicly available and thus it is difficult to reproduce reported results. Furthermore, some techniques are only evaluated on small datasets and thus it is not clear whether the results are generalizable. Thus, there is a need for a platform that allows various techniques to be compared with one another on a common pool containing a large number of bug reports with known defective source code files. In this paper, we address this need by proposing our Bug lOcalization experimental plATform (BOAT). BOAT is an extensible web application that contains thousands of bug reports with known defective source code files. Researchers can create accounts in BOAT, upload executables of their bug localization techniques, and see how these techniques perform in comparison with techniques uploaded by other researchers, with respect to some standard evaluation measures. BOAT is already preloaded with several bug localization techniques and thus researchers can directly compare their newly proposed techniques against these existing techniques. BOAT has been made available online since October 2013, and researchers could access the platform at: http://www.vlis.zju.edu.cn/blp."
785422,15517,8385,Making offline analyses continuous,2013,"Developers use analysis tools to help write, debug, and understand software systems under development. A developer's change to the system source code may affect analysis results. Typically, to learn those effects, the developer must explicitly initiate the analysis. This may interrupt the developer's workflow and/or the delay until the developer learns the implications of the change. The situation is even worse for impure analyses — ones that modify the code on which it runs — because such analyses block the developer from working on the code.     This paper presents Codebase Replication, a novel approach to easily convert an offline analysis — even an impure one — into a continuous analysis that informs the developer of the implications of recent changes as quickly as possible after the change is made. Codebase Replication copies the developer's codebase, incrementally keeps this copy codebase in sync with the developer's codebase, makes that copy codebase available for offline analyses to run without disturbing the developer and without the developer's changes disturbing the analyses, and makes analysis results available to be presented to the developer.     We have implemented Codebase Replication in Solstice, an open-source, publicly-available Eclipse plug-in. We have used Solstice to convert three offline analyses — FindBugs, PMD, and unit testing — into continuous ones. Each conversion required on average 436 NCSL and took, on average, 18 hours. Solstice-based analyses experience no more than 2.5 milliseconds of runtime overhead per developer action."
1624800,15517,23865,Lean GHTorrent: GitHub data on demand,2014,"In recent years, GitHub has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or jQuery) have chosen GitHub as their host and have migrated their code base to it. GitHub offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, GitHub data is, to date, largely underexplored. To facilitate studies of GitHub, we have created GHTorrent, a scalable, queriable, offline mirror of the data offered through the GitHub REST API. In this paper we present a novel feature of GHTorrent designed to offer customisable data dumps on demand. The new GHTorrent data-on-demand service offers users the possibility to request via a web form up-to-date GHTorrent data dumps for any collection of GitHub repositories. We hope that by offering customisable GHTorrent data dumps we will not only lower the barrier for entry even further for researchers interested in mining GitHub data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of GitHub studies (since a snapshot of the data on which the results were obtained can now easily accompany each study)."
877386,15517,8868,Finding rare numerical stability errors in concurrent computations,2013,"A numerical algorithm is called stable if an error, in all possible executions of the algorithm, does not exceed a predefined bound. Introduction of concurrency to numerical algorithms results in a significant increase in the number of possible computations of the same result, due to different possible interleavings of concurrent threads. This can lead to instability of previously stable algorithms, since rounding can result in a larger error than expected for some interleavings. Such errors can be very rare, since the particular combination of rounding can occur in only a small fraction of interleavings. In this paper, we apply the cross-entropy method -- a generic approach to rare event simulation and combinatorial optimization -- to detect rare numerical instability in concurrent programs. The cross-entropy method iteratively samples a small number of executions and adjusts the probability distribution of possible scheduling decisions to increase the probability of encountering an error in a subsequent iteration. We demonstrate the effectiveness of our approach on implementations of several numerical algorithms with concurrency and rounding by truncation of intermediate computations. We describe several abstraction algorithms on top of the implementation of the cross-entropy method and show that with abstraction, our algorithms successfully find rare errors in programs with hundreds of threads. In fact, some of our abstractions lead to a state space whose size does not depend on the number of threads at all. We compare our approach to several existing testing algorithms and argue that its performance is superior to other techniques."
756336,15517,517,Compositional Specifications for ioco Testing,2014,"Model-based testing is a promising technology for black-box software and hardware testing, in which test cases are generated automatically from high-level specifications. Nowadays, systems typically consist of multiple interacting components and, due to their complexity, testing presents a considerable portion of the effort and cost in the design process. Exploiting the compositional structure of system specifications can considerably reduce the effort in model-based testing. Moreover, inferring properties about the system from testing its individual components allows the designer to reduce the amount of integration testing. In this paper, we study compositional properties of the ioco-testing theory. We propose a new approach to composition and hiding operations, inspired by contract-based design and interface theories. These operations preserve behaviors that are compatible under composition and hiding, and prune away incompatible ones. The resulting specification characterizes the input sequences for which the unit testing of components is sufficient to infer the correctness of component integration without the need for further tests. We provide a methodology that uses these results to minimize integration testing effort, but also to detect potential weaknesses in specifications. While we focus on asynchronous models and the ioco conformance relation, the resulting methodology can be applied to a broader class of systems."
1357945,15517,23827,Patch verification via multiversion interprocedural control flow graphs,2014,"Software development is inherently incremental; however, it is challenging to correctly introduce changes on top of existing code. Recent studies show that 15%-24% of the bug fixes are incorrect, and the most important yet hard-to-acquire information for programming changes is whether this change breaks any code elsewhere. This paper presents a framework, called Hydrogen, for patch verification. Hydrogen aims to automatically determine whether a patch correctly fixes a bug, a new bug is introduced in the change, a bug can impact multiple software releases, and the patch is applicable for all the impacted releases. Hydrogen consists of a novel program representation, namely multiversion interprocedural control flow graph (MVICFG), that integrates and compares control flow of multiple versions of programs, and a demand-driven, path-sensitive symbolic analysis that traverses the MVICFG for detecting bugs related to software changes and versions. In this paper, we present the definition, construction and applications of MVICFGs. Our experimental results show that Hydrogen correctly builds desired MVICFGs and is scalable to real-life programs such as libpng, tightvnc and putty. We experimentally demonstrate that MVICFGs can enable efficient patch verification. Using the results generated by Hydrogen, we have found a few documentation errors related to patches for a set of open-source programs."
1569098,15517,23620,Geometry of synthesis III: resource management through type inference,2011,"Geometry of Synthesis  is a technique for compiling higher-level programming languages into digital circuits via their game semantic model. Ghica (2007) first presented the key idea, then Ghica and Smith (2010) gave a provably correct compiler into asynchronous circuits for Syntactic Control of Interference (SCI), an affine-typed version of Reynolds's Idealized Algol. Affine typing has the dual benefits of ruling out race conditions through the type system and having a finite-state game-semantic model for any term, which leads to a natural circuit representation and simpler correctness proofs. In this paper we go beyond SCI to full Idealized Algol, enhanced with shared-memory concurrency and semaphores.   Compiling ICA proceeds in three stages. First, an intermediate type system called Syntactic Control of Concurrency (SCC), is used to statically determine concurrency bounds on all identifiers in the program. Then, a program transformation called  serialization  is applied to the program to translate it into an equivalent SCC program in which all concurrency bounds are set to the unit. Finally, the resulting program can be then compiled into asynchronous circuits using a slightly enhanced version of the GoS II compiler, which can handle assignable variables used in non-sequential contexts."
607013,15517,20592,DSCRETE: automatic rendering of forensic information from memory images via application logic reuse,2014,"State-of-the-art memory forensics involves signature-based scanning of memory images to uncover data structure instances of interest to investigators. A largely unaddressed challenge is that investigators may not be able to interpret the content of data structure fields, even with a deep understanding of the data structure's syntax and semantics. This is very common for data structures with application-specific encoding, such as those representing images, figures, passwords, and formatted file contents. For example, an investigator may know that a buffer field is holding a photo image, but still cannot display (and hence understand) the image. We call this the data structure content reverse engineering challenge. In this paper, we present DSCRETE, a system that enables automatic interpretation and rendering of in-memory data structure contents. DSCRETE is based on the observation that the application in which a data structure is defined usually contains interpretation and rendering logic to generate human-understandable output for that data structure. Hence DSCRETE aims to identify and reuse such logic in the program's binary and create a scanner+renderer tool for scanning and rendering instances of the data structure in a memory image. Different from signature-based approaches, DSCRETE avoids reverse engineering data structure signatures. Our evaluation with a wide range of real-world application binaries shows that DSCRETE is able to recover a variety of application data -- e.g., images, figures, screenshots, user accounts, and formatted files and messages -- with high accuracy. The raw contents of such data would otherwise be unfathomable to human investigators."
2610537,15517,22232,Pushdown model checking for malware detection,2012,"The number of malware is growing extraordinarily fast. Therefore, it is important to have efficient malware detectors. Malware writers try to obfuscate their code by different techniques. Many of these well-known obfuscation techniques rely on operations on the stack such as inserting dead code by adding useless push and pop instructions, or hiding calls to the operating system, etc. Thus, it is important for malware detectors to be able to deal with the program's stack. In this paper we propose a new model-checking approach for malware detection that takes into account the behavior of the stack. Our approach consists in : (1) Modeling the program using a Pushdown System (PDS). (2) Introducing a new logic, called SCTPL, to represent the malicious behavior. SCTPL can be seen as an extension of the branching-time temporal logic CTL with variables, quantifiers, and predicates over the stack. (3) Reducing the malware detection problem to the model-checking problem of PDSs against SCTPL formulas. We show how our new logic can be used to precisely express malicious behaviors that could not be specified by existing specification formalisms. We then consider the model-checking problem of PDSs against SCTPL specifications. We reduce this problem to emptiness checking in Symbolic Alternating Buchi Pushdown Systems, and we provide an algorithm to solve this problem. We implemented our techniques in a tool, and we applied it to detect several viruses. Our results are encouraging."
47963,15517,9438,A Value-Oriented Approach to Business/IT Alignment – Towards Formalizing Purpose in System Engineering,2012,"It is widely recognized that a large percentage of IT initiatives fail from a business perspective. This is attributed to many factors, namely system complexity and change pace. We believe that the system development process itself is a crucial aspect of this state of affairs and a paradigm shift is required. There is a lack a common set of concepts and language to use through an IT de- velopment process. Essentially, appropriate models and founded theory for articulating the teleological and ontological perspectives of a system are neces- sary. In this paper, we present and discuss an innovative value-oriented approach to System Design and Engineering. Our contribution begins by identi- fying a relevant problem space regarding current approaches, particularly the lack of a sound structure to model a service system's purpose. We believe that system modeling with a market mindset will help improving quality and im- prove change response. The approach draws from a combination of theory based on Enterprise Engineering, Service Science and Value Modeling. A four- layer framework (System, Service, Value and Purpose) is pointed as a concep- tual solution for simultaneously representing relevant concerns for promoting dynamic alignment between Business and IT."
1709614,15517,8868,Practical static race detection for Java parallel loops,2013,"Despite significant progress in recent years, the important problem of static race detection remains open. Previous techniques took a general approach and looked for races by analyzing the effects induced by low-level concurrency constructs (e.g., java.lang.Thread). But constructs and libraries for expressing parallelism at a higher level (e.g., fork-join, futures, parallel loops) are becoming available in all major programming languages.     We claim that specializing an analysis to take advantage of the extra semantic information provided by the use of these constructs and libraries improves precision and scalability.     We present IteRace, a set of techniques that are specialized to use the intrinsic thread, safety, and data-flow structure of collections and of the new loop-parallelism mechanism to be introduced in Java 8. Our evaluation shows that IteRace is fast and precise enough to be practical. It scales to programs of hundreds of thousands of lines of code and it reports few race warnings, thus avoiding a common pitfall of static analyses. The tool revealed six bugs in real-world applications. We reported four of them, one had already been fixed, and three were new and the developers confirmed and fixed them."
1277102,15517,23827,Effects of using examples on structural model comprehension: a controlled experiment,2014,"We present a controlled experiment for the empirical evaluation of Example-Driven Modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate and undergraduate students from electrical and computer engineering (ECE), computer science (CS), and software engineering (SE) programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples in model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model abstractions only, with an improvement of 39% for diagram completeness, 30% for questions completeness, 71% for efficiency, and a reduction of 80% for the number of mistakes. We provide qualitative results showing that participants receiving model abstractions augmented with examples experienced lower perceived difficulty in performing the comprehension tasks, higher perceived confidence in their tasks' solutions, and asked fewer clarifying domain questions, a reduction of 90%. We also present participants' feedback regarding the usefulness of the provided examples, their number and types, as well as, the use of partial examples."
1235099,15517,23827,"Two's company, three's a crowd: a case study of crowdsourcing software development",2014,"Crowdsourcing is an emerging and promising approach which involves delegating a variety of tasks to an unknown workforce - the crowd. Crowdsourcing has been applied quite successfully in various contexts from basic tasks on Amazon Mechanical Turk to solving complex industry problems, e.g. InnoCentive. Companies are increasingly using crowdsourcing to accomplish specific software development tasks. However, very little research exists on this specific topic. This paper presents an in-depth industry case study of crowdsourcing software development at a multinational corporation. Our case study highlights a number of challenges that arise when crowdsourcing software development. For example, the crowdsourcing development process is essentially a waterfall model and this must eventually be integrated with the agile approach used by the company. Crowdsourcing works better for specific software development tasks that are less complex and stand-alone without interdependencies. The development cost was much greater than originally expected, overhead in terms of company effort to prepare specifications and answer crowdsourcing community queries was much greater, and the time-scale to complete contests, review submissions and resolve quality issues was significant. Finally, quality issues were pushed later in the lifecycle given the lengthy process necessary to identify and resolve quality issues. Given the emphasis in software engineering on identifying bugs as early as possible, this is quite problematic."
1224408,15517,20524,Characteristic studies of loop problems for structural test generation via symbolic execution,2013,"Dynamic Symbolic Execution (DSE) is a state-of-the-art test-generation approach that systematically explores program paths to generate high-covering tests. In DSE, the presence of loops (especially unbound loops) can cause an enormous or even infinite number of paths to be explored. There exist techniques (such as bounded iteration, heuristics, and summarization) that assist DSE in addressing loop problems. However, there exists no literature-survey or empirical work that shows the pervasiveness of loop problems or identifies challenges faced by these techniques on real-world open-source applications. To fill this gap, we provide characteristic studies to guide future research on addressing loop problems for DSE. Our proposed study methodology starts with conducting a literature-survey study to investigate how technical problems such as loop problems compromise automated software-engineering tasks such as test generation, and which existing techniques are proposed to deal with such technical problems. Then the study methodology continues with conducting an empirical study of applying the existing techniques on real-world software applications sampled based on the literature-survey results and major open-source project hostings. This empirical study investigates the pervasiveness of the technical problems and how well existing techniques can address such problems among real-world software applications. Based on such study methodology, our two-phase characteristic studies identify that bounded iteration and heuristics are effective in addressing loop problems when used properly. Our studies further identify challenges faced by these techniques and provide guidelines for effectively addressing these challenges."
2785682,15517,20592,Static detection of access control vulnerabilities in web applications,2011,"Access control vulnerabilities, which cause privilege escalations, are among the most dangerous vulnerabilities in web applications. Unfortunately, due to the difficulty in designing and implementing perfect access checks, web applications often fall victim to access control attacks. In contrast to traditional injection flaws, access control vulnerabilities are application-specific, rendering it challenging to obtain precise specifications for static and runtime enforcement. On one hand, writing specifications manually is tedious and time-consuming, which leads to non-existent, incomplete or erroneous specifications. On the other hand, automatic probabilistic-based specification inference is imprecise and computationally expensive in general.#R##N##R##N#This paper describes the first static analysis that automatically detects access control vulnerabilities in web applications. The core of the analysis is a technique that statically infers and enforces implicit access control assumptions. Our insight is that source code implicitly documents intended accesses of each role and any successful forced browsing to a privileged page is likely a vulnerability. Based on this observation, our static analysis constructs sitemaps for different roles in a web application, compares per-role sitemaps to find privileged pages, and checks whether forced browsing is successful for each privileged page. We implemented our analysis and evaluated our tool on several real-world web applications. The evaluation results show that our tool is scalable and detects both known and new access control vulnerabilities with few false positives."
2402358,15517,21022,Efficient deterministic multithreading through schedule relaxation,2011,"Deterministic multithreading (DMT)  eliminates many pernicious software problems caused by nondeterminism. It works by constraining a program to repeat the same thread interleavings, or  schedules , when given same input. Despite much recent research, it remains an open challenge to build  both deterministic and efficient  DMT systems for general programs on commodity hardware. To deterministically resolve a data race, a DMT system must enforce a deterministic schedule of shared memory accesses, or  mem-schedule , which can incur prohibitive overhead. By using schedules consisting only of synchronization operations, or  sync-schedule , this overhead can be avoided. However, a sync-schedule is deterministic only for race-free programs, but most programs have races.   Our key insight is that races tend to occur only within minor portions of an execution, and a dominant majority of the execution is still race-free. Thus, we can resort to a mem-schedule only for the racy portions and enforce a sync-schedule otherwise, combining the efficiency of sync-schedules and the determinism of mem-schedules. We call these combined schedules  hybrid schedules .   Based on this insight, we have built Peregrine, an efficient deterministic multithreading system. When a program first runs on an input, Peregrine records an execution trace. It then  relaxes  this trace into a hybrid schedule and reuses the schedule on future compatible inputs efficiently and deterministically. Peregrine further improves efficiency with two new techniques:  determinism-preserving slicing  to generalize a schedule to more inputs while preserving determinism, and  schedule-guided simplification  to precisely analyze a program according to a specific schedule. Our evaluation on a diverse set of programs shows that Peregrine is deterministic and efficient, and can frequently reuse schedules for half of the evaluated programs."
1684783,15517,20754,Hidden GEMs: Automated Discovery of Access Control Vulnerabilities in Graphical User Interfaces,2014,"Graphical user interfaces (GUIs) are the predominant means by which users interact with modern programs. GUIs contain a number of common visual elements or widgets such as labels, text fields, buttons, and lists, and GUIs typically provide the ability to set attributes on these widgets to control their visibility, enabled status, and whether they are writable. While these attributes are extremely useful to provide visual cues to users to guide them through an application's GUI, they can also be misused for purposes they were not intended. In particular, in the context of GUI-based applications that include multiple privilege levels within the application, GUI element attributes are often misused as a mechanism for enforcing access control policies. In this work, we introduce GEMs, or instances of GUI element misuse, as a novel class of access control vulnerabilities in GUI-based applications. We present a classification of different GEMs that can arise through misuse of widget attributes, and describe a general algorithm for identifying and confirming the presence of GEMs in vulnerable applications. We then present GEM Miner, an implementation of our GEM analysis for the Windows platform. We evaluate GEM Miner over a test set of three complex, real-world GUI-based applications targeted at the small business and enterprise markets, and demonstrate the efficacy of our analysis by finding numerous previously unknown access control vulnerabilities in these applications. We have reported the vulnerabilities we discovered to the developers of each application, and in one case have received confirmation of the issue."
768999,15517,23827,Dual ecological measures of focus in software development,2013,"Work practices vary among software developers. Some are highly focused on a few artifacts; others make wideranging contributions. Similarly, some artifacts are mostly authored, or “owned”, by one or few developers; others have very wide ownership. Focus and ownership are related but different phenomena, both with strong effect on software quality. Prior studies have mostly targeted ownership; the measures of ownership used have generally been based on either simple counts, information-theoretic views of ownership, or social-network views of contribution patterns. We argue for a more general conceptual view that unifies developer focus and artifact ownership. We analogize the developer-artifact contribution network to a predator-prey food web, and draw upon ideas from ecology to produce a novel, and conceptually unified view of measuring focus and ownership. These measures relate to both cross-entropy and Kullback-Liebler divergence, and simultaneously provide two normalized measures of focus from both the developer and artifact perspectives. We argue that these measures are theoretically well-founded, and yield novel predictive, conceptual, and actionable value in software projects. We find that more focused developers introduce fewer defects than defocused developers. In contrast, files that receive narrowly focused activity are more likely to contain defects than other files."
2408416,15517,339,Poster: a path-cutting approach to blocking XSS worms in social web networks,2011,"Worms exploiting JavaScript XSS vulnerabilities rampantly infect millions of webpages, while drawing the ire of helpless users. To date, users across all of the popular social networks, including FaceBook, MySpace, Orkut and Twitters, have been vulnerable to XSS worms. We propose PathCutter as a new approach to severing the self-propagation path of JavaScript worms. PathCutter works by blocking two critical steps in the propagation path of an XSS worm: (i) DOM access to different views at the client-side and (ii) unauthorized HTTP request to the server. As a result, although an XSS vulnerability is successfully exercised at the client, the XSS worm is prevented from successfully propagating to the would be victim's own social network page. PathCutter is effective against all of the current forms of XSS worms, including those that exploit traditional XSS, DOM-based XSS, and content sniffing XSS vulnerabilities. We demonstrate PathCutter using WordPress and perform a preliminary evaluation on a proof-of-concept JavaScript Worm."
1032567,15517,339,mXSS attacks: attacking well-secured web-applications by using innerHTML mutations,2013,"Back in 2007, Hasegawa discovered a novel Cross-Site Scripting (XSS) vector based on the mistreatment of the backtick character in a single browser implementation. This initially looked like an implementation error that could easily be fixed. Instead, as this paper shows, it was the first example of a new class of XSS vectors, the class of mutation-based XSS (mXSS) vectors, which may occur in innerHTML and related properties. mXSS affects all three major browser families: IE, Firefox, and Chrome.   We were able to place stored mXSS vectors in high-profile applications like Yahoo! Mail, Rediff Mail, OpenExchange, Zimbra, Roundcube, and several commercial products. mXSS vectors bypassed widely deployed server-side XSS protection techniques (like HTML Purifier, kses, htmlLawed, Blueprint and Google Caja), client-side filters (XSS Auditor, IE XSS Filter), Web Application Firewall (WAF) systems, as well as Intrusion Detection and Intrusion Prevention Systems (IDS/IPS). We describe a scenario in which seemingly immune entities are being rendered prone to an attack based on the behavior of an involved party, in our case the browser. Moreover, it proves very difficult to mitigate these attacks: In browser implementations, mXSS is closely related to performance enhancements applied to the HTML code before rendering; in server side filters, strict filter rules would break many web applications since the mXSS vectors presented in this paper are harmless when sent to the browser.   This paper introduces and discusses a set of seven different subclasses of mXSS attacks, among which only one was previously known. The work evaluates the attack surface, showcases examples of vulnerable high-profile applications, and provides a set of practicable and low-overhead solutions to defend against these kinds of attacks."
2069115,15517,343,Bootstrapping energy debugging on smartphones: a first look at energy bugs in mobile devices,2011,"This paper argues that a new class of bugs faced by millions of smartphones, energy bugs or  ebugs , have become increasingly prominent that already they have led to significant user frustrations. We take a first look at this emerging important technical challenge faced by the smartphones, ebugs, broadly defined as an error in the system (application, OS, hardware, firmware, external conditions or combination) that causes an  unexpected  amount of high energy consumption by the system as a whole. We first present a taxonomy of the kinds of ebugs based on mining over 39K posts (1.2M before filtering) from 4 online mobile user forum and mobile OS bug repositories. The taxonomy shows the highly diverse nature of smartphone ebugs. We then propose a roadmap towards developing a systematic diagnosing framework for debugging ebugs on smartphones."
1319185,15517,339,Crouching tiger - hidden payload: security risks of scalable vectors graphics,2011,"Scalable Vector Graphics (SVG) images so far played a rather small role on the Internet, mainly due to the lack of proper browser support. Recently, things have changed: the W3C and WHATWG draft specifications for HTML5 require modern web browsers to support SVG images to be embedded in a multitude of ways. Now SVG images can be embedded through the classical method via specific tags such as   or  , or in novel ways, such as with   tags, CSS or inline in any HTML5 document. SVG files are generally considered to be plain images or animations, and security-wise, they are being treated as such (e.g., when an embedment of local or remote SVG images into websites or uploading these files into rich web applications takes place). Unfortunately, this procedure poses great risks for the web applications and the users utilizing them, as it has been proven that SVG files must be considered fully functional, one-file web applications potentially containing HTML, JavaScript, Flash, and other interactive code structures. We found that even more severe problems have resulted from the often improper handling of complex and maliciously prepared SVG files by the browsers.   In this paper, we introduce several novel attack techniques targeted at major websites, as well as modern browsers, email clients and other comparable tools. In particular, we illustrate that SVG images embedded via   tag and CSS can execute arbitrary JavaScript code. We examine and present how current filtering techniques are circumventable by using SVG files and subsequently propose an approach to mitigate these risks. The paper showcases our research into the usage of SVG images as attack tools, and determines its impact on state-of-the-art web browsers such as Firefox 4, Internet Explorer 9, and Opera 11."
1649963,15517,339,FlashOver: automated discovery of cross-site scripting vulnerabilities in rich internet applications,2012,"The last fifteen years have transformed the Web in ways that would seem unimaginable to anyone of the few Internet users of the year 1995 [8]. What began as a simple set of protocols and mechanisms facilitating the exchange of static documents between remote computers is now an everyday part of billions' of users life, technical and non-technical alike. The sum of a user's daily experience is composed of open standards, such as HTML, JavaScript and Cascading Style Sheets as well as proprietary plugins, such as Adobe's Flash [1] and Microsoft's Silverlight [6]."
2435290,15517,339,ALETHEIA: Improving the Usability of Static Security Analysis,2014,"The scale and complexity of modern software systems complicate manual security auditing. Automated analysis tools are gradually becoming a necessity. Specifically, static security analyses carry the promise of efficiently verifying large code bases. Yet, a critical usability barrier, hindering the adoption of static security analysis by developers, is the excess of false reports. Current tools do not offer the user any direct means of customizing or cleansing the report. The user is thus left to review hundreds, if not thousands, of potential warnings, and classify them as either actionable or spurious. This is both burdensome and error prone, leaving developers disenchanted by static security checkers.   We address this challenge by introducing a general technique to refine the output of static security checkers. The key idea is to apply statistical learning to the warnings output by the analysis based on user feedback on a small set of warnings. This leads to an interactive solution, whereby the user classifies a small fragment of the issues reported by the analysis, and the learning algorithm then classifies the remaining warnings automatically. An important aspect of our solution is that it is user centric. The user can express different classification policies, ranging from strong bias toward elimination of false warnings to strong bias toward preservation of true warnings, which our filtering system then executes.   We have implemented our approach as the Aletheia tool. Our evaluation of Aletheia on a diversified set of nearly 4,000 client-side JavaScript benchmarks, extracted from 675 popular Web sites, is highly encouraging. As an example, based only on 200 classified warnings, and with a policy biased toward preservation of true warnings, Aletheia is able to boost precision by a threefold factor (x 2.868), while reducing recall by a negligible factor (x 1.006). Other policies are enforced with a similarly high level of efficacy."
1217289,15517,339,The (un)reliability of NVD vulnerable versions data: an empirical experiment on Google Chrome vulnerabilities,2013,"NVD is one of the most popular databases used by researchers to conduct empirical research on data sets of vulnerabilities. Our recent analysis on Chrome vulnerability data reported by NVD has revealed an abnormally phenomenon in the data where almost vulnerabilities were originated from the first versions. This inspires our experiment to validate the reliability of the NVD vulnerable version data. In this experiment, we verify for each version of Chrome that NVD claims vulnerable is actually vulnerable. The experiment revealed several errors in the vulnerability data of Chrome. Furthermore, we have also analyzed how these errors might impact the conclusions of an empirical study on foundational vulnerability. Our results show that different conclusions could be obtained due to the data errors."
1720465,15517,517,Fifty Shades of Grey in SOA Testing,2013,"Testing is undisputedly a fundamental verification principle in the software landscape. Today's products require us to effectively handle and test huge, complex systems and in this context to tackle challenging traits like heterogeneity, distribution and controllability to name just a few. The advent of Service-Oriented Architectures with their inherent technological features like dynamics and heterogeneity exacerbated faced challenges, requiring us to evolve our technology. The traditional view of white or black box testing, for example, does not accommodate the multitude of shades of grey one should be able to exploit effectively for system-wide tests. Today, while there are a multitude of approaches for testing single services, there is still few work on methodological system tests for SOAs. In this paper we propose a corresponding workflow for tackling SOA testing and diagnosis, discuss SOA test case generation in more detail, and report preliminary research in that direction."
2365232,15517,517,An Abstract Operational Framework for Dependence Models in Software Debugging,2011,"In this article we introduce an abstract operational framework employing Aspect's notion of abstract dependences. We exemplify that this framework allows one for motivating certain desirable model properties in an intuitive and straightforward way. Moreover, we propose an algorithm that allows for obtaining a static program slice from an abstract execution trace. This computation resembles computing a dynamic slice for a certain test case, but unlike to a dynamic slice, does not suffer from error masking since it covers all possible executions. We show how to employ abstract dependences to determine those variables upon which computation of the slice is most beneficial. Moreover, we show how to best use the obtained static slices to compute diagnoses, that is - fault candidates, in terms of hitting sets."
947665,15517,339,Fuzzing the ActionScript virtual machine,2013,"Fuzz testing is an automated testing technique where random data is used as an input to software systems in order to reveal security bugs/vulnerabilities. Fuzzed inputs must be binaries embedded with compiled bytecodes when testing against ActionScript virtual machines (AVMs). The current fuzzing method for JavaScript-like virtual machines is very limited when applied to compiler-involved AVMs. The complete source code should be both grammatically and semantically valid to allow execution by first passing through the compiler. In this paper, we present  ScriptGene , an algorithmic approach to overcome the additional complexity of generating valid ActionScript programs. First, nearly-valid code snippets are randomly generated, with some controls on instruction flow. Second, we present a novel mutation method where the former code snippets are lexically analyzed and mutated with runtime information of the AVM, which helps us to build context for undefined behaviours against compiler-check and produce a high code coverage. Accordingly, we have implemented and evaluated  ScriptGene  on three different versions of Adobe AVMs. Results demonstrate that  ScriptGene  not only covers almost all the blocks of the official test suite (Tamarin), but also is capable of nearly twice the code coverage. The discovery of six bugs missed by the official test suite demonstrates the effectiveness, validity and novelty of  ScriptGene ."
690143,15517,339,Scanning of real-world web applications for parameter tampering vulnerabilities,2014,"Web applications require exchanging parameters between a client and a server to function properly. In real-world systems such as online banking transfer, traversing multiple pages with parameters contributed by both the user and server is a must, and hence the applications have to enforce workflow and parameter dependency controls across multiple requests. An application that applies insufficient server-side input validations is however vulnerable to parameter tampering attacks, which manipulate the exchanged parameters. Existing fuzzing-based scanning approaches however neglected these important controls, and this caused their fuzzing requests to be dropped before they can reach any vulnerable code. In this paper, we propose a novel approach to identify the workflow and parameter dependent constraints, which are then maintained and leveraged for automatic detection of server acceptances during fuzzing. We realized the approach by building a generic blackbox parameter tampering scanner. It successfully uncovered a number of severe vulnerabilities, including one from the largest multi-national banking website, which other scanners miss."
1443187,15517,517,Introducing Combinatorial Testing in a Large Organization: Pilot Project Experience Report,2014,"This poster gives an overview of the experience of eight pilot projects, over two years, applying combinatorial testing in Lockheed Martin (LM), one of the world's largest aerospace firms. Lockheed Martin and NIST developed a Co-operative Research and Development Agreement (CRADA) to evaluate effectiveness and areas of suitable application for combinatorial testing in a real-world industrial setting with complex software requirements. (One of the ways in which NIST conducts joint research with US industry is through CRADAs, which allow federal laboratories to work with US industry and provide flexibility in structuring projects, intellectual property rights, and in protecting industry proprietary information and research results.)"
2300198,15517,369,Implementing Situation Awareness for Car-to-X Applications Using Domain Specific Languages,2013,"Car-to-X i.e. Car-to-Anything communication based on standardized IEEE 802.11p radio technology is comprised with wireless communication between cars (Car-to-Car) and between vehicles and the environment (Car-to-Infrastructure). In order to develop Car-to-X applications based on this standard one needs to model parameters such as the vehicle's position, velocity, acceleration etc. and parameters of the vehicle's environment. Typically, the underlying domain models are designed in an ad-hoc manner and the domain rules become hard-coded into the source- code of the application software. In this paper we describe an alternative and more flexible approach. The model is described in almost plain English using a Domain Specific Language (DSL) and translated into target code via parser technology based on the ANTLR tool-chain. This provides more flexibility not only in creating and maintaining the domain rules, but also with regards to generating code for entirely different target languages and technology environments. For instance, we demonstrate to generate Java code for a simulation environment and C-code for the embedded device from the same rule definitions."
1421348,15517,517,Applying Pattern-Based Graphical Validation Rules to Business Process Models,2014,"Business Process Models (BPMs) are widely used for documentation and (model driven) software development. Due to the increasing complexity of BPMs automated checking is unavoidable to ensure the quality of BPMs. This requires formal specification methods that address high level (domain-specific) requirements (e.g. Compliance) in a manner that is comprehensible for stakeholders without mathematical background and provide a high level of generalizability. In previous publications we conceptually introduced the G-CTL notation for formal graphical validation rules, tackling these issues. Based on G-CTL, we present a pattern based specification and matching mechanism for such graphical validation rules. We describe the mapping of the graphical G-CTL rules into textual CTL instances for the particular BPM to be checked. Moreover, the approach enhances the generalizability by supporting the abstraction from concrete names of elements in BPMs and it's application is not limited to a particular BPM notation."
1278627,15517,339,Deriving common malware behavior through graph clustering,2011,"Detection of malicious software (malware) continues to be a problem as hackers devise new ways to evade available methods. The proliferation of malware and malware variants requires methods that are both powerful, and fast to execute. This paper proposes a method to derive the common execution behavior of a family of malware instances. For each instance, a graph is constructed that represents kernel objects and their attributes, based on system call traces. The method combines these graphs to develop a supergraph for the family. This supergraph contains a subgraph, called the  HotPath , which is observed during the execution of all the malware instances. The proposed method is scalable, identifies previously-unseen malware instances, shows high malware detection rates, and false positive rates close to 0%."
1955934,15517,374,Preventing web application injections with complementary character coding,2011,"Web application injection attacks, such as SQL injection and cross-site scripting (XSS) are major threats to the security of the Internet. Several recent research efforts have investigated the use of dynamic tainting to mitigate these threats. This paper presents complementary character coding, a new approach to character level dynamic tainting which allows efficient and precise taint propagation across the boundaries of server components, and also between servers and clients over HTTP. In this approach, each character has two encodings, which can be used to distinguish trusted and untrusted data. Small modifications to the lexical analyzers in components, such as the application code interpreter, the database management system, and (optionally) the web browser, allow them to become complement aware components, capable of using this alternative character coding scheme to enforce security policies aimed at preventing injection attacks, while continuing to function normally in other respects. This approach overcomes some weaknesses of previous dynamic tainting approaches. Notably, it offers a precise protection against persistent cross-site scripting attacks, as taint information is maintained when data is passed to a database and later retrieved by the application program. A prototype implementation with LAMP and Firefox is described. An empirical evaluation shows that the technique is effective on a group of vulnerable benchmarks and has low overhead."
1592655,15517,517,GrowthTracker: Diagnosing Unbounded Heap Growth in C++ Software,2013,"Unbounded growth of heap memory degrades performance and eventually causes program failure. Traditional memory leaks are the most commonly recognized, but not the only cause of this issue. Large software systems use many aggregate data structures that can grow arbitrarily, and application behavior that produces unbounded growth of these structures is common. This growth can remain undetected by both memory leak and staleness detection tools. In this paper, we present an approach for reliably identifying aggregate data structures that can grow without bound over the lifetime of a program. Our solution tracks all aggregates over the lifetime of the program and utilizes heuristics to identify non-convergent growth. Our diagnostic method continuously reduces false positives and false negatives during execution, producing more accurate reports for as long as it is allowed to continue execution. In addition, we present techniques to utilize this method in large, pre-existing C++ software without requiring extensive code modification. Our tool identified data structures with this issue in Google's Chrome web browser and Apple's Safari browser among others."
1956175,15517,517,A Systematic Test Case Generation Approach for Testing Message Length Variability,2011,"Variable length messages have been in use for a long time for efficient delivery of information. As there are many different ways, with varying complexity, to utilize message length variability, it is crucial to thoroughly test the capability of the parsers of such messages to ensure that they correctly handle the variability. However, testing techniques for message length variability were developed only in fragments in the past and therefore test developers who are faced with the task of testing variable length messages are left with little guidance or few techniques for handling them systematically. This paper develops an approach for systematic test cases generation for testing the parsers of variable length messages. To do so, we develop taxonomy of message length variabilities, and derive test requirement patterns for them using the taxonomy. Then test requirements for particular protocols can be derived from the patterns. A case study is conducted to show that the proposed taxonomy and the test requirement patterns are effective in deriving test cases for actual protocols. The results showed significant improvement over the conventional approach. It revealed many missed requirements that were not identified with the test requirements developed using the conventional intuition-based approach."
2426688,15517,517,Towards a Rule-Level Verification Framework for Property-Preserving Graph Transformations,2012,"We report in this paper a method for proving that a graph transformation is property-preserving. Our approach uses a relational representation for graph grammar and a logical representation for graph properties with first-order logic formulas. The presented work consists in identifying the general conditions for a graph grammar to preserve graph properties, in particular structural properties. We aim to implement all the relevant notions of graph grammar in the Isabelle/HOL proof assistant in order to allow a (semi) automatic verification of graph transformation with a reasonable complexity. Given an input graph and a set of graph transformation rules, we can use mathematical induction strategies to verify statically if the transformation preserves a particular property of the initial graph. The main highlight of our approach is that such a verification is done without calculating the resulting graph and thus without using a transformation engine."
2214176,15517,517,Utilizing Model Checking for Automatic Test Case Generation from Conjunctions of Predicates,2011,"Automatic test case generation from a pre-post style formal specification must deal with the issue of how to generate test cases from a conjunction of atomic predicate expressions, but unfortunately this problem has not been effectively solved due to its intrinsic difficulty. In this paper, we describe a practical approach to tackling this problem by utilizing the model checking technique. An algorithm that converts test case generation from a conjunction of atomic predicate expressions into model checking is proposed. We discuss how the algorithm deals with atomic predicate expressions involving only variables of numeric types, and then extend the discussion to variables of compound types such as set, sequence, and composite types. Finally, case studies are presented to assess the feasibility and effectiveness of our approach."
2137898,15517,517,SOA Grey Box Testing -- A Constraint-Based Approach,2013,"Service-Oriented Architectures (SOAs) offer attractive advantages in respect of reusability, interoperability and dynamics, and are nowadays widely accepted in industry. Achieving established software quality levels also with SOAs, while mandatory, is challenging, as, for instance, a SOA's dynamics and heterogeneity exacerbate verification issues like observability, controllability, and distribution. Regarding verification, we thus have to evolve available technology in order to enable the assessment of essential functional and non-functional system properties, including correctness, performance, stability, robustness and scalability. Adopting a model-based grey box testing approach that can exploit mixed description levels for individual (possibly 3 rd  party) services promises the required flexibility for successful development workflows. In this paper, we propose such a testing approach that, considering a SOA model, defines constraint satisfaction problems for the test case generation step. First empirical results for our approach are promising."
1924272,15517,517,A Taint Based Approach for Smart Fuzzing,2012,"Fuzzing is one of the most popular test-based software vulnerability detection techniques. It consists in running the target application with dedicated inputs in order to exhibit potential failures that could be exploited by a malicious user. In this paper we propose a global approach for fuzzing, addressing the main challenges to be faced in an industrial context: large-size applications, without source code access, and with a partial knowledge of the input specifications. This approach integrates several successive steps, and we mostly focus here on an important one which relies on binary-level dynamic taint analysis. We summarize the main problems to be addressed in this step, and we detail the solution we implemented to solve them."
2156096,15517,517,Probabilistic Error Propagation Modeling in Logic Circuits,2011,"Recent study has shown that accurate knowledge of the false negative rate (FNR) of tests can significantly improve the diagnostic accuracy of spectrum-based fault localization. To understand the principles behind FNR modeling in this paper we study three error propagation probability (EPP) modeling approaches applied to a number of logic circuits from the 74XXX/ISCAS-85 benchmark suite. Monte Carlo simulations for random injected faults show that a deterministic approach that models gate behavior provides high accuracy (O(1%)), while probabilistic approaches that abstract from gate modeling generate higher prediction errors (O(10%)), which increase with the number of injected faults."
1978302,15517,517,Model Driven Mutation Applied to Adaptative Systems Testing,2011,"Dynamically Adaptive Systems modify their behavior and structure in response to changes in their surrounding environment and according to an adaptation logic. Critical systems increasingly incorporate dynamic adaptation capabilities, examples include disaster relief and space exploration systems. In this paper, we focus on mutation testing of the adaptation logic. We propose a fault model for adaptation logics that classifies faults into environmental completeness and adaptation correctness. Since there are several adaptation logic languages relying on the same underlying concepts, the fault model is expressed independently from specific adaptation languages. Taking benefit from model-driven engineering technology, we express these common concepts in a metamodel and define the operational semantics of mutation operators at this level. Mutation is applied on model elements and model transformations are used to propagate these changes to a given adaptation policy in the chosen formalism. Preliminary results on an adaptive web server highlight the difficulty of killing mutants for adaptive systems, and thus the difficulty of generating efficient tests."
268132,15517,23827,Scalable Verification of Markov Decision Processes,2013,"Markov decision processes (MDP) are useful to model concurrent process optimisation problems, but verifying them with numerical methods is often intractable. Existing approximative approaches do not scale well and are limited to memoryless schedulers. Here we present the basis of scalable verification for MDPSs, using an O(1) memory representation of history-dependent schedulers. We thus facilitate scalable learning techniques and the use of massively parallel verification."
1414119,15517,23865,"What is software development productivity, anyway? (Keynote)",2013,"Businesses and consumers all want more software faster. The seemingly ever-increasing demand for more software suggests the need to not only increase production capabilities but also to produce more with the resources available for production. In other words, software development productivity needs to increase. But what is software development productivity anyway? In this talk, I will explore various ways in which productivity, both in general and for software development, has been characterized and will explore ways in which mining software repository information can help accelerate both software development productivity and innovation."
33597,15517,9438,Developing the GEA Method - Design Science and Case-Study Research in Action,2013,"This paper is concerned with the research methodology that was used in the GEA (General Enterprise Architecting) research programme. The goal of this programme was the development of a new approach for doing enterprise architecture. We discuss the motivations for starting the GEA programme, its focus, as well as its objectives. Based on this, the research methodology as it was used by the GEA programme is discussed and motivated. This involves a combination of design science and case study based research. The paper also discusses the way the GEA programme went about to actually implement the research methodology in a real-world situation, while also highlighting its results."
1129011,15517,23620,Proofs that count,2014,"Counting arguments are among the most basic proof methods in mathematics. Within the field of formal verification, they are useful for reasoning about programs with  infinite control , such as programs with an unbounded number of threads, or (concurrent) programs with recursive procedures. While counting arguments are common in informal, hand-written proofs of such programs, there are no fully  automated  techniques to construct counting arguments. The key questions involved in automating counting arguments are:  how to decide what should be counted? , and  how to decide when a counting argument is valid?  In this paper, we present a technique for automatically constructing and checking counting arguments, which includes novel solutions to these questions."
1142070,15517,507,Deciding monotone duality and identifying frequent itemsets in quadratic logspace,2013,"The monotone duality problem is defined as follows: Given two monotone formulas f and g in irredundant DNF, decide whether  f  and  g  are dual. This problem is the same as duality testing for hypergraphs, that is, checking whether a hypergraph  H  consists of precisely all minimal transversals of a hypergraph  G . By exploiting a recent problem-decomposition method by Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs, and thus for monotone DNFs, is feasible in DSPACE(log 2   n ), i.e., in quadratic logspace. As the monotone duality problem is equivalent to a number of problems in the areas of databases, data mining, and knowledge discovery, the results presented here yield new complexity results for those problems, too. For example, it follows from our results that whenever, for a Boolean-valued relation (whose attributes represent items), a number of maximal frequent itemsets and a number of minimal infrequent itemsets are known, then it can be decided in quadratic logspace whether there exist additional frequent or infrequent itemsets."
1612233,15517,517,AutoQUEST -- Automated Quality Engineering of Event-Driven Software,2013,"In this paper, we present AutoQUEST, a testing platform for Event-Driven Software (EDS) that decouples the implementation of testing techniques from the concrete platform they should be applied to. AutoQUEST provides the means to define testing techniques against an abstract Application Programming Interface (API) and provides plugins to port the testing techniques to distinct platforms. The requirements on plug-in implementations for AutoQUEST are kept low to keep the porting effort low. We implemented several testing techniques on top of AutoQUEST and provide five plugins for concrete software platforms, which demonstrates the capabililities of our approach."
1648251,15517,517,Events-Based Security Monitoring Using MMT Tool,2012,"MMT (Mont image Monitoring Tool) is a monitoring solution that combines: data capture, filtering and storage, events extraction, statistics collection, traffic analysis and reporting. In the context of the PIMI and DIAMONDS projects, Mont image is developing MMT-Security: a security analysis solution (part of MMT) that inspects network traffic against a set of security properties denoting both security rules and attacks. This tool has been applied to an industrial case study provided by Thales Group that consists of a QoS-aware ad-hoc radio communication protocol."
2167896,15517,339,The devil is in the (implementation) details: an empirical analysis of OAuth SSO systems,2012,"Millions of web users today employ their Facebook accounts to sign into more than one million  relying party  (RP) websites. This web-based single sign-on (SSO) scheme is enabled by OAuth 2.0, a web resource authorization protocol that has been adopted by major service providers. The OAuth 2.0 protocol has proven secure by several formal methods, but whether it is indeed secure in practice remains an open question. We examine the implementations of three major OAuth identity providers (IdP) (Facebook, Microsoft, and Google) and 96 popular RP websites that support the use of Facebook accounts for login. Our results uncover several critical vulnerabilities that allow an attacker to gain unauthorized access to the victim user's profile and social graph, and impersonate the victim on the RP website. Closer examination reveals that these vulnerabilities are caused by a set of design decisions that trade security for implementation simplicity. To improve the security of OAuth 2.0 SSO systems in real-world settings, we suggest simple and practical improvements to the design and implementation of IdPs and RPs that can be adopted gradually by individual sites."
1093933,15517,517,Dfuzzer: A D-Bus Service Fuzzing Tool,2014,"We present Dfuzzer, a fully automated tool for fuzz testing programs communicating via D-Bus. D-Bus is the prevalent modern mechanism for an inter-process communication in the GNU/Linux ecosystem. Programs receiving data over D-Bus should sanitize the inputs correctly as it may come from any application having access to the message bus. Unfortunately, it is often not the case as demonstrated by severe bugs found by the presented fuzzing tool. Dfuzzer is fully automated: using D-Bus introspection, it is able to acquire the structure of the parameters expected by the target program. It can then generate ballast data respecting this structure, so the target program starts using such data incorrectly if it does not carefully validate it. We have found numerous bugs in various parts of the GNU/Linux operating system, including GNOME Shell and system. The bugs usually result in crashes, but we have found other bugs like memory leaks and even a data-loss bug. We also discuss the software engineering aspects of fuzz testing D-Bus services. We have met developer opinions that the problems found do not constitute valid bugs, because the D-Bus interface is actually an internal API. The discussion is interesting by showing that D-Bus usage is not a fully mature area of engineering, and programmers do not have a shared understanding of its purpose."
1924827,15517,517,Model Checking a TTCAN Implementation,2011,"This paper describes how the SPIN model checker has been applied to find and correct problems in the software design of a distributed vessel control system currently under development at a control systems specialist in New Zealand. The system under development is a mission critical control system used on large marine vessels. Hence, the requirement to study the architecture and verify the implementation of the system. The model checking work reported here focused on analysing the implementation of the Time-Triggered Controller-Area-Network (TTCAN) protocol, as this is used as the backbone for communications between devices and thus is a crucial part of the control system. The starting point was to develop a set of general techniques for model checking TTCAN-like protocols. The techniques developed include modelling the progression of time efficiently in SPIN, TTCAN message transmission, TTCAN error handling, and CAN bus arbitration. These techniques form the basis of a set of models developed to check the implementation of TTCAN in the control system as well as the fault tolerance schemes added to the system. Descriptions of the models and properties developed to check the correctness of the implementation are given, and verification results are presented and discussed. This application of model checking to an industrial design problem has uncovered and corrected a number of potentially costly issues in the original design."
2349418,15517,339,"Code Injection Attacks on HTML5-based Mobile Apps: Characterization, Detection and Mitigation",2014,"Due to the portability advantage, HTML5-based mobile apps are getting more and more popular.Unfortunately, the web technology used by HTML5-based mobile apps has a dangerous feature, which allows data and code to be mixed together, making code injection attacks possible. In this paper, we have conducted a systematic study on this risk in HTML5-based mobile apps. We found a new form of code injection attack, which inherits the fundamental cause of Cross-Site Scripting attack~(XSS), but it uses many more channels to inject code than XSS. These channels, unique to mobile devices, include Contact, SMS, Barcode, MP3, etc. To assess the prevalence of the code injection vulnerability in HTML5-based mobile apps, we have developed a vulnerability detection tool to analyze 15,510 PhoneGap apps collected from Google Play. 478 apps are flagged as vulnerable, with only 2.30\% false-positive rate. We have also implemented a prototype called NoInjection as a Patch to PhoneGap in Android to defend against the attack."
1917488,15517,517,Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities,2011,"Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability."
2098180,15517,517,Mutation Testing of Go-Back Functions Based on Pushdown Automata,2011,"A go-back (GB) function for canceling recent user or system operations and going back to and resuming of previous state(s) is very often used regardless of the application domain. Therefore, faulty handling of them can cause severe damages in those applications. This paper proposes a mutation-based approach to testing GB functions modeled by pushdown automata. Novel mutation operators, recent coverage criteria, and a new algorithm for test case generation are introduced. A case study validates the approach and discusses its characteristics."
1442983,15517,517,Model-Based Exploratory Testing: A Controlled Experiment,2014,"Exploratory testing provides an intuitive way for testing a software product that testers can apply, but the advantages of exploratory testing are generally outweighed by its disadvantages, mainly the time and resources necessary to perform it manually. To address this problem, in this research, we propose Model-Based Exploratory Testing (MBET), an approach that incorporates the advantages of exploratory testing and Model-Based Testing (MBT) that automates the testing processes. To support the MBET approach, we implemented an automated testing tool, the Crushinator. To assess our approach, we conducted an experiment using an educational game application with multiple versions and we collected the number and type of defects detected with the MBET and MBT approaches. Our results showed that, overall, MBET detected more defects than MBT. The results also showed that MBET detected certain defect types better than MBT while MBT detected other types better than MBET."
1029249,15517,517,Experimental Evaluation of Mutation Testing Approaches to Python Programs,2014,"Mutation testing of Python programs raises a problem of incompetent mutants. Incompetent mutants cause execution errors due to inconsistency of types that cannot be resolved before run-time. We present a practical approach in which incompetent mutants can be generated, but the solution is transparent for a user and incompetent mutants are detected by a mutation system during test execution. Experiments with 20 traditional and object-oriented operators confirmed that the overhead can be accepted. The paper presents an experimental evaluation of the first- and higher-order mutation. Four algorithms to the 2nd and 3rd order mutant generation were applied. The impact of code coverage consideration on the process efficiency is discussed. The experiments were supported by the MutPy system for mutation testing of Python programs."
1241609,15517,517,SPaCiTE -- Web Application Testing Engine,2012,"Web applications and web services enjoy an ever-increasing popularity. Such applications have to face a variety of sophisticated and subtle attacks. The difficulty of identifying respective vulnerabilities steadily increases with the complexity of applications. Moreover, the art of penetration testing predominantly depends on the skills of highly trained test experts. The difficulty to test web applications hence represents a daunting challenge to their developers. As a step towards improving security analyses, model checking has, at the model level, been found capable of identifying complex attacks and thus moving security analyses towards a push-button technology. In order to bridge the gap with actual systems, we present Spa Cite. This tool relies on a dedicated model-checker for security analyses that generates potential attacks with regard to common vulnerabilities in web applications. Then, it semi-automatically runs those attacks on the System Under Validation (SUV) and reports which vulnerabilities were successfully exploited. We applied Spa Cite to Role-Based-Access-Control (RBAC) and Cross-Site Scripting (XSS) lessons of Web Goat, an insecure web application maintained by OWASP. The tool successfully reproduced RBAC and XSS attacks."
112722,15517,374,HI-CFG: Construction by Binary Analysis and Application to Attack Polymorphism,2013,"Security analysis often requires understanding both the con- trol and data-flow structure of a binary. We introduce a new program representation, a hybrid information- and control-flow graph (HI-CFG), and give algorithms to infer it from an instruction-level trace. As an application, we consider the task of generalizing an attack against a pro- gram whose inputs undergo complex transformations before reaching a vulnerability. We apply the HI-CFG to find the parts of the program that implement each transformation, and then generate new attack in- puts under a user-specified combination of transformations. Structural knowledge allows our approach to scale to applications that are infeasible with monolithic symbolic execution. Such attack polymorphism shows the insufficiency of any filter that does not support all the same trans- formations as the vulnerable application. In case studies, we show this attack capability against a PDF viewer and a word processor."
1888883,15517,517,Paraµ -- A Partial and Higher-Order Mutation Tool with Concurrency Operators,2011,"The mutation operators implemented in a mutation tool typically mutate the entire programs thoroughly and thus generate enormous number of mutants spread all over the programs. However, the complexity and difficulty level of methods implemented in a given program is less evenly distributed all over the program. Hence, the non-uniform distribution of complexity of methods in a program is an indication of testing efforts required for each method. We introduce partial mutations where only the complex parts of the programs are mutated instead of the entire programs. Paraµ is a mutation tool for Java programs enabling partial mutations. In addition to the class mutation, Paraµ implements concurrency mutation operators to address the recent advances in multicore systems and hence mutation testing of parallel and multi-threaded programs. Furthermore, Paraµ allows higher-order mutations by which the users are allowed to specify the order and the types of mutation operators and thus perform a combinatorial higher-order mutation testing."
1648517,15517,517,Exhaustive Exploration of Ajax Web Applications with Selective Jumping,2014,"Exploring modern web applications is a difficult task with the presence of client-side JavaScript code, as a crawler cannot jump or backtrack arbitrarily inside applications that maintain a state. In this paper, we present Web Mole, an automated crawler that implements a formal framework for web exploration that generalizes existing approaches. Web Mole uses an algorithm that explores an application without the need for arbitrary backtracking, it intercepts HTTP requests called from client-side code, and uses that information to perform selectively jump to pages while preserving the client-server state relationship. Comparisons with existing crawlers on various classes of graphs show that this strategy incurs a lower exploration cost."
2087095,15517,517,ACTS: A Combinatorial Test Generation Tool,2013,"In this paper, we introduce a combinatorial test generation research tool called Advanced Combinatorial Testing System (or ACTS). ACTS supports t-way combinatorial test generation with several advanced features such as mixed-strength test generation and constraint handling. To facilitate its use and integration with other tools, ACTS provides three types of external interface, including a graphic user interface, a command line interface, and an application programming interface. ACTS is a freely distributed research tool and has been downloaded by more than 1200 companies and organizations."
1506679,15517,517,Search-Based Testing of Relational Schema Integrity Constraints Across Multiple Database Management Systems,2013,"There has been much attention to testing applications that interact with database management systems, and the testing of individual database management systems themselves. However, there has been very little work devoted to testing arguably the most important artefact involving an application supported by a relational database - the underlying schema. This paper introduces a search-based technique for generating database table data with the intention of exercising the integrity constraints placed on table columns. The development of a schema is a process open to flaws like any stage of application development. Its cornerstone nature to an application means that defects need to be found early in order to prevent knock-on effects to other parts of a project and the spiralling bug-fixing costs that may be incurred. Examples of such flaws include incomplete primary keys, incorrect foreign keys, and omissions of NOT NULL declarations. Using mutation analysis, this paper presents an empirical study evaluating the effectiveness of our proposed technique and comparing it against a popular tool for generating table data, DBMonster. With competitive or faster data generation times, our method outperforms DBMonster in terms of both constraint coverage and mutation score."
1479352,15517,517,WebMate: Web Application Test Generation in the Real World,2014,"We present Web Mate, a tool for automatically generating test cases for Web applications. Given only the URL of the starting page, Web Mate automatically explores the functionality of a Web application, detecting differences across multiple browsers or operating systems, as well as across different revisions of the same Web application. Web Mate can handle full Web 2.0 functionality and explore sites as complex as Facebook. In addition to autonomously exploring the application, Web Mate can also leverage existing written or recorded test cases, and use these as an exploration base, this combination allows for quick expansion of the existing test base. Originating from research in generating test cases for specification mining, Web Mate is now the core product of a startup specializing in automated Web testing - a transfer that took us two years to complete. We report central lessons learned from this transfer, reflecting how robust, versatile, and pragmatic an innovative tool must be to be a success in the marketplace."
1779506,15517,517,MFL: Method-Level Fault Localization with Causal Inference,2013,"Recent studies have shown that use of causal inference techniques for reducing confounding bias improves the effectiveness of statistical fault localization (SFL) at the level of program statements. However, with very large programs and test suites, the overhead of statement-level causal SFL may be excessive. Moreover cost evaluations of statement-level SFL techniques generally are based on a questionable assumption-that software developers can consistently recognize faults when examining statements in isolation. To address these issues, we propose and evaluate a novel method-level SFL technique called MFL, which is based on causal inference methodology. In addition to reframing SFL at the method level, our technique incorporates a new algorithm for selecting covariates to use in adjusting for confounding bias. This algorithm attempts to ensure that such covariates satisfy the conditional exchangeability and positivity properties required for identifying causal effects with observational data. We present empirical results indicating that our approach is more effective than four method-level versions of well-known SFL techniques and that our confounder selection algorithm is superior to two alternatives."
2035803,15517,517,How Test Organizations Adopt New Testing Practices and Methods,2011,"Software testing process is an activity, in which the software is verified to comply with the requirements and validated to operate as intended. As software development adopts new development methods, this means also that the test processes need to be changed. In this qualitative study, we observe ten software organizations to understand how organizations develop their test processes and how they adopt new test methods. Based on our observations, organizations do only sporadic test process development, and are conservative when adopting new ideas or testing methods. Organizations need to have a clear concept of what to develop and how to implement the needed changes before they commit to process development."
2004599,15517,517,An Analysis of OO Mutation Operators,2011,"This paper presents results from empirical studies using object-oriented, class-level mutation operators. Class mutation operators modify OO programming language features such as inheritance, polymorphism, dynamic binding and encapsulation. Most previous empirical studies of mutation operators used statement-level operators, this study asked questions about the static and dynamic nature of class-level mutation operators. Results include statistics on the various types of mutants, how many are equivalent, new rules for avoiding creation of equivalent mutants, the difficulty of killing individual mutants, and the difficulty of killing mutants from the various operators. The paper draws conclusions about which mutation operators are more or less useful, leading to recommendations about how future OO mutation systems should be built."
1991242,15517,517,Systematic Testing of Database Engines Using a Relational Constraint Solver,2011,"We describe an automated approach for systematic black-box testing of database management systems (DBMS) using a relational constraint solver. We reduce the problem of automated database testing into generating three artifacts: (1) SQL queries for testing, (2) meaningful input data to populate test databases, and (3) expected results of executing the queries on the generated data. We leverage our previous work on ADUSA and the Automated SQL Query Generator to form high-quality test suites for testing DBMS engines. This paper presents a detailed description of our framework for Automated SQL Query Generation using the Alloy tool-set, and experimental results of testing database engines using our framework. We show how the main SQL grammar constraints can be solved by translating them to Alloy constraints to generate semantically and syntactically correct SQL queries. We also present experimental results of combining ADUSA and the Automated SQL Query Generator, and applying our framework to test the Oracle 11g database. Our framework generated 5 new queries, which reveal erroneous behavior of Oracle 11g."
1704725,15517,517,Search Based Testing of Embedded Systems Implemented in IEC 61131-3: An Industrial Case Study,2013,"This paper presents a case study of search-based test generation for embedded system software units developed using the Function Block Diagrams (FBDs), a graphical language in the IEC 61131-3 standard aimed at programmable logic controllers (PLCs). We consider 279 different components from the train control software developed by Bombardier Transportation, a major rail vehicle manufacturer. The software is compiled into C code with a particular structure. We use a modified hill climbing algorithm for generating test data to maximize MC/DC coverage for assignments with logical expressions in the C code, while retaining the semantics of the original FBD implementation. An experimental evaluation for comparing the effectiveness (coverage rate) and the efficiency (required number of executions) of hill climbing algorithm with random testing is presented. The results show that random testing performs well for most units under test, while around 30% of the artifacts significantly benefit from the hill climbing algorithm. Structural properties of the units that affect the performance of hill climbing and random testing are also discussed."
994095,15517,517,Towards Practical Debugging for Regression Faults,2012,"Regression faults are inevitably introduced in software development. Identifying and fixing regression faults can be tedious and time-consuming. The goal of my doctoral research is to provide an automated practical technique to effectively and efficiently locating failure-inducing changes. In this work, the research problem and related work is discussed first. Then, our approach, research questions, completed work and future work is presented. Finally, the expected contributions of my thesis are listed."
1981005,15517,517,Dependency-Based Test Case Selection and Prioritization in Embedded Systems,2012,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing."
1001947,15517,517,Evaluation of t-wise Approach for Testing Logical Expressions in Software,2013,"Pair-wise and, more generally, t-wise testing are the most common and powerful combinatorial testing approaches. This paper investigates the effectiveness of the t-wise approach for testing logical expressions in software in terms of its fault detecting capabilities. Effectiveness is evaluated experimentally using special software tools for generating logical expressions and t-wise test cases, simulating faults in expressions, testing faulty expressions, and evaluating effectiveness of the testing. T-wise testing effectiveness is measured in its totality and for specific types of faults; it is then compared with random testing. A detailed analysis of the experimental results is also provided."
1490193,15517,507,A dichotomy in the intensional expressive power of nested relational calculi augmented with aggregate functions and a powerset operator,2013,"The extensional aspect of expressive power---i.e., what queries can or cannot be expressed---has been the subject of many studies of query languages. Paradoxically, although efficiency is of primary concern in computer science, the intensional aspect of expressive power---i.e., what queries can or cannot be implemented efficiently---has been much neglected. Here, we discuss the intensional expressive power of  NRC ( Q , +, ·, ‏, ÷, Σ,  powerset ), a nested relational calculus augmented with aggregate functions and a powerset operation. We show that queries on structures such as long chains, deep trees, etc. have a dichotomous behaviour: Either they are already expressible in the calculus without using the powerset operation or they require at least exponential space. This result generalizes in three significant ways several old dichotomy-like results, such as that of Suciu and Paredaens that the complex object algebra of Abiteboul and Beeri needs exponential space to implement the transitive closure of a long chain. Firstly, a more expressive query language---in particular, one that captures SQL---is considered here. Secondly, queries on a more general class of structures than a long chain are considered here. Lastly, our proof is more general and holds for all query languages exhibiting a certain normal form and possessing a locality property."
1865275,15517,343,Toward a verifiable software dataplane,2013,"Software dataplanes are emerging as an alternative to traditional hardware switches and routers, promising programmability and short time to market. These advantages are set against the concern of introducing buggy or under-performing code into the network. We explore whether it is practical to formally prove that a software dataplane satisfies key properties that would ensure smooth network operation. In general, proving properties of real programs remains an elusive goal, but we argue that dataplanes are different: they typically follow a pipeline structure that enables our proposed approach, in which we verify pieces of the code in isolation, then compose the results to reason about the entire dataplane. We preliminarily demonstrate the potential of our approach by applying it on simple Click pipelines and proving that they are crash-free and execute a bounded number of instructions. This takes on the order of minutes, whereas a general-purpose state-of-the-art verifier fails to complete the same task within 12 hours."
1528202,15517,517,Designing Sequence Diagram Models for Robustness to Attacks,2014,"The omnipresence of complex distributed component-based systems offers numerous opportunities for malicious parties, especially thanks to the numerous communication mechanisms brought into play. This is particularly true for Smart Grids systems in which electricity networks and information technology are coupled to provide smarter and more efficient energy production-to-consumption chain. Indeed, Smart Grids are clearly security sensitive since a lot of components usually operate outside of the trusted company's border. In this paper, we propose a model-based methodology targeting the diagnostic of attacks with respect to some trusted components. The methodology combines UML sequence diagrams (SD) and formal symbolic techniques in order to model and analyze systems and threats from early design stages. We introduce a criterion that allows us to qualify or not a SD as robust with respect to an attack, also modeled as a SD. The criterion is defined by comparing traces as they are perceived by trusted components. We illustrate our approach with a UML sequence diagram issued from a Smart Grid case study."
2471178,15517,517,Identification of Anomalies in Processes of Database Alteration,2013,"Data, especially in large item sets, hide a wealth of information on the processes that have created and modified them. Often, a data-field or a set of data-fields are not modified only through well-defined processes, but also through latent processes; without the knowledge of the second type of processes, testing cannot be considered exhaustive. As a matter of fact, changes in the data deriving from unknown processes can cause anomalies not detectable by testing, which focuses on known data variation rules. History of data variations can yield information about the nature of the changes. In my work I focus on the elicitation of an evolution profile of data: the values data may assume, the change frequencies, the temporal variation of a piece of data in relation to other data, or other constraints that are directly connected to the reference domain. The profile of evolution is then used to detect anomalies in the database state evolution. Detecting anomalies in the database state evolution could strengthen the quality of a system, since a data anomaly could be the signal of a defect in the applications populating the database."
2074157,15517,517,Scenario Based Test Generation Using Test Designer,2011,"This paper presents a Scenario Based Testing approach for UML/OCL behavioral models. Scenarios are expressed using a regular expression syntax, that makes it possible to specify iterations and choices between sequences of operation calls, specific operation behaviors to be activated, and intermediate states that have to be reached by the scenario. This expressive scenario language describes sequences of operations that compose the scenario and restrict the execution of the model to representative subset that complements the functional test cases produced by a test generator. This approach is tool supported by a scenario editor and coupled with the model animation engine of the Test Designer tool that is used to animate unfolded test cases and check their consistency with respect to the initial test scenario. The resulting abstract test cases can then be integrated back to the Test Designer repository so as to benefit from the test publishers offered by the tool."
1441104,15517,339,Covert computation: hiding code in code for obfuscation purposes,2013,"As malicious software gets increasingly sophisticated and resilient to detection, new concepts for the identification of malicious behavior are developed by academia and industry alike. While today's malware detectors primarily focus on syntactical analysis (i.e., signatures of malware samples), the concept of semantic-aware malware detection has recently been proposed. Here, the classification is based on models that represent the underlying machine and map the effects of instructions on the hardware. In this paper, we demonstrate the incompleteness of these models and highlight the threat of malware, which exploits the gap between model and machine to stay undetectable. To this end, we introduce a novel concept we call  covert computation , which implements functionality in side effects of microprocessors. For instance, the flags register can be used to calculate basic arithmetical and logical operations. Our paper shows how this technique could be used by malware authors to hide malicious code in a harmless-looking program. Furthermore, we demonstrate the resilience of  covert computation  against semantic-aware malware scanners."
896763,15517,339,25 million flows later: large-scale detection of DOM-based XSS,2013,"In recent years, the Web witnessed a move towards sophis- ticated client-side functionality. This shift caused a signifi- cant increase in complexity of deployed JavaScript code and thus, a proportional growth in potential client-side vulnera- bilities, with DOM-based Cross-site Scripting being a high impact representative of such security issues. In this paper, we present a fully automated system to detect and validate DOM-based XSS vulnerabilities, consisting of a taint-aware JavaScript engine and corresponding DOM implementation as well as a context-sensitive exploit generation approach. Using these components, we conducted a large-scale analysis of the Alexa top 5000. In this study, we identified 6167 unique vulnerabilities distributed over 480 domains, show- ing that 9,6% of the examined sites carry at least one DOM- based XSS problem."
1821108,15517,517,CrowdOracles: Can the Crowd Solve the Oracle Problem?,2013,"Despite the recent advances in test generation, fully automatic software testing remains a dream: Ultimately, any generated test input depends on a test oracle that determines correctness, and, except for generic properties such as “the program shall not crash”, such oracles require human input in one form or another. CrowdSourcing is a recently popular technique to automate computations that cannot be performed by machines, but only by humans. A problem is split into small chunks, that are then solved by a crowd of users on the Internet. In this paper we investigate whether it is possible to exploit CrowdSourcing to solve the oracle problem: We produce tasks asking users to evaluate CrowdOracles - assertions that reflect the current behavior of the program. If the crowd determines that an assertion does not match the behavior described in the code documentation, then a bug has been found. Our experiments demonstrate that CrowdOracles are a viable solution to automate the oracle problem, yet taming the crowd to get useful results is a difficult task."
212669,15517,235,A Step Towards Usable Privacy Policy: Automatic Alignment of Privacy Statements,2014,"With the rapid development of web-based services, concerns about user privacy have heightened. The privacy policies of online websites, which serve as a legal agreement between service providers and users, are not easy for people to understand and therefore offer an opportunity for natural language processing. In this paper, we consider a corpus of these policies, and tackle the problem of aligning or grouping segments of policies based on the privacy issues they address. A dataset of pairwise judgments from humans is used to evaluate two methods, one based on clustering and another based on a hidden Markov model. Our analysis suggests a five-point gap between system and median-human levels of agreement with a consensus annotation, of which half can be closed with bag of words representations and half requires more sophistication."
2472832,15517,517,An Assessment of the Quality of Automated Program Operator Repair,2014,"Automated program repair (APR) techniques fix faults by repeatedly modifying suspicious code until a program passes a set of test cases. Although generating a repair is the goal of APR, a repair can have negative consequences. The quality of a repair is reduced when the repair introduces new faults and/or degrades maintainability by adding irrelevant but functionally benign code. We used two APR approaches to repair faulty binary operators: (1) find a repair in existing code by applying a genetic algorithm to replace suspicious code with other existing code as done by GenProg, and (2) mutate suspicious operators within a genetic algorithm. Mutating operators was clearly more effective in repairing faulty operators than using existing code for a repair. We also evaluated the approaches in terms of two potential negative effects: (1) the introduction of new faults and (2) a reduction of program maintainability. We found that repair processes that use tests that satisfy branch coverage reduce the number of new faults. In contrast, repair processes using tests that satisfy statement coverage and randomly generated tests introduce numerous new faults. We also demonstrate that a mutation based repair process produces repairs that should be more maintainable compared to those produced using existing code."
1731027,15517,517,Challenges for Addressing Quality Factors in Model Transformation,2012,"Designing a high quality model transformation is critical, because it is the pivotal mechanism in many mission applications for evolving the intellectual design described by models. This paper proposes solution ideas to assist modelers in developing high quality transformation models. We propose to initiate a design pattern movement in the context of model transformation. The resulting catalog of patterns shall satisfy quality attributes identified beforehand. Verification and validation of these patterns allow us to assess whether the cataloged design patterns are sound and complete with respect to the quality criteria. This will lead to techniques and tools that can detect bad designs and propose alternatives based on well-thought design patterns during the development or maintenance of model transformation."
1976907,15517,517,A Principled Evaluation of the Effect of Directed Mutation on Search-Based Statistical Testing,2011,"Statistical testing generates test inputs by sampling from a probability distribution that is carefully chosen so that the inputs exercise all parts of the software being tested. Sets of such inputs have been shown to detect more faults than test sets generated using traditional random and structural testing techniques. Search-based statistical testing employs a metaheuristic search algorithm to automate the otherwise labour-intensive process of deriving the probability distribution. This paper proposes an enhancement to this search algorithm: information obtained during fitness evaluation is used to direct the mutation operator to those parts of the representation where changes may be most beneficial. A principled empirical evaluation demonstrates that this enhancement leads to a significant improvement in algorithm performance, and so increases both the cost-effectiveness and scalability of search-based statistical testing. As part of the empirical approach, we demonstrate the use of response surface methodology as an effective and objective method of tuning algorithm parameters, and suggest innovative refinements to this methodology."
785062,15517,507,On XPath with transitive axes and data tests,2013,"We study the satisfiability problem for XPath with data equality tests. XPath is a node selecting language for XML documents whose satisfiability problem is known to be undecidable, even for very simple fragments. However, we show that the satisfiability for XPath with the rightward, leftward and downward reflexive-transitive axes (namely  following-sibling-or-self, preceding-sibling-or-self, descendant-or-self ) is decidable. Our algorithm yields a complexity of 3EXPSPACE, and we also identify an expressive-equivalent normal form for the logic for which the satisfiability problem is in 2EXPSPACE. These results are in contrast with the undecidability of the satisfiability problem as soon as we replace the reflexive-transitive axes with just transitive (non-reflexive) ones."
2076566,15517,517,Offset-Aware Mutation Based Fuzzing for Buffer Overflow Vulnerabilities: Few Preliminary Results,2011,"This article presents few preliminary results and future ideas related to smart fuzzing to detect buffer overflow vulnerabilities. The approach is based on the combination of lightweight static analysis techniques and mutation-based evolutionary strategies. First, a static taint-analysis allows to identify the most dangerous execution paths, containing vulnerable statements those execution depend on user input streams. Then, concrete input are produced and executed on the vulnerable program following an offset-aware mutation strategy: at each step, the current input streams are mutated with specific values, and at specific offsets, depending on their ability to activate a target execution path. We provide few empirical results on a benchmarking dataset as a proof of concept and discuss future extension."
1668653,15517,339,Obfuscation resilient binary code reuse through trace-oriented programming,2013,"With the wide existence of binary code, it is desirable to reuse it in many security applications, such as malware analysis and software patching. While prior approaches have shown that binary code can be extracted and reused, they are often based on static analysis and face challenges when coping with obfuscated binaries. This paper introduces trace-oriented programming (TOP), a general framework for generating new software from existing binary code by elevating the low-level binary code to C code with templates and inlined assembly. Different from existing work, TOP gains benefits from dynamic analysis such as resilience against obfuscation and avoidance of points-to analysis. Thus, TOP can be used for malware analysis, especially for malware function analysis and identification. We have implemented a proof-of-concept of TOP and our evaluation results with a range of benign and malicious software indicate that TOP is able to reconstruct source code from binary execution traces in malware analysis and identification, and binary function transplanting."
2278748,15517,122,Inferring ownership transfer for efficient message passing,2011,"One of the more popular paradigms for concurrent programming is the Actor model of message passing; it has been adopted in one form or another by a number of languages and frameworks. By avoiding a shared local state and instead relying on message passing, the Actor model facilitates modular programming. An important challenge for message passing languages is to transmit messages efficiently. This requires retaining the pass-by-value semantics of messages while avoiding making a deep copy on sequential or shared memory multicore processors. A key observation is that many messages have an ownership transfer semantics; such messages can be sent efficiently using pointers without introducing shared state between concurrent objects. We propose a conservative static analysis algorithm which infers if the content of a message is compatible with an ownership transfer semantics. Our tool, called SOTER (for Safe Ownership Transfer enablER) transforms the program to avoid the cost of copying the contents of a message whenever it can infer the content obeys the ownership transfer semantics. Experiments using a range of programs suggest that our conservative static analysis method is usually able to infer ownership transfer. Performance results demonstrate that the transformed programs execute up to an order of magnitude faster than the original programs."
1462963,15517,517,Verified Operational Patterns with Graph Transformation,2012,"Using object-oriented patterns such as design patterns, architectural patterns, and refactoring operations has considerably simplified the design process of software systems. With the proliferation of Domain-Specific Languages, the generalization of OO patterns is a natural demand. A straightforward idea is to adapt OO patterns with automated tool support to the practice of Domain-Specific Modeling as well. A possible solution for that is using graph transformations to formalize and realize such patterns. One may expect, however, that the patterns are realized in a way that they are correct and do exactly what we expect them to. In this paper, we present how one can precisely define the requirements for a domain-specific model pattern, and how to verify the requirements on the implemented patterns. The presented concept is motivated and illustrated with a case study from the state chart domain."
923367,15517,517,En Garde: Winning Coding Duels through Genetic Programming,2013,"In this paper we present a Genetic Programming system to solve coding duels on the Pex4Fun website. Users create simple puzzle methods in a .NET supported programming language, and other users have to `guess' the puzzle implementation through trial and error. We have replaced the human user who solves a puzzle (i.e. implements a program that matches the implementation of the puzzle) with a Genetic Programming system that tries to win such coding duels. During a proof of concept experiment we found that our system can indeed automatically generate code that matches the behaviour of a secret puzzle method. It takes on average 76.57 fitness evaluations to succeed."
1559741,15517,517,Online Model-Based Behavioral Fuzzing,2013,"Fuzz testing or fuzzing is interface robustness testing by stressing the interface of a system under test (SUT) with invalid input data. It aims at finding security-relevant weaknesses in the implementation that may result in a crash of the system-under-test or anomalous behavior. Fuzzing means sending invalid input data to the SUT, the input space is usually huge. This is also true for behavioral fuzzing where invalid message sequences are submitted to the SUT. Because systems are getting more and more complex, testing a single invalid message sequence becomes more and more time consuming due to startup and initialization of the SUT. We present an approach to make the test execution for behavioral fuzz testing more efficient by generating test cases at runtime instead of before execution, focusing on interesting regions of a message sequence based on a previously conducted risk analysis and reducing the test space by integrating already retrieved test results in the test generation process."
1746183,15517,517,Automatic XACML Requests Generation for Policy Testing,2012,"Access control policies are usually specified by the XACML language. However, policy definition could be an error prone process, because of the many constraints and rules that have to be specified. In order to increase the confidence on defined XACML policies, an accurate testing activity could be a valid solution. The typical policy testing is performed by deriving specific test cases, i.e. XACML requests, that are executed by means of a PDP implementation, so to evidence possible security lacks or problems. Thus the fault detection effectiveness of derived test suite is a fundamental property. To evaluate the performance of the applied test strategy and consequently of the test suite, a commonly adopted methodology is using mutation testing. In this paper, we propose two different methodologies for deriving XACML requests, that are defined independently from the policy under test. The proposals exploit the values of the XACML policy for better customizing the generated requests and providing a more effective test suite. The proposed methodologies have been compared in terms of their fault detection effectiveness by the application of mutation testing on a set of real policies."
1054132,15517,517,Repairing Selenium Test Cases: An Industrial Case Study about Web Page Element Localization,2013,"This poster presents an industrial case study about test automation and test suite maintenance in the context of Web applications. The Web application under test is a Learning Content Management System (eXact learning LCMS). We analysed the costs associated with the realignment of four equivalent Selenium WebDriver test suites, implemented using the page object pattern and different methods to locate web page elements, to a subsequent release of eXact learning LCMS. In our study, the two ID-based test suites required significantly less maintenance effort than the XPath-based ones."
2655340,15517,122,Concurrent breakpoints,2012,"In program debugging, reproducibility of bugs is a key requirement. Unfortunately, bugs in concurrent programs are notoriously difficult to reproduce because bugs due to concurrency happen under very specific thread schedules and the likelihood of taking such corner-case schedules during regular testing is very low. We propose concurrent breakpoints, a light-weight and programmatic way to make a concurrency bug reproducible. We describe a mechanism that helps to hit a concurrent breakpoint in a concurrent execution with high probability. We have implemented concurrent breakpoints as a light-weight library for Java and C/C++ programs. We have used the implementation to deterministically reproduce several known non-deterministic bugs in real-world concurrent Java and C/C++ programs with almost 100% probability."
1855183,15517,517,On the Use of Constraints in Dynamic Slicing for Program Debugging,2011,"During the past decades research in automated debugging has led to several approaches. Some of them are based on structural and some of them on behavioral characteristics of the source code. The quality of the obtained results in terms of computed fault candidates compared to the overall number of statement usually is better when considering the program's behavior or even more sophisticated models representing the structure and the semantics of the program. However, for larger programs such approaches are not feasible and more light-weighted techniques have to be used in order to keep running time as small as possible. In this paper, we introduce an approach that combines dynamic slicing with constraint computation in order to increase the debugging quality while retaining small computational footprint. Using the presented approach it was possible to remove all fault candidates except the correct root cause for a particular program."
1821210,15517,517,Munch: An Efficient Modularisation Strategy to Assess the Degree of Refactoring on Sequential Source Code Checkings,2011,"Software module clustering is the process of automatically partitioning the structure of the system using low-level dependencies in the source code, to improve the system's structure. There have been a large number of studies using the search-based software engineering approach to solve the software module clustering problem. This paper introduces the concept of seeding to modularise sequential source code software versions, in order to measure the degree of refactoring. We have developed a software clustering tool called Munch. We evaluated the efficiency of the modularisation by performing a set of experiments on the dataset. We initially experimented with few fitness functions and as a result chose what we believe the most suitable function EVMD to test on our unique dataset. The results of the experiments provide evidence to support the seeding strategy."
1069746,15517,517,OCCF: A Framework for Developing Test Coverage Measurement Tools Supporting Multiple Programming Languages,2013,"Although many programming languages and test coverage criteria currently exist, most coverage measurement tools only support select programming languages and coverage criteria. Consequently, multiple measurement tools must be combined to measure coverage for software which uses multiple programming languages such as web applications. However, such combination leads to inconsistent and inaccurate measurement results. In this paper, we describe a consistent and flexible framework for measuring coverage supporting multiple programming languages, called Open Code Coverage Framework (OCCF). OCCF allows users to add new extensions for supporting programming languages and coverage criteria with low development costs. To evaluate the effectiveness of OCCF, sample implementation to support statement coverage and decision coverage for eight programming languages (C, C++, C#, Java, JavaScript, Python, Ruby and Lua) are demonstrated. Additionally, applications of OCCF for localizing faults and minimizing tests are shown."
2015012,15517,517,"CPTEST: A Framework for the Automatic Fault Detection, Localization and Correction of Constraint Programs",2011,"Constraint programs, such as those written in high level constraint modeling languages, e.g., OPL (Optimization Programming Language), are more and more used in business critical programs. As any other critical programs, they require to be thoroughly tested and corrected to prevent catastrophic loss of money. This paper is a demonstrations tool of CPTEST, a first testing tool for constraint programs. In particular, the paper presents the design of CPTEST and the implementation of our approaches enabling so automatic detection, localization and correction of faults in OPL programs."
1977543,15517,517,Improving Test Suites Maintainability with the Page Object Pattern: An Industrial Case Study,2013,"The page object pattern is used in the context of web testing for abstracting the application's web pages in order to reduce the coupling between test cases and application under test. This paper reports on an industrial case study in a small Italian company (eXact learning solutions S.p.A.) investigating the potential benefits of adopting the page object pattern to improve the maintainability of Selenium WebDriver test cases. After a maintenance/evolution activity performed on the application under test, we compared two equivalent test suites, one built using the page object pattern and one without it. The results of our case study indicate a strong reduction in terms of time required (by a factor of about three) and number of modified LOCs (by a factor of about eight) to repair the test suite when the page object pattern is used."
1769967,15517,517,Conformance Testing from Message Sequence Charts,2011,"There are several industries in which Message Sequence Charts (MSCs) and the corresponding UML notation (Sequence Diagrams) are used to describe requirements. However, most work on model based testing has looked at testing from other languages such as input output transition systems and finite state machines. This paper explores the problem of testing on the basis of an MSC specification. We develop a formal test framework and explore the notion of a test hypothesis in this context. It transpires that there are several possible test architectures and each defines the observational power of the tester(s) and so we describe a flexible test architecture. In this paper we explore these alternatives and define corresponding implementation relations, explaining how verdicts can be produced for these relations. We then show how test suites can be generated and executed and define test coverage criteria."
923054,15517,339,Virtual browser: a virtualized browser to sandbox third-party JavaScripts with enhanced security,2012,"Third party JavaScripts not only offer much richer features to the web and its applications but also introduce new threats. These scripts cannot be completely trusted and executed with the privileges given to host web sites. Due to incomplete virtualization and lack of tracking all the data flows, all existing approaches without native sandbox support can secure only a subset of third party JavaScripts, and they are vulnerable to attacks encoded in non-standard HTML/-JavaScript (browser quirks) as these approaches will parse third party JavaScripts independently at server side without considering client-side non-standard parsing quirks. At the same time, native sandboxes are vulnerable to attacks based on unknown native JavaScript engine bugs.   In this paper, we propose Virtual Browser, a full browser-level virtualized environment within existing browsers for executing untrusted third party code. Our approach supports more complete JavaScript language features including those hard-to-secure functions, such as  with  and  eval . Since Virtual Browser does not rely on native browser parsing behavior, there is no possibility of attacks being executed through browser quirks. Moreover, given the third-party Javascripts are running in Virtual Browser instead of native browsers, it is harder for the attackers to exploit unknown vulnerabilities in the native JavaScript engine. In our design, we first completely isolate Virtual Browser from the native browser components and then introduce communication by adding data flows carefully examined for security. The evaluation of the Virtual Browser prototype shows that our execution speed is the same as Microsoft Web Sandbox[5], a state of the art runtime web-level sandbox. In addition, Virtual Browser is more secure and supports more complete JavaScript for third party JavaScript development."
1989326,15517,517,An Experience Report on an Industrial Case-Study about Timed Model-Based Testing with UPPAAL-TRON,2011,"Several theories have been proposed for timed model-based testing, but only few case-studies have been reported. In this paper, we report about our experience in using UPPAAL-TRON to test the conformance of an industrial implementation Auto trust of Automatic Trust Anchor Updating, a protocol to help securing DNS. We created models for specific parts of the protocol focussing on security key states and critical timing behaviours. We developed testing environments to test one or multiple keys and to check a security-relevant feature. This case-study also illustrates several challenges when testing timed systems, namely, quiescence, latency, and coverage."
2197721,15517,517,Tester Feedback Driven Fault Localization,2012,"Coincidentally correct test cases are those that execute faulty statements but do not cause failures. Such test cases reduce the effectiveness of spectrum-based fault localization techniques, such as Ochiai, because the correlation of failure with the execution of a faulty statement is lowered. Thus, coincidentally correct test cases need to be predicted and removed from the test suite used for fault localization. Techniques for predicting coincidentally correct test cases can produce false positives, such as when one predicts a fixed percentage that is higher than the actual percentage of coincidentally correct test cases. False positives may cause non-faulty statements to be assigned higher suspiciousness scores than the faulty statements. We propose an approach that iteratively predicts and removes coincidentally correct test cases. In each iteration, we present the tester the set of statements that share the highest Ochiai suspiciousness score. If the tester reports that these statements are not faulty, we use that feedback to determine a number that is guaranteed to be less than or equal to the actual number of coincidentally correct test cases. We predict and remove that number of coincidentally correct test cases, recalculate the suspiciousness scores of the remaining statements, and repeat the process. We evaluated our approach with the Siemens benchmark suite and the Unix utilities, grep and gzip. Our approach outperformed an existing approach that predicts a fixed percentage of test cases as coincidentally correct. The results with Ochiai were mixed. In some cases, our approach outperformed Ochiai by up to 67\%. In others, Ochiai was more effective."
2102647,15517,517,Behaviourally Adequate Software Testing,2012,"Identifying a finite test set that adequately captures the essential behaviour of a program such that all faults are identified is a well-established problem. Traditional adequacy metrics can be impractical, and may be misleading even if they are satisfied. One intuitive notion of adequacy, which has been discussed in theoretical terms over the past three decades, is the idea of behavioural coverage, if it is possible to infer an accurate model of a system from its test executions, then the test set must be adequate. Despite its intuitive basis, it has remained almost entirely in the theoretical domain because inferred models have been expected to be exact (generally an infeasible task), and have not allowed for any pragmatic interim measures of adequacy to guide test set generation. In this work we present a new test generation technique that is founded on behavioural adequacy, which combines a model evaluation framework from the domain of statistical learning theory with search-based white-box test generation strategies. Experiments with our BESTEST prototype indicate that such test sets not only come with a statistically valid measurement of adequacy, but also detect significantly more defects."
1529608,15517,517,Automated Bug Finding in Video Games: A Case Study for Runtime Monitoring,2014,"Runtime verification is the process of observing a sequence of events generated by a running system and comparing it to some formal specification for potential violations. We show how the use of a runtime monitor can greatly speed up the testing phase of a video game under development, by automating the detection of bugs when the game is being played. We take advantage of the fact that a video game, contrarily to generic software, follows a special structure that contains a game loop, this game loop can be used to centralize the instrumentation and generate events based on the game's internal state. We report on experiments made on a sample of five real-world video games of various genres and sizes, by successfully incrementing and efficiently monitoring various temporal properties over their execution-including actual bugs reported in the games' bug tracking database in the course of their development."
2511667,15517,8385,Effective communication of software development knowledge through community portals,2011,"Knowledge management plays an important role in many software organizations. Knowledge can be captured and distributed using a variety of media, including traditional help files and manuals, videos, technical articles, wikis, and blogs. In recent years, web-based community portals have emerged as an important mechanism for combining various communication channels. However, there is little advice on how they can be effectively deployed in a software project.   In this paper, we present a first study of a community portal used by a closed source software project. Using grounded theory, we develop a model that characterizes documentation artifacts along several dimensions, such as content type, intended audience, feedback options, and review mechanisms. Our findings lead to actionable advice for industry by articulating the benefits and possible shortcomings of the various communication channels in a knowledge-sharing portal. We conclude by suggesting future research on the increasing adoption of community portals in software engineering projects."
1994283,15517,517,Scenario-Based Testing Applied in Two Real Contexts: Healthcare and Employability,2011,"In this paper we focus on the automatic application of UML scenario-based testing to two real case studies developed in the Integrated Project TAS3: one focuses on issues from healthcare, the other on those from an employability context. The aim was the derivation of integration test plans. We report on our experience, and the results obtained from the two test plan executions, which resulted in the discovery of some critical bugs in both integration trials, thereby opening up the possibility of discussing and applying immediate corrective actions."
1890098,15517,122,From relational verification to SIMD loop synthesis,2013,"Existing pattern-based compiler technology is unable to effectively exploit the full potential of SIMD architectures. We present a new program synthesis based technique for auto-vectorizing performance critical innermost loops. Our synthesis technique is applicable to a wide range of loops, consistently produces performant SIMD code, and generates correctness proofs for the output code. The synthesis technique, which leverages existing work on relational verification methods, is a novel combination of deductive loop restructuring, synthesis condition generation and a new inductive synthesis algorithm for producing loop-free code fragments. The inductive synthesis algorithm wraps an optimized depth-first exploration of code sequences inside a CEGIS loop. Our technique is able to quickly produce SIMD implementations (up to 9 instructions in 0.12 seconds) for a wide range of fundamental looping structures. The resulting SIMD implementations outperform the original loops by 2.0x-3.7x."
2479854,15517,517,An Evaluation of Mutation and Data-Flow Testing: A Meta-analysis,2011,"Mutation testing is a fault-based testing technique for assessing the adequacy of test cases in detecting synthetic faulty versions injected to the original program. The empirical studies report the effectiveness of mutation testing. However, the inefficiency of mutation testing has been the major drawback of this testing technique. Though a number of studies compare mutation to data ?ow testing, the summary statistics for measuring the magnitude order of effectiveness and efficiency of these two testing techniques has not been discussed in literature. In addition, the validity of each individual study is subject to external threats making it hard to draw any general conclusion based solely on a single study. This paper introduces a novel meta-analytical approach to quantify and compare mutation and data ?ow testing techniques based on findings reported in research articles. We report the results of two statistical meta-analyses performed on comparing and measuring the effectiveness as well as efficiency of mutation and data-?ow testing based on relevant empirical studies. We focus on the results of three empirical research articles selected from the premier venues with their focus on comparing these two testing techniques. The results show that mutation is at least two times more effective than data-?ow testing, i.e., odds ratio= 2.27. However, mutation is three times less efficient than data-?ow testing, i.e., odds ratio= 2.94."
2202085,15517,517,Factors Limiting Industrial Adoption of Test Driven Development: A Systematic Review,2011,"Test driven development (TDD) is one of the basic practices of agile software development and both academia and practitioners claim that TDD, to a certain extent, improves the quality of the code produced by developers. However, recent results suggest that this practice is not followed to the extent preferred by industry. In order to pinpoint specific obstacles limiting its industrial adoption we have conducted a systematic literature review on empirical studies explicitly focusing on TDD as well as indirectly addressing TDD. Our review has identified seven limiting factors viz., increased development time, insufficient TDD experience/knowledge, lack of upfront design, domain and tool specific issues, lack of developer skill in writing test cases, insufficient adherence to TDD protocol, and legacy code. The results of this study is of special importance to the testing community, since it outlines the direction for further detailed scientific investigations as well as highlights the requirement of guidelines to overcome these limiting factors for successful industrial adoption of TDD."
1742443,15517,517,Analysis and Prediction of Mandelbugs in an Industrial Software System,2013,"Mandelbugs are faults that are triggered by complex conditions, such as interaction with hardware and other software, and timing or ordering of events. These faults are considerably difficult to detect with traditional testing techniques, since it can be challenging to control their complex triggering conditions in a testing environment. Therefore, it is necessary to adopt specific verification and/or fault-tolerance strategies for dealing with them in a cost-effective way. In this paper, we investigate how to predict the location of Mandelbugs in complex software systems, in order to focus V&V activities and fault tolerance mechanisms in those modules where Mandelbugs are most likely present. In the context of an industrial complex software system, we empirically analyze Mandelbugs, and investigate an approach for Mandelbug prediction based on a set of novel software complexity metrics. Results show that Mandelbugs account for a noticeable share of faults, and that the proposed approach can predict Mandelbug-prone modules with greater accuracy than the sole adoption of traditional software metrics."
1383063,15517,517,Semi-automatic Incompatibility Localization for Re-engineered Industrial Software,2014,"After a legacy system is re-engineered, it is important to perform compatibility testing so as to identify the differences and reduce the introduced bugs. We can first apply symbolic execution to obtain an exhaustive set of test cases, then use them to check the compatibility of the old system and the new one. However there may be a lot of failed test cases which are a mix of erroneous and allowable incompatibilities. To locate the causes of failures detected during the testing, we apply multiple statistical bug localization techniques. We are able to localize 90% of the incompatibilities in 10% of the code for an industrial application with around 20k lines by Tarantula. And we identify the characteristics of failure causes which are difficult to be detected by statistical bug localization."
1635316,15517,507,Data management perspectives on business process management: tutorial overview,2013,"Traditional approaches to Business Process Management (BPM) focus primarily on the process aspects, and treat the persistent data accessed and manipulated by the business processes as second class citizens. A recent approach to BPM, based on business artifacts, is centered on a modeling framework that places data and process on an equal footing. The approach has been shown useful in various application domains, and one variant of business artifacts forms the basis of the emerging OMG Case Management Model and Notation (CMMN) standard. Research results have been developed around conceptual models, enterprise interoperation, business intelligence, and verification. This data-centric approach has the potential to provide the basis for a new generation of BPM technology in support of diverse application, and fueled by the insights into abstraction and data management that have been the hallmark of database research since the 70's."
2460662,15517,517,Analysis of Mistakes as a Method to Improve Test Case Design,2011,"Test Design -- how test specifications and test cases are created -- inherently determines the success of testing. However, test design techniques are not always properly applied, leading to poor testing. We have developed an analysis method based on identifying mistakes made when designing the test cases. Using an extended test case template and an expert review, the method provides a systematic categorization of mistakes in the test design. The detailed categorization of mistakes provides a basis for improvement of the Test Case Design, resulting in better tests. In developing our method we have investigated over 500 test cases created by novice testers. In a comparison with industrial test cases we could confirm that many of these mistake categories remain relevant also in an industrial context. Our contribution is a new method to improve the effectiveness of test case construction through proper application of test design techniques, leading to an improved coverage without loss of efficiency."
952370,15517,517,Performance Testing Web Applications on the Cloud,2014,"Web applications are increasingly resorting to public cloud platforms such as the Amazon Web Services (AWS) Elastic Compute Cloud (EC2). However, previous studies have shown that the virtualized infrastructures used in a cloud environment can introduce performance issues. Hence, it is crucial to test the performance of the cloud to support Web applications. This is challenging because the performance effect of the cloud platform cannot be easily isolated from other extraneous factors. In this paper, we present a systematic experimental process to address this challenge. Following our proposed process, we test the performance of Web applications running on different types of AWS EC2 instances. Results suggest that Web server response times can fluctuate up to 1000% for the same workload under certain circumstances such as instance type, time-of-the-day, and day-of-the-week."
2001996,15517,517,Assessing the Impact of Using Fault Prediction in Industry,2011,"Software developers and testers need realistic ways to measure the practical effects of using fault prediction models to guide software quality improvement methods such as testing, code reviews, and refactoring. Will the availability of fault predictions lead to discovery of different faults, or to more efficient means of finding the same faults? Or do fault predictions have no practical impact at all? In this challenge paper we describe the difficulties of answering these questions, and the issues involved in devising meaningful ways to assess the impact of using prediction models. We present several experimental design options and discuss the pros and cons of each."
1738061,15517,517,Software Product Line Testing -- A 3D Regression Testing Problem,2012,"In software product line engineering, testing for regression concerns not only versions, as in one-off product development, but also regression across variants. We propose a 3D process model, with the dimensions of level, version and variant, to help analyze, plan and manage software product line testing. We derive the model from empirical observations of regression testing practice and software product line testing theory and practice, and look forward to see the model evaluated in practitioner-oriented research."
1724636,15517,517,Mitigating the Effect of Coincidental Correctness in Spectrum Based Fault Localization,2012,"Coincidentally correct test cases are those that execute faulty statements but do not cause failures. Such test cases reduce the effectiveness of spectrum-based fault localization techniques, such as Ochiai. These techniques calculate a suspiciousness score for each statement. The suspiciousness score estimates the likelihood that the program will fail if the statement is executed. The presence of coincidentally correct test cases reduces the suspiciousness score of the faulty statement, thereby reducing the effectiveness of fault localization. We present two approaches that predict coincidentally correct test cases and use the predictions to improve the effectiveness of spectrum based fault localization. In the first approach, we assign weights to passing test cases such that the test cases that are likely to be coincidentally correct obtain low weights. Then we use the weights to calculate suspiciousness scores. In the second approach, we iteratively predict and remove coincidentally correct test cases, and calculate the suspiciousness scores with the reduced test suite. In this dissertation, we investigate the cost and effectiveness of our approach to predicting coincidentally correct test cases and utilizing the predictions. We report the results of our preliminary evaluation of effectiveness and outline our research plan."
2166310,15517,517,Automated Significant Load Testing for WS-BPEL Compositions,2013,"Web service compositions must provide services to hundreds even thousands of users concurrently. These applications must be load tested to ensure that they can function properly under high load. We propose in this paper a solution for load testing of WS-BPEL compositions. It is based on Timed Automata as model for specifying requirements to test under various load conditions, a distributed testing architecture, an algorithm for online load test generation and execution, and finally an automated log analysis technique. We also illustrate our approach by means of a case study."
1237443,15517,8868,Analysis of performance regression testing data by transaction profiles,2013,"Performance regression testing is an important step in the software development lifecycle, especially for enterprise applications. Commonly the analysis of performance regression testing to find anomalies is carried out manually and therefore can be error-prone, time consuming and sensitive to the input load. In our research, we propose a new technique that overcomes the above problems which helps the performance testing teams to improve their process and speeds up the entire software production process."
94120,15517,9438,Business model ontologies in OLAP cubes,2013,"Business model ontologies capture the complex interdependencies between business objects. The analysis of the hence formalized knowledge eludes traditional OLAP systems which operate on numeric measures. Many real-world facts, however, do not boil down to a single number but are more accurately represented by business model ontologies. In this paper, we adopt business model ontologies for the representation of non-numeric measures in OLAP cubes. We propose modeling guidelines and adapt traditional OLAP operations for ontology-valued measures."
956036,15517,8385,A cost-effectiveness criterion for applying software defect prediction models,2013,"Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a defect prediction model."
2749302,15517,22232,Symbiotic 2: More Precise Slicing (Competition Contribution),2014,"Symbiotic 2 keeps the concept and the structure of the original#N#bug-finding tool Symbiotic, but it uses a more precise slicing#N#based on a field-sensitive pointer analysis instead of#N#field-insensitive analysis of the original tool. The paper#N#discusses this improvement and its consequences. We also#N#briefly recall basic principles of the tool, its strong and#N#weak points, installation, and running instructions. Finally,#N#we comment the results achieved by Symbiotic 2 in the#N#competition."
139157,15517,374,Key exchange in IPsec revisited: formal analysis of IKEv1 and IKEv2,2011,"The IPsec standard aims to provide application-transparent end-to-end security for the Internet Protocol. The security properties of IPsec critically depend on the underlying key exchange protocols, known as IKE (Internet Key Exchange).#R##N##R##N#We provide the most extensive formal analysis so far of the current IKE versions, IKEv1 and IKEv2. We combine recently introduced formal analysis methods for security protocols with massive parallelization, allowing the scope of our analysis to go far beyond previous formal analysis. While we do not find any significant weaknesses on the secrecy of the session keys established by IKE, we find several previously unreported weaknesses on the authentication properties of IKE."
1390534,15517,517,The SmartLogic Tool: Analysing and Testing Smart Card Protocols,2012,"This paper introduces the Smart Logic, which is a flexible smart card research tool that gives complete control over the smart card communication channel for eavesdropping, man-in-the-middle attacks, relaying and card emulation. The hardware is available off-the-shelf at a price of about 100 euros. Furthermore, the necessary firm- and software is open source. The Smart Logic provides essential functionality for smart card protocol research and testing. This is demonstrated by reproducing two attack scenarios. The first attack is on an implementation of the EMV payment protocol where a payment terminal is forced to do a rollback to plaintext PIN instead of using encrypted PIN. The second attack is a relay of a smart card payment over a 20 km distance. We also show that this distance can be increased to at least 10.000 km."
2045830,15517,122,CUDA-NP: realizing nested thread-level parallelism in GPGPU applications,2014,"Parallel programs consist of series of code sections with different thread-level parallelism (TLP). As a result, it is rather common that a thread in a parallel program, such as a GPU kernel in CUDA programs, still contains both se-quential code and parallel loops. In order to leverage such parallel loops, the latest Nvidia Kepler architecture intro-duces dynamic parallelism, which allows a GPU thread to start another GPU kernel, thereby reducing the overhead of launching kernels from a CPU. However, with dynamic parallelism, a parent thread can only communicate with its child threads through global memory and the overhead of launching GPU kernels is non-trivial even within GPUs. In this paper, we first study a set of GPGPU benchmarks that contain parallel loops, and highlight that these bench-marks do not have a very high loop count or high degrees of TLP. Consequently, the benefits of leveraging such par-allel loops using dynamic parallelism are too limited to offset its overhead. We then present our proposed solution to exploit nested parallelism in CUDA, referred to as CUDA-NP. With CUDA-NP, we initially enable a high number of threads when a GPU program starts, and use control flow to activate different numbers of threads for different code sections. We implemented our proposed CUDA-NP framework using a directive-based compiler approach. For a GPU kernel, an application developer only needs to add OpenMP-like pragmas for parallelizable code sections. Then, our CUDA-NP compiler automatically gen-erates the optimized GPU kernels. It supports both the reduction and the scan primitives, explores different ways to distribute parallel loop iterations into threads, and effi-ciently manages on-chip resource. Our experiments show that for a set of GPGPU benchmarks, which have already been optimized and contain nested parallelism, our pro-posed CUDA-NP framework further improves the perfor-mance by up to 6.69 times and 2.18 times on average."
1618998,15517,517,Experimental Evaluation of SDL and One-Op Mutation for C,2014,"Mutation analysis modifies a program by applying syntactic rules, called mutation operators, systematically to create many versions of the program (mutants) that differ in small ways. Testers then design tests to cause the mutants to behave differently from the original program. Mutation testing is widely considered to result in very effective tests, however, it is also quite costly. Cost comes from the many mutants that are created, the number of tests that are needed to kill the mutants, and the difficulty of deciding whether mutants behave equivalently to the original program. One-op mutation theorizes that cost can be reduced by using a single, very powerful, mutation operator that leads to tests that are almost as effective as if all operators are used. Previous research proposed the statement deletion operator (SDL) and found promising results. This paper investigates the use of SDL-mutation in a new context, the language C, and poses additional empirical questions, including whether other operators can be used. We carried out a controlled experiment in which cost and effectiveness of each individual C mutation operator were collected for 39 different subject programs. Experimental data are used to define a cost-effectiveness metric to choose the best single operator for one-op mutation."
1855878,15517,122,Time skewing made simple,2011,"Time skewing and loop tiling has been known for a long time to be a highly beneficial acceleration technique for nested loops especially on bandwidth hungry multi-core processors, but it is little used in practice because efficient implementations utilize complicated code and simple or abstract ones show much smaller gains over naive nested loops. We break this dilemma with an essential time skewing scheme that is both compact and fast."
1701624,15517,517,Automated Multi-platform Testing and Code Coverage Analysis of the CP2K Application,2014,"CP2K is a widely used application for atomistic simulation that can execute on a range of architectures. Consisting of more than one million lines of Fortran 95 code, the application is tested for correctness with a set of about 2,500 inputs using a dedicated regression testing environment. CP2K can be built with many compilers and executed on different serial and parallel platforms, thus making comprehensive testing even more challenging. This paper presents an effort to improve the existing testing process of CP2K in order to better support its continuing development. Enhancements have been made to the regression testing environment to support multi-platform testing and a new automated multi-platform testing system has been developed to check the code on a regular basis. Also, tools have been used to gain code coverage information for different test configurations. All the information is aggregated and displayed on the dedicated web page."
2014530,15517,517,Designing Deletion Mutation Operators,2014,"As a test criterion, mutation analysis is known for yielding very effective tests. It is also known for creating many test requirements, each of which is represented by a mutant that must be killed. In recent years, researchers have found that these test requirements have a lot of duplication, in that many test requirements yield the same tests. Put another way, hundreds of mutants can usually be killed by only a few dozen tests. If we could reduce this duplication without reducing mutation's effectiveness, mutation testing could become more cost-effective. One avenue of this research has been to use only one type of mutant, the statement deletion mutation operator. Researchers have found that statement deletion mutation has relatively few mutants, but yields tests that are almost as effective as using all mutants, with the significant benefit that fewer equivalent mutants are generated. This paper extends this idea by asking a simple question: if deleting statements is a cost-effective way to design tests, will deleting other program elements also be effective? This paper presents results from mutation operators that delete variables, operators, and constants, finding that indeed, this is an efficient and effective approach."
2477084,15517,517,SoftwareHardware Hybrid Systems Verification,2011,"Verification of softwarehardware hybrid systems is difficult because of the inter process communication, concurrency and synchronization and the configuration of processors. Multi-Processor Systems on Chip (MPSoC) are examples of such hybrid systems, targeted for embedded systems. We focus on the verification of MPSoC software (application running on MPSoC) and the system consisting of the application running on the MPSoC platform. We aim at generating test cases for verifying the software while overcoming the limitations of previous approaches. Metaheuristic approaches get blocked in local optima and constraint programming approaches are slow on complex programs, to satisfy given coverage criteria. We propose to combine metaheuristic and constraint programming solvers in a novel way: executing both the solvers sequentially and splitting and reducing the individual input variable domains. We also propose to generate test cases from the functional test objectives to check the proper functioning of the software running on the hardware platform. We report the results of the preliminary experiments."
2233214,15517,517,A Framework for the Automatic Correction of Constraint Programs,2011,"Constraint programs, such as those written in high-level constraint modelling languages, e.g., OPL (Optimization Programming Language), COMET, ZINC or ESSENCE, are more and more used in business-critical programs. As any other critical programs, they require to be thoroughly tested and corrected to prevent catastrophic loss of money. This paper presents a framework for the automatic correction of constraint programs that takes into account the specificity of the software development process of these programs as well as their typical faults. We implemented this framework in our testing platform CPTEST for OPL programs. Using mutation testing, our experimental results show that well-known constraint programs written in OPL can be automatically corrected using our framework."
1045316,15517,339,Automatic generation of vaccines for malware immunization,2012,"Inspired by the biological vaccines, we explore the possibility of developing similar vaccines for malware immunization. We provide the first systematic study towards this direction and present a prototype system, AGAMI, for automatic generation of vaccines for malware immunization. With a novel use of several dynamic malware analysis techniques, we show that it is possible to extract a lightweight vaccine from current malware, and after injecting such vaccine on clean machines, they can be immune from future infection from the same malware family. We evaluate AGAMI on a large set of real-world malware samples and successfully extract working vaccines for many families such as Conficker and Zeus. We believe it is an appealing complementary technique to existing malware defense solutions."
1736297,15517,8385,From pixels to bytes: evolutionary scenario based design with video,2012,"Change and user involvement are two major challenges in agile software projects. As change and user involvement usually arise spontaneously, reaction to change, validation and communication are thereby expected to happen in a continuous way in the project lifecycle. We propose Evolutionary Scenario Based Design, which employs video in fulfilling this goal, and present a new idea that supports video production using SecondLife-like virtual world technology."
1168432,15517,23827,Asymmetric software structures in the Linux kernel,2014,"We investigated the asymmetry in the structure of complex software. After studying the degree distribution of the call graphs corresponding to the Linux kernel modules of 223 different versions, we found the asymmetry between the in-degree and out-degree distributions. After analyzing the behaviors of the newly added nodes in each version, we found that the preferential attachment behaviors of the new nodes are not only related with the degree of nodes, also related with the age of nodes, especially in the out-degree. In addition, the new nodes tend to cluster in Linux kernel."
547311,15517,9438,An Outlook on Patterns as an Aid for Business and IT Alignment with Capabilities,2014,"Patterns have established themselves as a useful and practicable instrument for capturing reusable solutions to reoccurring problems in a multitude of domains. This paper discusses three cases of pattern application – at Riga City Council, Kongsberg Automotive, and Proton Engineering, An outlook on how pattern based approaches should be developed to support business and IT alignment and the concept of capability as means to deliver context dependent organizational solutions is also presented."
47394,15517,8422,A quantifier elimination algorithm for linear modular equations and disequations,2011,We present a layered bit-blasting-free algorithm for existentially quantifying variables from conjunctions of linear modular (bitvector) equations (LMEs) and disequations (LMDs). We then extend our algorithm to work with arbitrary Boolean combinations of LMEs and LMDs using two approaches - one based on decision diagrams and the other based on SMT solving. Our experiments establish conclusively that our technique significantly outperforms alternative techniques for eliminating quantifiers from systems of LMEs and LMDs in practice.
121308,15517,22232,Polyglot: systematic analysis for multiple statechart formalisms,2013,"Polyglot is a tool for the systematic analysis of systems integrated from components built using multiple Statechart formalisms. In Polyglot, Statechart models are translated into a common Java representation with pluggable semantics for different Statechart variants. Polyglot is tightly integrated with the Java Pathfinder verification tool-set, providing analysis and test-case generation capabilities. The tool has been applied in the context of safety-critical software systems whose interacting components were modeled using multiple Statechart formalisms."
1327891,15517,20561,Badges of Friendship: Social Influence and Badge Acquisition on Stack Overflow,2014,"Badges can provide a number of advantages to networked, self-directed learners, including making visible social networks of support and direction. If badges do allow for this, we would expect to see badge acquisition to be predicted by the presence of a particular badge among a learner's social connections. In examining the badges and tags used on the question-and-answer site Stack Overflow. We find that the more general badges are closely related to tenure on the site, while the numerous tag badges provide for more socially-determined differentiation."
1682548,15517,8385,Integrating approaches for feature implementation,2014,"Compositional and annotative approaches are two competing yet complementary candidates for implementing feature-oriented software product lines. While the former provides real modularity, the latter excels concerning expressiveness. To combine the respective advantages of compositional and annotative approaches, we aim at unifying their underlying representations by leveraging the snippet system instead of directories and files. In addition, to exploit this unification, we propose different editable views."
2330227,15517,10973,LICS: Logic in Computer Security -- Some Attacker's Models and Related Decision Problems,2013,"Logic plays an important role in formal aspects of computer security, for instance in access control, security of communications or even intrusion detection. The peculiarity of security problems is the presence of an attacker, whose goal is to break the intended properties of a system/database/protocol... In this tutorial, we will consider several attacker's models and study how to find attacks (or to get security guarantees) on communication protocols in these different models."
210551,15517,23827,Models of Knowledge Transfer for Sustainable Development,2012,Sustainable development requires that distant organisations connect to one another and exchange knowledge. Online social networks have the potential to support these processes. It is however important to understand and model the processes of knowledge transfer that can effectively occur on the Internet. This paper explores recent literature with the purpose to highlight relevant formal approaches that can help to model and analyse the processes of inter-organisational knowledge transfer for sustainable development.
2340336,15517,517,Object-Oriented Mutation Applied in Common Intermediate Language Programs Originated from C#,2011,Application of object-oriented mutation operators in C# programs using a parser-based tool can be precise but requires compilation of mutants. Mutations can be introduced faster directly to the Common Intermediate Language of. NET. It can be simple for traditional mutation operators but more complicated for the object-oriented ones. We propose the reconstruction of complex object-oriented faults on the intermediate language level. The approach was tested in the ILMutator tool implementing few object-oriented mutation operators in the intermediate code derived from compiled C# programs. Exemplary mutation and performance results are given and compared to results of the parser-based mutation tool CREAM.
2139583,15517,517,Test Generation and Evaluation from High-Level Properties for Common Criteria Evaluations -- The TASCCC Testing Tool,2013,"In this paper, we present a model-based testing tool resulting from a research project, named TASCCC. This tool is a complete tool chain dedicated to property-based testing in UML/OCL, that integrates various technologies inside a dedicated Eclipse plug-in. The test properties are expressed in a dedicated language based on property patterns. These properties are then used for two purposes. First, they can be employed to evaluate the relevance of a test suite according to specific coverage criteria. Second, it is possible to generate test scenarios that will illustrate or exercise the property. These test scenarios are then unfolded and animated on the Smartesting's Certify It model animator, that is used to filter out infeasible sequences. This tool has been used in industrial partnership, aiming at providing an assistance for Common Criteria evaluations, especially by providing test generation reports used to show the link between the test cases and the Common Criteria artefacts."
1549520,15517,517,Lazart: A Symbolic Approach for Evaluation the Robustness of Secured Codes against Control Flow Injections,2014,"In the domain of smart cards, secured devices must be protected against high level attack potential [1]. According to norms such as the Common Criteria [2], the vulnerability analysis must cover the current state-of-the-art in term of attacks. Nowadays, a very classical type of attack is fault injection, conducted by means of laser based techniques. We propose a global approach, called Lazart, to evaluate code robustness against fault injections targeting control flow modifications. The originality of Lazart is two folds. First, we encompass the evaluation process as a whole: starting from a fault model, we produce (or establish the absence of) attacks, taking into consideration software countermeasures. Furthermore, according to the near state-of-the-art, our methodology takes into account multiple transient fault injections and their combinatory. The proposed approach is supported by an effective tool suite based on the LLVM format [3] and the KLEE symbolic test generator [4]."
1869085,15517,517,Combinatorial Test Architecture Design Using Viewpoint Diagram,2013,"Software test has recently been a large-scale and complicated artifact, as is the software itself. It is necessary to reduce huge combinatorial test cases. This paper focuses on reduction of test parameters and combinations in test architectural design. First we will mention the test architecture design phase in TDLC: Test Development Life Cycle. Second we will introduce NGT: Notation for Generic Testing, which is a set of concepts or notation for design of software test architecture. This paper shows four examples of test architecture design patterns: Interaction-Viewpoint Conversion pattern, Interaction Cluster Partitioning Pattern, Interaction Demotion Pattern and Interaction Necessity Analysis."
1693041,15517,517,On an Embedded Software Design Architecture for Improving the Testability of In-vehicle Multimedia Software,2014,"In-vehicle multimedia software quality remains critical to assuring vehicle-owner satisfaction and loyalty. Automated software testing has been identified as fundamental to software quality assurance but the success of such automated testing depends largely on the testability of the software under test. Accordingly, this paper presents an industry experience report which discusses a software design architecture that helps to improve software testability. This architecture which is currently being used in our software verification laboratory is simple, easily scalable, efficient, reliable and practical to implement. It enforces close collaboration between the software engineers and automation tool developers (testers) which helps to alleviate limitations of off-the-shelf test automation solutions such as tool compatibility, reusability, maintenance and usefulness."
1343552,15517,517,An Extended LLRP Model for RFID System Test and Diagnosis,2012,"In recent years, radio frequency identification (RFID) systems have been increasingly used in critical domains such as medical field or real-time processing domains. Although important efforts have been made to make this technology more reliable and fault-tolerant, more research is needed to meet the increasing dependability requirements. In this article, we propose a dependability approach consisting in two steps. The first one is an analysis of RFID systems in order to identify potential failures of RFID middleware components and their effects on the whole system. The second one is the modelling of the communication behaviour of RFID systems (that is represented by Low Level Reader Protocol) as a finite state machine. This protocol FSM will be extended to take into consideration the failures identified in the first step and serves as a diagnosis and test tool for the RFID system."
2510621,15517,122,Revisiting loop fusion in the polyhedral framework,2014,"Loop fusion is an important compiler optimization for improving memory hierarchy performance through enabling data reuse. Traditional compilers have approached loop fusion in a manner decoupled from other high-level loop optimizations, missing several interesting solutions. Recently, the polyhedral compiler framework with its ability to compose complex transformations, has proved to be promising in performing loop optimizations for small programs. However, our experiments with large programs using state-of-the-art polyhedral compiler frameworks reveal suboptimal fusion partitions in the transformed code. We trace the reason for this to be lack of an effective cost model to choose a good fusion partitioning among the possible choices, which increase exponentially with the number of program statements. In this paper, we propose a fusion algorithm to choose good fusion partitions with two objective functions - achieving good data reuse and preserving parallelism inherent in the source code. These objectives, although targeted by previous work in traditional compilers, pose new challenges within the polyhedral compiler framework and have thus not been addressed. In our algorithm, we propose several heuristics that work effectively within the polyhedral compiler framework and allow us to achieve the proposed objectives. Experimental results show that our fusion algorithm achieves performance comparable to the existing polyhedral compilers for small kernel programs, and significantly outperforms them for large benchmark programs such as those in the SPEC benchmark suite."
2517357,15517,517,Towards a Language and Framework for Penurious Testing,2011,"Today's testing technologies and tools are all essentially based on the assumption that testers have full, unlimited access to systems under test along with the actual and expected results of test executions. This reflects the fact that, historically, software systems were usually developed by a single organization using components completely under its own control, and were tested in isolation under carefully controlled, offline conditions. However, with the rise of service-oriented architectures and the drive for more software reuse, these conditions are changing and the assumption that testers must be given full, trusted access to systems under test is no longer universally valid. In this paper we make the case for a new form of testing -- penurious testing -- in which testers do not have full, trusted access to the entities they test. By means of small examples we motivate its use and describe a language and framework that supports this new form of testing."
1086549,15517,517,Software-Based Remote Attestation for Safety-Critical Systems,2013,"Assuring system integrity to a remote communication partner through attestation is a security concept which also is very important for safety-critical systems facing security threats. Most remote attestation methods are based on integrity measurement mechanisms embedded in the underlying hardware or software (e.g. operating system). Alternatively, the application software can measure itself, whereas the security of this approach relies on obscurity of the measurement mechanism. There are several tools available to introduce such obscurity through automatic code transformations, but these tools cannot be applied to safety-critical systems, because automatic code transformations are difficult to justify during safety certification. We present a software-based remote attestation concept for safety-critical systems and apply it to an automation system case study. The attestation concept utilizes the safety-related black channel principle to allow the application of code protection tools in order to protect the attestation mechanism without increasing the safety certification effort for the system."
1590388,15517,517,Impediments for Automated Testing -- An Empirical Analysis of a User Support Discussion Board,2014,"To better understand the challenges encountered by users and developers of automatic software testing, we have performed an empirical investigation of a discussion board used for support of a test automation framework having several hundred users. The messages on the discussion board were stratified into problem reports, help requests, development information, and feature requests. The messages in the problem report and help request strata were then sampled and analyzed using thematic analysis, searching for common patterns. Our analysis indicate that a large part of the impediments discussed on the board are related to issues related to the centralized IT environment, and to erroneous behaviour connected to the use of the framework and related components. We also observed a large amount of impediments related to the use of software development tools. Turning to the help requests, we found that the majority of the help requests were about designing test scripts and not about the areas that appear to be most problematic. From our results and previous publications, we see a clear need to simplify the use, installation, and configuration of test systems of this type. The problems attributable to software development tools suggest that testers implementing test automation need more skills in handling those tools, than historically has been assumed. Finally, we propose that further research into the benefits of centralization of tools and IT environments, as well as structured deployment and efficient use of test automation, is performed."
1576754,15517,517,A Method and Tool for Test Optimization for Automotive Controllers,2013,"Completely automatic generation of tests from formal executable test models of industrial size still looks like a “holy grail”, in spite of significant progress in model-based testing research and tool development. Realizing this, we follow a more down-to-earth approach by assuming that, even if a test model is available, the test expert manually derives powerful test fragments and what remains to be automated is chaining them into an optimal test. Focusing on this task, we develop a test optimization framework using an FSM extended with input variables and clocks, which reflects important features of Simulink/Stateflow statecharts. The test optimization is expressed as the Asymmetric Travelling Salesman Problem (ATSP). We show how this approach can be used for solving some testing problems specific to automotive controllers. We describe a proof-of-concept prototype, implementing the proposed approach, which we tested on a case study of a particular controller available along with some tests. Experiments with the prototype indicate that the approach scales well for hundreds of tests."
1780063,15517,517,Generating Test Data to Distinguish Conjunctive Queries with Equalities,2014,"The widespread use of databases in software systems has increased the importance of unit testing the queries that form the interface to these databases. Mutation analysis is a powerful testing technique that has been adapted to test database queries. But each of the existing mutation approaches to testing database queries has one or more of the following shortcomings: inability to recognize equivalent mutants, inability to generate test databases automatically, or inability to mutate all aspects of a query. In this paper we address all three of these challenges by adapting results from the rich literature on query rewriting. We restrict attention to the class of conjunctive queries with equalities. In return for this restriction, we give an algorithm that recognizes equivalent mutants, generates a test database that distinguishes each nonequivalent mutant, and applies to arbitrary mutations, as long at the mutation is also a conjunctive query with equalities. The paper presents the test database generation algorithm and proves that it is sound and complete for conjunctive queries with equalities. We then illustrate the algorithm on a sample query. We evaluate mutations of the query both with the new technique and compare the results to existing mutation techniques for databases."
1261673,15517,517,Isolating First Order Equivalent Mutants via Second Order Mutation,2012,"In this paper, a technique named I-EQM, able to dynamically isolate first order equivalent mutants, is proposed. I-EQM works by employing a novel dynamic execution scheme that integrates both first and second order mutation. The proposed approach combines the impact on the program execution of the first order mutants with the impact on the output of second order ones, to isolate likely to be first order equivalent mutants. Experimental results on a benchmark set of manually classified mutants, selected from real word programs, reveals that I-EQM achieves to classify equivalent mutants with a 71% and 82% classification precision and recall respectively. These results improve the previously proposed approaches by selecting (retrieving) a considerably higher number of killable mutants with only a limited loss on the classification precision."
983966,15517,517,On Modeling and Testing Security Properties of Vehicular Networks,2014,"In this paper a new model to formally represent some units of a vehicular network system is presented. We show how this formalism, based on finite state machines augmented with variables, allows us to represent such kind of system. We focus in a scenario of vehicle to infrastructure (V2I) communication with the Dynamic Route Planning (DRP) service as a case study. We enrich this model by a new negotiation scenario. Next, we present the notion of a test in our framework, and discuss some testing scenarios which compile some security and interoperability properties. To support the theoretical framework we translate the system specification on an IF code which we will use to generate test cases using the TestGen-IF tool. These test cases allow us to perform experiments to verify the security and interoperability properties."
1708144,15517,517,Toward Harnessing High-Level Language Virtual Machines for Further Speeding Up Weak Mutation Testing,2012,"High-level language virtual machines (HLL VMs) are now widely used to implement high-level programming languages. To a certain extent, their widespread adoption is due to the software engineering benefits provided by these managed execution environments, for example, garbage collection (GC) and cross-platform portability. Although HLL VMs are widely used, most research has concentrated on high-end optimizations such as dynamic compilation and advanced GC techniques. Few efforts have focused on introducing features that automate or facilitate certain software engineering activities, including software testing. This paper suggests that HLL VMs provide a reasonable basis for building an integrated software testing environment. As a proof-of-concept, we have augmented a Java virtual machine (JVM) to support weak mutation analysis. Our mutation-aware HLL VM capitalizes on the relationship between a program execution and the underlying managed execution environment, thereby speeding up the execution of the program under test and its associated mutants. To provide some evidence of the performance of our implementation, we conducted an experiment to compare the efficiency of our VM-based implementation with a strong mutation testing tool (muJava). Experimental results show that the VM-based implementation achieves speedups of as much as 89% in some cases."
2108517,15517,122,Cooperative reasoning for preemptive execution,2011,"We propose a cooperative methodology for multithreaded software, where threads use traditional synchronization idioms such as locks, but additionally document each point of potential thread interference with a yield annotation. Under this methodology, code between two successive yield annotations forms a serializable transaction that is amenable to sequential reasoning. This methodology reduces the burden of reasoning about thread interleavings by indicating only those interference points that matter. We present experimental results showing that very few yield annotations are required, typically one or two per thousand lines of code. We also present dynamic analysis algorithms for detecting cooperability violations, where thread interference is not documented by a yield, and for yield annotation inference for legacy software."
1425632,15517,517,Assessing the Influence of Multiple Test Case Selection on Mutation Experiments,2014,"Mutation testing is widely used in experiments. Some papers experiment with mutation directly, while others use it to introduce faults to measure the effectiveness of tests created by other methods. There is some random variation in the mutation score depending on the specific test values used. When generating tests to use in experiments, a common, although not universal practice, is to generate multiple sets of tests to satisfy the same criterion or according to the same procedure, and then to compute their average performance. Averaging over multiple test sets is thought to reduce the variation in the mutation score. This practice is extremely expensive when tests are generated by hand (as is common) and as the number of programs increase (a current positive trend in software engineering experimentation). The research reported in this short paper asks a simple and direct question: do we need to generate multiple sets of test cases? That is, how do different test sets influence the cost and effectiveness results? In a controlled experiment, we generated 10 different test cases to be adequate for the Statement Deletion (SSDL) mutation operator for 39 small programs and functions, and then evaluated how they differ in terms of cost and effectiveness. We found that averaging over multiple programs was effective in reducing the variance in the mutation scores introduced by specific tests."
1162150,15517,517,A 5-Step Hunt for Faults in Java Implementations of Algebraic Specifications,2013,"Executing thorough test suites allows programmers to strengthen the confidence on their software systems. However, given some failed test cases, finding the faults' locations can be a difficult task, thereby techniques that make it easier for the programmer to locate the faulty components are much desirable. In this paper we focus on finding faults in object-oriented, more precisely Java, implementations of data types that are described by algebraic specifications. We capitalize on models for the specification under study and JUnit test suites that cover all axioms of the specification, and present a collection of techniques and underlying methodology, that give the programmer, in a transparent way, a means to locate the fault that causes the implementation to violate the specification. We also present a comparative experiment that was carried out to evaluate this approach where very encouraging results were obtained."
1877779,15517,517,CITLAB: A Laboratory for Combinatorial Interaction Testing,2012,"Although the research community around combinatorial interaction testing has been very active for several years, it has failed to find common solutions on some issues. First of all, there is not a common abstract nor concrete language to express combinatorial problems. Combinatorial testing generator tools are strongly decoupled making difficult their interoperability and the exchange of models and data. In this paper, we propose an abstract and concrete specific language for combinatorial problems. It features and formally defines the concepts of parameters and types, constraints, seeds, and test goals. The language is defined by means of XTEXT, a framework for the definition of domain-specific languages. XTEXT is used to derive a powerful editor integrated with eclipse and with all the expected features of a modern editor. Eclipse is also used to build an extensible framework in which test generators, importers, and exporters can be easily added as plugins."
940512,15517,517,Numerical Constraints for Combinatorial Interaction Testing,2012,"Constraints can be found in many specifications of a software system. The impact of constraints varies with the test problem, but their presence causes problems for many existing combinatorial interaction testing (CIT) tools. Of the numerous existing tools supporting CIT design only a few offer full constraints support. Of these few tools those with full published details are even rarer. In extension to existing Boolean constraints we propose numerical constraints. We discuss definition, usage and handling in this work and integrate results with the classification tree method."
1106314,15517,517,Combinatorial Interaction Testing of a Java Card Static Verifier,2014,"We present a combinatorial interaction testing approach to perform validation testing of a fundamental component for the security of Java Cards: the byte code verifier. Combinatorial testing of all states of the Java Card virtual machine has been adopted as the coverage criteria. We developed a formal model of the Java Card byte code syntax to enable the combinatorial enumeration of well-formed states, and a formal model of the byte code semantic rules to be able to distinguish between well-typed and ill-typed ones, and to derive actual test programs from them. A complete framework has been implemented, enabling fully automated application and evaluation of the conformance tests to any verifier implementation."
2122119,15517,517,Breeding High-Impact Mutations,2011,"Mutation testing was developed to measure the adequacy of a test suite by seeding artificial bugs (mutations) into a program, and checking whether the test suite detects them. An undetected mutation either indicates a insufficiency in the test suite and provides means for improvement, or it is an equivalent mutation that cannot be detected because it does not change the program's semantics. Impact metrics-that quantify the difference between a run of the original and the mutated version of a program-are one way to detectnon-equivalent mutants. In this paper we present a genetic algorithm that aims to produce a set of mutations that have a high impact, are not detected by the test suite, and at the same time are well spread all over the code. We believe that such a set is useful for improving a test suite, as a high impact of a mutation implies it caused a grave damage, which is not detected by the test suite, and that the mutation is likely to be non-equivalent. First results are promising: The number of undetected mutants in a set of evolved mutants increases from 20 to over 70 percent, the average impact of these undetected mutants grows at the same time by a factor of 5."
1430778,15517,517,Empirical Evaluation of the Statement Deletion Mutation Operator,2013,"Mutation analysis is widely considered to be an exceptionally effective criterion for designing tests. It is also widely considered to be expensive in terms of the number of test requirements and in the amount of execution needed to create a good test suite. This paper posits that simply deleting statements, implemented with the statement deletion (SDL) mutation operators in Mothra, is enough to get very good tests. A version of the SDL operator for Java was designed and implemented inside the muJava mutation system. The SDL operator was applied to 40 separate Java classes, tests were designed to kill the non-equivalent SDL mutants, and then run against all mutants."
2510790,15517,122,NDetermin: inferring nondeterministic sequential specifications for parallelism correctness,2012,"Nondeterministic Sequential (NDSeq) specifications have been proposed as a means for separating the testing, debugging, and verifying of a program's parallelism correctness and its sequential functional correctness. In this work, we present a technique that, given a few representative executions of a parallel program, combines dynamic data flow analysis and Minimum-Cost Boolean Satisfiability (MinCostSAT) solving for automatically inferring a likely NDSeq specification for the parallel program. For a number of Java benchmarks, our tool NDetermin infers equivalent or stronger NDSeq specifications than those previously written manually."
924264,15517,517,Comparing Model-Based Testing with Traditional Testing Strategies: An Empirical Study,2014,"Testing is an important activity on the project life-cycle. Test suites are generated from system models and can be explicit or not. This generation can follow a hand-crafted approach or an automated one, using strategies such as model-based testing (MBT). The literature acknowledges MBT advantages, but to the best of our knowledge most successful reported experiences are case studies. There are few contributions on the empirical comparison of model-based strategies with manual ones. Therefore, in this work we designed and conducted an empirical experiment comparing two approaches: i) manual ad hoc tests and ii) MBT using TaRGeT tool. For such evaluation, we defined metrics for the time spent by tests and the quantity of detected defects. Furthermore, we correlated these metrics with factors such as the testers experience and systems' complexity. The experiment counted on 54 experimental units, testing 27 use cases across the two approaches and detected a total of 82 new defects on a real project under development for the Federal Police of Brazil. From the experiment results, we observe that both approaches are statistically equivalent, but each one has particularities finding certain types of defects. Thus, we discuss major advantages and drawbacks of each approach."
1875251,15517,517,Probability-Based Semantic Interpretation of Mutants,2014,"Mutation analysis is a stringent and powerful technique for evaluating the ability of a test suite to find faults. It generates a large number of mutants and applies the test suite to them one at a time. As mutation analysis is computationally expensive, it is usually performed on a subset of mutants. The competent programmer hypothesis suggests that experienced software developers are more likely to make small mistakes. It is prudent therefore to focus on semantically small mutants that represent mistakes developers are likely to make. We previously introduced a technique to assess mutant semantics using static analysis by comparing the numerical range of their symbolic output expressions. This paper extends our previous work by considering the probability the output of a mutant is the same as the original program. We show how probability-based semantic interpretation can be used to select mutants that are semantically more similar than those selected by our previous technique. In addition to numerical outputs, it also provides support for modelling the semantics of Boolean variables, strings and composite objects."
930837,15517,517,MESSI: Mutant Evaluation by Static Semantic Interpretation,2012,"Mutation testing is effective at measuring the adequacy of a test suite, but it can be computationally expensive to apply all the test cases to each mutant. Previous research has investigated the effect of reducing the number of mutants by selecting certain operators, sampling mutants at random, or combining them to form new higher-order mutants. In this paper, we propose a new approach to the mutant reduction problem using static analysis. Symbolic representations are generated for the output along the paths through each mutant and these are compared with the original program. By calculating the range of their output expressions, it is possible to determine the effect of each mutation on the program output. Mutants with little effect on the output are harder to kill. We confirm this using random testing and an established test suite. Competent programmers are likely to only make small mistakes in their programming code. We argue therefore that test suites should be evaluated against those mutants that are harder to kill without being equivalent to the original program."
1435742,15517,517,Model-Based Testing of Cryptographic Components -- Lessons Learned from Experience,2013,"We present an approach to use techniques of model-based testing (MBT) applied on security cryptographic components. This application of MBT is done in the context of a qualification testing phase made by an entity independent from designers, developers and sponsors of the cryptographic components under test. This qualification phase targets both hardware and software cryptographic components and the testing activities cover functional and security testing objectives. In this context, we present the application of MBT for two cryptographic components (one hardware and one software) and show the complementary of test selection criteria based on one side on a structural coverage of the behavioral model used for test generation, and on the other side on a test purpose approach to meet some security test objectives. The test purpose language used in this project is novel and has been designed to complete behavioral model coverage criteria in the MBT process."
1632413,15517,517,A Functional Testing Approach for Hybrid Safety Properties with Incomplete Information,2013,"This paper proposes a functional testing approach for safety properties formalized as hybrid automata. We first propose a formalism inspired from the concept of operational profile to specify test requirements for hybrid automata. We propose an associated parametric adequacy criterion that measures to what extent a given test suite satisfies these requirements. We also develop a set of hypothesis under which the proposed criterion can be evaluated when testing from a black box system when time is discretized and some signals of the automaton are not observable on the concrete system under test. We finally present the HyATT tool prototype that was developed to implement the proposed approach, and report practical feedback of applying it on a case study."
1427466,15517,517,Combinatorial Testing with Order Requirements,2014,"Combinatorial Test Design, CTD, does not easily lend itself to the modeling of ordered parameter-values. Such modeling is much needed in practice, e.g. For the testing of sequences of API calls or parameterized events. We extend the CTD paradigm to address this need. We define a test as an ordered tuple of the parameter-values of the model, and introduce the concepts of ordered restrictions and ordered interaction coverage requirements. We develop an efficient algorithm for generating a small set of tests that satisfy the ordered and unordered interaction coverage requirements and evaluate it on several real-life examples."
2477972,15517,517,Test Sequence Generation from Classification Trees,2012,"The combinatorial test design and combinatorial interaction testing are well studied topics. For the generation of dynamic test sequences from a formal specification of combinatorial problems, there has not been much work yet. The classification tree method implements aspects from the field of combinatorial testing. This paper extends the classification tree with additional information to allow the interpretation of the classification tree as a hierarchical concurrent state machine. Using this state machine, our new approach then uses a Multi-agent System to generate test sequences by finding and rating valid paths through the state machine."
1405405,15517,517,Leveraging Light-Weight Analyses to Aid Software Maintenance,2013,"We will evaluate our techniques and tools on large, real-world systems, comprising tens of millions of lines of code and thousands of defects. The proposed work will attempt to reduce the cost of three specific maintenance tasks: triaging automatically-generated defect reports, automatically synthesizing defect repairs, and automatically identifying out-of-date or incomplete system documentation. We hope to address bottlenecks in each of these three areas and show concrete time and effort savings for each process. The rest of this section describes each maintenance process and our proposed improvements in each case."
972287,15517,517,Technical Debt in Test Automation,2012,"Automated test execution is one of the more popular and available strategies to minimize the cost for software testing, and is also becoming one of the central concepts in modern software development as methods such as test-driven development gain popularity. Published studies on test automation indicate that the maintenance and development of test automation tools commonly encounter problems due to unforeseen issues. To further investigate this, we performed a case study on a telecommunication subsystem to seek factors that contribute to inefficiencies in use, maintenance, and development of the automated testing performed within the scope of responsibility of a software design team. A qualitative evaluation of the findings indicates that the main areas of improvement in this case are in the fields of interaction design and general software design principles, as applied to test execution system development."
1549750,15517,517,Smartesting CertifyIt: Model-Based Testing for Enterprise IT,2013,"This paper presents Smartesting CertifyIt, a general-purpose model-based testing solution dedicated to the IT domain. In the process supported by this model-based testing solution, test generation models are developed using a subset of UML and BPMN notations. The automated test generation phase is led by several kinds of test selection criteria such as requirements coverage or scenario-based criteria. It targets both manually executed and automated test cases generation, and supports the publication of generated tests in a test management tool. In this paper, we describe the model-based testing process with Smartesting Certify-It for Enterprise IT applications based on composing business process and behavioral models."
1110654,15517,517,Coverage-Based Test Cases Selection for XACML Policies,2014,"XACML is the de facto standard for implementing access control policies. Testing the correctness of policies is a critical task. The test of XACML policies involves running requests and checking manually the correct response. It is therefore important to reduce the manual test effort by automatically selecting the most important requests to be tested. This paper introduces the XACML smart coverage selection approach, based on a proposed XACML policy coverage criterion. The approach is evaluated using mutation analysis and is compared on the one side with a not-reduced test suite, on the other with random and greedy optimal test selection approaches. We performed the evaluation on a set of six real world policies. The results show that our selection approach can reach good mutation scores, while significantly reducing the number of tests to be run."
2465163,15517,517,Mutation-Based Test Generation from Security Protocols in HLPSL,2011,"In the recent years, important efforts have been made for offering a dedicated language for modelling and verifying security protocols. Outcome of the European project AVISPA, the High-Level Security Protocol Language (HLPSL) aims at providing a means for verifying usual security properties (such as data secrecy) in message exchanges between agents. Nevertheless, verifying the security protocol model does not guarantee that the actual implementation of the protocol will fulfil these properties. We propose in this paper a testing technique that makes it possible to validate an implementation of a security protocol, based on a HLPSL model. We introduce a set of mutation operators for HLPSL models that aim at introducing leaks in the security protocols. The mutated models are then analysed by the AVISPA tool set that will produce counter-example traces leading to the leaks, thus providing the test cases. We report an experiment of our mutation technique on a wide range of security protocols and discuss the relevance of the proposed mutation operators."
1989947,15517,517,Model-Based Testing for the Second Generation of Integrated Modular Avionics,2011,"In this paper the authors present the current research and development activities regarding automated testing of Integrated Modular Avionics controllers in the European research project SCARLETT. The authors describe the goals of the SCARLETT project and explain its background of Integrated Modular Avionics. Furthermore, they explain different levels of testing of components required for certification. A domain-specific modelling language designed for the IMA platform is presented. This language is used to create models from which tests of different levels can be generated automatically. The authors expect significant improvements in terms of effort to create and maintain test procedures compared to conventional test creation."
1949269,15517,517,Using Algebraic Petri Nets for Testing the Models of Ad Hoc Secure Routing Protocols in Mobility Scenarios,2014,"So far, model checkers cannot be used to verify the properties of complex systems, because of the state space explosion. Such systems are for instance the security protocols designed for ad hoc networks. But by limiting the parameters that are taken into consideration for the systems, the model checkers can be used as testing tools of the constructed models. In this paper we present how AlPiNA, a symbolic model checker based on algebraic Petri nets, can be used to test the model of ARAN secure routing protocol in mobility scenarios."
893357,15517,517,XACMUT: XACML 2.0 Mutants Generator,2013,"Testing of security policies is a critical activity and mutation analysis is an effective approach for measuring the adequacy of a test suite. In this paper, we propose a set of mutation operators addressing specific faults of the XACML 2.0 access control policy and a tool, called XACMUT (XACml MUTation) for creating mutants. The tool generates the set of mutants, provides facilities to run a given test suite on the mutants set and computes the test suite effectiveness in terms of mutation score. The tool includes and enhances the mutation operators of existing security policy mutation approaches."
668182,15517,339,Software decoys for insider threat,2012,"Decoy technology and the use of deception are useful in securing critical computing systems by confounding and confusing adversaries with fake information. Deception leverages uncertainty forcing adversaries to expend considerable effort to differentiate realistic useful information from purposely planted false information. In this paper, we propose software-based decoy system that aims to deceive insiders, to detect the exfiltration of proprietary source code. The proposed system generates believable Java source code that appear to an adversary to be entirely valuable proprietary software. Bogus software is generated iteratively using code obfuscation techniques to transform original software using various transformation methods. Beacons are also injected into bogus software to detect the exfiltration and to make an alert if the decoy software is touched, compiled or executed. Based on similarity measurement, the experimental results demonstrate that the generated bogus software is different from the original software while maintaining similar complexity to confuse an adversary as to which is real and which is not."
1477133,15517,339,Self destructive tamper response for software protection,2011,"A method of creating tamper resistant software that is resistant to unauthorized modification is proposed. It utilizes a primitive that combines self-modifying based instruction camouflage and self integrity verification, and a method to construct a structure in which the multiple primitives are interlocked each other. Tamper resistant software created by the proposed method contains multiple camouflaged instructions in the object program, so that it is difficult for attacker to correctly understand the content of processing using static analysis. When attacker tries to do dynamic analysis, anti-debugging techniques prevent the attempt. The tamper resistant software, at runtime, continuously executes detecting and preventing dynamic analysis, verifying its integrity, and self-modifying itself in such a way that target of self-modifying is dynamically determined according to result of self integrity verification. If unauthorized modification is detected, then it self-modifies a part of instruction which is different from the part of camouflaged instruction to be self-modified, and executes different instructions from its original. As a result, it generates a series of unpredictable abnormal self destructive behaviors such as error or termination, so that attacker's analysis and modification are strongly disturbed. Cost of analysis is increased as the numbers of self integrity verification and instruction camouflage are increased, hence, the tamper resistance can be strengthened quantitatively."
1269496,15517,507,Querying contract databases based on temporal behavior,2011,"Considering a broad definition for service contracts (beyond web services and software, e.g. airline tickets and insurance policies), we tackle the challenges of building a high performance broker in which contracts are both specified and queried through their temporal behavior. The temporal dimension, in conjunction with traditional relational attributes, enables our system to better address difficulties arising from the great deal of information regarding the temporal interaction of the various events cited in contracts (e.g. No refunds are allowed  after  a reschedule of the flight, which can be requested only  before  any flight leg has been used). On the other hand, querying large repositories of temporal specifications poses an interesting indexing challenge. In this paper, we introduce two distinct and complementary indexing techniques that enable our system to scale the evaluation of a novel and theoretically sound notion of  permission  of a temporal query by a service contract. Our notion of permission is inspired by previous work on model checking but, given the specific characteristic of our problem, does not reduce to it. We evaluate experimentally our implementation, showing that it scales well with both the number and the complexity of the contracts."
1859340,15517,517,Verifying Code and Its Optimizations: An Experience Report,2011,"We present our experience in formally verifying the correctness of a 200-line industrial C implementation of Cyclic Redundancy Check (CRC) and its optimizations. Our experience indicates that (a) both the specification and verification of even such small code can be hard to automate and needs intense manual effort, and (b) verifying certain optimizations to an existing code is relatively easier. It took us 10 days to specify and 5 days verify the correctness of this 200 line program. Our initial attempt to verify the correctness, using the model checker CBMC, did not scale up due to a huge loop unwinding (220 times). Since it is hard to scale model checking for such huge loops, we first verified each function's correctness independently using CBMC and then verified the correctness of the loop through manual application of induction using CBMC. This traditional modular and inductive approach helped CBMC verify the implementation within 25 seconds. Furthermore, the implementation was poorly coded to begin with and had to undergo several optimizations to meet its performance requirement. With our optimizations, the CRC implementation met its performance requirement. Due to the changes in code for the optimizations, we needed to reverify the complete implementation. To lessen the verification rework, we proved one optimization at a time, in each function, by proving that the behavior of the function is preserved by the optimization. It took us 7 days to code the optimizations and 1 day to verify them. This optimized correct implementation was accepted and shipped by our client along with the client's automotive software. In future, we would like to generalize this approach to verify behavior preserving changes, and we believe that some of these steps can be automated."
1114124,15517,339,BitShred: feature hashing malware for scalable triage and semantic analysis,2011,"The sheer volume of new malware found each day is growing at an exponential pace. This growth has created a need for automatic malware triage techniques that determine what malware is similar, what malware is unique, and why. In this paper, we present BitShred, a system for large-scale malware similarity analysis and clustering, and for automatically uncovering semantic inter- and intra-family relationships within clusters. The key idea behind BitShred is using feature hashing to dramatically reduce the high-dimensional feature spaces that are common in malware analysis. Feature hashing also allows us to mine correlated features between malware families and samples using co-clustering techniques. Our evaluation shows that BitShred speeds up typical malware triage tasks by up to 2,365x and uses up to 82x less memory on a single CPU, all with comparable accuracy to previous approaches. We also develop a parallelized version of BitShred, and demonstrate scalability within the Hadoop framework."
2111718,15517,517,Empirically Identifying the Best Greedy Algorithm for Covering Array Generation,2013,"Covering array generation is a key issue in combinatorial testing. A number of researchers have been applying greedy algorithms for covering array construction. A greedy framework has been built to integrate most greedy algorithms and evaluate new approaches derived from this framework. However, this framework is affected by multiple factors, which makes its deployment and optimization very challenging. In order to identify the best configuration, we propose a search method that combines pairwise coverage with either base choice or hill climbing techniques. We conduct three different groups of experiments based on six decisions of the greedy framework. The influence of these decisions and their interactions are studied systematically, and the selected greedy algorithm for covering array generation is shown to be better than the existing greedy algorithms."
1739471,15517,339,Context-sensitive auto-sanitization in web templating languages using type qualifiers,2011,"Scripting vulnerabilities, such as cross-site scripting (XSS), plague web applications today. Most research on defense techniques has focused on securing existing legacy applications written in general-purpose languages, such as Java and PHP. However, recent and emerging applications have widely adopted web templating frameworks that have received little attention in research. Web templating frameworks offer an ideal opportunity to ensure safety against scripting attacks by secure construction, but most of today's frameworks fall short of achieving this goal.   We propose a novel and principled type-qualifier based mechanism that can be bolted onto existing web templating frameworks. Our solution permits rich expressiveness in the templating language while achieving backwards compatibility, performance and formal security through a context-sensitive auto-sanitization (CSAS) engine. To demonstrate its practicality, we implement our mechanism in Google Closure Templates, a commercially used open-source templating framework that is used in GMail, Google Docs and other applications. Our approach is fast, precise and retrofits to existing commercially deployed template code without requiring any changes or annotations."
1913148,15517,517,Software Behavior and Failure Clustering: An Empirical Study of Fault Causality,2012,"To cluster executions that exhibit faulty behavior by the faults that cause them, researchers have proposed using internal execution events, such as statement profiles, to (1) measure execution similarities, (2) categorize executions based on those similarity results, and (3) suggest the resulting categories as sets of executions exhibiting uniform fault behavior. However, due to a paucity of evidence correlating profiles and output behavior, researchers employ multiple simplifying assumptions in order to justify such approaches. In this paper we present an empirical study of profile correlation with output behavior, and we reexamine the suitability of such simplifying assumptions. We examine over 4 billion test-case outputs and execution profiles from multiple programs with over 9000 versions. Our data provides evidence that with current techniques many executions should be omitted from the clustering analysis to provide clusters that each represent a single fault. In addition, our data reveals the previously undocumented effects of multiple faults on failures, which has implications for techniques' ability (and inability) to properly cluster. Our results suggest directions for the improvement of future failure-clustering techniques that better account for software-fault behavior."
2115066,15517,339,Scheduling black-box mutational fuzzing,2013,"Black-box mutational fuzzing is a simple yet effective technique to find bugs in software. Given a set of program-seed pairs, we ask how to schedule the fuzzings of these pairs in order to maximize the number of unique bugs found at any point in time. We develop an analytic framework using a mathematical model of black-box mutational fuzzing and use it to evaluate 26 existing and new randomized online scheduling algorithms. Our experiments show that one of our new scheduling algorithms outperforms the multi-armed bandit algorithm in the current version of the CERT Basic Fuzzing Framework (BFF) by finding 1.5x more unique bugs in the same amount of time."
1506018,15517,517,An Integrated Model-Driven Approach for Mechatronic Systems Testing,2012,"Mechatronic systems integrate mechanical, electrical and software subsystems. They are increasingly important in mission-critical infrastructures in different domains including automotive, healthcare, energy and transportation sectors. As such, the adequate testing and validation of such infrastructures are of prime importance. Mechatronic systems testing has been supported by different isolated industrial and research approaches including automated test infrastructures, model-based testing, and test execution engines. While these approaches have individual benefits, they are usually applied in isolation, suffering from different weaknesses. This paper reports on an integrated model-driven approach for mechatronic systems testing, where existing approaches are combined and enhanced. The result is unprecedented levels of automation and testability. We discuss the design and implementation of our approach, illustrating its use with a hybrid calculator example."
1075688,15517,517,Symbolic System Time in Distributed Systems Testing,2012,"We propose an extension of symbolic execution of distributed systems to test software parts related to timing. Currently, the execution model is limited to symbolic input for individual nodes, not capturing the important class of timing errors resulting from varying network conditions. In this paper, we introduce symbolic system time in order to systematically find timing-related bugs in distributed systems. Instead of executing time events at a concrete time, we execute them at a set of times and analyse possible event interleaving son demand. We detail on the resulting problem space, discuss possible algorithmic optimisations, and highlight our future research directions."
1177156,15517,517,Combinatorial Methods for Event Sequence Testing,2012,"Many software testing problems involve sequences of events. This paper applies combinatorial methods to testing problems that have n distinct events, where each event occurs exactly once. The methods described in this paper were motivated by testing needs for systems that may accept multiple communication or sensor connections and generate output to several communication links and other interfaces, where it is important to test the order in which connections occur. Although pair wise event order testing (both A followed by B and B followed by A) has been described, our algorithm ensures that any t events will be tested in every possible t-way order."
868647,15517,339,WAPTEC: whitebox analysis of web applications for parameter tampering exploit construction,2011,"Parameter tampering attacks are dangerous to a web application whose server fails to replicate the validation of user-supplied data that is performed by the client. Malicious users who circumvent the client can capitalize on the missing server validation. In this paper, we describe WAPTEC, a tool that is designed to automatically identify parameter tampering vulnerabilities and generate exploits by construction to demonstrate those vulnerabilities. WAPTEC involves a new approach to whitebox analysis of the server's code. We tested WAPTEC on six open source applications and found previously unknown vulnerabilities in every single one of them."
1185722,15517,339,S3: A Symbolic String Solver for Vulnerability Detection in Web Applications,2014,"Motivated by the vulnerability analysis of web programs which work on string inputs, we present S3, a new symbolic string solver. Our solver employs a new algorithm for a constraint language that is expressive enough for widespread applicability. Specifically, our language covers all the main string operations, such as those in JavaScript. The algorithm first makes use of a symbolic representation so that membership in a set defined by a regular expression can be encoded as string equations. Secondly, there is a constraint-based generation of instances from these symbolic expressions so that the total number of instances can be limited. We evaluate S3 on a well-known set of practical benchmarks, demonstrating both its robustness (more definitive answers) and its efficiency (about 20 times faster) against the state-of-the-art."
2401030,15517,517,Risk-Based Security Testing in Cloud Computing Environments,2011,"Assuring the security of a software system in terms of testing nowadays still is a quite tricky task to conduct. Security requirements are taken as a foundation to derive tests to be executed against a system under test. Yet, these positive requirements by far do not cover all the relevant security aspects to be considered. Hence, especially in the event of security testing, negative requirements, derived from risk analysis, are vital to be incorporated. If considering today's emerging trend in the adoption of cloud computing, security testing even has a more important significance. Due to a cloud's openness, in theory there exists an infinite number of tests. Hence, a concise technique to incorporate the results of risk analysis in security testing is inevitable. We therefore propose a new model-driven methodology for the security testing of cloud environments, ingesting misuse cases, defined by negative requirements derived from risk analysis."
2312709,15517,30,Optimizing Medical Data Quality Based on Multiagent Web Service Framework,2012,"One of the most important issues in e-healthcare information systems is to optimize the medical data quality extracted from distributed and heterogeneous environments, which can extremely improve diagnostic and treatment decision making. This paper proposes a multiagent web service framework based on service-oriented architecture for the optimization of medical data quality in the e-healthcare information system. Based on the design of the multiagent web service framework, an evolutionary algorithm (EA) for the dynamic optimization of the medical data quality is proposed. The framework consists of two main components; first, an EA will be used to dynamically optimize the composition of medical processes into optimal task sequence according to specific quality attributes. Second, a multiagent framework will be proposed to discover, monitor, and report any inconstancy between the optimized task sequence and the actual medical records. To demonstrate the proposed framework, experimental results for a breast cancer case study are provided. Furthermore, to show the unique performance of our algorithm, a comparison with other works in the literature review will be presented."
2492859,15517,517,Generating Checking Sequences for Nondeterministic Finite State Machines,2012,"A checking sequence is a single input sequence which is able to reveal all the faults in a given fault domain. There are many methods for generating checking sequences for deterministic finite state machines (FSM), however, we are not aware of any generalization to no deterministic machines. No deterministic specifications are needed for software testing, as they describe the behavior of a wider class of reactive systems than deterministic FSMs when depending on the environment conditions, a no deterministic system is allowed to take different runs under the same input sequence. In this paper, we propose a method for constructing checking sequences when both the specification and implementations under test are modeled by no deterministic FSMs."
1977367,15517,517,Execution Hijacking: Improving Dynamic Analysis by Flying off Course,2011,"Typically, dynamic-analysis techniques operate on a small subset of all possible program behaviors, which limits their effectiveness and the representativeness of the computed results. To address this issue, a new paradigm is emerging: execution hijacking, consisting of techniques that explore a larger set of program behaviors by forcing executions along specific paths. Although hijacked executions are infeasible for the given inputs, they can still produce feasible behaviors that could be observed under other inputs. In such cases, execution hijacking can improve the effectiveness of dynamic analysis without requiring the (expensive) generation of additional inputs. To evaluate the usefulness of execution hijacking, we defined, implemented, and evaluated several variants of it. Specifically, we performed an empirical study where we assessed whether execution hijacking could improve the effectiveness of a common dynamic analysis: memory error detection. The results of the study show that execution hijacking, if suitably performed, can indeed improve dynamic analysis."
1606332,15517,517,Assessing Quality and Effort of Applying Aspect State Machines for Robustness Testing: A Controlled Experiment,2013,"Aspect-Oriented Modeling (AOM) has been the subject of intense research over the last decade and aims to provide numerous benefits to modeling, such as enhanced modularization, easier evolution, higher quality as well as reduced modeling effort. However, these benefits can only be obtained at the cost of learning and applying new modeling approaches. Studying their applicability is therefore important to assess whether they are worth using in practice. In this paper, we report a controlled experiment to assess the applicability of AOM, focusing on a recently published UML profile (AspectSM). This profile was originally designed to support model-based robustness testing in an industrial context but is applicable to the behavioral modeling of other crosscutting concerns. This experiment assesses the applicability of AspectSM from two aspects: the quality of derived state machines and the effort required to build them. With AspectSM, a crosscutting behavior is modeled using an “aspect state machine”. The applicability of aspect state machines is evaluated by comparing them with standard UML state machines that directly model the entire system behavior, including crosscutting concerns. The quality of both aspect and standard UML state machines derived by subjects is measured by comparing them against predefined reference state machines. Results show that aspect state machines derived with AspectSM are significantly more complete and correct though AspectSM took significantly more time than the standard approach."
2062161,15517,517,A Genetic Algorithm for Computing Class Integration Test Orders for Aspect-Oriented Systems,2012,"In this paper we present an approach for the class integration test order problem in aspect-oriented programs. Several approaches have been proposed for aspect-oriented systems, but the proposed approach is the first, to our best knowledge, to consider the indirect impact of aspects. This approach relies on a genetic algorithm and can reduce the testing efforts when many methods are indirectly impacted by aspects. We detail the algorithm and then discuss its parameters. The approach has been implemented for Aspect J systems, and to validate it, has been applied to a motivating example."
937312,15517,517,Techniques for Automatic Detection of Metamorphic Relations,2014,"Much software lacks test oracles, which limits automated testing. Metamorphic testing is one proposed method for automating the testing process for programs without test oracles. Unfortunately, finding appropriate metamorphic relations for use in metamorphic testing remains a labor intensive task, which is generally performed by a domain expert or a programmer. We are investigating novel approaches for automatically predicting metamorphic relations using machine learning techniques. Preliminary results show that the proposed techniques are highly effective in predicting metamorphic relations."
