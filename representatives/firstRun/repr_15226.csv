ID_Article,communityId,ID_RelatedVenue,title,year,abstract
587031,15226,11052,Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science,2014,"In part one of the Critique of Judgment, Immanuel Kant wrote that \the judgment of taste . . . is not a cognitive judgment, and so not logical, but is aesthetic (1). While the condition of aesthetic discernment has long been the subject of philosophical discourse, the role of the arbiters of that judgment has more often been assumed than questioned. The art historian, critic, connoisseur, and curator have long held the esteemed position of the aesthetic judge, their training, instinct, and eye part of the inimitable subjective processes that Kant described as occurring upon artistic evaluation. Although the concept of intangible knowledge in regard to aesthetic theory has been much explored, little discussion has arisen in response to the development of new types of articial intelligence as a challenge to the seemingly ineable abilities of the human observer. This paper examines the developments in the eld of computer vision analysis of paintings from canonical movements within the history of Western art and the reaction of art historians to the application of this technology in the eld. Through an investigation of the ethical consequences of this innovative technology, the unquestioned authority of the art expert is challenged and the subjective nature of aesthetic judgment is brought to philosophical scrutiny once again."
1529536,15226,9099,Networking of multimedia women event beyond epsilon science: where to look and how to realize new opportunities,2011,"Networking of Multimedia Women event is a continuation of an on-going conversation in the multimedia research community and efforts by the ACM SIGMM to engage and promote female researchers in multimedia community, enable networking of junior and senior female researchers, and give insights towards successful professional careers based on examples.   This year, the event will have a theme, called Beyond Epsilon Science, where preeminent senior female researchers from academia, industry and government, Svetha Venkatesh, Nalini Venkatasubramanian, Dulce Ponceleon, Susanne Boll, and Maria Zemankova will present and discuss how to go beyond epsilon science, where to look for big ideas with high social impact, as well as how to obtain funding to realize these ideas, innovations and opportunities. Their current research projects and funding efforts, and their personal experiences will drive the event's discussions, awareness of major research and funding initiatives, answers to open questions and insights into successful professional careers."
1541821,15226,22279,AVSS2011 demo session: Real-time human detection using fast contour template matching for visual surveillance,2011,"Summary form only given. Anthropomatics addresses the symbiosis between humans and machines, focusing on a deeper understanding of the cooperation, interaction and coexistence between humans and machines stimulating and strengthen advanced and deep research in response to the challenges of increasingly smart environments and multimodal access to various complex technical systems. At KIT the Focus Anthropomatics and Robotics - APR has been set up by a number of research groups focusing on the research field of Anthropomatics and Robotics with more than 250 researchers. Modelling humans and their capabilities requires a deep understanding of the principle of biomechanics and kinematics, as well as the underlaying neural control principles and the perceptive and actuatoric system. Modelling and understanding of the sensomotoric mechanisms, learning and developement of skills and cognititve capabilities to enable humans to interact with the world is of high importance to design technical systems operating closely and interactively with humans via various modalities like speech, haptics, vision, grasping and locomotion. Typical research fields are related to active vision, interpretation of scenes and human activities, recognition and tracking technologies multimodal & perceptual user interfaces, understanding and translation of speech. Complementary research needed is related to the retrieval & access and summarization of multimedia data sources, translation of spoken text, context aware learning computers, implicit services and many more. The robotics application field ranges from interactive industrial robotics, service robotic companions, humanoids and medical robotics. In all domains the integrating aspects are focusing on algorithms processing real word data as well as open self-organizing architectures which allow autonomy, skill an"
2132473,15226,30,Toward Semantic Interoperability of Electronic Health Records,2012,"Although the goal of achieving semantic interoperability of electronic health records (EHRs) is pursued by many researchers, it has not been accomplished yet. In this paper, we present a proposal that smoothes out the way toward the achievement of that goal. In particular, our study focuses on medical diagnoses statements. In summary, the main contributions of our ontology-based proposal are the following: first, it includes a canonical ontology whose EHR-related terms focus on semantic aspects. As a result, their descriptions are independent of languages and technology aspects used in different organizations to represent EHRs. Moreover, those terms are related to their corresponding codes in well-known medical terminologies. Second, it deals with modules that allow obtaining rich ontological representations of EHR information managed by proprietary models of health information systems. The features of one specific module are shown as reference. Third, it considers the necessary mapping axioms between ontological terms enhanced with so-called path mappings. This feature smoothes out structural differences between heterogeneous EHR representations, allowing proper alignment of information."
745865,15226,30,A Resource-Efficient Planning for Pressure Ulcer Prevention,2012,"Pressure ulcer is a critical problem for bed-ridden and wheelchair-bound patients, diabetics, and the elderly. Patients need to be regularly repositioned to prevent excessive pressure on a single area of body, which can lead to ulcers. Pressure ulcers are extremely costly to treat and may lead to several other health problems, including death. The current standard for prevention is to reposition at-risk patients every 2 h. Even if it is done properly, a fixed schedule is not sufficient to prevent all ulcers. Moreover, it may result in nurses being overworked by turning some patients too frequently. In this paper, we present an algorithm for finding a nurse-effort optimal repositioning schedule that prevents pressure ulcer formation for a finite planning horizon. Our proposed algorithm uses data from a commercial pressure mat assembled on the bed's surface and provides a sequence of next positions and the time of repositioning for each patient."
1524016,15226,9896,"Aesthetic capital: what makes london look beautiful, quiet, and happy?",2014,"In the 1960s, Lynch's 'The Image of the City' explored what impression US city neighborhoods left on its inhabitants. The scale of urban perception studies until recently was considerably constrained by the limited number of study participants. We here present a crowdsourcing project that aims to investigate, at scale, which visual aspects of London neighborhoods make them appear beautiful, quiet, and/or happy. We collect votes from over 3.3K individuals and translate them into quantitative measures of urban perception. In so doing, we quantify each neighborhood's aesthetic capital. By then using state-of-the-art image processing techniques, we determine visual cues that may cause a street to be perceived as being beautiful, quiet, or happy. We identify effects of color, texture and visual words. For example, the amount of greenery is the most positively associated visual cue with each of three qualities; by contrast, broad streets, fortress-like buildings, and council houses tend to be associated with the opposite qualities (ugly, noisy, and unhappy)."
1934503,15226,22279,AVSS 2011 demo session: Interactive person-retrieval in a distributed camera network,2011,"Summary form only given. Two fundamental pillars of Software Engineering practice are formalism and structure. Formalism allows engineers to reason rigorously about the system in hand; structure allows them to understand its purposes and behaviours. In the constructive activity of system development structure must therefore take precedence. The central role of formalism is to check and verify — or, where necessary, correct — the products of more informal modes of thought. In this talk these ideas are explored in the context of an illustrative system. The large structure of the system functionality is discussed, together with the nature of the components of that structure. Informal criteria of functional simplicity are presented. The inescapable mismatch between an intelligible functional structure and implementable software architecture is exposed. The role of formalism in these concerns is suggested."
1485102,15226,30,Monitoring Kinematic Changes With Fatigue in Running Using Body-Worn Sensors,2012,"In this paper, we investigate monitoring of kinematic changes evoked by fatigue in running using wearable technology. Movement data were recorded with ETHOS devices. ETHOS is the ETH Orientation Sensor, a customized inertial measurement unit for unconstrained monitoring of human movement. We perform two real-world experiments, in which 21 runners of different skill levels participated. The real-world experiments capture two exhausting 45-min runs: one on a treadmill and one on a conventional outdoor track. We describe and evaluate algorithms to extract kinematic parameters from the sensor data. We identified parameters that change with fatigue for all runners, ones that change for runners of distinct skill levels, and ones that are dependent on an individual's running technique. Overall, we found that observations from treadmill running are not always generalizable to outdoor running. We, thus, argue for using wearable technology to provide athletes and trainers with continuous, quantitative objective measurements of running technique. These could be used to further gain insight into the complex relationship of running kinematics, injury risk, fatigue, and running economy."
2727060,15226,30,Novel wireless-communicating textiles made from multi-material and minimally-invasive fibers.,2014,"The ability to integrate multiple materials into miniaturized fiber structures enables the realization of novel biomedical textile devices with higher-level functionalities and minimally-invasive attributes. In this work, we present novel textile fabrics integrating unobtrusive multi-material fibers that communicate through 2.4 GHz wireless networks with excellent signal quality. The conductor elements of the textiles are embedded within the fibers themselves, providing electrical and chemical shielding against the environment, while preserving the mechanical and cosmetic properties of the garments. These multi-material fibers combine insulating and conducting materials into a well-defined geometry, and represent a cost-effective and minimally-invasive approach to sensor fabrics and bio-sensing textiles connected in real time to mobile communications infrastructures,"
1973329,15226,30,Context-Based Electronic Health Record: Toward Patient Specific Healthcare,2012,"Due to the increasingly data-intensive clinical environment, physicians now have unprecedented access to detailed clinical information from a multitude of sources. However, applying this information to guide medical decisions for a specific patient case remains challenging. One issue is related to presenting information to the practitioner: displaying a large (irrelevant) amount of information often leads to information overload. Next-generation interfaces for the electronic health record (EHR) should not only make patient data easily searchable and accessible, but also synthesize fragments of evidence documented in the entire record to understand the etiology of a disease and its clinical manifestation in individual patients. In this paper, we describe our efforts toward creating a context-based EHR, which employs biomedical ontologies and (graphical) disease models as sources of domain knowledge to identify relevant parts of the record to display. We hypothesize that knowledge (e.g., variables, relationships) from these sources can be used to standardize, annotate, and contextualize information from the patient record, improving access to relevant parts of the record and informing medical decision making. To achieve this goal, we describe a framework that aggregates and extracts findings and attributes from free-text clinical reports, maps findings to concepts in available knowledge sources, and generates a tailored presentation of the record based on the information needs of the user. We have implemented this framework in a system called Adaptive EHR, demonstrating its capabilities to present and synthesize information from neurooncology patients. This paper highlights the challenges and potential applications of leveraging disease models to improve the access, integration, and interpretation of clinical patient data."
1183687,15226,30,Application of Evolutionary Fuzzy Cognitive Maps for Prediction of Pulmonary Infections,2012,"In this paper, a new evolutionary-based fuzzy cognitive map (FCM) methodology is proposed to cope with the forecasting of the patient states in the case of pulmonary infections. The goal of the research was to improve the efficiency of the prediction. This was succeeded with a new data fuzzification procedure for observables and optimization of gain of transformation function using the evolutionary learning for the construction of FCM model. The approach proposed in this paper was validated using real patient data from internal care unit. The results emerged had less prediction errors for the examined data records than those produced by the conventional genetic-based algorithmic approaches."
1767881,15226,30,A Holistic Environment for the Design and Execution of Self-Adaptive Clinical Pathways,2011,"One of the main challenges to be confronted by modern health care, so as to increase treatment quality, is the personalization of treatment. The treatment personalization requires the continuous reconfiguration and adaptation of the selected treatment schemes according to the “current” clinical status of each patient and “current” circumstances inside a health care organization that change rapidly, as well as the updated medical knowledge. In this paper, we present an innovative software environment that provides an integrated IT solution concerning the adaptation of health care processes (clinical pathways) during execution time. The software comprises a health care process execution engine assisted by a semantic infrastructure for reconfiguring the clinical pathways. During the execution of clinical pathways, the system reasons over the rules and reconfigures the next steps of the treatment. A graphical designer interface is implemented for the definition of the rule-set for the clinical pathways adaptation in a user-friendly way."
1935724,15226,30,Noninvasive Biological Sensor System for Detection of Drunk Driving,2011,"Systems capable of monitoring the biological condition of a driver and issuing warnings during instances of drowsiness have recently been studied. Moreover, many researchers have reported that biological signals, such as brain waves, pulsation waves, and heart rate, are different between people who have and have not consumed alcohol. Currently, we are developing a noninvasive system to detect individuals driving under the influence of alcohol by measuring biological signals. We used the frequency time series analysis to attempt to distinguish between normal and intoxicated states of a person as the basis of the sensing system."
1682912,15226,30,Guest Editorial Introduction to the Special Section: 4G Health—The Long-Term Evolution of m-Health,2012,"In the last decade, the seminal term and concept of “m-health” were first defined and introduced in this transactions as “mobile computing, medical sensor, and communications technologies for healthcare.” Since that special section, the m-health concept has become one of the key technological domains that reflected the key advances in remote healthcare and e-health systems. The m-health is currently bringing together major academic research and industry disciplines worldwide to achieve innovative solutions in the areas of healthcare delivery and technology sectors. From the wireless communications perspective, the current decade is expected to bring the introduction of new wireless standards and network systems with true mobile broadband and fast internet access healthcare services. These will be developed around what is currently called the fourth-generation (4G) mobile communication systems. In this editorial paper, we will introduce the new and novel concept of 4G health that represents the long-term evolution of m-health since the introduction of the concept in 2004. The special section also presents a snapshot of the recent advances in these areas and addresses some of the challenges and future implementation issues from the evolved m-health perspective. It will also present some of the concepts that can go beyond the traditional “m-health ecosystem” of the existing systems. The contributions presented in this special section represent some of these developments and illustrate the multidisciplinary nature of this important and emerging healthcare delivery concept."
1193731,15226,208,Healthy: A Diary System Based on Activity Recognition Using Smartphone,2013,"An activity-diary system, named Healthy, is presented in this paper. Healthy can infer users diary of physical activities and energy expenditure based on METS (Metabolic Equivalents) values via recognizing general human activities. In this system, we design a two-layer classifier which costs less energy and memory with satisfactory accuracy. Our classifier divides the activities into two categories: periodic and nonperiodic. And a different sub-classifier is applied for each category. Meanwhile, We design a state listener to recognize more complicated activities. To further improve recognition accuracy, in the second layer sub-classifier, we put forward an adaptive framing algorithm based on the period length of periodical activities to determine the time during which features are extracted. By testing Healthy in real situation, we obtained an average recognition accuracy of 98.0%."
1952249,15226,30,Intuitionistic Fuzzy Cognitive Maps for Medical Decision Making,2011,"Medical decision making can be regarded as a process, combining both analytical cognition and intuition. It involves reasoning within complex causal models of multiple concepts, usually described by uncertain, imprecise, and/or incomplete information. Aiming to model medical decision making, we propose a novel approach based on cognitive maps and intuitionistic fuzzy logic. The new model, called intuitionistic fuzzy cognitive map (iFCM), extends the existing fuzzy cognitive map (FCM) by considering the expert's hesitancy in the determination of the causal relations between the concepts of a domain. Furthermore, a modification in the formulation of the new model makes it even less sensitive than the original model to missing input data. To validate its effectiveness, an iFCM with 34 concepts representing fuzzy, linguistically expressed patient-specific data, symptoms, and multimodal measurements was constructed for pneumonia severity assessment. The results obtained reveal its comparative advantage over the respective FCM model by providing decisions that match better with the ones made by the experts. The generality of the proposed approach suggests its suitability for a variety of medical decision-making tasks."
839156,15226,30,A Service-Oriented Distributed Semantic Mediator: Integrating Multiscale Biomedical Information,2012,"Biomedical research continuously generates large amounts of heterogeneous and multimodal data spread over multiple data sources. These data, if appropriately shared and exploited, could dramatically improve the research practice itself, and ultimately the quality of health care delivered. This paper presents DIstributed Semantic MEDiator (DISMED), an open source semantic mediator that provides a unified view of a federated environment of multiscale biomedical data sources. DISMED is a Web-based software application to query and retrieve information distributed over a set of registered data sources, using semantic technologies. It also offers a user-friendly interface specifically designed to simplify the usage of these technologies by nonexpert users. Although the architecture of the software mediator is generic and domain independent, in the context of this paper, DISMED has been evaluated for managing biomedical environments and facilitating research with respect to the handling of scientific data distributed in multiple heterogeneous data sources. As part of this contribution, a quantitative evaluation framework has been developed. It consist of a benchmarking scenario and the definition of five realistic use-cases. This framework, created entirely with public datasets, has been used to compare the performance of DISMED against other available mediators. It is also available to the scientific community in order to evaluate progress in the domain of semantic mediation, in a systematic and comparable manner. The results show an average improvement in the execution time by DISMED of 55% compared to the second best alternative in four out of the five use-cases of the experimental evaluation."
1078237,15226,30,Enhancement of Initial Equivalency for Protein Structure Alignment Based on Encoded Local Structures,2012,"Most alignment algorithms find an initial equivalent residue pair followed by an iterative optimization process to explore better near-optimal alignments in the surrounding solution space of the initial alignment. It plays a decisive role in determining the alignment quality since a poor initial alignment may make the final alignment trapped in an undesirable local optimum even with an iterative optimization. We proposed a vector-based alignment algorithm with a new initial alignment approach accounting for local structure features called MIRAGE-align. The new idea is to enhance the quality of the initial alignment based on encoded local structural alphabets to identify the protein structure pair whose sequence identity falls in or below twilight zone. The statistical analysis of alignment quality based on match index and computation time demonstrated that MIRAGE-align algorithm outperformed four previously published algorithms, i.e., the residue-based algorithm, the vector-based algorithm, TM-align, and Fr-TM-align. MIRAGE-align yields a better estimate of initial solution to enhance the quality of initial alignment and enable the employment of a noniterative optimization process to achieve a better alignment."
1310837,15226,30,Medically Relevant Criteria used in EEG Compression for Improved Post-Compression Seizure Detection,2014,"Biomedical signals aid in the diagnosis of different disorders and abnormalities. When targeting lossy compression of such signals, the medically relevant information that lies within the data should maintain its accuracy and thus its reliability. In fact, signal models that are inspired by the bio-physical properties of the signals at hand allow for a compression that preserves more naturally the clinically significant features of these signals. In this paper, we illustrate this through the example of EEG signals; more specifically, we analyze three specific lossy EEG compression schemes. These schemes are based on signal models that have different degrees of reliance on signal production and physiological characteristics of EEG. The resilience of these schemes is illustrated through the performance of seizure detection post compression."
2503770,15226,390,Elastic registration of chest CT images with log un-biased deformations and rigidity constraint,2011,This work presents an integrated framework for elastic image registration with log-unbiased deformations and a spatially variable constraint to reduce image folding and preserve the rigidity of bony structures. The framework has been applied to the data provided by the workshop on Evaluation of Methods for Pulmonary Image Registration 2010 (EMPIRE10). We have compared our new method to the classic elastic registration technique and have found that our method has an improved performance.
2279737,15226,30,Limb Movements Classification Using Wearable Wireless Transceivers,2011,"A feasibility study, where small wireless transceivers are used to classify some typical limb movements used in physical therapy processes is presented. Wearable wireless low-cost commercial transceivers operating at 2.4 GHz are supposed to be widely deployed in indoor settings and on people's bodies in tomorrow's pervasive computing environments. The key idea of this work is to exploit their presence by collecting the received signal strength measured between those worn by a person. The measurements are used to classify a set of kinesiotherapy activities. The collected data are classified by using both support vector machine and K-nearest neighbor methods, in order to recognise the different activities."
2493313,15226,390,Understanding dynamics of biological macromolecular complexes by estimating a mechanical model via statistical mechanics from cryo electron microscopy images,2011,"Cryo electron microscopy (cryo EM) imaging experiments can lead to stochastic models for biological macromolecular complexes. However, interpreting the statistical variability is difficult. In some situations, the variability in the original complexes is due primarily to thermal fluctuations which are snap frozen in place during the preparation of the specimen. In this case the images are images of samples of the equilibrium statistical mechanics ensemble of the complex. Based on representing the complex by a spring-mass mechanical model, an estimation problem for determining the masses and spring constants is described and demonstrated on synthetic data. With a model, quantities such as normal modes can be computed, which provide insight into the dynamics of biological complexes."
1907855,15226,65,Detecting occluded people for robotic guidance,2014,"Often overlooked in human-robot interaction is the challenge of people detection. For natural interaction, a robot must detect people without waiting for them to face the camera, get far enough away to be fully present, or center themselves fully within the field of view. Furthermore, it must happen without requiring immense amounts of processing that are not practical for real systems. In this work we focus on person detection in a guidance scenario, where occlusion is particularly prevalent. Using a layered approach with depth images, we can substantially improve detection rates under high levels of occlusion, and enable a robot to detect a target that is moving into and out of the field of view."
1902813,15226,30,Fast Localization and Segmentation of Optic Disk in Retinal Images Using Directional Matched Filtering and Level Sets,2012,"The optic disk (OD) center and margin are typically requisite landmarks in establishing a frame of reference for classifying retinal and optic nerve pathology. Reliable and efficient OD localization and segmentation are important tasks in automatic eye disease screening. This paper presents a new, fast, and fully automatic OD localization and segmentation algorithm developed for retinal disease screening. First, OD location candidates are identified using template matching. The template is designed to adapt to different image resolutions. Then, vessel characteristics (patterns) on the OD are used to determine OD location. Initialized by the detected OD center and estimated OD radius, a fast, hybrid level-set model, which combines region and local gradient information, is applied to the segmentation of the disk boundary. Morphological filtering is used to remove blood vessels and bright regions other than the OD that affect segmentation in the peripapillary region. Optimization of the model parameters and their effect on the model performance are considered. Evaluation was based on 1200 images from the publicly available MESSIDOR database. The OD location methodology succeeded in 1189 out of 1200 images (99% success). The average mean absolute distance between the segmented boundary and the reference standard is 10% of the estimated OD radius for all image sizes. Its efficiency, robustness, and accuracy make the OD localization and segmentation scheme described herein suitable for automatic retinal disease screening in a variety of clinical settings."
2519019,15226,390,On MR experiment design with quadratic regularization,2011,"The design of MRI experiments represents a trade-off between acquisition time, signal-to-noise ratio (SNR), and resolution. For fixed acquisition time and reconstruction resolution, it has been widely believed that the optimal acquisition strategy is to avoid collecting k-space data at frequencies higher than the nominal image resolution. While this belief is true under certain metrics, we observe in this work that a high-resolution acquisition strategy, combined with an appropriate linear filtering/regularization strategy, leads to significantly improved SNR/resolution efficiency for the majority of common resolution metrics. Analysis of this surprising result leads to practical methods for the improved design of imaging experiments and the selection of efficient quadratic regularization penalties."
1295341,15226,30,ReTrust: Attack-Resistant and Lightweight Trust Management for Medical Sensor Networks,2012,"Wireless medical sensor networks (MSNs) enable ubiquitous health monitoring of users during their everyday lives, at health sites, without restricting their freedom. Establishing trust among distributed network entities has been recognized as a powerful tool to improve the security and performance of distributed networks such as mobile ad hoc networks and sensor networks. However, most existing trust systems are not well suited for MSNs due to the unique operational and security requirements of MSNs. Moreover, similar to most security schemes, trust management methods themselves can be vulnerable to attacks. Unfortunately, this issue is often ignored in existing trust systems. In this paper, we identify the security and performance challenges facing a sensor network for wireless medical monitoring and suggest it should follow a two-tier architecture. Based on such an architecture, we develop an attack-resistant and lightweight trust management scheme named ReTrust. This paper also reports the experimental results of the Collection Tree Protocol using our proposed system in a network of TelosB motes, which show that ReTrust not only can efficiently detect malicious/faulty behaviors, but can also significantly improve the network performance in practice."
2450564,15226,30,Personalization and Adaptation to the Medium and Context in a Fall Detection System,2012,"The main objective of this paper is to present a distributed processing architecture that explicitly integrates capabilities for its continuous adaptation to the medium, the context, and the user. This architecture is applied to a falling detection system through: (1) an optimization module that finds the optimal operation parameters for the detection algorithms of the system devices; (2) a distributed processing architecture that provides capabilities for remote firmware update of the smart sensors. The smart sensor also provides an estimation of activities of daily living (ADL), which results very useful in monitoring of the elderly and patients with chronic diseases. The developed experiments have demonstrated the feasibility of the system and specifically, the accuracy of the proposed algorithms and procedures (100% success for impact detection, 100% sensitivity and 95.68% specificity rates for fall detection, and 100% success for ADL level classification). Although the experiments have been developed with a cohort of young volunteers, the personalization and adaption mechanisms of the proposed architecture related to the concepts of design for all and design space will significantly ease the adaptation of the system for its application to the elderly."
2493627,15226,390,Left ventricle motion classification in the medial surface shape space,2013,"The wall thickness is known as a valuable measure for the cardiac diagnosis. From the geometric point of view, it can be considered as a function defined on the 2D manifold of the medial surface. This paper presents a novel classification method based on medial representation to diagnose and detect the myopathic regions on the left ventricle. A shape space is proposed and constructed based on the changes of the left ventricle wall thickness, in which two shape descriptors are introduced which show remarkable performance to distinguish normal and abnormal left ventricle deformations. The experimental results show that this method can automatically classify the healthy and myopathic subjects and detect myopathic regions on the left ventricle well."
2502945,15226,390,Current challenges in image analysis for in toto imaging of zebrafish,2011,"We are developing an approach called in toto imaging whose goal is to track all the cell movements and divisions that give rise to an embryo. We can also capture protein expression and localization throughout development using GFP transgenics. Our long term goal is to integrate these data into a “Digital Fish” that shows how the genetic circuits encoded in the genome turn an egg into an embryo. We have a two pronged approach. We use confocal and 2-photon, time-lapse microscopy to capture very high spatial and temporal resolution movies of developing zebrafish embryos which permit single cell tracking but for only a portion of an embryo. We also use a robotic, 96-well plate based system that can screen 5000 embryos per day but at much lower resolution. Both approaches generate ∼100,000 images per experiment. We are developing software systems for analyzing these large image sets."
945877,15226,65,Visual people detection for safe Human-Robot Interaction,2013,"The ability of any robot to robustly detect and recognize people is a fundamental issue to allow safe Human-Robot Interactions. This ability allows the system to deal with situations that are not determined a priori, while safely performing its tasks. Some research has been carried out on this topic, although most of the proposed approaches restrict the systems autonomy, flexibility and/or performance time. In this paper, we present a visual application for human detection and recognition without limiting the systems capabilities."
2146556,15226,30,EEG Signal Description with Spectral-Envelope-Based Speech Recognition Features for Detection of Neonatal Seizures,2011,"In this paper, features which are usually employed in automatic speech recognition (ASR) are used for the detection of seizures in newborn EEG. In particular, spectral envelope-based features, composed of spectral powers and their spectral derivatives are compared to the established feature set which has been previously developed for EEG analysis. The results indicate that the ASR features which model the spectral derivatives, either full-band or localized in frequency, yielded a performance improvement, in comparison to spectral-power-based features. Indeed it is shown here that they perform reasonably well in comparison with the conventional EEG feature set. The contribution of the ASR features was analyzed here using the support vector machines (SVM) recursive feature elimination technique. It is shown that the spectral derivative features consistently appear among the top-rank features. The study shows that the ASR features should be given a high priority when dealing with the description of the EEG signal."
2525532,15226,390,A non parametric mixed-effect model for population analysis: Application to Alzheimer's disease data,2012,"In population analysis, the images of different groups can be compared to locate the effects of a particular disease or treatment and also to generate biomarkers that help in the diagnosis process. Voxel-Based Morphometry (VBM) is a set of widely extended techniques to compare groups of images. VBM involves image normalization, image smoothing, statistical map generation and correction for hypothesis testing. In this paper, we propose the use of a nonparametric mixed-effect model to study Alzheimer's Disease (AD). The proposed method can handle covariates and through the integration of the smoothing and statistical map generation, individual specificities can be controlled. Moreover, it allows the reconstruction of the typical shapes for each group and it can be advantageously used as another VBM implementation."
1490536,15226,65,Improving existing cascaded face classifier by adding occlusion handling,2012,Recent face detectors used in human robot interaction are boosted cascades. These cascades can detect upright faces but are very sensible to occlusions. We propose a generic framework to handle occlusions at prediction time in a boosted cascade. The contribution is a probabilistic formulation of the cascade structure that considers the uncertainty introduced by missing weak classifiers. This new formulation involves two problems: 1) the approximation of posterior probabilities on each level and 2) the computation of thresholds on these probabilities to make a decision. Both problems are studied and solutions are proposed and evaluated. The method is then applied on the problem of occluded faces detection. Experimental results are provided on classic databases to evaluate the proposed solution related to the basic one.
2521107,15226,390,A primal-dual reconstruction algorithm for fluorescence and bioluminescence tomography,2011,"We introduce a new primal-dual reconstruction algorithm for fluorescence and bioluminescence tomography. As often in optical tomography, image reconstruction is performed by optimizing a multi-term convex cost function. Current reconstruction methods employed in the field are usually limited to cost functions with a smooth data fidelity term; quadratic in general. In addition, the use of a composite regularization term (a sum of multiple terms) requires a substantial adaptation of these methods. Typically one would have to solve a subproblem via a primal-dual method at each iteration. The primal-dual scheme presented here is designed to handle directly cost functions composed of multiple, possibly non-smooth, terms. This allows more freedom for the design of tailored cost functions leading to enhanced reconstructions. We illustrate the method on two cases. First, we use a cost function composed of l 1  fidelity and regularization terms. We compare to the reconstructions obtained with the quadratic fidelity counterpart. Second, we employ a cost function composed of three terms : l 1  for data fidelity, total-variation plus (2,1)-mixed norms for regularization."
348460,15226,9616,Using k-nearest neighbors to handle missing weak classifiers in a boosted cascade,2012,We propose a generic framework to handle missing weak classifiers at prediction time in a boosted cascade. The contribution is a probabilistic formulation of the cascade structure that considers the uncertainty introduced by missing weak classifiers. This new formulation involves two problems: 1) the approximation of posterior probabilities on each level and 2) the computation of thresholds on these probabilities to make a decision. Both problems are studied and solutions are proposed and evaluated. The method is then applied on a popular computer vision application: detecting occluded faces. Experimental results are provided on classic databases to evaluate the proposed solution related to the basic one.
980457,15226,30,Increasing the Signal-to-Noise Ratio by Using Vertically Stacked Phased Array Coils for Low-Field Magnetic Resonance Imaging,2012,"A new method is introduced to increase the signal-to-noise ratio (SNR) in low-field magnetic resonance imaging (MRI) systems by using a vertically stacked phased coil array. It is shown theoretically that the SNR is increased with the square root of the number of coils in the array if the array signals are properly combined to remove the mutual coupling effect. Based on this, a number of vertically stacked phased coil arrays have been designed and characterized by a numerical simulation method. The performance of these arrays confirms the significant increase of SNR by increasing the number of coils in the arrays. This provides a simple and efficient method to improve the SNR for low-field MRI systems."
831044,15226,30,Near-Affine-Invariant Texture Learning for Lung Tissue Analysis Using Isotropic Wavelet Frames,2012,"We propose near-affine-invariant texture descriptors derived from isotropic wavelet frames for the characterization of lung tissue patterns in high-resolution computed tomography (HRCT) imaging. Affine invariance is desirable to enable learning of nondeterministic textures without a priori localizations, orientations, or sizes. When combined with complementary gray-level histograms, the proposed method allows a global classification accuracy of 76.9% with balanced precision among five classes of lung tissue using a leave-one-patient-out cross validation, in accordance with clinical practice."
82582,15226,11052,Linking People in Videos with Their Names Using Coreference Resolution,2014,������� �������������� ������������ ���� ����������� �������������� ��������� �� �������������������� ������������ ���� ����������� �������������� ��������� �� �������� �������� �������� ���������� ������ �!����������������� �� �!���������������� �� �� �� �� �� ���� �������� � ����������� � �� �� �� �� ����������� ) ���������������� ����������� � ���������������� ������������� � ���������������� ���������������� ������ � ������� ������� ������� � ��! ��!������������� �� ��� ����� ������� ��������������� ��� ����! ���������������� ������� #����� ������� ������#� ���������� ����#� ��������� �!���� �������������� ��!����� ����� ����� ������������������ �� $�����#�
2502718,15226,9004,A Compact Active Stereovision System with Dynamic Reconfiguration for Endoscopy or Colonoscopy Applications,2014,��� ��� ��� ��� ��� ��� ��� ��� ���� ������ ���� ������ ���� ������ ���� ������ ������ ������ �������� ������ �������� ������ ��������� ���������� ��������� ���������� ������� ������� ������� ������� ��������� ���������� ��������� ���������� ������������� ������������� ������������ �������� ������������ �������� ����������� ����� ������� ������������ ������������ ��� ��� ��� ����� ����� ����� ���������� ������������ ������������ ������� ��������������� � ������������� ��������������� ������������ ������������ ! ��� ����������� ��� ��� ��� ��� ��� ��� ���� ��� ��� ���� ��� ��� ��������� � ��������� � ���� ������ ���� ��� ���� ������ ���� ��� ���
32792,15226,11052,Untangling Object-View Manifold for Multiview Recognition and Pose Estimation,2014,����� ���� ����� � ���� ��� ���� ����� ���� ����� � ���� ��� ���� ���� ����� � ���� ��� ���� � ������� �� ��������� ���� ���� ���� ������ ������ ���� ���������������� ������ ������������������ ������������� ��������������� �� ��������������� �� �� �������������������� �������������������� ���� † † † †
223770,15226,11187,Towards Context-Dependence Eye Movements Prediction in Smart Meeting Rooms,2014,�� �� �� ������������ �������� ���������� �������������� �������� ������� �������������� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ��������� � ���� ��� ���� ��� ���� ��� ���� ��� ���� ��������� ����������� ������������ ������� ����������� ����������� ��������� ��������� ����������� ������������ ������� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ����� ����� ����� ����� ����� ����� ����� ���� ����� ����� ����� ���� ����� ����� ����� � ���� ��� ���� ��� ���� ��� ���� ��� ��������� ���������� ���������� ���������� ���� �� ��������� ���������� ��������� ���������� ���������� ���� ��
888824,15226,9099,Acoustic and multimodal processing for multimedia content analysis,2011,This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2011.
816521,15226,9099,Privacy concerns in multimedia and their solutions,2012,This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2012.
1514496,15226,9099,Privacy concerns of sharing multimedia in social networks,2013,This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2013.
1452218,15226,9099,Workshop summary for the 5th international workshop on multimedia for cooking and eating activities (CEA'13),2013,This summary introduces the aim of the CEA'13 workshop and the list of papers presented in the workshop.
1098243,15226,30,Guest EditorialMultimedia Services and Technologies for E-Health (MUST-EH),2012,The 11 papers in this special section focus on multimedia services and technologies for E-Health (MUST-EH).
256956,15226,11052,Movement Pattern Histogram for Action Recognition and Retrieval,2014,© Springer International Publishing Switzerland 2014. The original publication is available at www.springerlink.com
248672,15226,11052,Joint Semantic Segmentation and 3D Reconstruction from Monocular Video,2014,© Springer International Publishing Switzerland 2014. The original publication is available at www.springerlink.com
590635,15226,10994,Action-Gons: Action Recognition with a Discriminative Dictionary of Structured Elements with Varying Granularity,2014,"LNCS v. 9007 entitled: Computer Vision -- ACCV 2014: 12th Asian Conference on Computer ..., Part 5"
1869289,15226,22130,Open-world Person Re-Identification by Multi-Label Assignment Inference.,2014,(c) 2014. The copyright of this document resides with its authors.#R##N#It may be distributed unchanged freely in print or electronic forms.
670134,15226,22130,Unsupervised learning of generative topic saliency for person re-identification,2014,(c) 2014. The copyright of this document resides with its authors.#R##N#It may be distributed unchanged freely in print or electronic forms.
1259770,15226,22130,Mining Structure Fragments for Smart Bundle Adjustment,2014,© 2014. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.
1235165,15226,22130,Re-id: Hunting Attributes in the Wild.,2014,© 2014. The copyright of this document resides with its authors.#R##N#It may be distributed unchanged freely in print or electronic forms.
1280371,15226,22130,An Image Based Approach to Recovering the Gravitational Field of Asteroids,2014,© 2014. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.
1079816,15226,9616,The Contribution of Morphological Features in the Classification of Prostate Carcinoma in Digital Pathology Images,2014,"2014 22nd International Conference on Pattern Recognition (ICPR), Stockholm, Sweden, 24-28 August 2014"
815672,15226,22130,An In Depth View of Saliency.,2013,"Presented at the 24th British Machine Vision Conference (BMVC 2013), 9-13 September 2013, Bristol, UK."
122140,15226,21989,Perceived Multimedia Quality: The Impact of Device Characteristics,2011,This is the post-print version of the Article. The official published version can be accessed from the link below - Copyright @ 2011 Springer Verlag
1168393,15226,22130,Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency,2014,"This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF - 1231216."
799633,15226,22035,Welcome message from the QoMEX 2013 chairs,2013,"It is our pleasure to welcome you to QoMEX 2013, the Fifth International Workshop on Quality of Multimedia Experience, in Klagenfurt am Worthersee, Austria."
2203007,15226,22279,Continuous recovery for real time pan tilt zoom localization and mapping,2011,We propose a method for real time recovering from tracking failure in monocular localization and mapping with a Pan Tilt Zoom camera (PTZ).
1844391,15226,9099,Image search 2.0,2011,This abstract sketches my PhD research towards establishing a generic mechanism for exploiting social intelligence for next-generation image search.
1322696,15226,21106,Text detection and recognition in urban scenes,2011,"Text detection and recognition in real images taken in unconstrained environments, such as street view images, remain surprisingly challenging in Computer Vision."
2386040,15226,22130,Face Alignment with Part-Based Modeling,2011,"We propose a new method for face alignment with part-based modeling. This method is competitive in terms of precision with existing methods such as Active Appearance Models, but is more robust and  ..."
2780182,15226,11052,Towards Predicting Good Users for Biometric Recognition Based on Keystroke Dynamics,2014,The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-16181-5_54
2005140,15226,21106,Object detection grammars,2011,"In this talk I will discuss various aspects of object detection using compositional models, focusing on the framework of object detection grammars, discriminative training and efficient computation."
722763,15226,390,Picro-Sirius-HTX stain for blind color decomposition of histopathological prostate tissue,2014,Gleason grading is the most widely used system for determining the severity of prostate cancer. The Gleason grade is determined visually under a microscope from prostate tissue that is most often s ...
1216768,15226,21106,"Visual object classification by robots, using on-line, self-supervised learning",2011,"The challenge addressed in this paper is the classification of visual objects by robots. Visual classification is an active field within Computer Vision, with excellent results achieved recently."
12268,15226,11052,RGBD salient object detection: A benchmark and algorithms,2014,"Conference Name:13th European Conference on Computer Vision, ECCV 2014. Conference Address: Zurich, Switzerland. Time:September 6, 2014 - September 12, 2014."
643591,15226,20358,A proposal for automatic diagnosis of malaria: extended abstract,2013,This paper presents a methodology for automatic diagnosis of malaria using computer vision techniques combined with artificial intelligence. We had obtained an accuracy rate of 74% in the detection system.
2667017,15226,10994,Effective Drusen Segmentation from Fundus Images for Age-Related Macular Degeneration Screening,2014,Full paper can be freely downloaded here: https://www.semanticscholar.org/paper/Effective-Drusen-Segmentation-from-Fundus-Images-Liu-Xu/62e1d400c692b3c92ddacd4e330dce6b3a656fca
2516913,15226,11052,A Low-Level Active Vision Framework for Collaborative Unmanned Aircraft Systems,2014,"Micro unmanned aerial vehicles are becoming increasingly interesting for aiding and collaborating with human agents in myriads of applications, but in particular they are useful for monitoring inac ..."
1230933,15226,22130,Teaching Stereo Perception to YOUR Robot,2012,This paper describes a method for generation of dense stereo ground-truth using a consumer depth sensor such as the Microsoft Kinect. Such ground-truth allows adaptation of stereo algorithms to a s ...
145078,15226,9004,Non-rigid Deformation Pipeline for Compensation of Superficial Brain Shift,2013,"The correct visualization of anatomical structures is a critical component of neurosurgical navigation systems, to guide the surgeon to the areas of interest as well as to avoid brain damage. A maj ..."
1096691,15226,30,Guest Editorial Introduction to the Special Issue on Citizen Centered e-Health Systems in a Global Healthcare Environment: Selected Papers From ITAB 2009,2011,"The 20 papers in this special issue were originally presented in the International Special Topic Conference on Information Technology in Biomedicine, held in October 2009, in Larnaka, Cyprus."
1515464,15226,22130,Biologically Inspired Online Learning of Visual Autonomous Driving,2014,"While autonomously driving systems accumulate more and more sensors as well as highly specialized visual features and engineered solutions, the human visual system provides evidence that visual inp ..."
2551737,15226,9616,An Evaluation of the Faster STORM Method for Super-resolution Microscopy,2014,Development of new stochastic super-resolution methods together with fluorescence microscopy imaging enables visualization of biological processes at increasing spatial and temporal resolution. Qua ...
2268996,15226,390,Geometry-constrained coronary arteries motion estimation from 2D angiograms - Application to injection side recognition,2011,"This paper deals with the 2D motion estimation of coronary arteries. It exploits the geometry of acquisition to strongly constrain the problem, thereby ensuring smooth and robust motion fields."
2417361,15226,10994,Fast Segmentation of Sparse 3D Point Trajectories Using Group Theoretical Invariants,2014,We present a novel approach for segmenting different motions from 3D trajectories. Our approach uses the theory of transformation groups to derive a set of invariants of 3D points located on the sa ...
1608645,15226,9616,Skull Segmentation in MRI by a Support Vector Machine Combining Local and Global Features,2014,Magnetic resonance (MR) images lack information about radiation transport-a fact which is problematic in applications such as radiotherapy planning and attenuation correction in combined PET/MR ima ...
777650,15226,22130,"Simultaneous pose, focal length and 2D-to-3D correspondences from noisy observations",2013,Presentado al 24th BMVC celebrado en Bristol (UK) del 9 al 13 de septiembre 2013.-- The copyright of this document resides with its authors.
1989287,15226,21106,Smooth object retrieval using a bag of boundaries,2011,We describe a scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. The approach is illustrated on sculptures.
1286483,15226,23735,"A fast, low-cost, computer vision approach for tracking surgical tools",2014,"University of Minnesota M.S. thesis. August 2013. Major: Mechanical Engineering. Advisor: Timothy M. Kowalewski. 1 computer file (PDF); ix, 198 pages, appendices A-D."
2137531,15226,390,Functional cardiovascular ultrasound imaging: Quantification of plaque vulnerability and cardiac function,2011,"With ultrasound strain imaging, the function of tissue and organs can be identified. The technique uses multiple images, acquired from tissue under different degrees of deformation. We developed techniques for cardiovascular applications."
1695941,15226,11491,Image and video browsing with a cylindrical 3D storyboard,2011,We demonstrate an interactive 3D storyboard that take advantage of 3D graphics in order to overcome certain limitations of conventional 2D storyboards when used for the task of image and video browsing.
202583,15226,11052,Good Edgels to Track: Beating the Aperture Problem with Epipolar Geometry,2014,"An open issue in multiple view geometry and structure from motion, applied to real life scenarios, is the sparsity of the matched key-points and of the reconstructed point cloud. We present an appr ..."
1766846,15226,9078,"Scalable, self-organizing 3D camera network for non-intrusive people tracking and counting",2014,"In this proposal we present a scalable, self-organizing people tracking system for the ICIP 2014 Show&Tell. We describe the people tracking system and show why it is a good candidate for an interactive live demo."
1609890,15226,390,Utilization of in-depth photon counting detectors towards x-ray spectral imaging: The benefits from the depth information,2014,The in-depth photon counting x-ray detector (PCXD) is a multi-layer detector arrangement which has been introduced to tackle photon count rate limitations of current systems. The capability of reso ...
2594953,15226,10994,3D Interaction Through a Real-Time Gesture Search Engine,2014,"3D gesture recognition and tracking are highly desired features of interaction design in future mobile and smart environments. Specifically, in virtual/augmented reality applications, intuitive int ..."
2347931,15226,11470,Radon-based Audio Classification Features,2012,This paper presents novel features for audio classification based on the Radon transform. These features are evaluated against widely accepted MFCC based features in terms of classification accuracy for a wide range of audio data sets.
842539,15226,9078,A new minimal path selection algorithm for automatic crack detection on pavement images,2014,This paper proposes a new algorithm for crack detection based on the selection of minimal paths. It takes account of both photometric and geometric characteristics and requires few information a priori. It is validated on synthetic and real images.
1608978,15226,21106,3D dynamics analysis in Teichmüller space,2011,"Dynamics analysis for 3D surface sequence is an important task in vision. In this work, 2D shape analysis method [16] based on Teichmuller space theory is generalized to handle deforming surfaces."
1666023,15226,9616,Semantic Urban Maps,2014,A novel region based 3D semantic mapping method is proposed for urban scenes. The proposed Semantic Urban Maps (SUM) method labels the regions of segmented images into a set of geometric and semant ...
735257,15226,11470,Expert Talk for Time Machine Session: High Order Entropy Coding -- From Conventional Video Coding to Distributed Video Coding,2012,High order entropy coding has proved to be critical for improving the coding efficiency of conventional image/video coding. What role will it play in distributed video coding? This talk intends to shed some light on that.
1859444,15226,21106,Tasting families of features for image classification,2011,"Using multiple families of image features is a very efficient strategy to improve performance in object detection or recognition. However, such a strategy induces multiple challenges for machine learning methods, both from a computational and a statistical perspective."
1545673,15226,390,Fast single molecule localization using graphics processing units,2012,We demonstrate maximum likelihood estimations of fluorophore positions in 2D and 3D using algorithms optimized for or accelerated by graphical processing units. The result is a reduction in computation time by two orders of magnitude.
2714939,15226,23922,Minimax Algorithm for Learning Rotations,2011,It is unknown what is the most suitable regularization for rotation matrices and how to maintain uncertainty over rotations in an online setting. We propose to address these questions by studying the minimax algorithm for rotations and begin by working out the 2-dimensional case.
1127504,15226,9078,Rotations in the Mojette space,2013,"In this paper, we develop an exact, reversible and scale change rotation in the Mojette projection space. The whole process is performed using 1D fast operators and has the advantage to be consistent with standard tomographic geometry."
810940,15226,9099,Introduction to the special section of best papers of ACM multimedia 2011,2012,"Reference EPFL-ARTICLE-184335doi:10.1145/2348816.2348817View record in Web of Science Record created on 2013-02-27, modified on 2016-08-09"
2371658,15226,21106,"Stochastic models for semantic parsing, multi-faceted topic discovery, and causal event inference: Perspectives from natural language processing",2011,"In this talk I will address three important problems in Natural Language Processing with direct relevance to Image Understanding: Semantic Parsing, Multi-Faceted Topic Discovery, and Causal Event Inference."
2016612,15226,9616,Recognizing Point Clouds Using Conditional Random Fields,2014,"Trabajo presentado a la 22nd International Conference on Pattern Recognition (ICPR-2014), celebrada en Estocolmo (Suecia) del 24 al 28 de agosto."
2504335,15226,22279,AVSS 2011 demo session: OUTLIER - online learning and visualization of unusual events,2011,"Summary form only given. We introduce to the surveillance community the VIRAT Video Dataset[1], which is a new large-scale surveillance video dataset designed to assess the performance of event recognition algorithms in realistic scenes 1 ."
1889891,15226,390,Complexity of cerebral atrophy evaluation in relapsing-remitting multiple sclerosis context,2011,"In Multiple Sclerosis (MS) research, it is of interest to evaluate accurately the whole brain atrophy. To address this purpose, a lot of methods are available in the literature. However, no gold standards are available to study the effects of a treatment."
2411596,15226,11470,"A web system for ontology-based multimedia annotation, browsing and search",2011,"In this paper we present a complete system for semantic and syntactic annotation, browsing and search of multimedia data, that is based on a service oriented architecture, with web-based interfaces developed following the Rich Internet Application paradigm."
2194813,15226,23735,3D object recognition in range images using visibility context,2011,"Recognizing and localizing queried objects in range images plays an important role for robotic manipulation and navigation. Even though it has been steadily studied, it is still a challenging task for scenes with occlusion and clutter."
1177611,15226,390,Direct mapping of T 2 * signal changes induced by Transcranial Direct Current Stimulation,2013,Transcranial Direct Current Stimulation (tDCS) evokes changes in brain function. In this work we show that direct effects of tDCS on T*2 fMRI signal in the cortex can be mapped using the method of time-series intervention analysis.
1098554,15226,22035,Subjective and objective evaluation of an audiovisual subjective dataset for research and development,2013,"In 2011, the Video Quality Experts Group (VQEG) ran subjects through the same audiovisual subjective test at six different international laboratories. That small dataset is now publically available for research and development purposes."
907492,15226,21106,A real-time multi-cue framework for determining optical flow confidence,2011,"In recent years, many dense optical flow algorithms have been presented. Besides the actual result, the confidence of the result is of uttermost importance for safety-critical applications using flow. We focus on the determination of the confidence of optical flow."
2616948,15226,9004,Structure and context in prostatic gland segmentation and classification,2012,"A novel gland segmentation and classification scheme applied to an HE 79% for normal v. cancer glands, and 77% for discriminating all three classes. The proposed method outperforms state of the art methods in terms of segmentation and classification accuracies and computational efficiency."
810228,15226,22035,"A system for subjective evaluation of audio, video and audiovisual quality using MUSHRA and SAMVIQ methods",2012,"This paper presents a system for the subjective evaluation of audio, video and audiovisual quality. The system combines the well known MUSHRA and SAMVIQ methods for evaluation of audio and video quality. The implementation of the system uses inexpensive commercial of the shelf hardware."
996304,15226,21106,Criteria and metrics for thresholded AU detection,2011,"Implementing a computerized facial expression analysis system for automatic coding requires that a threshold for the system's classifier outputs be selected. However, there are many potential ways to select a threshold. How do different criteria and metrics compare?"
843663,15226,9099,Modeling and representing events in multimedia,2011,"This paper presents an overview of the Joint Workshop on Modeling and Representing Events (JMRE), which is held as part of ACM Multimedia 2011. JMRE is concerned with the understanding of events from multimedia, and with using events in order to better organize and consume multimedia."
1304337,15226,22130,Multi-View Depth Map Estimation With Cross-View Consistency,2014,)# 1.790 1.722 1.626 1.602 1.092 1.179 1.099 Mean Rel. Error of LP+HF+CVF on Pixels of Other Methods ( 10 3)# 1.102 1.068 1.142 1.292 1.319 1.368
2481199,15226,22035,A proposal project for a blind image quality assessment by learning distortions from the full reference image quality assessments,2012,This short paper presents a perspective plan to build a null reference image quality assessment. Its main goal is to deliver both the objective score and the distortion map for a given distorted image without the knowledge of its reference image.
831671,15226,11470,Delta interpolation for upsampling imaging solutions,2014,"An apparatus comprising one or more processors configured to process a first image at a first resolution to generate a first solved image at the first resolution, determine a difference between the first solved image and a function of the first image, and generate a second solved image at a second resolution higher than the first resolution based at least in part on the difference between the first solved image and the function of the first image."
2461655,15226,9616,Position estimation of near point light sources using a clear hollow sphere,2012,"We present a novel method for estimating 3-D positions of near light sources by using highlights on the outside and inside of a single clear hollow sphere. Conventionally, the positions of near light sources have been estimated by using observed highlights on multiple reference objects, e.g. mirror balls."
2290587,15226,390,Noise estimation and removal in MR imaging: The variance-stabilization approach,2011,"We develop optimal forward and inverse variance-stabilizing transformations for the Rice distribution, in order to approach the problem of magnetic resonance (MR) image filtering by means of standard denoising algorithms designed for homoskedastic observations."
634588,15226,9616,Using spatial pyramids with compacted VLAT for image categorization,2012,"In this paper, we propose a compact image signature based on VLAT. Our method integrates spatial information while significantly reducing the size of original VLAT by using two pojection steps. we carry out experiments showing our approach is competitive with state of the art signatures."
1386616,15226,9099,TEEVE endpoint: towards the ease of 3D tele-immersive application development,2013,"We present TEEVE Endpoint, which is a runtime engine to handle the creation, transmission and rendering of 3D Tele-immersive (3DTI) data and provides application programming interfaces (APIs) to developers to easily create 3DTI applications."
1896225,15226,390,Wavelet frames on graphs defined by fMRI functional connectivity,2011,Multiscale representations such as the wavelet transform are useful for many signal processing tasks. Graphs are flexible models to represent complex networks and a spectral graph wavelet transform (SGWT) has recently been developed as a generalization of conventional wavelet designs.
1784533,15226,390,Model-free analysis of time-dependent single-molecule spectroscopy: Dynamics of biological macromolecules,2012,"Rigorous and unbiased statistical analysis of singlemolecule data is now recognized as a crucial component to substantiate any discovery made through such experiments. This presentation summarizes many such tools, and ventures some future directions in the context of the dynamics of biological macromolecules."
1302158,15226,9099,ACM multimedia interactive art program: interaction stations,2011,"The Interaction Stations Exhibit features screen-based, interactive works that align with the new conference themes, and integrate into the physical setting of the conference center. In this paper we describe our motivation for this format, as well as the works selected and larger connections."
1458288,15226,22035,On the impact of speech intelligibility on speech quality in the context of voice over IP telephony,2014,"This contribution presents a study on the relation between speech intelligibility and quality ratings of transmitted speech with packet loss degradations, which was found to be nonlinear: in case of high speech intelligibility, quality can still vary across the whole range; in case of low intelligibility, quality is mainly determined by intelligibility."
668411,15226,9004,Supervised Feature Learning for Curvilinear Structure Segmentation,2013,"Keywords: filter learning ; boosting ; curvilinear structure ; delineation ; segmentation Reference EPFL-CONF-187757View record in Web of Science Record created on 2013-07-31, modified on 2016-08-09"
1406296,15226,9099,Care and scale: Fifteen years of music retrieval,2013,"The co-founder of The Echo Nest, a music intelligence company that now powers recommendation and discovery for most music services, discusses the notion of care and scale, cultural analysis of music, a brief history of music retrieval, and how and why The Echo Nest got started."
1638782,15226,9099,HuEvent'14: 2014 workshop on human-centered event understanding from multimedia,2014,This workshop focuses on the human-centered aspects of understanding events from multimedia content. This includes the notion of objects and their relation to events. The workshop brings together researchers from the different areas in multimedia and beyond that are interested in understanding the concept of events.
1267166,15226,21106,The evolution of stochastic grammars for representation and recognition of activities in videos,2011,"The speaker is one of the privileged many to have been taught syntactic pattern recognition methods by the Late Prof. K.S. Fu. In this talk, I will discuss the evolution of stochastic image grammars from the early seventies to now with a focus on image and video understanding applications."
2193307,15226,22035,Defining aesthetic principles for automatic media gallery layout for visual and audial event summarization based on social networks,2012,"In this paper, we present and define aesthetic principles for the automatic generation of media galleries based on media items retrieved from social networks that—after a ranking and pruning step—can serve to authentically summarize events and their atmosphere from a visual and an audial standpoint."
2528542,15226,390,Regularization design for isotropic spatial resolution in motion-compensated image reconstruction,2011,"Patient motion degrades image quality in medical imaging. Gating can reduce motion artifacts by using part of the acquired data, but can increase noise. Motion-compensated image reconstruction (MCIR) utilizes all collected data with motion information to reduce motion artifacts and noise."
282481,15226,9616,Made to measure top hats,2012,"Floodings, i.e. reconstruction openings, may be tailored such that only selected wells get filled, whereas others remain empty, resulting in particular morphological filters. Their residues constitute top-hats. Razings, i.e. reconstruction openings have the same effect on peaks."
2092314,15226,11470,Media rate control for large scale immersive communications,2011,A video rate control mechanism based on a modified LMS algorithm is presented. The control mechanism is used within a real-time IP multicast based system. Simulation results show that such a control mechanism allows the system to adapt to the varying network conditions of receiving clients.
201725,15226,11052,Hybrid Consensus Learning for Legume Species and Cultivars Classification,2014,"Fil: Larese, Monica Graciela. Consejo Nacional de Investigaciones Cientificas y Tecnicas. Centro Cientifico Tecnologico Rosario. Centro Internacional Franco Argentino de Ciencias de la Informacion y Sistemas; Argentina"
2298766,15226,390,Estimation of uncertainty in respiratory-gated PET images,2011,"Positron Emission Tomography (PET) is a powerful tool for patient diagnosis and assessment of therapy response, which is carried out through comparison of PET measurements from longitudinal scans. However, as with any measurement tool, an estimate of error or uncertainty is required for meaningful comparison of such measurements."
2001307,15226,9099,Scene segmentation of wedding party videos by scenario-based matching with example videos,2011,"We propose a method for scene segmentation of a wedding party video. Recently, it has become popular to take videos of a wedding ceremony and its party. Especially, because of its length, each scene of a wedding party video needs to be indexed with each event for efficient browsing. The proposed method segments a wedding party video into scenes of events by scenario- based matching with example videos that are synthesized by combining scenes from other wedding party videos according to a scenario."
715550,15226,11470,Your heart might give away your emotions,2014,"ESTIMATING EMOTIONAL RESPONSES TO PICTURES BASED ON HEART RATE MEASUREMENTS: Variations in Heart Rate serves as an important clinical health indicator, but potentially also as a window into cognitive reactions to presented stimuli, as a function of both stimuli, context and previous cognitive state."
1970250,15226,9616,Local tangent space based manifold entropy for image retrieval,2012,"This paper proposes a new manifold entropy function based on local tangent space (LTS). With this entropy function, we further propose a framework for image retrieval. The retrieval is treated as searching for ordered cycles by categories in image datasets. The optimal cycles can be found by minimizing our manifold entropy of images."
1120658,15226,9099,Semantic computing in multimedia,2011,"This short overview describes the contents of the tutorial Semantic computing in multimedia, which was offered to the participants of ACM Multimedia 2011.   Given the impossibility of summarizing properly the contents of the tutorial in just two pages, the purpose of this overview is mainly to introduce the reader to the relevant bibliography."
1675024,15226,9099,Sound-Light Giblet,2014,"We describe an audiovisual live coding performance created using  Gibber , a creative coding environment that runs in the browser. The performance takes advantage of novel affordances for rapidly creating music, shaders, and mappings that tie together audio and visual modalities."
733540,15226,9773,Chromatic / Achromatic Separation in Noisy Document Images,2011,This paper presents a new method to split an image into chromatic and achromatic zones. The proposed algorithm is dedicated to document images. It is robust to the color noise introduced by scanners and image compression. It is also parameter-free since it automatically adapts to the image content.
1031602,15226,9099,Interactive photomosaic system using GPU,2012,"A photomosaic is a type of decorative art made up from various other photographs. We present a method for quickly generating photomosaics and propose an interactive recursive photomosaic system. Users can operate the system by using a large display with a touch input function, which allows them to alter the appearance of the image dynamically."
795459,15226,9078,Decoupled coarse-to-fine matching and nonlinear regularization for efficient motion estimation,2012,"A simple motion estimation algorithm, light-weighted both in memory and in time, is presented in this paper. This simplicity is achieved by decoupling the matching and the regularization stages in the estimation process. Experiments show that the obtained results are comparable with state-of-the-art algorithms that are much more computationally demanding."
751109,15226,22021,Authentication over noisy data with the use of memory containing metric functions,2012,"We propose an authentication scheme where the verifier makes the decision based on the value of the metric function, which is assigned depending on the type of the vector stored in the database at the enrollment stage. The scheme has a better performance than the maximum likelihood verification algorithm even for binary symmetric channels."
1825818,15226,20358,I-SEARCH: a multimodal search engine based on rich unified content description (RUCoD),2012,"In this paper, we report on work around the I-SEARCH EU (FP7 ICT STREP) project whose objective is the development of a multimodal search engine. We present the project's objectives, and detail the achieved results, amongst which a Rich Unified Content Description format."
351193,15226,9616,Hash-based structural similarity for semi-supervised Learning on attribute graphs,2012,We present an efficient method to compute similarity between graph nodes by comparing their neighborhood structures rather than proximity. The key is to use a hash for avoiding expensive subgraph comparison. Experiments show that the proposed algorithm performs well in semi-supervised node classification.
816214,15226,11491,Image exploration using online feature extraction and reranking,2012,We present an image meta-search engine that allows content-based exploration of the results obtained from various sources (mostly based on keyword query). The online feature extraction and the particle physics model are the two key features of our demo application that shows very promising results.
1096073,15226,9078,Validation of Mojette reconstruction from Radon acquisitions,2013,Two new methods to perform interpolation mapping from Radon sinogram to Mojette domain are presented. Reconstructions are made from both spaces using FBP and SART algorithms. Assessment of the methods is made both from Shepp-Logan phantom and actual data and demonstrate the efficiency of the proposed algorithms.
410077,15226,20332,Proposal and Initial Study for Animal Crowdsourcing,2014,"We focus on animals as a resource of processing capability in crowdsourcing and propose an Animal Crowdsourcing (we call it Animal Cloud”, too) that resolves problems with cooperation between computers and human or animals. This paper gives an overview of Animal Crowdsourcing and reports on the interim results of our learning experiments using rats (Long-Evans rats) to verify the feasibility of Animal Crowdsourcing."
1387033,15226,11470,DRM-interoperable MPEG-dash end-to-end architecture,2014,"Today a variety of media streaming formats exist, which increases complexity, when distributing media to as many devices as possible. In this paper we present new standards that aim at unifying media delivery over the Internet, driving DRM-interoperability and enabling better Web applications."
1055632,15226,9078,Coupled distributed arithmetic coding,2011,"In this paper, we propose a novel scheme of coupled distributed arithmetic coding to overcome the de-synchronization problem caused by causal decoding in existing distributed arithmetic coding system. Simulation results show that decoding performance is significantly improved and longer sequences outperform shorter sequences using this approach."
1006906,15226,9099,Gesture-based control of physical modeling sound synthesis: a mapping-by-demonstration approach,2013,"We address the issue of mapping between gesture and sound for gesture-based control of physical modeling sound synthesis. We propose an approach called mapping by demonstration, allowing users to design the mapping by performing gestures while listening to sound examples. The system is based on a multimodal model able to learn the relationships between gestures and sounds."
1148614,15226,9099,CAMMA: contextual advertising system for multimodal news aggregations,2013,This demo paper describes a system for contextual advertising on aggregations of multimodal news items. The prototype is intended to demonstrate how modern content analysis techniques can be profitably used to automate tasks commonly performed by humans such as the planning of the computer-assisted advertising content.
734735,15226,9078,Well-composed images and rigid transformations,2013,"We study the conditions under which the topological properties of a 2D well-composed binary image are preserved under arbitrary rigid transformations. This work initiates a more global study of digital image topological properties under such transformations, which is a crucial but under-considered problem in the context of image processing, e.g., for image registration and warping."
2504869,15226,9616,"A new algorithm for labeling connected-components and calculating the Euler number, connected-component number, and hole number",2012,"Labeling connected components and calculating the Euler number, connected-component number, and hole number in a binary image are usually necessary for image analysis, pattern recognition, and computer (robot) vision. This paper presents a new algorithm for calculating the Euler number, connected-component number, and hole number in a binary image by labeling connected components in the binary image. The experimental results demonstrated that our algorithm is more efficient than convention algorithms."
2010630,15226,390,Stochastic optical fluctuation imaging,2012,"The presentation gives an introduction and overview of our recently developed Stochastic Optical Fluctuation Imaging microscopy technique. The theoretical foundations are given and the method is exemplified on several model systems. Its advantages and limitations are discussed, and many applications of SOFI in cell microscopy are presented."
1365168,15226,9099,Internet multimedia advertising: techniques and technologies,2011,"The explosive growth of multimedia data on the Internet creates huge opportunities for multimedia advertising. In this tutorial, we present the techniques and technologies for Internet multimedia advertising. The tutorial aims at bringing together recent insights from the research on multimedia advertising that addresses the theoretical fundamentals, solution concepts, and the issues related to the development of modern multimedia advertising schemes."
1600449,15226,22279,AVSS 2011 demo session: People flow analysis,2011,"Based on a unique video stream analysis and combined with the Sony Smartcamera architecture, Blue Eye Video stand alone solution is able to determine how many persons are waiting in a queue, the customer behaviour when moving in a department store, airports, theatre or stadium."
1611985,15226,11104,A new built-up presence index based on density of corners,2012,A new built-up presence index based on the density of corners is presented. Corners are detected by multi scale Harris detector based on Differential Morphological Decompositions before being aggregated in a density of corners. Experiments are conducted with several high resolution panchromatic images and the density of corners is shown to be highly correlated to the PanTex built-up index [1].
2378266,15226,9616,Learning a selectivity-invariance-selectivity feature extraction architecture for images,2012,"Selectivity and invariance are thought to be important ingredients in biological or artificial visual systems. A fundamental problem is, however, to know what the visual system should be selective to and what to be invariant to. Building a statistical model of images, we learn here a three-layer feature extraction system where the selectivity and invariance emerges from the properties of the images."
2228252,15226,9078,Multiple histogram matching,2013,"Histogram Matching (HM) is a common technique for finding a monotonic map between two histograms. However, HM cannot deal with cases where a single mapping is sought between two sets of histograms. This paper presents a novel technique that finds such a mapping in an optimal manner under various histograms distance measures."
1114510,15226,11491,Active learning of custom sound taxonomies in unstructured audio data,2012,In this paper we describe a system for content-based retrieval of audio clips from a large unstructured database. The system allows users to devise their own sound taxonomies for organizing sounds. An active learning algorithm for Support Vector Machines (SVM) is used for reducing the effort of annotating sounds in the database.
58978,15226,21106,A human vs. machine challenge in fashion color classification,2012,"For this demo, we present a set of stark applications designed to evaluate the performance of a color similarity retrieval system against human operators performance in the same tasks. The proposed series of tests give some interesting insights about the perception of color classes and the reliability of manual annotation in the fashion context."
1069186,15226,9078,A general probability framework for improving similarity based approaches for face verification,2013,"This paper introduces a probability model for face verification, aiming at improve various similarity comparison approaches transplanted directly from face identification algorithms. Experiences demonstrate that, when embedded with a few well known subspace based similarity comparison approaches, our probability model can efficiently reduce the error rates in face verification tasks."
336899,15226,9616,Composite likelihood estimation for restricted Boltzmann machines,2012,"Generally, learning the parameters of graphical models by using the maximum likelihood estimation is difficult and requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation and are higher-order generalizations of the maximum pseudo-likelihood estimation. In this paper, we propose a composite likelihood method and investigate its properties. Furthermore, we apply this to restricted Boltzmann machines."
189562,15226,9004,A Combinatorial Method for 3D Landmark-Based Morphometry: Application to the Study of Coronal Craniosynostosis,2012,"We present a new method to analyze, classify and character- ize 3D landmark-based shapes. It is based on a framework provided by oriented matroid theory, that is on a combinatorial encoding of convex- ity properties. We apply this method to a set of skull shapes presenting various types of coronal craniosynostosis."
1427950,15226,9475,Vision based control of aerial robotic vehicles using the port Hamiltonian framework,2011,This paper investigates the formulation of sensor based control for aerial robotic vehicles based on the port Hamiltonian framework. The paper considers the particular case of vision based control and develops a model for an “infinite dimensional” visual energy port that uses optical flow in the image plane as a ‘velocity’ in the port Hamiltonian formalism.
736099,15226,9616,Speeding up optimum-path forest training by path-cost propagation,2012,"In this paper we present an optimization of the Optimum-Path Forest classifier training procedure, which is based on a theoretical relationship between minimum spanning forest and optimum-path forest for a specific path-cost function. Experiments on public datasets have shown that the proposed approach can obtain similar accuracy to the traditional one but with faster data training."
1512705,15226,9078,Energetic lattice for optimizing over hierarchies of partitions,2014,"This theoretical paper introduces a novel continuous representation of hierarchy of partitions, and generalizes the conditions of h-increasingness and scale increasingness [1] to obtain a global-local optimum on the hierarchy. It studies in particular the Lagrange optimization problem and gives the condition on the energy to achieve constrained optimization, in the lattice of cuts in the hierarchy."
405904,15226,9616,Generalized ordinary moment based blur invariant descriptors for face recognition with degraded images,2012,"In this paper, we introduce an alternative definition for ordinary moments and use them to redefine moment based blur invariants. With this change, we are able to increase the discriminative power significantly, as well as robustness to blur. The superiority of our proposed method is illustrated in comparison to another similar descriptor in an experiment on the FRGC database."
670120,15226,20411,Graph-cut based tag enrichment,2011,"In this paper, a graph cut based tag enrichment approach is proposed. We build a graph for each image with its initial tags. The graph is with two terminals. Nodes of the graph are full connected with each other. Min-cut/max-flow algorithm is utilized to find the relevant tags for the image. Experiments on Flickr dataset demonstrate the effectiveness of the proposed graph-cut based tag enrichment approach."
1824006,15226,11491,RetrievalLab: a programming tool for content based retrieval,2011,"In this paper we present RetrievalLab, a content based retrieval tool that was designed for both educational and research purposes. It is a tool to facilitate the testing of new features, segmentations, machine learning approaches, and evaluation methods, by presenting a Matlab-like programming interface which illuminates the fundamental processes and algorithms in content based retrieval."
133520,15226,21106,A tai chi training system based on fast skeleton matching algorithm,2012,"In this paper, we introduce a Tai Chi training system based on Microsoft's Kinect, which automatically evaluates a user's performance and provides real-time feedback for the user to refine his current posture. A novel method to measure posture is also described. The experimental results are promising, demonstrating the effectiveness of our approach."
1262665,15226,9099,Multimedia systems research: The first twenty years and lessons for the next twenty,2013,This retrospective article examines the past two decades of multimedia systems research through the lens of three research topics that were in vogue in the early days of the field and offers perspectives on the evolution of these research topics. We discuss the eventual impact of each line of research and offer lessons for future research in the field.
1000197,15226,9099,Optically coupled oscillators (OCOs) -- LED fireflies,2012,"We design oscillators controlled by optical input, which we call LED fireflies, and study their synchronization phenomena. The most distinctive feature of LED fireflies is their collective behavior. The LED fireflies produce a huge variety of synchronous patterns. We demonstrate an optical art work that behaves like living entities by making use of their property."
931948,15226,23735,Confidence fusion based emotion recognition of multiple persons for human-robot interaction,2012,"Emotional interaction with human beings is desirable for robots. In this study, we propose an integrated system which has ability to track multiple people at the same time, to recognize their facial expressions, and to identify social atmosphere. Consequently, robots can easily recognize facial expression, emotion variations of different people, and can respond properly."
1847962,15226,11470,Remote lab in virtual world for remote control of industrial processes,2011,This paper addresses ways users can remotely control industrial hardware devices over the Internet. We employ a J2EE-based Remote Lab platform to describe the interface of the remote devices and to relay commands and results between users and devices. Collaboration is accomplished by embedding the remote lab platform in an Open Wonderland virtual world.
1797427,15226,11470,"Technical overview of VP8, an open source video codec for the web",2011,"VP8 is an open source video compression format supported by a consortium of technology companies. This paper provides a technical overview of the format, with an emphasis on its unique features. The paper also discusses how these features benefit VP8 in achieving high compression efficiency and low decoding complexity at the same time."
437875,15226,9616,A fast wavelet-packet-based algorithm for texture synthesis,2012,"We propose a fast texture synthesis algorithm based on wavelet packet transform. It decomposes the input image into wavelet packet coefficients, then a 2-step matching, specifically coarse matching based on low frequency wavelet packet coefficients followed by fine matching based on high frequency wavelet packet coefficients, is used for the texture synthesis task. Experimental results show that the proposed algorithm is preferable in terms of computation time."
1251426,15226,11491,A Visualization Tool for Violent Scenes Detection,2014,We present a browser-based visualization tool that allows users to explore movies and online videos based on the violence level of these videos. The system offers visualizations of annotations and results of the MediaEval 2012 Affect Task and can interactively download and analyze content from video hosting sites like YouTube.
2430210,15226,11470,Context-aware prioritized game streaming,2011,"In this paper, we describe our proposed mechanism for a context-aware progressive 3D streaming for virtual environments (VEs) such as games running on mobile handheld devices. Our goal is to optimize the efficiency of streaming update messages in such environments by providing a new, dynamic, and context-aware method for selecting and prioritizing objects."
557915,15226,20332,Aesthetic considerations for automated platformer design,2012,"We describe ANGELINA3, a system that can automatically develop games along a defined theme, by selecting appropriate multimedia content from a variety of sources and incorporating it into a game's design. We discuss these capabilities in the context of the FACE model for assessing progress in the building of creative systems, and discuss how ANGELINA3 can be improved through further work."
2244076,15226,11470,A robust pipeline for logo detection,2011,"We present a method for detecting appearances of logos in low-resolution video sequences. The method is based on matching of SIFT descriptors, plus several heuristics. The logos must come from a small database of possible logos. The emphasis is not on speed but on reliability, although the method can be executed in real time using a parallel computer."
1446005,15226,11470,A Multi-User Interaction System Based on Kinect and Wii Remote,2012,"We will demonstrate a multi-user interaction system that uses Kinect and Wii Remote for manipulating windows in both desktop and wall-sized environment. This system combines the gesture information collected by Kinect and other sensor information such as acceleration from Wii Remote, therefore providing a more accurate control and a more nature experience for users."
900590,15226,11470,Efficient compression of rhythmic motion using spatial segmentation and temporal blending,2013,We investigate the effectiveness of combining spatial segmentation and temporal blending to improve compression ratio and rendered quality on motion capture data of rhythmic nature. Experimental results demonstrate the feasibility of our method. We also observe that visual quality can be further improved by adopting an automatic weight propagation strategy.
1514719,15226,9078,Accuracy improvement of histogram-based image filtering,2013,"We propose a method for improving the accuracy of histogram-based image filtering. With this method, we define a histogram called intensity-stacked histogram. An image histogram generally consists of a frequency (number of pixels) for each bin. On the other hand, intensity-stacked histogram stores the sum of intensity values for each bin. The intensity-stacked histogram can be calculated in constant time similar to a standard histogram. We apply the intensity-stacked histogram to histogram-based image algorithms for median and bilateral filters. The histogram-based image filter with the intensity-stacked histogram works effectively when using a few bins. We confirmed that the accuracy of histogram-based image filters using intensity-stacked histogram is higher than that using standard histogram."
94649,15226,21106,Application of backward stochastic differential equations to reconstruction of vector-valued images,2012,In this paper we explore the problem of reconstruction of vector-valued images with additive Gaussian noise. In order to solve this problem we use backward stochastic differential equations. Our numerical experiments show that the new approach gives very good results and compares favourably with deterministic partial differential equation methods.
1872766,15226,11104,Snow particle automatic classification with texture operators,2011,"The backscattered data recorded by the meteorological radar is exploited for rainfall/snowfall rate calculation according to the Z-R relation. This relation is governed by parameters, which are influenced by the size and shape of the falling particles. The variety of snowflake types as well as the in class shape and size differences make this problem very difficult."
1807023,15226,390,Towards active segmentation of cell images,2011,This paper presents ongoing work towards creating a framework for the active segmentation and classification of cell assay images. In this paper we focus on the learning of a probabilistic boundary model followed by an extended segmentation method. The abilities are demonstrated on a variety of cell images. We conclude by outlining approaches for the active segmentation of cell images.
1167165,15226,9616,Pruning the 3D Curve Skeleton,2014,"A new pruning algorithm is introduced to remove peripheral branches of scarce relevance from the curve skeleton of 3D objects, while keeping the more relevant branches untouched. The algorithm does not require fine tuning of the parameters involved in the criteria suggested to evaluate branch significance and, in the average, produces satisfactory results."
1175424,15226,11470,Brief introduction into information systems & management research in media industries,2013,"Within the scope of this paper, a new research field - information systems and management in media industries is introduced. The paper illustrates the importance of viewing the media firm as holistic digital firm from the information systems and management perspective. It reviews current literature, research work, and provides an overview of opened research gaps."
1147978,15226,23735,Selecting good measurements via ℓ 1 relaxation: A convex approach for robust estimation over graphs,2014,"© 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works."
896614,15226,23735,Direct Superpixel Labeling for Mobile Robot Navigation Using Learned General Optical Flow Templates,2014,"© 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works."
1694721,15226,390,Automated surgical OSATS prediction from videos,2014,"© 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works."
1584409,15226,20411,Location disambiguation for geo-tagged images,2011,"In this poster, we address the problem of location disambiguation for geotagged Web photo resources. We propose an approach for analyzing and partitioning large geotagged photo collections using geographic and semantic information. By organizing the dataset in a structural scheme, we resolve the location ambiguity and clutter problem yield by massive volume of geotagged photos."
739558,15226,9078,Morphological floodings and optimal cuts in hierarchies,2014,"The non-horizontal cuts of a hierarchy and the floodings of an image are well-established tools for image segmenting and filtering respectively. We present definitions of non-horizontal cuts and of floodings in the same framework of hierarchies of partitions. We show that, given a hierarchy, there is a one-to-one correspondence between the non-horizontal cuts and the floodings. This opens the door to optimal image filtering based on non-horizontal cuts and, conversely, to nonhorizontal cuts obtained by morphological floodings, or more generally by connected filterings."
2659283,15226,9099,Empathetic heartbeat,2012,"Empathy is important for our society and individuals, since it is what facilitates our interactions and connections to the other people around us. Our experience-based installation empathetic heartbeat aims to have the participant remind the existence of his/her heart in the internal body and recognize that our bodies are medium for feeling empathy with others. Here we describe the concept of the installation and participant's experience."
1884026,15226,390,"Chronic liver disease staging classification based on ultrasound, clinical and laboratorial data",2011,"In this work the identification and diagnosis of various stages of chronic liver disease is addressed. The classification results of a support vector machine, a decision tree and a k-nearest neighbor classifier are compared. Ultrasound image intensity and textural features are jointly used with clinical and laboratorial data in the staging process."
999397,15226,21106,BiCoS: A Bi-level co-segmentation method for image classification,2011,"The objective of this paper is the unsupervised segmentation of image training sets into foreground and background in order to improve image classification performance. To this end we introduce a new scalable, alternation-based algorithm for co-segmentation, BiCoS, which is simpler than many of its predecessors, and yet has superior performance on standard benchmark image datasets."
687230,15226,22035,Perceptual performance analysis for visual attention model based asymmetric stereoscopic 3D video coding,2014,Asymmetric quality stereoscopic coding has proved to be an effective method in reducing the bandwidth without altering the visual quality. Visual attention cues can also be used in asymmetric coding that has not been widely studied yet. This study reports the performance limits of visual attention aided asymmetric stereoscopic video coding over conventional asymmetric coding.
2431120,15226,9099,OTMedia: the French TransMedia news observatory,2013,"Who said What, Where and How? How are images, video and stories spreading out? Who produces the information? OTMedia addresses these questions by collecting, enriching and analysing continuously more than 1500 streams of French media from TV Radio, Web, AFP, and Twitter. Two studies on media produced by end users with the OTMedia framework are presented."
52169,15226,21106,Syntactic algorithm of two-dimensional scene analysis for unmanned flying vehicles,2012,In this paper the approach to on-line object recognition for autonomous flying agent is considered. The method is divided into two parts. First the algorithm for scene objects vectorization is introduced. As the second step of the overall method we present the rotation and scale invariant algorithm for vectorized object identification based on syntactic language.
332224,15226,9616,Joint multi-frame super-resolution and matting,2012,Matting and super-resolution of frames from an image sequence have been studied independently in the literature. We propose a unified formulation to solve both inverse problems by assimilating matting within the super-resolution model. We adopt a multi-frame approach which uses data from adjacent frames to increase the resolution of the matte as well as foreground.
2330696,15226,9616,Stable discriminative dictionary learning via discriminative deviation,2012,Discriminative learning of sparse-code based dictionaries tends to be inherently unstable. We show that using a discriminative version of the deviation function to learn such dictionaries leads to a more stable formulation that can handle the reconstruction/discrimination trade-off in a principled manner. Results on Graz02 and UCF Sports datasets validate the proposed formulation.
779979,15226,9099,The Yael Library,2014,"This paper introduces Yael, a library implementing computationally intensive functions used in large scale image retrieval, such as neighbor search, clustering and inverted files. The library offers interfaces for C, Python and Matlab. Along with a brief tutorial, we analyze and discuss some of our implementation choices, and their impact on efficiency."
1432260,15226,9078,Skeleton-based human segmentation in still images,2012,"In this paper we propose a skeleton-based model for human segmentation in static images. Our approach explores edge information, orientation coherence and anthropometric-estimated parameters to generate a graph, and the desired contour is a path with maximal cost. Experimental results show that the proposed technique works well in non trivial images."
1200397,15226,11470,"Expert Talk for Time Machine Session: Affective Multimedia Analysis: Introduction, Background and Perspectives",2012,"The term affective computing was coined by Rosalind Picard in 1995. She presented her ideas about how to use affect for interaction with and analysis of multimedia. Her ideas were inspiring to the studies and applications on affective multimedia analysis in the last decade. In this talk, the initial ideas and their development to the current state as well as challenges and perspectives are presented."
274459,15226,9616,Binary invariant cross color descriptor using galaxy sampling,2012,"In this paper, we propose a new descriptor which is computed by comparing invariant cross color channels of pairs of points in the local patch. To efficiently obtain the sampled pairs of points, a galaxy sampling pattern is proposed. As shown in the experiments, our descriptor using invariant cross color channels and the galaxy sampling can achieve the best performance in most cases with slight computation time increasing."
847383,15226,9078,End-to-end distortion estimation for H.264 with unconstrained intra prediction,2012,This paper presents a macroblock-level algorithm to estimate the end-to-end distortion of H.264/AVC-based video transmission that allows unrestricted intra prediction. Some fast approximations are developed to reduce its complexity so that it can be used in real-time applications. Experimental results demonstrate the performance of the algorithm and the impact of the unconstrained intra prediction.
708443,15226,9078,Scalable compressive video,2011,The paper presents a scalable compressive sampling (CS) scheme for video acquisition. The proposed solution enables progressive reconstruction of video frames with novel measurement matrices. Simulation results show significant performance improvements over the traditional CS technique for the base layer and slightly better performance for the final enhancement layer.
1921493,15226,22279,Resource-aware sensor selection and task assignment,2011,"Multimedia sensor networks [1] and visual sensor networks (VSN) [3] have been increasingly studied in recent years. However, the aspect of resource-awareness has just recently moved into the focus of research interests. Especially energy-aware systems that may be deployed in areas without fixed infrastructure have only recently achieved attention."
2019962,15226,22279,Extended feature-based object tracking in presence of data association uncertainty,2011,"This paper proposes and algorithm for extended object tracking using sparse feature points. The described technique is based on the Rao-Blackwellized Particle Filter. In particular, two different data association techniques that take into consideration clutter and missed detections, are coupled and tested in order to provide a comparison of their performance for the problem of extended object tracking."
834361,15226,21106,Learning and the language of thought,2011,Logic and probability are key themes of cognitive science that have long had an uneasy coexistence. I will describe the Probabilistic Language of Thought approach that brings them together into compositional representations with probabilistic meaning - formalized as stochastic lambda calculus. I will describe how this general framework is realized in the probabilistic programming language Church.
1556232,15226,9099,"Personalized access to cultural heritage: multimedia by the crowd, for the crowd",2012,"The primary goal of this workshop is to gather researchers and practitioners from different fields, e.g., multimedia retrieval, user interaction, arts and heritage curation, interface design and user modeling, in order to showcase novel applications and discuss opportunities that grow from the connections between users and multimedia systems in the cultural heritage domain."
1223038,15226,9099,Robust image representation for efficient recognition and retrieval,2011,Image representation plays an essential role in image categorization and retrieval applications. Images span on visual and spatial space. Encoding both visual and spatial information for effective and efficient image matching remains a fundamental problem in computer vision. The objective of my research is to construct a robust image representation for efficient recognition and retrieval.
2233038,15226,21089,VSEM: An open library for visual semantics representation,2013,"VSEM is an open library for visual semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability. The software is accompanied by a website with supporting documentation and examples."
558090,15226,9616,Invariant signatures for omnidirectional visual place recognition and robot localization in unknown environments,2012,The paper introduces a novel approach to place representation for robot localization and mapping. It uses classical invariance theory while proposing an adaptive kernel to omnidirectional images and exploiting only the main significant visual information in the images. The approach is validated in real world robot exploration and localization and compared to color histograms.
664632,15226,21106,Estimating surface normals from spherical stokes reflectance fields,2012,In this paper we introduce a novel technique for estimating surface normals from the four Stokes polarization parameters of specularly reflected light under a single spherical incident lighting condition that is either unpolarized or circularly polarized. We illustrate the practicality of our technique by estimating surface normals under uncontrolled outdoor illumination from just four observations from a fixed viewpoint.
57408,15226,21089,Structure-Preserving Pipelines for Digital Libraries,2011,"Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved."
1381186,15226,9078,Anisotropic color image denoising and sharpening,2014,"In this paper, a new model for color image enhancement based on diffusion-shock filter coupling is presented. The proposed method is based on the consideration of single vectors of the gradient magnitude and the second derivatives as a manner to relate the processing of the different color components of the image. This solution avoids the apparition of color artefacts in the enhanced image."
2676782,15226,22113,Autonomous object manipulation: a semantic-driven approach,2011,"The problem of grasping is widely studied in the robotics community. This project focuses on the identification of object graspable features using images and object structural information. The primary aim is the creation of a framework in which the information gathered by the vision system can be integrated with automatically generated knowledge, modelled by means of fuzzy description logics."
1143200,15226,390,PdEs-based morphology on graphs for cytological slides segmentation and clustering,2012,"In this paper, we propose a new graph-based approach to address the problem of cytological computer-aided diagnostic. Such approach uses our previously introduced formalism of PdEs-based morphology and geometric diffusion on graphs. The approach is illustrated through two applications in cytopathology (involving Feulgen and Papanicolaou colorations), with examples of nucleus extraction and classification."
1230443,15226,21106,Recursive Live Dense Reconstruction: Some comments on established and imaginable new approaches,2011,"In the present position paper, I formulate some (in part critical) remarks related to some techniques which are successfully used in contemporary live dense reconstruction approaches. Main issues are feature based correspondence vs. image-based matching, the generalization of the brightness constancy assumption, and the handling of featureless regions which do not allow direct determination of correspondence."
1716663,15226,9078,Image deconvolution using tree-structured Bayesian group sparse modeling,2014,"In this paper, we propose to incorporate wavelet tree structures into a recently developed wavelet modeling method, called VBMM. We show that, using overlapped groups, tree-structured modeling can be integrated into the highperformance non-convex sparsity-inducing VBMM method, and can achieve significant performance gains over the coefficient-sparse version of the algorithm."
792379,15226,9078,Physiologically-based detection of computer generated faces in video,2014,"We describe a new forensic technique for distinguishing between computer generated and human faces in video. This technique identifies tiny fluctuations in the appearance of a face that result from changes in blood flow. Because these changes result from the human pulse, they are unlikely to be found in computer generated imagery. We use the absence or presence of this physiological signal to distinguish computer generated from human faces."
788316,15226,9078,Improving image quality in small animal diffusion tensor imaging at 7T,2012,"Diffusion tensor imaging is being increasingly used as a means to elucidate the brain's fiber structure. High spatial resolution is needed to capture details of the anatomy for tractography. However, image deteriorating factors such as low contrast-to-noise ratio, partial volume effects and subject's displacements affect the analysis."
1952556,15226,9078,Lifetime map reconstruction in frequency-domain fluorescence lifetime imaging microscopy,2012,We propose a robust statistical framework for reconstructing lifetime map corrupted by vesicle motion in frequency domain FLIM imaging. Instrumental noise is taken into account to improve lifetime estimation. Robust M-estimators and ML-estimators allow to jointly estimate motion and lifetime. Performances are demonstrated on simulated and real samples.
1757392,15226,8502,Plant classification system for crop /weed discrimination without segmentation,2014,This paper proposes a machine vision approach for plant classification without segmentation and its application in agriculture. Our system can discriminate crop and weed plants growing in commercial fields where crop and weed grow close together and handles overlap between plants. Automated crop / weed discrimination enables weed control strategies with specific treatment of weeds to save cost and mitigate environmental impact.
1303081,15226,9616,Multiphase Image Segmentation Using the Deformable Simplicial Complex Method,2014,"The deformable simplicial complex method is a generic method for tracking deformable interfaces. It provides explicit interface representation, topological adaptivity, and multiphase support. As such, the deformable simplicial complex method can readily be used for representing active contours in image segmentation based on deformable models. We show the benefits of using the deformable simplicial complex method for image segmentation by segmenting an image into a known number of segments characterized by distinct mean pixel intensities."
460270,15226,9616,Video figure ground labeling,2012,Figure-ground labeling is a classical problem in computer vision in which the goal is to label different parts of the visual input as figural or background. Yet most existing approaches focuses on single image figure-ground labeling with little emphasis on video. We present a method which integrates several cues to achieve figure-ground labeling on video sequences. The method is evaluated on challenging video sequences.
1515850,15226,9099,"Towards a privacy preserving personal photo album manager with semantic classification, indexing and querying capabilities",2011,"This paper presents the prototype of a personal photo album manager that we build over three years. It integrates state of the art techniques of both database systems and computer vision to propose to publishers a privacy preserving sharable photo album manager where the publisher decides what each user is allowed to see. In addition, the photo album manager provides tools for an easy semantic annotation, classification and querying of the images."
274209,15226,9616,Ensemble learning for change-point prediction,2012,"In this paper, we propose a novel algorithm for the problem of predicting change-points. We assume that the causes for change-points can be characterized by the time interval between a change-point and its symptom. Based on this assumption, we first generate weak classifiers for capturing each characteristic, and then build an ensemble classifier with the weak classifiers. Experimental results show our algorithm improves the F-measure by 11% in the best case."
1949924,15226,9616,Image enhancement by wavelet multi-scale edge statistics,2012,"The distribution of wavelet modulus maxima across wavelet scales can be used to characterize edges in an image. In this paper, we present a novel algorithm that performs image enhancement by mapping the distribution of the wavelet modulus maxima of the blurred image to that of a generic sharp image. Experimental results confirm that the proposed algorithm is able to perform image enhancement without introducing unpleasant visual artifacts."
52451,15226,21106,A new method to segment x-ray microtomography images of lamellar titanium alloy based on directional filter banks and gray level gradient,2012,This paper presents a method for segmentation of 2D texture images of titanium alloys. The procedure is fully automated and is able to find and recognize so-called α-colonies from the image. The algorithm combines nonsubsampled directional filter banks (NSDFB) from the contourlet transform and gradient gray-level value to recognize directional orientations of α-colony.
1559572,15226,9078,Dense interest features for video processing,2014,"We propose two novel feature detection methods for action recognition, based on the dense interest points described by Tuytelaars [1]. The first one is an extension of dense interest points to three dimensions. In the second one, trajectories are constructed starting from dense interest points. We present an analysis of the properties of these methods and conclude that both give higher classification accuracies than dense sampling when less features are used."
1117130,15226,390,Optimal 3D single-molecule super-resolution microscopy with engineered point spread functions,2012,Three-dimensional single-molecule localization is a fundamental problem in biological and biophysical experiments such as super-resolution microscopy. Point spread function engineering accompanied by corresponding reconstruction procedures provides a unique approach to increase localization precision. Joint optical/digital design and image reconstruction are investigated.
759223,15226,390,A statistical analysis of spatial colocalization using Ripley's K function,2013,"The author of this work present an appendix paper entitled: A statistical analysis of spatial colocalization using Ripley's K function. Thibault Lagache *, Vannary Meas-Yedid, lean-Christophe Olivo-Marin Institut Pasteur, Quantitative Image Analysis Unit, F-75015 Paris, France CNRS URA 2582, F-75015 Paris, France"
1929266,15226,9616,Vanishing point estimation by spherical gradient,2012,"In this paper we propose a novel method of estimating vanishing point by spherical gradient. In contrast with the conventional methods in which vanishing point is estimated from lines, the proposed method does not necessarily extract lines, but employs the spherical gradient cues of edge points. Based on the observation that spherical gradient is aligned with the normal vector of the projection plane of space lines, the vanishing point is estimated directly from spherical gradient of edge points by the Hough Transform."
391155,15226,20515,A two-factor protection scheme for MCC fingerprint templates,2014,"Minutia Cylinder-Code (MCC) is an effective representation for robust and fast fingerprint matching. To avoid that MCC templates can disclose sensitive information about position and angle of minutiae, a protected MCC representation was recently introduced (called P-MCC). In spite of a satisfactory level of accuracy and irreversibility, P-MCC templates cannot be revoked. In this paper we propose a two-factor protection scheme that makes P-MCC templates revocable."
2017691,15226,9004,Direct surgeon control of the computer in the operating room,2011,"This paper describes the design and evaluation of a joysticklike device that allows direct surgeon control of the computer in the operating room. The device contains no electronic parts, is easy to use, is unobtrusive, has no physical connection to the computer, and makes use of an existing surgical tool. The device was tested in comparison to a mouse and to verbal dictation."
2021632,15226,9099,Million-scale near-duplicate video retrieval system,2011,"In this paper, we present a novel near-duplicate video retrieval system serving one million web videos. To achieve both the effectiveness and efficiency, a visual word based approach is proposed, which quantizes each video frame into a word and represents the whole video as a bag of words. The system can respond to a query in 41ms with 78.4% MAP on average."
1929131,15226,9616,Enhancing motion segmentation by combination of complementary affinities,2012,"Complementary information, when combined in the right way, is capable of improving clustering and segmentation problems. In this paper, we show how it is possible to enhance motion segmentation accuracy with a very simple and inexpensive combination of complementary information, which comes from the column and row spaces of the same measurement matrix. We test our approach on the Hopkins155 dataset where it outperforms all other state-of-the-art methods."
2510846,15226,390,Lifetime estimation of moving vesicles in frequency-domain fluorescence lifetime imaging microscopy,2012,We propose a robust statistical framework for correcting the movements of vesicles in frequency domain FLIM imaging. Movement and lifetime are jointly estimated in a three-step procedure. Robust M-estimators are mainly used to improve accuracy in temporally-varying noisy images. The performance of the proposed method is demonstrated on both simulated and real samples.
2421475,15226,9704,GEC-based multi-biometric fusion,2011,"In this paper, we use Genetic and Evolutionary Computation (GEC) to optimize the weights assigned to the biometric modalities of a multi-biometric system for score-level fusion. Our results show that GEC-based multi-biometric fusion provides a significant improvement in the recognition accuracy over evenly fused biometric modalities, increasing the accuracy from 90.77% to 95.24%."
2476431,15226,9099,"Spacetime freeview generation using image-based rendering, relighting, and augmented telepresence",2012,"This paper proposes an freeview generation technique providing the users to change their viewpoints beyond time and space. The study consists of three technical elements: image-based rendering, relighting, and augmented telepresence. Before now, we have developed two systems relating this study: an augmented telepresence system and a full spherical HDR aerial imaging system."
1480255,15226,9078,Curvature scale-space of open curves: Theory and shape representation,2013,"The problem of extending the curvature scale-space (CSS) technique to represent open curves is addressed. Various approaches for dealing with the endpoint problem of open curves are considered, and one is selected which allows us to handle the evolution of the open curves as a special case of the evolution of closed curves. The convergence theory of evolved open curves is established, and the CSS shape representation is investigated."
2525301,15226,8502,Groupwise pose normalization for craniofacial applications,2011,A general framework is proposed for solving groupwise pose normalization problems and is analyzed in detail under different feature spaces. The analysis shows that using principal component analysis for pose normalization is a special case of using the proposed framework under a special feature space. The experimental results on two cranio-facial datasets show the proposed method achieved promising results for solving groupwise pose normalization problems for craniofacial applications.
1054323,15226,9099,Towards a real time public transport awareness system: case study in dublin,2011,"In this paper we discuss our experience with the design of a public transport awareness application developed for the city of Dublin. The application is capable to ingest, analyze and visualize in real-time high volumes of traffic data coming from a variety of sources. We address the challenges encountered during the design of the application and propose novel solutions to tackle those challenges."
42958,15226,9616,Semi-Automated Identification of Leopard Frogs,2014,Principal component analysis is used to implement a semi-automatic recognition system to identify recaptured northern leopard frogs (Lithobates pipiens). Results of both open set and closed set experiments are given. The presented algorithm is shown to provide accurate identification of 209 individual leopard frogs from a total set of 1386 images
285622,15226,9616,Nonlocal processing of 3D colored point clouds,2012,"In this paper we present a methodology for nonlocal processing of 3D colored point clouds using regularization of functions defined on weighted graphs. To adapt it to nonlocal processing of 3D data, a new definition of patches for 3D point clouds is introduced and used for nonlocal filtering of 3D data such as colored point clouds. Results illustrate the benefits of our nonlocal approach to filter noisy 3D colored point clouds (either on spatial or colorimetric information)."
1769855,15226,9773,A Tool for Tuning Binarization Techniques,2011,"In this paper a user friendly tool appropriate to get user feedback for the application of binarization algorithms is presented. The human feedback is very useful in order to apply next the algorithm to similar images. The tool supports Image Selection and Display, Selection of Binarization Algorithm and Parameter Configuration, Feedback gathering and Creation of log file for further processing."
862884,15226,11491,nepDroid: an intelligent mobile music player,2012,"Mobile music consumption has been spiraling during the past couple of years. The interaction techniques provided to sift through the ever increasing amounts of music available on smart devices unfortunately have not. In this paper, we address this issue and present an intelligent mobile user interface that enables the user to browse her mobile music collection in a joyful and informed way."
392695,15226,9616,An intrinsic coordinate system for 3D face registration,2012,"We present a method to estimate, based on the horizontal symmetry, an intrinsic coordinate system of faces scanned in 3D. We show that this coordinate system provides an excellent basis for subsequent landmark positioning and model-based refinement such as Active Shape Models, outperforming other -explicit- landmark localisation methods including the commonly-used ICP+ASM approach."
1995691,15226,9078,VOW: Variance-optimal wavelets for the steerable pyramid,2014,"We study the issue of localization in the context of isotropic wavelet frames. We define a variance-type measure of localization and propose an algorithm based on calculus of variations to minimize this criterion under the constraint of a tight wavelet frame. Based on these calculations, we design the variance-optimal wavelet (VOW). Finally, we demonstrate the advantage of better localization in a practical image-processing task."
1427480,15226,11491,Explicit diversification of image search,2013,Search result diversification can increase user satisfaction in answering a particular information need. There are many ways of diversify search results. In some cases the user has a clear idea of how they would like to see their results diversified. This work presents a system that is capable of diversifying search results along specific user-specified axes of diversity.
1088270,15226,9099,PhacePhinder: harnessing social networks to build social face databases for mobile devices,2012,This demo presents a client-server application which collects images and personal information from social networks to build face recognition databases. The client runs on a mobile phone allowing any face captured by a mobile phone's camera to be identified. Using the personal information collected we give back the most meaningful social connection between the user and the recognized individual.
1430082,15226,9773,Detecting Main Body Size in Document Images,2013,"In this paper, two techniques are presented, appropriate to detect the text main body size in a document image. One measures it directly, while the other estimates the baselines first. Both are segmentation free. Experimental results are presented over a collection of handwritten text, as well as for a small collection of 10 printed document images, in order to give more objective results."
1495293,15226,9078,MS lesions segmentation in 3D MR images using FCM and SVM,2014,This paper proposes an approach to automatically segment MS lesions in MR images using fuzzy c-means (FCM) and a support vector machines (SVM) based on the sequential minimal optimization (SMO) in learning step. A postprocessing based on morphological operations was applied to refine the obtained results. The proposed approach was tested on 3D MR images and the obtained results are encouraging.
1192148,15226,390,Axon segmentation in microscopy images — A graphical model based approach,2012,"Image segmentation of very large and complex microscopy images are challenging due to variability in the images and the need for algorithms to be robust, fast and able to incorporate various types of information and constraints in the segmentation model. In this paper we propose a graphical model based image segmentation framework that combines the information in images regions with the information in their boundary in a unified probabilistic formulation."
2229867,15226,9616,Symbol spotting for technical documents: An efficient template-matching approach,2012,"Symbol retrieval for technical documents is still a hot challenge in the document analysis community. In this paper we propose another way to spot symbols. A pixel-based template operator which is an adaptation of the hit-or-miss transform is defined. This operator is robust to translation, rotation and reflection. Experimental results on a real application show the efficiency of our approach."
1499528,15226,9099,Visual-based transmedia events detection,2012,"This paper presents a visual-based media event detection system based on the automatic discovery of the most circulated images across the main news media (news websites, press agencies, TV news and newspapers). Its main originality is to rely on the transmedia contextual information to denoise the raw visual detections and consequently focus on the most salient transmedia events."
1846014,15226,22279,Naturalistic data sets for image and behavior analysis - “normal” versus “anomalous” events,2011,"The approach and rationale used to create controlled data sets encompassing video, track, and reference data and meta-data is described in this paper. The data sets are designed to support the development of automated technologies for the detection of situational and operational behaviors of interest. A custom tool for the concurrent visualization and access of the data is also introduced."
2511790,15226,390,Application of compressed sensing to optical tomography,2011,"A concrete experimental application of compressed sensing is described. It uses a pair of digital micromirrors to perform diffuse optical tomography. Specific constraints arise due to the requirement of rapid imaging, which entails using of binary illumination patterns. Benefits and limitations of compressed sensing towards reaching these goals are discussed with preliminary results."
1087528,15226,9078,Eikonal-based vertices growing and iterative seeding for efficient graph-based segmentation,2014,In this paper we propose to use the Eikonal equation on graphs for generalized data clustering. We introduce a new potential function that favors the creation of homogeneous clusters together with an iterative algorithm that place seeds vertices at smart locations. Oversegmentation application shows the effectiveness of our approach and gives results comparable to the state-of-the-art methods.
2316446,15226,21106,Data-Driven 3D Primitives for Single Image Understanding,2013,What primitives should we use to infer the rich 3D world behind an image? We argue that these primitives should be both visually discriminative and geometrically informative and we present a technique for discovering such primitives. We demonstrate the utility of our primitives by using them to infer 3D surface normals given a single image. Our technique substantially outperforms the state-of-the-art and shows improved cross-dataset performance.
1409702,15226,9078,Surface completion of shape and texture based on energy minimization,2011,"In this paper, we propose a novel surface completion method to generate plausible shapes and textures for missing regions of 3D models. The missing regions are filled in by minimizing two energy functions for shape and texture, which are both based on similarities between the missing region and the rest of the object; in doing so, we take into account the positive correlation between shape and texture. We demonstrate the effectiveness of the proposed method experimentally by applying it to two models."
2408677,15226,390,Retrospective illumination correction of retinal fundus images from gradient distribution sparsity,2012,We present a novel technique for retrospective illumination correction of retinal fundus images from the sparsity property of image gradient distribution. It can automatically estimate the illumination inhomogeneity given an arbitrary retinal fundus image. Experimental results on 665 high resolution fundus images show both the efficiency of our algorithm on illumination correction and its high value on improving the accuracy of blood-vessel segmentation.
1154419,15226,9078,View interpolation sensitive to pixel positions,2013,"Theoretical analysis and an optimization scheme to solve the basic problem with view interpolation are presented. Two error factors, i.e., disparity error and pixel interpolation error, are considered. I found that the method of view interpolation should change according to which error factor is dominant. When disparity information is very accurate, the method of view interpolation should be especially sensitive to pixel positions, which has rarely been noticed in previous studies."
1556398,15226,9099,"Online video delivery: Past, present, and future",2013,"Video streaming is the core technology for online video delivery systems. Initial research on this technology faced many challenges. In this article, lessons learned from beginning trials are discussed; some pioneering works that provided early solutions and inspired subsequent research are presented; and new techniques required for emerging applications are examined."
1927828,15226,21089,Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions,2012,"Blogs and forums are widely adopted by online communities to debate about various issues. However, a user that wants to cut in on a debate may experience some difficulties in extracting the current accepted positions, and can be discouraged from interacting through these applications. In our paper, we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability."
1885122,15226,9616,A direction Change-based algorithm for polygonal approximation,2012,"A linear-time algorithm is proposed for polygonal approximation of digital curves. The direction changes of the x- and y-coordinates are traced to generate a new, compact representation of curves. The algorithm, Direction Change-based Polygonal Approximation (DCPA), has two advantages: linear time complexity and insensitivity to parameter setting. Benchmark results demonstrate the competitive performance of DCPA using standard assessment techniques."
2418362,15226,9078,Dual-domain image denoising,2013,"Image denoising methods have been implemented in both spatial and transform domains. Each domain has its advantages and shortcomings, which can be complemented by each other. State-of-the-art methods like block-matching 3D filtering (BM3D) therefore combine both domains. However, implementation of such methods is not trivial. We offer a hybrid method that is surprisingly easy to implement and yet rivals BM3D in quality."
1151170,15226,9099,A closer look at photographers' intentions: a test dataset,2012,"Taking a photo is a process typically triggered by an intention. Some people want to document the progress of a task, others just want to capture the moment to re-visit the situation later on. In this contribution we present a novel, openly available dataset with 1,309 photos and annotations specifying the intentions of the photographers, which were eventually validated using Amazon Mechanical Turk."
1698212,15226,21106,Salient object detection by composition,2011,Conventional saliency analysis methods measure the saliency of individual pixels. The resulting saliency map inevitably loses information in the original image and finding salient objects in it is difficult. We propose to detect salient objects by directly measuring the saliency of an image window in the original image and adopt the well established sliding window based object detection paradigm.
1962530,15226,11104,Weighted laplacian differences based multispectral anisotropic diffusion,2011,We study a multichannel version of nonlinear diffusion PDE which is used to restore noisy multispectral images. Weighted coupling of interchannel edges is done by utilizing fast total variation for each channel. Anisotropic intrachannel smoothing is included to denoise and preserve edges. Numerical results on noisy multispectral images show the advantage of the proposed hybrid approach.
1663277,15226,9078,Localization of Drosophila embryos using connected components in scale space,2012,"Localization of Drosophila embryos in images is a fundamental step in an automatic computational system for the exploration of gene-gene interaction on Drosophila. In this paper, we introduce a localization framework based on the analysis of connected components in the Gaussian scale space of an embryonic image. We propose three criteria for the selection of the optimal scale. The experiment results show the promise of the proposed methods."
470692,15226,9616,"A game theory inspired, decentralized, local information based algorithm for community detection in social graphs",2012,"Motivated by the observation that communities in real world social networks form due to actions of rational individuals in networks, we propose a novel game theory inspired algorithm to determine communities in networks. The algorithm is decentralized and only uses local information at each node. We show the efficacy of the proposed algorithm through extensive experimentation on several real world social network data sets."
720042,15226,11470,Expert Talk for Time Machine Session: Designing Calm Technology as Refreshing as Taking a Walk in the Woods,2012,"Weiser, and Brown predicted the era of Ubiquitous Computing but what they called the most profound change has been almost completely abandoned, overlooked or misrepresented. Designing calm technology requires a deeper understanding of how we multi-task, but it has the potential to becalm human computer interaction, decreasing stress, and mitigating human error as a cause of accidents."
49748,15226,11052,Generalized roof duality for multi-label optimization: optimal lower bounds and persistency,2012,"We extend the concept of generalized roof duality from pseudo-boolean functions to real-valued functions over multi-label variables. In particular, we prove that an analogue of the persistency property holds for energies of any order with any number of linearly ordered labels. Moreover, we show how the optimal submodular relaxation can be constructed in the first-order case."
2363441,15226,23735,Perception for the manipulation of socks,2011,"We consider the perceptual challenges inherent in the robotic manipulation of previously unseen socks, with the end goal of manipulation by a household robot for laundry. The task poses challenging problems in modeling the appearance, shape and configuration of these textile items that tend to exhibit high variability in texture, design, and style while being highly articulated objects."
841407,15226,9773,Determining Document Skew Using Inter-line Spaces,2011,"We present a novel method of determining a global text page orientation. The method is based on Hough transform, but, unlike the existing methods, it does not use the letters themselves and relies on establishing the orientation of interline regions. The method is robust and also works even when a single line of text is present. Experimental evaluation is shown, comparing the method to other methods proposed in the literature."
243858,15226,9616,Confidence-assisted classification result refinement for object recognition featuring TopN-Exemplar-SVM,2012,"This paper proposes a cascaded classifier framework for better image recognition. The proposed method is based on the confidence values given by the classifiers. By using our proposed topN-Exemplar SVM in the second stage and comparing the confidence values with those from the first stage, the classification results with less confidence are successfully updated. The validity of our algorithm has been demonstrated by the experiments using three standard image datasets."
2531042,15226,390,Recursive ultrasonic tomographic imaging using propagation and back-propagation method,2011,"This paper presents a recursive ultrasonic tomographic imaging method using the propagation and back propagation for solving a Helmholtz equation with a fixed frequency. The proposed algorithm recursively processes data collected on the eight boundaries of an octagon in a circular mode. Using numerical examples, we demonstrate that the proposed algorithm results in high quality images with fast convergence."
2450154,15226,11470,H.264 video transmission oveR 2×2 OFDM-MIMO,2011,"Higher capacity is achieved by using multichannel transmission technology. In this paper, we investigate the performance of H.264 standard over a 2 × 2 MIMO channel. With the use of data partitioning in H.264, it is possible to allocate the information, in an optimal way, through multiple channels with different SNR values. The results of our proposed MIMO communication show an improvement of 0.85dB by using a data-partitioning over a SISO case."
1432795,15226,9078,Global scheme for iterative mojette reconstructions,2014,"In this paper, we develop a global iterative algorithm for tomographic reconstructions from Mojette projections. Since Spline-Mojette projections are obtained by convolving Dirac-Mojette values with a specific uniform projection kernel, we decorrelate iterative reconstructions from projection model and provide a global scheme available for all Mojette models. We refer iterative algorithms to their Radon based counterparts and propose a comparative study from several Mojette acquisitions."
840149,15226,9099,Smart VideoCooKing: a multimedia cooking recipe browsing application on portable devices,2012,"This demo presents Smart VideoCooKing which is a multimedia cooking recipe browsing application on portable Android devices. A multimedia cooking recipe is a cooking recipe where each cooking operation is associated with a corresponding video clip describing it, aimed to facilitate the understanding of cooking operations. In combination with third-party applications, Smart VideoCooKing provides useful functions such as playing cooking video clips describing cooking operations quickly, searching information of ingredients easily, and reading aloud cooking directions."
1954256,15226,390,"Neuronal electrical activity, energy consumption and mitochondrial ATP restoration dynamics: A physiological based model for FMRI",2011,"fMRI is a widely used method to detect the activated brain regions due to a stimulus application. Commonly, it employs the BOLD contrast, which is based on the correlation between physiological function, energy metabolism and haemodynamics. However, the BOLD signal is weak and noisy, so that, an accurate mathematical model to describe the Haemodynamic Response Function (HRF) to activation is needed."
1487004,15226,9078,Active contours without level sets,2012,"This paper deals with the problem of segmenting an image with active contours. We explain how recent convexification methods allow now to use active contours without level sets with simple and efficient first order schemes. We recall different algorithms proposed in the literature, and we propose a new variant. Numerical experiments in 2D and 3D confirm the interest of the approach."
1389414,15226,9078,3D rectification of distorted document image based on tiled rectangle fragments,2014,This paper presents an approach for document rectification using a quasi-isometric mapping derived from a novel model that represents a deformed document surface. The model is composed of rectangle fragments of the developed document plane. This was realized by introducing gap relaxation which permits unconnected fragments. Our experiments show that our rectification approach can be applied to different types of document deformation.
1793995,15226,11470,Real-time eating action recognition system on a smartphone,2014,"Recently, many mobile applications to record everyday meals for dieting have been popular. Some of them can recognize names of food items in meals by only taking photos. However, such image-recognition-based food recording systems requires taking meal photos before eating, which are not applicable for the meals in which the amount of food to be eaten is not decided before eating such as large platter for sharing and barbecue-style dishes."
1233498,15226,9896,Video chat with multiple cameras,2013,"This work provides the first rigorous investigation of multi-camera video chat, concentrating especially on the ability of a user at one end of the conversation to switch between multiple views at both ends of the conversation. A user study of 23 individuals and comprehensive benchmark experiments employing up to four webcams simultaneously demonstrate that multi-camera video chat is both desirable and feasible on consumer hardware."
1436233,15226,9078,Improving the quality of mesh simplification with texture saliency measurement,2014,"The objective of this paper is to study the effect of using visual attention (saliency) of an object's texture to improve the quality of mesh simplification. We present an extension for QSlim method [Garland and Zhou, ‘Quadric-based simplification in any dimension’, 2005] which uses saliency measurement and we evaluate simplified meshes in terms of geometry loss and synthetic view-dependent image-based measurement."
1885564,15226,20649,Design Methods for Augmented Reality In-Vehicle Infotainment Systems,2014,"We have experienced rapid development of augmented reality (AR) systems and platforms in the automotive industry. However, to bring AR into production cars, we still face a range of challenges to design an AR system that meets vehicle specific requirements. Based on our experience with an AR prototype car, we analyze the influence of augmented reality on the design of the in-vehicle electric/electronic (E/E) architecture."
970694,15226,9078,A fast adaptive binarization method for complex scene images,2012,"A novel adaptive binarization method based on wavelet filter is proposed in this paper, which shows comparable performance to other similar methods and processes faster, so that it is more suitable for real-time processing and applicable for mobile devices. The proposed method is evaluated on complex scene images of ICDAR 2005 Robust Reading Competition, and experimental results provide a support for our work."
1841620,15226,11104,Potential of linear features detection in a Mediterranean landscape from 3D VHR optical data: Application to terrace walls,2012,"Despite having an high commission rate, the potential of an automated terrace walls detection from VHR optical 3D scene (DTM) is high using a line segment detection method on a simple DTM transform. The development of a vector post-process would probably help to eliminate false positive segments since they are short and non-aligned."
1407518,15226,9078,Local shape recognition for mobile applications,2012,"The paper presents a proof-of-concept shape-based visual recognition method with efficient local implementation for mobile devices, not relying on network or cloud processing. In such situations the focus is on effectiveness and simplicity, preserving a high level of functionality. Applications include offline object recognition and template matching (e.g. authorization, blind aid, product recognition)."
1084762,15226,9099,Towards synergy between the open source and the research multimedia communities,2011,This panel extends current efforts from the ACM Multimedia 2011 Organization Committee in taking an important step towards open source projects. The panelists include speakers who are among the leading figures from the open source community. The goal is to provide a shared space for discussion and interaction among consolidated and new open source projects and multimedia researchers.
1534187,15226,9099,1st international ACM workshop on user experience in e-learning and augmented technologies in education,2012,"UXeLATE2012 is the 1st International ACM Workshop on User Experience in e-Learning and Augmented Technologies in Education in conjunction with the ACM International Multimedia Conference (MM'12) at Nara, Japan. The workshop has a half day program, with a selection of six papers, and one keynote talk of a recognized expert in the field of usability, mobile technology and education."
1761195,15226,9099,"OpenMusic: visual programming environment for music composition, analysis and research",2011,OpenMusic is an open source environment dedicated to music composition. The core of this environment is a full-featured visual programming language based on Common Lisp and CLOS (Common Lisp Object System) allowing to design processes for the generation or manipulation of musical material. This language can also be used for general purpose visual programming and other (possibly extra-musical) applications.
1489491,15226,11104,Automatic extraction of geometric structures for 3D reconstruction from tomographic SAR data,2012,In this paper we introduce a method that allows automatic extraction of planar features from tomographic SAR data. Our approach takes advantage of the spatial connectivity of pixels from tomographic height maps in order to retrieve planar patches from noise corrupted complex scenes. We demonstrate how our method outperforms the well-known RANSAC algorithm over synthetic and experimental data.
1589478,15226,9078,Hazardous material sign detection and recognition,2013,"In this paper we describe two methods for hazardous material (hazmat) sign recognition. The first method is based on segment detection and grouping using geometric constraints. The second method is based on the use of a saliency map and convex quadrilateral detection. Our experimental results show a detection accuracy of 57.7% on a set of hazmat signs taken in the field under various lightning conditions, distances, and perspectives."
676530,15226,9099,Tell me what happened here in history,2013,"This demo shows our system that takes a landmark image as input, recognizes the landmark from the image and returns historical events of the landmark with related photos. Different from existing landmark related researches, we focus on the temporal dimension of a landmark. Our system automatically recognizes the landmark, shows historical events chronologically and provides detailed photos for the events. To build these functions, we fuse information from multiple online resources."
1259828,15226,9078,Multi-scale Non-Local Kernel Regression for super resolution,2011,"In this paper, we propose an extension of the Non-Local Kernel Regression (NL-KR) method and apply it to super-resolution (SR) tasks. The proposed method extends NL-KR via generalizing the self-similarity from single-scale to multi-scale, and propose an effective SR algorithm using the proposed multi-scale NL-KR model. Experimental results on both synthetic and real images demonstrate the effectiveness of the proposed method."
189346,15226,9748,Parallel Processing Model for Syntactic Pattern Recognition-Based Electrical Load Forecast,2013,"A model of a recognition of distorted/fuzzy patterns for a electrical load forecast is presented in the paper. The model is based on a syntactic pattern recognition approach. Since a system implemented on the basis of the model is to perform in a real-time mode, it is parallelized. An architecture for parallel processing and a method of tasks distribution is proposed. First experimental results are also provided and discussed."
848637,15226,9078,Saliency detection via statistical non-redundancy,2012,"A novel algorithm based on statistical non-redundancy is proposed for saliency detection in natural images. By modeling site neighbourhoods as realizations of other site neighbourhoods under a Gaussian process, the saliency of any arbitrary site can be characterized by the statistical non-redundancy of its site neighbourhood with respect to other site neighbourhoods in a given image. Preliminary results using natural images show that the proposed method provides improved precision vs. recall characteristics over previous methods such as spectral residuals and spectral whitening."
1037191,15226,9078,Grid warping in total variation image enhancement methods,2014,"An approach of including warping algorithms into total variation image enhancement methods is suggested. The idea of warping is to make edges sharper using pixel grid transform so that the pixels near edges move closer to the edges. The advantage of using warping approach is that the change of the total variation value is small, so it can be used to improve the results of total variation image enhancement methods like deblurring, deringing and resampling."
2108183,15226,11470,Photobook creation and social sharing on facebook,2011,"Photobook is a popular form adopted by consumers to organize photos with text into pages for memory preserving. In this paper, a photobook creation and social sharing application developed on facebook platform is presented with the focus on the following perspectives: 1) imaging and layout technologies for photobook creation; 2) cloud based architecture design; 3) social photobook sharing. The application is live on facebook with proven data on user growth."
832393,15226,8502,Learning mid-level features from object hierarchy for image classification,2014,We propose a new approach for constructing mid-level visual features for image classification. We represent an image using the outputs of a collection of binary classifiers. These binary classifiers are trained to differentiate pairs of object classes in an object hierarchy. Our feature representation implicitly captures the hierarchical structure in object classes. We show that our proposed approach outperforms other baseline methods in image classification.
1106684,15226,9078,Adaptive post-filtering based on Local Binary Patterns,2012,"In this paper, a novel adaptive post-filtering method is introduced for effective video coding. Specifically, an enhanced filtering capability is obtained by partitioning a reconstructed video frame into non-overlapping segments based on local pattern information. Experimental results show that the proposed approach has the potential to produce an enhanced video frame reconstruction with more implementable filters when compared to existing adaptive post-filtering."
1864345,15226,390,Stochastic 3-D signal reconstruction from noisy projection data for heterogeneous instances of objects in electron microscopy imagery,2011,"Different instances of a biological macromolecular complex need not share the same 3-D electron scattering distribution function due to stoichiometric variability, flexibility and vibrations, etc. Cryo electron microscopy provides 2-D images of each of many such complexes where each image is roughly a projection. A statistical estimation problem is described and demonstrated for determining a statistical description of the complex from such imagery."
1942422,15226,22279,AVSS 2011 demo session: Construction site monitoring from highly-overlapping MAV images,2011,"Summary form only given. We report on a disruption in organizational dynamics arising from the introduction of model-driven development tools in General Motors. The introduction altered the balance of collaboration deeply, and the organization is still negotiating with its aftermath. Our report illustrates one consequence of tool adoption in groups, and that these consequences should be understood to facilitate technical change."
63237,15226,21106,Estimation of Position and Radius of Light Probe Images,2012,Image Based Lighting technique needs light probe images. Light probe images are measurements of the scene light. Spherical and hemispherical mirrors (light probe measurement devices) and camera are used for the light probe image acquisition. In the paper is proposed and analyzed computational requirement of position and radius estimation of the hemispherical mirror with stripe pattern flange. Proposed solution reduces computation cost and allows processing of 4k image in 3 minutes.
1797659,15226,11470,Design issues of ambient social media for better decision making,2011,"Since our daily life is becoming more and more complex, it is difficult to make a decision for a variety of issues about ourselves. Of course, the Internet gives us a possibility to reactively find necessary information through search engines. But, it requires users to understand what they like to know currently. Also, they need significant cognitive efforts to collect a set of necessary information reactively for making their decisions."
1461442,15226,9616,Noise-Resistant Image Retrieval,2014,"We present a content-based image retrieval method which is particularly designed for noisy images. The images are retrieved according to histogram similarity. To reach high robustness to noise, the histograms are described by novel features which are insensitive to convolution with a Gaussian kernel, i.e. insensitive to a Gaussian additive noise in original images. The advantage of the new method is demonstrated experimentally on real data."
2022207,15226,23735,Gaussian Process for lens distortion modeling,2012,"When calibrating a camera, the radial component of lens distortion is the dominant source of image distortion. To model this lens distortion, camera models incorporate a radial distortion model that conforms to a certain parametric form. In practice however, multiple parametric forms can be used to model distortion for a given lens. Ideally, one would choose the best suited parametric form using a model selection procedure."
810401,15226,9616,On Performance Evaluation Metrics for Lane Estimation,2014,"Accurate and efficient lane estimation is a critical component of active safety systems in automobiles such as lane departure warning systems. A substantial number of lane estimation methods have been proposed and evaluated in literature. However, a common set of evaluation metrics that assess different components of lane estimation process has not been addressed. This paper proposes a set of performance evaluation metrics for lane estimation process, that can be deployed to evaluate different kinds of lane estimation algorithms. Evaluation by applying the proposed metrics is demonstrated using a recent lane estimation method."
2181839,15226,9099,Multimedia analysis for ecological data,2012,"The ACM International Workshop on Multimedia Analysis for Ecological Data (MAED'12) is held as part of ACM Multimedia 2012. MAED'12 is concerned with the processing, interpretation, and visualization of ecology-related multimedia content with the aim to support biologists in their investigations for analyzing and monitoring natural environments, with particular attention to living organisms and pollution effects."
1975648,15226,11470,A real-time body tracking system for smart rooms,2011,"We present a real-time human body tracking system for a single user in a Smart Room scenario. In this paper we propose a novel system that involves a silhouette-based cost function using variable windows, a hierarchical optimization method, parallel implementations of pixel-based algorithms and efficient usage of a low-cost hardware structure. Results in a Smart Room setup are presented."
598901,15226,9616,Predicting onsets of genocide with sparse additive models,2012,"Prevention of genocide is one of the most important challenges before the international community. In this paper we apply recent machine learning techniques to forecast the onset of political instability and genocide. Specifically, we employ sparse additive models which are both flexible and maintain interpretability of the results. Our model demonstrates a reasonable degree of forecasting performance over the hold-out period 1988–2003."
1023068,15226,9078,Image fusion based on a sparse linear system,2013,"This paper proposes an image fusion algorithm based on a sparse linear equation system, which uses local extreme of high resolution image and intensity of multi-spectral image to construct the system. Based on this sparse system, the multi-scale image decomposition algorithm can be implemented. This algorithm extracts the details of the high-resolution image and integrates with the multi-spectral information to derive the fused image."
844285,15226,21106,Perceptually motivated automatic sharpness enhancement using hierarchy of non-local means,2011,"We address the problem of sharpness enhancement of images. Existing hierarchical techniques that decompose an image into a smooth image and high frequency components based on Gaussian filter and bilateral filter suffer from halo effects, whereas techniques based on weighted least squares extract low contrast features as detail. Other techniques require multiple images and are not tolerant to noise."
1448929,15226,9078,What is the right center/surround for Retinex?,2014,"In this work we propose to analyze the formal properties of the center/surround versions of Retinex. Our main goal is to clarify what the “best” surround should be. Two conditions are sound or necessary from an image theoretical viewpoint: scale invariance and integrability. Then, we present a new kernel, which finds an acceptable compromise between these two conditions. This new kernel is compared with different kernels obtained from some center-surround methods."
882219,15226,11470,A Novel SVM Based Food Recognition Method for Calorie Measurement Applications,2012,"Emerging food classification methods play an important role in nowadays food recognition applications. For this purpose, a new recognition algorithm for food is presented, considering its shape, color, size, and texture characteristics. Using various combinations of these features, a better classification will be achieved. Based on our simulation results, the proposed algorithm recognizes food categories with an approval recognition rate of 92.6%, in average."
443326,15226,10994,Disambiguation in unknown object detection by integrating image and speech recognition confidences,2012,"This paper presents a new method to detect unknown objects and their unknown names in object manipulation through man-robot dialog. In the method, the detection is carried out by using the information of object images and user's speech in an integrated way. Originality of the method is to use logistic regression for the discrimination between unknown and known objects. The accuracy of the unknown object detection was 97% in the case when there were about fifty known objects."
866366,15226,390,3D saddle point detection and applications in cardiac imaging,2012,"We investigate saddle points in 3D cardiac images. We do so by improving a critical point detection algorithm, the 3D winding number, or Poincare index. We consider two different applications. We estimate cardiac motion from 3D tagged MRI data, based on tracking of saddle points. We also employ our method for saddle point extraction in blood flow data, acquired by phase contrast MRI."
2339315,15226,11104,Using worldwide available TerraSAR-X data to calibrate the geo-location accuracy of optical sensors,2011,"A method to calibrate the geo-location accuracy of optical sensors is presented which is based on a novel multi-modal image matching strategy. This concept enables to transfer points from highly accurate TerraSAR-X imagery to optical images. These points are then used to register the images or to update the optical sensor models. The potential of the methodology is demonstrated on Spot 5, Ikonos and RapidEye images."
616771,15226,9004,A holistic approach for the detection of media-adventitia border in IVUS,2011,In this paper we present a methodology for the automatic detection of media-adventitia border (MAb) in Intravascular Ultrasound. A robust computation of the MAb is achieved through a holistic approach where the position of the MAb with respect to other tissues of the vessel is used. A learned quality measure assures that the resulting MAb is optimal with respect to all other tissues. The mean distance error computed through a set of 140 images is 0.2164 (±0.1326) mm.
650626,15226,11052,On tensor-based PDEs and their corresponding variational formulations with application to color image denoising,2012,"The case when a partial differential equation (PDE) can be considered as an Euler-Lagrange (E-L) equation of an energy functional, consisting of a data term and a smoothness term is investigated. We show the necessary conditions for a PDE to be the E-L equation for a corresponding functional. This energy functional is applied to a color image denoising problem and it is shown that the method compares favorably to current state-of-the-art color image denoising techniques."
1717322,15226,20338,Broadcast yourself: understanding YouTube uploaders,2011,"YouTube uploaders are the central agents in the YouTube phenomenon. We conduct extensive measurement and analysis of YouTube uploaders. We estimate YouTube scale and examine the uploading behavior of YouTube users. We demonstrate the positive reinforcement between on-line social behavior and uploading behavior. Furthermore, we examine whether YouTube users are truly broadcasting themselves, via characterizing and classifying videos as either user generated or user copied."
747658,15226,9078,Pruning phantom detections from multiview foreground intersection,2012,"Homography mapping and fusion of foreground regions from multiple camera views is an effective technique for moving object detection. However, the intersections of non-corresponding foreground regions frequently cause phantom detections. In this paper, an algorithm using colour template matching is proposed to identify such phantoms from the multiview foreground intersection. Experiments on real-world video sequences have been carried out."
1350123,15226,9773,Modified Two-Class LDA Based Compound Distance for Similar Handwritten Chinese Characters Discrimination,2011,This paper proposes a modified two-class LDA based compound distance for similar handwritten Chinese characters discrimination. First the definition of the Intersecting Subspace (IS) between two classes and the modified between-class scatter matrix is given. Then we prove that the modified between-class scatter matrix can supply additional information. Our experiments demonstrate that the additional information can be used to discriminate points in the IS and the proposed method outperforms the previous LDA based method.
1252782,15226,22035,QoE Alchemy 2.0: An improved test setup for the pecuniary bias of QoE,2013,"Examining monetary aspects related to QoE research, e.g. as bias, requires specifically defined empirical testing and data analysis routines. This paper presents an advanced setup for investigating the impact of purchasing behaviors in an interactive Video-on-demand scenario, and, based on lessons learned from previous and current experiments, provides a set of recommendations for further research in this direction."
1891559,15226,21056,The EBU MIM-SCAIE content set for automatic information extraction on broadcast media,2014,This paper describes a content set that has been made available by the European Broadcasting Union (EBU). The content in the set consists of broadcast media content collected from different broadcasters around the world. This content set is made available to the research community in order to evaluate automatic information extraction tools on this broadcast media. The set also contains ground truth data and annotations for several automatic information extraction tasks.
2019305,15226,9616,Deblurring depth blur and motion blur simultaneously by using space-time coding,2012,"In recent years, various methods have been proposed for recovering depth blur and motion blur by coding camera optics, such as aperture and exposure. However, these methods are limited to deblurring just a single type of blur, such as depth blur or motion blur. In this paper, we propose a method, which enables us to deblur the depth blur and the motion blur simultaneously by coding image capture both in space and time. The validity and the advantages of the proposed method are shown by real-image experiments and quantitative evaluations using lens simulator."
1579975,15226,390,Coupled signed-distance functions for implicit surface reconstruction,2012,"We present a coupled signed-distance function method for reconstructing closed implicit surfaces from unstructured point clouds. The method can capture high curvature without the need for adaptive grids and is easy to implement. We present the method, benchmark it on artificial data, and apply it to two biological point data sets from protein surfaces and PALM microscopy."
43638,15226,9004,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,2013,"We use deep max-pooling convolutional neural networks to detect mi- tosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin."
568810,15226,9616,Descriptor correlation analysis for remote sensing image multi-scale classification,2012,"This paper addresses the problem of remote sensing image multi-scale classification by: (i) showing that using multiple scales does improve classification results, but not all scales have the same importance; (ii) showing that image descriptors do not offer the same contribution at all scales, as commonly thought, and some of them are very correlated; (iii) introducing a simple approach to automatically select segmentation scales, descriptors, and classifiers based on correlation and accuracy analysis."
2789388,15226,9616,Face Templates Creation for Surveillance Face Recognition System,2014,This paper addresses the problem of face templates creation for facial recognition system. The application of a face recognition system in real-world conditions requires compact and representative face templates in order to maintain low error rate and low classification time. The paper presents four approaches to face templates creation. The influence of different face template creation approaches was assessed on PubFig and IFaViD database. The achieved results show that appropriate face template creation methods have a significant influence on face recognition system performance.
2298085,15226,22288,A pedestrian detection method based on Dirichlet distribution and histogram of components,2012,"Pedestrian detection is a hot topic in computer vision recently. It is a challenging issue to detect the pedestrian in images accurately and quickly. In this paper, we propose a quick and accurate pedestrian detection method based on Dirichlet distribution and histogram of components. This method can find out the pedestrian in images accurately. Because of the introduction of Dirichlet distribution and histogram of components, our method reduces the time cost on detecting pedestrian. The experiment also showed that it can detect the pedestrian sheltered from other object or in the complicated background."
2318849,15226,9704,SSGA & EDA based feature selection and weighting for face recognition,2011,"In this paper, we compare genetic and evolutionary feature selection (GEFeS) and weighting (GEFeW) using a number of biometric datasets. GEFeS and GEFeW have been implemented as instances of Steady-State Genetic and Estimation of Distribution Algorithms. Our results show that GEFeS and GEFeW dramatically improve recognition accuracy as well as reduce the number of features needed for facial recognition. Our results also show that the Estimation of Distribution Algorithm implementation of GEFeW has the best overall performance."
161822,15226,21106,Architecture of Algorithmically Optimized MPEG-4 AVC/H.264 Video Encoder,2012,Architecture of algorithmically optimized MPEG-4 AVC/ H.264 video encoder is presented in the paper. The paper reveals details of implementation for the proposed MPEG-4 AVC video encoder. The presented MPEG-4 AVC encoder was tested with test video sequences from the point of view of computational performance and coding efficiency. The runtime of the optimized video encoder is 37 to 132 times smaller relative to runtime of the reference MPEG-4 AVC encoder for comparable encoder compression performance.
67836,15226,21106,Discrete geometric modeling of thick pelvic organs with a medial axis,2012,"Modeling of soft pelvic organs and their thicknesses is a difficult task, especially when inputs are noisy and scattered. In order to define the geometric step for a global pelvic surgery simulator, we define a new method based only on geometry while considering the problem of error transfer between outer and inner organ surfaces. We compare this approach with a parametric formulation and a mass-spring system."
1673722,15226,11470,OpenGL SC Implementation over an OpenGL ES 1.1 Graphics Board,2012,"OpenGL SC, the safety critical profile of OpenGL plays the major role for the graphical user interfaces, especially in the safety-critical markets, including avionics, military, medical and automotive applications. In other side, OpenGL ES, the embedded systems version of OpenGL, has many commercial implementations. In this demonstration, we show that the OpenGL SC features can be provided over the wide-spread OpenGL ES graphics boards. This is the most cost-effective way of implementing OpenGL SC, at this time. Our result is the first implementation based on OpenGL ES 1.1 hardware. We will demonstrate this OpenGL SC-over-OpenGL ES 1.1 implementation, and show its successful behaviors."
809076,15226,9078,Impact of hierarchical structures in image categorization systems,2012,"Image categorization refers to the process of assigning images to a number of predefined categories. The difficulty of problem solving is proportional to the number of categories the system addresses. This paper proposes an image categorization system, and studies the impact of dividing the categorization problem into smaller problems in a hierarchical structure. We compare examples solved with and without the proposed approach, to conclude its pros and cons."
2567055,15226,22035,Mapping of Received Signal Strength Indicator to QoE in VoIP applications over wlan,2012,"This paper presents mapping of Received Signal Strength Indicator (RSSI) to Quality of Experience (QoE) in mobile devices for end to end VoIP communications over Wi-Fi. Through extensive experiments, relationships between RSSI and QoE were investigated and a mapping table from RSSI to QoE was created. The effectiveness of the proposed approach in terms of energy saving was also demonstrated by comparing it with conventional QoE monitoring approach."
495863,15226,9616,8-D reflectance field for computational photography,2012,"Some computational photography techniques have been proposed to control the focus and illumination of captured images. However, the relationship between the techniques have been unclear because they were developed independently for different purposes. In this research we propose a unified framework to explain the computational photography techniques in the computation of an 8-D reflectance field. Moreover, for an 8-D reflectance field we show that the synthetic aperture, the image-based relighting, and the confocal imaging techniques can be realized using the same measuring device."
1342984,15226,9099,Video2GPS: a demo of multimodal location estimation on flickr videos,2011,"The following article describes our demo of an approach to determine the geo-coordinates of the recording place of Flickr videos based on both textual metadata and visual cues. The underlying system has been tested on the MediaEval 2010 Placing Task evaluation data, which consists of 5091 unfiltered test videos is able to classify 14% of the videos to within an accuracy of 10m."
1135518,15226,390,Variational myocardial tracking from CINE-MRI with non-linear regularization,2013,We present a new motion estimation approach for cardiac Magnetic Resonance Imaging (MRI) data from a variational framework. The improved performance of variational approach has been achieved by designing a new regularization term that properly handles motion discontinuities. This approach was applied to both synthetic and real data. The quantitative evaluation revealed the superior performance of the proposed method against reference approaches.
1558083,15226,9078,A fast and accurate re-calibration technique for misaligned stereo cameras,2013,"In this paper, we propose a practical approach for robust rectification of stereo camera setups without use of calibration pattern. Our solution simplifies the process to a non-general case of rectification to avoid explicit use of Fundamental Matrix estimation. The solution shows better or comparable robustness than some of recent solutions, but for much lower computational cost and code complexity."
1744989,15226,11104,Unsupervised river detection in RapidEye data,2012,"Remote sensing is a widely-used utility in supporting multilateral environmental treaties such as the Water Framework Directive (WFD). Regarding the WFD most remote sensing applications aim on the assessment of the biochemical status of surface water, while the general detection of water networks is disregarded. Therefore, a methodology for the automatic extraction of river networks from multispectral satellite data is presented."
116642,15226,21106,Multimodal Segmentation of Dense Depth Maps and Associated Color Information,2012,"An integrated segmentation approach for color images and depth maps is proposed. The 3D pointclouds are characterized by normal vectors and then grouped into planar, concave or convex faces. The empty regions in the depth map are filled by segments of the associated color image. In the experimental part two types of depth maps are analysed: generated by the MS-Kinect sensor or by a stereo-pair of cameras."
2037612,15226,9078,Fuzzy logic and local features based medical image segmentation,2013,A fuzzy logic based active contour model for medical image segmentation is proposed. Image local features are incorporated in active contour model. Fuzzy logic is used to assign weights to pixels. Higher weights are assigned to pixels having less entropy and local variance whereas Lower weights are assigned to pixels having high entropy and local variance. Simulation results show the improvement in segmentation results in terms of efficiency and accuracy.
524477,15226,9616,Towards automated classification of fine-art painting style: A comparative study,2012,This paper presents a comparative study of different classification methodologies for the task of fine-art genre classification. 2-level comparative study is performed for this classification problem. 1st level reviews the performance of discriminative vs. generative models while 2 nd  level touches the features aspect of the paintings and compares semantic-level features vs low-level and intermediate level features present in the painting.
1189110,15226,9616,VC-Dimension of Rule Sets,2014,"Abstract—In this paper, we give and prove lower bounds ofthe VC-dimension of the rule set hypothesis class where the inputfeatures are binary or continuous. The VC-dimension of the ruleset depends on the VC-dimension values of its rules and thenumber of inputs.Index Terms—VC-Dimension, Rule sets I. I NTRODUCTION Rule induction, an old branch of machine learning, isconcentrated on extracting rule sets from data. A rule setis typically composed of an ordered list of rules, where arule is composed of a conjunction of a list of conditions [1].Depending on the type of the input attribute, the conditionsare of the form • x i = v:ifx i is discrete • x i ≤ θ or x i >θ:ifx i is continuousAruleissaidtocover an instance, if that instance satisﬁesall conditions in that rule. Each rule is associated with a classlabel, and class label of the ﬁrst covering rule is assigned toan instance. If none of the rules cover an instance, the defaultclass label is assigned to that instance. An example rule setcomposed of three rules is given below.If x"
514661,15226,11491,LifeCLEF: Multimedia Life Species Identification,2014,"Building accurate knowledge of the identity, the geographicdistribution and the evolution of living species is essentialfor a sustainable development of humanity as well as forbiodiversity conservation. In this context, using multimediaidentication tools is considered as one of the most promisingsolution to help bridging the taxonomic gap. With therecent advances in digital devices/equipment, network bandwidthand information storage capacities, the production ofmultimedia big data has indeed become an easy task. In parallel,the emergence of citizen sciences and social networkingtools has fostered the creation of large and structured communitiesof nature observers (e.g. eBird, Xeno-canto, TelaBotanica, etc.) that have started to produce outstandingcollections of multimedia records. Unfortunately, the performanceof the state-of-the-art multimedia analysis techniqueson such data is still not well understood and is far fromreaching the real world's requirements in terms of identi-cation tools. The LifeCLEF lab proposes to evaluate thesechallenges around 3 tasks related to multimedia informationretrieval and ne-grained classication problems in 3 livingworlds. Each task is based on large and real-world data andthe measured challenges are dened in collaboration withbiologists and environmental stakeholders in order to reflect realistic usage scenarios."
829949,15226,11104,An assimilation algorithm of satellite-derived LST observations for the operational production of soil moisture maps,2012,"The knowledge of the soil moisture state in a region plays an important role in hydrology, with particular reference to the flood events prediction. A valid tool for the evaluation of the saturation state at watershed scale is given by remote sensing imagery. In this work temporal sequences of LST images from satellite platform (MSG-SEVIRI and Terra-MODIS) have been used in an assimilation procedure, ACHAB, in order to retrieve estimations of the land surface energy balance components and daily maps of soil moisture saturation index (SMSI). The simulation has been performed over the Italian territory for seven years (2005–2011) with about 5 km of spatial resolution. A climatology of the SMSI maps has been computed and reliability index maps have been provided. This study was realized in the framework of “OPERA - Protezione Civile dalle Alluvioni” ([1]), a project of the Italian Civil Protection aimed to the operational use of satellite data for floods prediction and management."
2665702,15226,20358,Songrium: a music browsing assistance service with interactive visualization and exploration of protect a web of music,2014,"This paper describes a music browsing assistance service, Songrium (http://songrium.jp), which increases user enjoyment when listening to songs and allows visualization and exploration of a ``Web of Music''. We define a Web of Music in this paper to be a network of ``web-native music'', which we define in turn to be music that is published, shared, and remixed (has derivative works created) entirely on the web. Songrium was developed as an attempt to realize a Web of Music, by showing relations between both original songs and derivative works and offering an enriched listening experience. Songrium has analyzed over 600,000 music video clips on the most popular Japanese video-sharing service, Niconico, which contains original songs of web-native music and their derivative works such as covers and dance arrangements. Analysis of over 100,000 original songs reveals that over 500,000 derivative works were generated and have contributed to enrich the Web of Music."
1896357,15226,9099,"Stoicheia: Architecture, Sound and Tesla's Apotheosis",2014,"Stoicheia, is an immersive installation focusing on the making of a synthetic ecology and informing the process of architecture using sound. After the reception of enigmatic radio signals in 1899, Tesla began work for many years to perfect the receiving and transmitting equipment that was needed to better pick up and translate his aural discoveries. Stoicheia explores similar notions of Telsa's Aether using specific data feeds and scanning techniques to create a dynamic spatial soundscape drawing upon an experience of immediacy, and ephemeral interplay between the agencies that maintain a continuously self-adaptive sonic and physical environment."
959992,15226,21056,GreenCache: augmenting off-the-grid cellular towers with multimedia caches,2013,"The growth of smartphones combined with advances in mobile networking have revolutionized the way people consume multimedia data. In particular, users in developing countries primarily rely on smartphones since they often do not have access to more powerful (and more expensive) computing devices. Unfortunately, cellular networks in developing countries have historically had low reliability, due to grid instability and lack of infrastructure. The situation has led network operators to experiment with running cellular towers off the grid using intermittent renewable energy sources. In parallel, network operators are also experimenting with co-locating server caches close to cell towers to reduce access latency and back-haul bandwidth. In this paper, we study techniques for optimizing multimedia caches for intermittent renewable energy sources. Specifically, we examine how to apply a blinking abstraction proposed in prior work, which rapidly transitions servers between an active and inactive state, to improve the performance of a multimedia cache powered by renewables, called GreenCache. Our results show that GreenCache's staggered load-proportional blinking policy, which coordinates when servers are active over brief intervals, results in 3X less buffering (or pause) time by the client compared to an activation blinking policy, which simply activates and deactivates servers over long periods as power fluctuates, for realistic power variations from renewable energy sources."
812338,15226,11317,FeelCraft: crafting tactile experiences for media using a feel effect library,2014,"FeelCraft is a media plugin that monitors events and states in the media and associates them with expressive tactile content using a library of feel effects (FEs). A feel effect (FE) is a user-defined haptic pattern that, by virtue of its connection to a meaningful event, generates dynamic and expressive effects on the user's body. We compiled a library of more than fifty FEs associated with common events in games, movies, storybooks, etc., and used them in a sandbox-type gaming platform. The FeelCraft plugin allows a game designer to quickly generate haptic effects, associate them to events in the game, play them back for testing, save them and/or broadcast them to other users to feel the same haptic experience. Our demonstration shows an interactive procedure for authoring haptic media content using the FE library, playing it back during interactions in the game, and broadcasting it to a group of guests."
2562401,15226,390,Trace driven registration of neuron confocal microscopy stacks,2011,"Active research in the area of 3-D neurite tracing has predominantly focused on single sections. Ultimately, however, neurobiologists desire to study the long range connectivity of the brain, which requires tracing axons across multiple serially-cut sections. Registration of axonal sections is challenging due to several factors, such as sparseness of the axons and complications of the sectioning process, including tissue deformation and loss. This paper investigates a method for registering sections using centerline traces which provide the locations of axons at section boundaries and the angles at which the axons approach the boundaries. This information is used to determine correspondences between two serial sections. Both global and local differences are accounted for using rigid and non-rigid transforms. Results show that utilizing information from traced axons allows axon continuity across sections to be restored."
1897530,15226,30,Unobtrusive Assessment of Motor Patterns During Sleep Based on Mattress Indentation Measurements,2011,"This study investigates how integrated bed measurements can be used to assess motor patterns (movements and postures) during sleep. An algorithm has been developed that detects movements based on the time derivate of mattress surface indentation. After each movement, the algorithm recognizes the adopted sleep posture based on an image feature vector and an optimal separating hyperplane constructed with the theory of support vector machines. The developed algorithm has been tested on a dataset of 30 fully recorded nights in a sleep laboratory. Movement detection has been compared to actigraphy, whereas posture recognition has been validated with a manual posture scoring based on video frames and chest orientation. Results show a high sensitivity for movement detection (91.2%) and posture recognition (between 83.6% and 95.9%), indicating that mattress indentation provides an accurate and unobtrusive measure to assess motor patterns during sleep."
1844337,15226,30,A Medical-Grade Wireless Architecture for Remote Electrocardiography,2011,"In telecardiology, electrocardiogram (ECG) signals from a patient are acquired by sensors and transmitted in real time to medical personnel across a wireless network. The use of IEEE 802.11 wireless LANs (WLANs), which are already deployed in many hospitals, can provide ubiquitous connectivity and thus allow cardiology patients greater mobility. However, engineering issues, including the error-prone nature of wireless channels and the unpredictable delay and jitter due to the nondeterministic nature of access to the wireless medium, need to be addressed before telecardiology can be safely realized. We propose a medical-grade WLAN architecture for remote ECG monitoring, which employs the point-coordination function (PCF) for medium access control and Reed-Solomon coding for error control. Realistic simulations with uncompressed two-lead ECG data from the MIT-BIH arrhythmia database demonstrate reliable wireless ECG monitoring; the reliability of ECG transmission exceeds 99.99% with the initial buffering delay of only 2.4 s."
2508198,15226,30,In-Depth Analysis and Evaluation of Diffusive Glioma Models,2012,"Glioma is one of the most aggressive types of brain tumor. Several mathematical models have been developed during the past two decades, toward simulating the mechanisms that govern the development of glioma. The most common models use the diffusion-reaction equation (DRE) for simulating the spatiotemporal variation of tumor cell concentration. Nevertheless, despite the applications presented, there has been little work on studying the details of the mathematical solution and implementation of the 3-D diffusion model and presenting a qualitative analysis of the algorithmic results. This paper presents a complete mathematical framework on the solution of the DRE using different numerical schemes. This framework takes into account all characteristics of the latest models, such as brain tissue heterogeneity, anisotropic tumor cell migration, chemotherapy, and resection modeling. The different numerical schemes presented have been evaluated based upon the degree to which the DRE exact solution is approximated. Experiments have been conducted both on real datasets and a test case for which there is a known algebraic expression of the solution. Thus, it is possible to calculate the accuracy of the different models."
2408897,15226,10192,Performance Assessment of Wireless ECG Transmission over IEEE 802.11 WLANs,2011,"We explore the use of IEEE 802.11 wireless technology to support health monitoring application, by providing ubiquitous connectivity while allowing patients greater mobility. The acquisition of electrocardiogram (ECG) is used as an example to highlight the engineering choices, such as packetization and access control rules, that must be made in designing a wireless transport. The error-prone nature of wireless channels, together with the unpredictable delay and jitter caused by the non-deterministic nature of access to the wireless medium, need to be considered. We discuss how our design choice with respect to each issue affects the performance of the network in a high-quality ECG monitoring service with bounded delay. We evaluate our schemes using simulations with non-compressed 2-lead ECG data from the MIT-BIH arrhythmia database."
1568071,15226,30,High-Grade Glioma Diffusive Modeling Using Statistical Tissue Information and Diffusion Tensors Extracted from Atlases,2012,"Glioma, especially glioblastoma, is a leading cause of brain cancer fatality involving highly invasive and neoplastic growth. Diffusive models of glioma growth use variations of the diffusion-reaction equation in order to simulate the invasive patterns of glioma cells by approximating the spatiotemporal change of glioma cell concentration. The most advanced diffusive models take into consideration the heterogeneous velocity of glioma in gray and white matter, by using two different discrete diffusion coefficients in these areas. Moreover, by using diffusion tensor imaging (DTI), they simulate the anisotropic migration of glioma cells, which is facilitated along white fibers, assuming diffusion tensors with different diffusion coefficients along each candidate direction of growth. Our study extends this concept by fully exploiting the proportions of white and gray matter extracted by normal brain atlases, rather than discretizing diffusion coefficients. Moreover, the proportions of white and gray matter, as well as the diffusion tensors, are extracted by the respective atlases; thus, no DTI processing is needed. Finally, we applied this novel glioma growth model on real data and the results indicate that prognostication rates can be improved."
2249116,15226,30,Prediction of the Timing and the Rhythm of the Parkinsonian Subthalamic Nucleus Neural Spikes Using the Local Field Potentials,2012,"In this paper, we discuss the use of a nonlinear cascade model to predict the subthalamic nucleus spike activity from the local field potentials recorded in the motor area of the nucleus of Parkinson's disease patients undergoing deep brain stimulation. We use a segment of appropriately selected and processed data recorded from five nuclei to acquire the information of the spike timing and rhythm of a single neuron and estimate the model parameters. We then use the rest of each recording to assess the model's accuracy in predicting spike timing, rhythm, and interspike intervals. We show that the cumulative distribution function (CDF) of the predicted spikes remains inside the 95% confidence interval of the CDF of the recorded spikes. By training the model appropriately, we prove its ability to provide quite accurate predictions for multiple-neuron recordings as well, and we establish its validity as a simple yet biologically plausible model of the intranuclear spike activity recorded from Parkinson's disease patients."
2139771,15226,30,Equipment Location in Hospitals Using RFID-Based Positioning System,2012,"Throughout various complex processes within hospitals, context-aware services and applications can help to improve the quality of care and reduce costs. For example, sensors and radio frequency identification (RFID) technologies for e-health have been deployed to improve the flow of material, equipment, personal, and patient. Bed tracking, patient monitoring, real-time logistic analysis, and critical equipment tracking are famous applications of real-time location systems (RTLS) in hospitals. In fact, existing case studies show that RTLS can improve service quality and safety, and optimize emergency management and time critical processes. In this paper, we propose a robust system for position and orientation determination of equipment. Our system utilizes passive (RFID) technology mounted on flooring plates and several peripherals for sensor data interpretation. The system is implemented and tested through extensive experiments. The results show that our system's average positioning and orientation measurement outperforms existing systems in terms of accuracy. The details of the system as well as the experimental results are presented in this paper."
725013,15226,30,Automatic Segmentation of Coronary Arteries in CT Imaging in the Presence of Kissing Vessel Artifacts,2012,"In this paper, we present a novel two-step algorithm for segmentation of coronary arteries in computed tomography images based on the framework of active contours. In the proposed method, both global and local intensity information is utilized in the energy calculation. The global term is defined as a normalized cumulative distribution function, which contributes to the overall active contour energy in an adaptive fashion based on image histograms, to deform the active contour away from local stationary points. Possible outliers, such as kissing vessel artifacts, are removed in the postprocessing stage by a slice-by-slice correction scheme based on multiregion competition, where both arteries and kissing vessels are identified and tracked through the slices. The efficiency and the accuracy of the proposed technique are demonstrated on both synthetic and real datasets. The results on clinical datasets show that the method is able to extract the major branches of arteries with an average distance of 0.73 voxels to the manually delineated ground truth data. In the presence of kissing vessel artifacts, the outer surface of the entire coronary tree, extracted by the proposed algorithm, is smooth and contains fewer erroneous regions, originating in kissing vessel artifacts, as compared to the initial segmentation."
2323970,15226,30,Revisiting Intensity-Based Image Registration Applied to Mammography,2011,"The detection of architectural distortions and abnormal structures in mammographic images can be based on the analysis of bilateral and temporal cases using image registration. This paper presents a quantitative evaluation of state-of-the art intensity based image registration methods applied to mammographic images. These methods range from a global and rigid transformation to local deformable paradigms using various metrics and multiresolution approaches. The aim of this study is to assess the suitability of these methods for mammographic image analysis. Evaluation using temporal cases based on quantitative analysis and a multiobserver study is presented which gives an indication of the accuracy and robustness of the different algorithms. Although previous studies suggested that local deformable methods were not suitable due to the generation of unrealistic distortions, in this work we show that local deformable paradigms (multiresolution B-Spline deformations) obtain the most accurate registration results."
1845177,15226,30,Geometric Calibration of a Micro-CT System and Performance for Insect Imaging,2011,"Micro-CT with a high spatial resolution in combination with computer-based-reconstruction techniques is considered a powerful tool for morphological study of insects. The quality of CT images crucially depends on the precise knowledge of the scan geometry of the micro-CT system. In this paper, we have proposed a method to calculate the deviation of rotating axis for compensating deficiency of existing methods. A practical application of this geometric calibration method of the micro-CT system for insect imaging is presented. We have performed the computer-simulation study and experimental study with our prototype micro-CT system. The results demonstrate that the proposed technique is accurate and robust. In addition, we have evaluated the imaging characteristics of the detector in terms of modulation-transfer function (MTF). Finally, insect imaging performance and image reconstruction from data acquired with different energies are presented."
1120397,15226,9773,The ICDAR2011 Arabic Writer Identification Contest,2011,"Arabic writer identification is a very active research field. However, no standard benchmark is available for researchers in this field. The aim of this competition is to gather researchers and compare recent advances in Arabic writer identification. This competition was hosted by Kaggle, it has attracted thirty participants from both academia and industry. This paper gives details on this competition, including the evaluation procedure, description of participating methods and their performances."
488329,15226,20332,Kinect@Home: Crowdsourcing a Large 3D Dataset of Real Environments,2012,"We present Kinect@Home, aimed at collecting a vast RGB-D dataset from real everyday living spaces. This dataset is planned to be the largest real world image col- lection of everyday environments to date, making use of the availability of a widely adopted robotics sensor which is also in the homes of millions of users, the Mi- crosoft Kinect camera."
790448,15226,9078,Performance evaluation of cluster-based hyperspectral target detection algorithms,2012,"Detection of targets in background clutter using hyperspectral imaging sensors, is a problem of great practical interest [1]. This paper addresses some practical problems related to the adaptive estimation of clutter models and their effects on the performance of matched-signature detection algorithms. More specifically, we compare clutter estimation algorithms using spatially-local adaptation or spectral clustering to deal with the nonstationarity of hyperspectral backgrounds."
1412339,15226,9078,Reversible non-expansive symmetric convolution for M-channel lifting based linear-phase filter banks,2012,"This paper presents an effective signal boundary solution in lossy-to-lossless image coding which is the unification of lossy and lossless image coding. Although M-channel filter banks (FBs) for lossy image coding have several effective signal boundary solutions, M-channel lifting based FBs (L-FBs) for lossless image coding do not have such an effective signal boundary solutions due to rounding error in each lifting step. This paper proposes reversible non-expansive symmetric convolution for M-channel lifting based linear-phase FBs (L-LPFBs) to apply lossy-to-lossless image coding. Our proposal is validated by comparing with the periodic extension in lossy-to-lossless image coding."
1554860,15226,9078,A reduced-reference perceptual quality metric for texture synthesis,2014,This paper presents a reduced-reference quality metric that quantifies the perceptual quality of the synthesized textures. The metric is based on the change in perceived regularity between the original and the synthesized textures. The perceived regularity is quantified through a modified texture regularity metric based on visual attention. It is shown through subjective testing that the proposed metric has a strong correlation with the Mean Opinion Score for the fidelity of synthesized textures and outperforms the state-of-the-art full-reference quality metrics.
1333420,15226,9099,Automatic cinemagraphs for ranking beautiful scenes,2012,"This work addresses the NHK challenge that aims at automatic recognition of beautiful scenes in broadcast programs. We propose a method that can be used to automatically extract beautiful scenes from videos and rank the scenes in terms of beauty. In particular, we introduce cinemagraphs as an alternative manner for presenting beautiful scenes, and a notion of beauty based on the presence of interesting motions. The method is fully automatic, requires no training phase, and produces beautiful scene cinemagraphs as by-products."
2184984,15226,11470,A Novel Edge Detection Framework by Component Tree Construction,2012,This paper proposes a new edge detection framework with component tree construction. This open framework is efficient for edge property computation and convenient for subsequent image processing. We detect edges according to the properties which are customized by framework rules. Experiments on using the framework for a new efficient implementation of Canny edge detector are reported. The results demonstrate that the tree construction is efficient and the framework is flexible.
2057592,15226,390,Joint 3D cell segmentation and classification in the Arabidopsis root using energy minimization and shape priors,2013,"This paper presents a discrete energy minimization approach to integrate different prior knowledge and image cues for simultaneous cell segmentation and classification. When there are multiple types of cells to segment, the segmentation of cells and the classification of the cell types are dependent on each other. The presented approach selects the optimal segmentations from hypotheses and infers the cell types in the same process. The approach is applied to the volumetric data of Arabidopsis roots."
1276273,15226,11491,Supporting browsing of user generated video on a tablet,2012,"In this demo paper, we describe our user-generated video search system, compromising of an iPad interface communicating with a remote server. The goal of this system is to provide an easy access to video content lacking textual annotations by clustering key frames. Moreover, the graphical user interface allows users to filter video content based on various semantic concepts."
2141502,15226,9099,Hierarchical video browsing with a 3D carousel,2011,We present a video browsing tool that combines advantages of the hierarchical browsing concept with 3D projection and multi-threaded programming in order to provide a convenient and efficient interface. The tool allows for instantaneous hierarchical browsing of video and uses a dynamic approach (i.e. tree of playable video segments instead of static key frames)that also supports parallel playback.
1146743,15226,9099,Eyeke: what you hear is what you see,2012,"This demonstration shows an interactive visual-to-auditory scene sensing system called Eyeke (Eye Mike), which converts visual features of what it sees into sound output aiming at helping us to interact with our surroundings. Eyeke works robustly by restricting colors in its image processing and easy-and-effective calibration. Demonstrations of Eyeke as a music instrument and a camera-based scene sonar are shown."
2324985,15226,390,The equivalence of linear spherical deconvolution and model-free linear transform methods for diffusion MRI,2013,"This work provides a theoretical analysis of linear spherical deconvolution methods in diffusion MRI, building off of a theoretical framework that was previously developed for model-free linear transforms of the Fourier 2-sphere. It is demonstrated that linear spherical deconvolution methods have an equivalent representation as model-free linear transform methods. This perspective is used to study the characteristics of linear spherical deconvolution from the point of view of the diffusion propagator. Practical results are shown with experimental brain MRI data."
1902433,15226,9099,ARA: the active reading application,2011,"The Active Reading Application (ARA) brings the familiar experience of writing on paper to the tablet. The application augments paper-based practices with audio, the ability to review annotations, and sharing. It is designed to make it easier to review, annotate, and comment on documents by individuals and groups. ARA incorporates several patented technologies and draws on several years of research and experimentation."
824653,15226,9099,AIEMPro 2011: the 4th international workshop on automated media analysis and production for novel TV services,2011,"The ACM AIEMPro 2011 workshop presents research on automated media content analysis and production for, amongst others, the development of novel TV services. The program of the workshop has two sessions. The first one is composed of three papers on video and TV content structuring and indexing. The second session is also composed of three papers on media production and retrieval systems and applications."
1688189,15226,9099,Human Computer Interface for Quadriplegic People Based on Face Position/gesture Detection,2014,"This paper proposes a human computer interface using a single depth camera for quadriplegic people. The nose position is employed to control the cursor along with the commands provided by mouth's status. The detection of nose position and mouth's status is based on randomized decision tree algorithm.The experimental results show that the proposed interface is comfortable, easy to use, robust, and outperforms the existing assistive technology."
717773,15226,21106,Dense disparity maps from sparse disparity measurements,2011,"In this work we propose a method for estimating disparity maps from very few measurements. Based on the theory of Compressive Sensing, our algorithm accurately reconstructs disparity maps only using about 5% of the entire map. We propose a conjugate subgradient method for the arising optimization problem that is applicable to large scale systems and recovers the disparity map efficiently. Experiments are provided that show the effectiveness of the proposed approach and robust behavior under noisy conditions."
2740872,15226,9804,In-home detection of distress calls: the case of aged users.,2013,"In the context of technologies development aiming at helping aged people to live independently at home, the CIRDO1 project aims at implementing an ASR system into a social inclusion product designed for elderly people in order to detect distress situations and provide capability to call for help. In this context we present a system able to detect distress and call for help sentences on line."
2029692,15226,9099,Video genre detection using a multimodality approach,2011,"In this abstract, we introduce the idea of our new multimodality approach towards video genre detection. The two novelties of this algorithm are that firstly it is based on identifying relationships between semantic concepts and genres in videos, which can later help in distinguishing videos in different genres. Secondly, we apply topic level genre detection such as a 'talk show' genre can be further categorized as 'political talk show' or 'economic talk show', etc., which is a more precise categorization."
588247,15226,9616,Exploiting subclass information in Support Vector Machines,2012,In this paper a new variation of Support Vector Machines (SVM) is introduced. The proposed method is called Subclass Support Vector Machine (SSVM) and makes use of principles from Discriminant Analysis field using subclasses. The major difference over SVM is that it takes into account the existence of subclasses in the classes and tries to minimize the distribution of the samples within each subclass. Experiments over various databases are conducted and the results are compared against other classifiers.
828362,15226,9099,OpenCast Matterhorn 1.1: reaching new heights,2011,"This paper gives a short overview of the Opencast Matterhorn system. Built by an open community of individuals and institutions, Matterhorn provides a lecture capture platform for both research and production environments. Matterhorn is comprehensive and scalable, and includes components for the acquisition, processing, and playback of content. Matterhorn is licensed under the liberal Educational Community License (ECL 2.0), a flexible OSI approved open source license, and the Opencast community is free for all institutions, corporations, or individuals to join."
2482315,15226,30,Motor Unit Number Reductions in Paretic Muscles of Stroke Survivors,2011,"The objective of this study is to assess whether there is evidence of spinal motoneuron loss in paretic muscles of stroke survivors, using an index measurement called motor unit number index (MUNIX). MUNIX, a recently developed novel neurophysiological technique, provides an index proportional to the number of motor units in a muscle, but not necessarily an accurate absolute count. The MUNIX technique was applied to the first dorsal interosseous (FDI) muscle bilaterally in nine stroke subjects. The area and power of the maximum M-wave and the interference pattern electromyogram (EMG) at different contraction levels were used to calculate the MUNIX. A motor unit size index (MUSizelndex) was also calculated using maximum M-wave recording and the MUNIX values. We observed a significant decrease in both maximum M-wave amplitude and MUNIX values in the paretic FDI muscles, as compared with the contralateral muscles. Across all subjects, the maximum M-wave amplitude was 6.4 ± 2.3 mV for the paretic muscles and 9.7 ± 2.0 mV for the contralateral muscles (p <; 0.001). These measurements, in combination with voluntary EMG recordings, resulted in the MUNIX value of 109 ± 53 for the paretic muscles, much lower than the MUNIX value of 153 ± 38 for the contralateral muscles (p <; 0.01). No significant difference was found in MUSizelndex values between the paretic and contralateral muscles. However, the range of MUSizelndex values was slightly wider for paretic muscles (48.8-93.3 μV) than the contralateral muscles (51.7-84.4 μV). The findings from the index measurements provide further evidence of spinal motoneuron loss after a hemispheric brain lesion."
1286076,15226,22130,Leveraging feature uncertainty in the PnP problem,2014,"Trabajo presentado a la 25th British Machine Vision Conference (BMVC), celebrada en Nottingham (UK) del 1 al 5 de septiembre de 2014.-- Este item (excepto textos e imagenes no creados por el autor) esta sujeto a una licencia de Creative Commons: Attribution-NonCommercial-NoDerivs 3.0 Spain."
2657742,15226,20332,An Intelligent Nutritional Assessment System,2012,"Higher life expectancies lead to an increased prevalenceof dementia in older adults, which is projected torise dramatically in the future. The link between malnutritionand dementia highlights the need to closelymonitor nutrition as early as possible. However, currentself-report assessment methods are labor-intensive,time-consuming and inaccurate. Technology has the potentialof assisting in nutritional analysis by alleviatingthe cognitive load of recording food intake and lesseningthe burden of care for the elderly. Therefore, we proposean intelligent nutritional assessment system thatwill monitor the dietary patterns of older adults with dementiaat their homes. Our computer vision-based systemconsists of food recognition and portion estimationalgorithms that, together, provide nutritional analysisof an image of a meal. We create a novel food imagedataset on which we achieve an 87.2% recognition accuracy.We apply several well-known segmentation andrecognition algorithms and analyze their suitability tothe food recognition problem."
1736689,15226,30,An Examination of the Motor Unit Number Index (MUNIX) in Muscles Paralyzed by Spinal Cord Injury,2012,"The objective of this study was to assess whether there is evidence of motor unit loss in muscles paralyzed by spinal cord injury (SCI), using a measurement called motor unit number index (MUNIX). The MUNIX technique was applied in SCI (n=12) and neurologically intact (n=12) subjects. The maximum M waves and voluntary surface electromyography (EMG) signals at different muscle contraction levels were recorded from the first dorsal interosseous (FDI) muscle in each subject. The MUNIX values were estimated using a mathematical model describing the relation between the surface EMG signal and the ideal motor unit number count derived from the M wave and surface EMG measurements. We recorded a significant decrease in both maximum M wave amplitude and in estimated MUNIX values in paralyzed FDI muscles, as compared with neurologically intact muscles. Across all subjects, the maximum M wave amplitude was 8.3±4.4 mV for the paralyzed muscles and 14.4±2.0 mV for the neurologically intact muscles (p <; 0.0001). These measurements, when combined with voluntary EMG recordings, resulted in a mean MUNIX value of 112±71 for the paralyzed muscles, much lower than the mean MUNIX value of 228±49 for the neurologically intact muscles (p <; 0.00001). A motor unit size index was also calculated using the maximum M wave recording and the MUNIX values. We found that paralyzed muscles showed a mean motor unit size index value of 80.7±17.7 μV, significantly higher than the mean value of 64.9±10.1 μV obtained from neurologically intact muscles (p <; 0.001). The MUNIX method used in this study offers several practical benefits compared with the traditional motor unit number estimation technique because it is noninvasive, induces minimal discomfort due to electrical nerve stimulation, and can be performed quickly. The findings from this study help understand the complicated determinants of SCI induced muscle weakness and provide further evidence of motoneuron degeneration after a spinal injury."
1675388,15226,22288,Towards Quality Aware Collaborative Video Analytic Cloud,2012,"As cloud diversifies into different application fields, understanding and characterizing the specific workloadsand application requirements play important roles in thedesign of efficient cloud infrastructure and system softwaresupport. Video analytic is a rapidly advancing field and it iswidely used in many application domains (i.e., health, medicalcare, surveillance, and defense). To support video analyticapplications efficiently in cloud, one has to overcome manychallenges such as lack of understanding of the relationship andtradeoff between analytic performance metrics and resourcerequirements. Furthermore, cloud computing has grown fromthe early model of resource sharing to data sharing andworkflow sharing. To address the challenges and to leverageemerging trends, we propose and experiment with a domainspecific cloud environment for video analytic applications. Wedesign a cloud infrastructure framework for sharing videodata, analytic software, and workflow. In addition, we create avideo analytic quality aware resource plan model to guaranteeusers QoS and optimize usage of resources based on predictiveknowledge of video analytic softwares performance metrics anda resource planning model that optimizes the overall analyticservice quality under users constraints (i.e., time and cost).The predictive knowledge is represented as input and analyticsoftware specific predictors. The experimental results show thatthe video analytic quality aware resource planning model canbalance the tradeoff between analytic quality and resourcerequirements, and achieve optimal or near-optimal planning forvideo analytic workloads with constraints in a resource sharedenvironment. Simulation studies show that resource planningresults using ground truth and video analytic performancepredictions are very similar, which indicates that our analytic quality/resource predictors are very accurate."
1682514,15226,20561,Personal Health Records Success: Why Google Health Failed and What Does that Mean for Microsoft HealthVault?,2014,"Five years of experimenting with Personal Health Records has not yielded the results that big companies like Google and Microsoft expected. Whereas Google pulled the plug on its product offering, Microsoft struggles to reach sufficient critical mass. This study adopts a user perspective (51 interviews) in conjunction with grounded theory, to offer explanations why Google Health failed and predictions relative to Microsoft's ability to reach a tipping point with respect to product/service viability. Noteworthy, vendors ignore relevance, or perceived usefulness when designing PHRs. Moreover, low trust and high risks do not bode well for long-term success, with the widely used information systems success models often neglecting the latter two critical dimensions."
1396640,15226,30,A Review on Digital ECG Formats and the Relationships Between Them,2012,"A plethora of digital ECG formats have been proposed and implemented. This heterogeneity hinders the design and development of interoperable systems and entails critical integration issues for the healthcare information systems. This paper aims at performing a comprehensive overview on the current state of affairs of the interoperable exchange of digital ECG signals. This includes 1) a review on existing digital ECG formats, 2) a collection of applications and cardiology settings using such formats, 3) a compilation of the relationships between such formats, and 4) a reflection on the current situation and foreseeable future of the interoperable exchange of digital ECG signals. The objectives have been approached by completing and updating previous reviews on the topic through appropriate database mining. 39 digital ECG formats, 56 applications, tools or implantation experiences, 47 mappings/converters, and 6 relationships between such formats have been found in the literature. The creation and generalization of a single standardized ECG format is a desirable goal. However, this unification requires political commitment and international cooperation among different standardization bodies. Ongoing ontology-based approaches covering ECG domain have recently emerged as a promising alternative for reaching fully fledged ECG interoperability in the near future."
2188296,15226,9773,Automatic Estimation of the Legibility of Binarised Historic Documents for Unsupervised Parameter Tuning,2011,"Document enhancement tools are a valuable help in the study of historic documents. Given proper filter settings, many effects that impair the legibility can be evened out (e.g. washed out ink, stained and yellowed paper). However, because of differing authors, languages, handwritings, fonts and paper conditions, no single filter parameter set fits all documents. Therefore, the parameters are usually tuned in a time-consuming manual process to every individual document. To simplify this procedure, this paper introduces a classifier for the legibility of an enhanced historic text document. Experiments on the binarisation of a set of documents from 1938 to 1946 show that the classifier can be used to automatically derive robust filter settings for a variety of documents."
1828554,15226,30,An Integrated Healthcare Information System for End-to-End Standardized Exchange and Homogeneous Management of Digital ECG Formats,2012,"This paper investigates the application of the enterprise information system (EIS) paradigm to standardized cardiovascular condition monitoring. There are many specifications in cardiology, particularly in the ECG standardization arena. The existence of ECG formats, however, does not guarantee the implementation of homogeneous, standardized solutions for ECG management. In fact, hospital management services need to cope with various ECG formats and, moreover, several different visualization applications. This heterogeneity hampers the normalization of integrated, standardized healthcare information systems, hence the need for finding an appropriate combination of ECG formats and a suitable EIS-based software architecture that enables standardized exchange and homogeneous management of ECG formats. Determining such a combination is one objective of this paper. The second aim is to design and develop the integrated healthcare information system that satisfies the requirements posed by the previous determination. The ECG formats selected include ISO/IEEE11073, Standard Communications Protocol for Computer-Assisted Electrocardiography, and an ECG ontology. The EIS-enabling techniques and technologies selected include web services, simple object access protocol, extensible markup language, or business process execution language. Such a selection ensures the standardized exchange of ECGs within, or across, healthcare information systems while providing modularity and accessibility."
2482127,15226,30,Interoperability of Medical Device Information and the Clinical Applications: An HL7 RMIM based on the ISO/IEEE 11073 DIM,2011,"Medical devices are essential to the practice of modern healthcare services. Their benefits will increase if clinical software applications can seamlessly acquire the medical device data. The need to represent medical device observations in a format that can be consumable by clinical applications has already been recognized by the industry. Yet, the solutions proposed involve bilateral mappings from the ISO/IEEE 11073 Domain Information Model (DIM) to specific message or document standards. Considering that there are many different types of clinical applications such as the electronic health record and the personal health record systems, the clinical workflows, and the clinical decision support systems each conforming to different standard interfaces, detailing a mapping mechanism for every one of them introduces significant work and, thus, limits the potential health benefits of medical devices. In this paper, to facilitate the interoperability of clinical applications and the medical device data, we use the ISO/IEEE 11073 DIM to derive an HL7 v3 Refined Message Information Model (RMIM) of the medical device domain from the HL7 v3 Reference Information Mode (RIM). This makes it possible to trace the medical device data back to a standard common denominator, that is, HL7 v3 RIM from which all the other medical domains under HL7 v3 are derived. Hence, once the medical device data are obtained in the RMIM format, it can easily be transformed into HL7-based standard interfaces through XML transformations because these interfaces all have their building blocks from the same RIM. To demonstrate this, we provide the mappings from the developed RMIM to some of the widely used HL7 v3-based standard interfaces."
773692,15226,30,Multichannel ECG Data Compression Based on Multiscale Principal Component Analysis,2012,"In this paper, multiscale principal component analysis (MSPCA) is proposed for multichannel electrocardiogram (MECG) data compression. In wavelet domain, principal components analysis (PCA) of multiscale multivariate matrices of multichannel signals helps reduce dimension and remove redundant information present in signals. The selection of principal components (PCs) is based on average fractional energy contribution of eigenvalue in a data matrix. Multichannel compression is implemented using uniform quantizer and entropy coding of PCA coefficients. The compressed signal quality is evaluated quantitatively using percentage root mean square difference (PRD), and wavelet energy-based diagnostic distortion (WEDD) measures. Using dataset from CSE multilead measurement library, multichannel compression ratio of 5.98:1 is found with PRD value 2.09% and the lowest WEDD value of 4.19%. Based on, gold standard subjective quality measure, the lowest mean opinion score error value of 5.56% is found."
1409798,15226,30,EMG Signal Decomposition Using Motor Unit Potential Train Validity,2013,"A system to resolve an intramuscular electromyographic (EMG) signal into its component motor unit potential trains (MUPTs) is presented. The system is intended mainly for clinical applications where several physiological parameters of motor units (MUs), such as their motor unit potential (MUP) templates and mean firing rates, are of interest. The system filters an EMG signal, detects MUPs, and clusters and classifies the detected MUPs into MUPTs. Clustering is partially based on the K-means algorithm, and the supervised classification is implemented using a certainty-based algorithm. Both clustering and supervised classification algorithms use MUP shape and MU firing pattern information along with signal dependent assignment criteria to obtain robust performance across a variety of EMG signals. During classification, the validity of extracted MUPTs are determined using several supervised classifiers; invalid trains are corrected and the assignment threshold for each train is adjusted based on the estimated validity (i.e., adaptive classification). Performance of the developed system in terms of accuracy (A e ), assignment rate (A r ), correct classification rate (CC r ), and the error in estimating the number of MUPTs represented in the set of detected MUPs (E NMUPTs ) was evaluated using 32 simulated and 30 real EMG signals comprised of 3-11 and 3-15 MUPTs, respectively. The developed system, with average CC r  of 86.4% for simulated and 96.4% for real data, outperformed a previously developed EMG decomposition system, with average CC r  of 71.6% and 89.7% for simulated and real data, by 14.7% and 6.7%, respectively. In terms of E NMUPTs , the new system, with average E NMUPTs  of 0.3 and 0.2 for simulated and real data respectively, was better able to estimate the number of MUPTs represented in a set of detected MUPs than the previous system, with average E NMUPTs  of 2.2 and 0.8 for simulated and real data respectively. For both the simulated and real data used, variations in A c , A r , and E NMUPTs  for the newly developed system were lower than for the previous system, which demonstrates that the new system can successfully adjust the assignment criteria based on the characteristics of a given signal to achieve robust performance across a wide variety of EMG signals, which is of paramount importance for successfully promoting the clinical application of EMG signal decomposition techniques."
1805581,15226,30,Synthesis of the 12-Lead Electrocardiogram From Differential Leads,2011,"A new approach is proposed for synthesizing the standard 12-lead ECG from three differential leads formed by pairs of proximal electrodes on the body surface. The method is supported by a statistical analysis that gives the best personalized positions of electrodes. The measurements from multichannel ECGs were used to calculate the differential leads. Our algorithm searches for optimal differential leads and the corresponding personalized transformation matrix that is used to synthesize the standard 12-lead ECG. The algorithm has been evaluated on 99 multichannel ECGs measured on 30 healthy subjects and 35 patients scheduled for elective cardiac surgery. It is shown that the algorithm significantly outperforms the synthesis based on the EASI lead system with medians of correlation coefficients greater than 0.954 for all 12 standard leads. To determine the optimal number of differential leads, the syntheses for two, three, and four differential leads were calculated. The analysis shows that 3 is the optimal number of differential leads for practical applications. Because of the proximity of the differential electrodes, the proposed approach offers an opportunity for the synthesis of a standard 12-lead ECG with wireless electrodes."
2011555,15226,30,Effect of Posture Change on the Geometric Features of the Healthy Carotid Bifurcation,2011,"Segmented cross-sectional MRI images were used to construct 3-D virtual models of the carotid bifurcation in ten healthy volunteers. Geometric features, such as bifurcation angle, internal carotid artery (ICA) angle, planarity angle, asymmetry angle, tortuosity, curvature, bifurcation area ratio, ICA/common carotid artery (CCA), external carotid artery (ECA)/CCA, and ECA/ICA diameter ratios, were calculated for both carotids in two head postures: 1) the supine neutral position; and 2) the prone sleeping position with head rotation to the right (~80°). The results obtained have shown that head rotation causes 1) significant variations in bifurcation angle [32% mean increase for the right carotid (RC) and 21% mean decrease for the left carotid (LC)] and internal carotid artery angle (97% mean increase for the RC, 43% mean decrease for the LC); 2) a slight increase in planarity and asymmetry angles for both RC and LC; 3) minor and variable curvature changes for the CCA and for the branches; 4) slight tortuosity changes for the braches but not for the CCA; and 5) unsubstantial alterations in area and diameter ratios (percentage changes <;10%). The significant geometric changes observed in most subjects with head posture may also cause significant changes in bifurcation hemodynamics and warrant future investigation of the hemodynamic parameters related to the development of atherosclerotic disease such as low oscillating wall shear stress and particle residence times."
1623267,15226,30,BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification,2014,"Dental biofilm is the deposition of microbial material over a tooth substratum. Several methods have recently been reported in the literature for biofilm quantification; however, at best they provide a barely automated solution requiring significant input needed from the human expert. On the contrary, state-of-the-art automatic biofilm methods fail to make their way into clinical practice because of the lack of effective mechanism to incorporate human input to handle praxis or misclassified regions. Manual delineation, the current gold standard, is time consuming and subject to expert bias. In this paper, we introduce a new semi-automated software tool, BiofilmQuant, for dental biofilm quantification in quantitative light-induced fluorescence (QLF) images. The software uses a robust statistical modeling approach to automatically segment the QLF image into three classes (background, biofilm, and tooth substratum) based on the training data. This initial segmentation has shown a high degree of consistency and precision on more than 200 test QLF dental scans. Further, the proposed software provides the clinicians full control to fix any misclassified areas using a single click. In addition, BiofilmQuant also provides a complete solution for the longitudinal quantitative analysis of biofilm of the full set of teeth, providing greater ease of usability."
1872077,15226,30,Intensive Care Window: Real-Time Monitoring and Analysis in the Intensive Care Environment,2011,"This paper introduces a novel, open-source middleware framework for communication with medical devices and an application using the middleware named intensive care window (ICW). The middleware enables communication with intensive care unit bedside-installed medical devices over standard and proprietary communication protocol stacks. The ICW application facilitates the acquisition of vital signs and physiological parameters exported from patient-attached medical devices and sensors. Moreover, ICW provides runtime and post-analysis procedures for data annotation, data visualization, data query, and analysis. The ICW application can be deployed as a stand-alone solution or in conjunction with existing clinical information systems providing a holistic solution to inpatient medical condition monitoring, early diagnosis, and prognosis."
2017847,15226,30,Nonlinear Unsharp Masking for Mammogram Enhancement,2011,"This paper introduces a new unsharp masking (UM) scheme, called nonlinear UM (NLUM), for mammogram enhancement. The NLUM offers users the flexibility 1) to embed different types of filters into the nonlinear filtering operator; 2) to choose different linear or nonlinear operations for the fusion processes that combines the enhanced filtered portion of the mammogram with the original mammogram; and 3) to allow the NLUM parameter selection to be performed manually or by using a quantitative enhancement measure to obtain the optimal enhancement parameters. We also introduce a new enhancement measure approach, called the second-derivative-like measure of enhancement, which is shown to have better performance than other measures in evaluating the visual quality of image enhancement. The comparison and evaluation of enhancement performance demonstrate that the NLUM can improve the disease diagnosis by enhancing the fine details in mammograms with no a priori knowledge of the image contents. The human-visual-system-based image decomposition is used for analysis and visualization of mammogram enhancement."
839502,15226,30,Data Interoperability and Multimedia Content Management in e-Health Systems,2012,"e-Health systems provide a collaborative platform for sharing patients' medical data typically stored in distributed autonomous healthcare data sources. Each autonomous source stores its medical and multimedia data without following any global structure. This causes heterogeneity in the underlying sources with respect to the data and storage structure. Therefore, a data interoperability mechanism is required for sharing the data among the heterogeneous sources. A proper metadata structure is also necessary to represent multimedia content in the sources to enable efficient query processing. Considering these needs, we present an interoperability solution for sharing data among heterogeneous data sources. We also propose a metadata management framework for medical multimedia content including X-ray, ECG, MRI, and ultrasound images. The framework identifies features, generates and represents metadata, and produces identifiers for the medical multimedia content to facilitate efficient query processing. The framework has been tested with various user queries and the accuracy of the query results evaluated by means of precision, recall, and user feedback methods. The results confirm the effectiveness of the proposed approach."
917224,15226,65,Early childhood education by hand gesture recognition using a smartphone based robot,2014,"We propose a light and fast hand gesture recognition method using geometric feature for a smartphone based robot and apply it to early childhood mathematics education. The feature of hand gesture is defined by the number of extrema in the plot for the distances between the center point of hand and the outer points of hand from active contour model or snakes. The region of interest (ROI) is continuously updated by Continuously Adaptive Mean Shift Algorithm (CamShift) algorithm, and the snake model is used to make the outer points sequential efficiently. A mathematics learning application for an Android OS smartphone based robot is developed using the hand gesture recognition algorithm. The experiment with Korean children (5–6 years of age) is conducted to evaluate if hand gesture based HRI could promote their mathematics learning. The result suggests that the idea of hand gesture based HRI for early childhood education is feasible and that children can learn mathematics by hand gesture based interaction with a robot."
2389650,15226,30,Noise-Assisted Data Processing With Empirical Mode Decomposition in Biomedical Signals,2011,"In this paper, a methodology is described in order to investigate the performance of empirical mode decomposition (EMD) in biomedical signals, and especially in the case of electrocardiogram (ECG). Synthetic ECG signals corrupted with white Gaussian noise are employed and time series of various lengths are processed with EMD in order to extract the intrinsic mode functions (IMFs). A statistical significance test is implemented for the identification of IMFs with high-level noise components and their exclusion from denoising procedures. Simulation campaign results reveal that a decrease of processing time is accomplished with the introduction of preprocessing stage, prior to the application of EMD in biomedical time series. Furthermore, the variation in the number of IMFs according to the type of the preprocessing stage is studied as a function of SNR and time-series length. The application of the methodology in MIT-BIH ECG records is also presented in order to verify the findings in real ECG signals."
2673736,15226,30,CIDI-lung-seg: a single-click annotation tool for automatic delineation of lungs from CT scans.,2014,"Accurate and fast extraction of lung volumes from computed tomography (CT) scans remains in a great demand in the clinical environment because the available methods fail to provide a generic solution due to wide anatomical variations of lungs and existence of pathologies. Manual annotation, current gold standard, is time consuming and often subject to human bias. On the other hand, current state-of-the-art fully automated lung segmentation methods fail to make their way into the clinical practice due to their inability to efficiently incorporate human input for handling misclassifications and praxis. This paper presents a lung annotation tool for CT images that is interactive, efficient, and robust. The proposed annotation tool produces an as accurate as possible initial annotation based on the fuzzy-connectedness image segmentation, followed by efficient manual fixation of the initial extraction if deemed necessary by the practitioner. To provide maximum flexibility to the users, our annotation tool is supported in three major operating systems (Windows, Linux, and the Mac OS X). The quantitative results comparing our free software with commercially available lung segmentation tools show higher degree of consistency and precision of our software with a considerable potential to enhance the performance of routine clinical tasks."
1854412,15226,30,Multiscale Amplitude-Modulation Frequency-Modulation (AM–FM) Texture Analysis of Ultrasound Images of the Intima and Media Layers of the Carotid Artery,2011,"The intima-media thickness (IMT) of the common carotid artery (CCA) is widely used as an early indicator of cardiovascular disease (CVD). Clinically, there is strong interest in identifying how the composition and texture of the media layer (ML) can be associated with the risk of stroke. In this study, we use 2-D amplitude-modulation frequency-modulation (AM-FM) analysis of the intima-media complex (IMC), the ML, and intima layer (IL) of the CCA to detect texture changes as a function of age and sex. The study was performed on 100 ultrasound images acquired from asymptomatic subjects at risk of atherosclerosis. To investigate texture variations associated with age, we separated them into three age groups: 1) patients younger than 50; 2) patients aged between 50 and 60 years old; and 3) patients over 60 years old. We also separated the patients by sex. The IMC, ML, and IL were segmented manually by a neurovascular expert and also by a snake-based segmentation system. To reject strong edge artifacts, we prefilter with an AM-FM filterbank that is centered along the horizontal frequency axis (parallel to the long axis of the IMC, ML, and IL), while removing the low-pass filter estimates and frequency bands with large, vertical frequency components. To investigate significant texture changes, we extract the instantaneous amplitude (IA) and the magnitude of the instantaneous frequency (IF) over each layer component, for low-, medium-, and high-frequency AM-FM components. We detected significant texture differences between the higher risk age group of >;60 years versus the lower risk age group of  ;60 groups, we found significant differences in the medium-scale IA extracted from the IMC. Between the >;60 and the 50-60 groups, we found significant texture changes in the low scale IA and high-scale IF magnitude extracted from the IMC, and the low-scale IA extracted from the IL. Also, we noted that the IA for the ML showed significant differences between males and females for all age groups. The AM-FM features provide complimentary information to classical texture analysis features like the gray-scale median, contrast, and coarseness. These findings provide evidence that AM-FM texture features can be associated with the progression of cardiovascular risk for disease and the risk of stroke with age. However, a larger scale study is needed to establish the application in clinical practice."
917277,15226,390,Segmentation of kidney in 3D-ultrasound images using Gabor-based appearance models,2014,"ABSTRACT This paper presents a new segmentation method for 3D ultrasound imag es of the pediatric kidney. Based on the popular active shape models, the algorithm is tailored to deal with the particular challenges raised by US images. First, a weighted statistical shape model allows to compensate the image variation with the propagat ion direction of the US wavefront. Second, an orientation correction approach is used to create a Gabor - based appearance model for each landmark at different scales. This multiscale characteristic is incorporated into the segmentation algorithm, creating a hierarchical approach where different appearance models are considered as the segmentation process evolves. The performance of the algorithm was evaluated on a dataset of 14 cases, both healthy and pathological, obtaining an average Dice's coefficient of 0.85, an average point -to-point distance of 4.07 mm, and 0.12 average relative volume difference. \ Index Terms ² Segmentation, Statistical Shape Model, Kidney, Hydronephrosis, Ultrasound. 1. INTRODUCTION Ultrasound (US) imaging is one of the most widely and conveniently used medical imaging methods. The non -ionizing and non -invasive properties of sonography, along with its real -time nature, safety, and relatively low cost, make US imaging especially useful in the pediatric population. Thus, renal US is one of the most common pediatric US studies, allowing to observe the state of the kidneys and the urinary tract quickly and safely. In particular, the most common abnormal finding in these studies is hydronep hrosis, the dilation of the renal pelvis and calyces due to obstruction of the urinary tract, affecting 2 -2.5% of children [1]. In this context, a n early diagnosis is very important in order to distinguish those kidneys that require surgery from those that do not. Additionally, the accurate parameterization and segmentation of the kidney anatomy plays an important role in the diagnosis of renal diseases [2], [3] and intervention planning. Although the quality of US images has increased in recent years, they still suffer from low signal -to-noise ratio, speckle, signal attenuation and dropout, and missing boundaries due to the orientation dependence of acquisition. These peculiarities make the detection of organs and object of interest form US images particula rly challenging, even when performed manually by a trained expert [4]. The improvement in image quality achieved in recent years has led to an increasing interest in developing new segmentation methods for sonographic images [5]. However, unlike other appl ication areas like echocardiography, or transrectal ultrasound (TRUS), kidney segmentation from renal US has received limited attention from the scientific community. To the best of our knowledge, only a few kidney segmentation approaches have already been reported [6] -[8], most of them focused exclusively on 2DUS. Xie et al [6] presented a 2D segmentation method based on texture and shape priors. From the set of features extracted by means of a Gabor filter bank, they create a texture model using an expect ation -maximization Gaussian -mixture approach, and a shape model to improve the robustness of the method. Though additional experiments with other type of images are provided, the validation of the method was limited. More recently, the work presented by Me ndoza et al. [8 ] takes into consideration the image formation process in US to combine the traditional Active Shape Models (ASM) [9] with a novel texture orientation correction. However, in spite of the promising results reported, the method is only app lied over 2D longitudinal kidney sections, which must be manually selected by an expert radiologist. Martin -Fernandez and Alberola -Lopez [7] proposed a segmentation approach for the kidney in 3DUS images using a probabilistic Bayesian method. However, the slices of the 3D volume are processed independently, trying to identify the kidney contour on each 2D image, and thus, loosing relevant information about the true 3D shape of the organ. Second, the significant user intervention required to adjust the templ ate reduces the automation and the reproducibility of the method. In this paper we present a new variant of the classic ASM, Gabor -based Appearance Models (GAM). This new approach addresses the inherent limitations of the original ASM when dealing with 3DU S renal images. First, a weighted statistical shape model correction compensates the image dependency with the propagation direction of the US wavefront. Second, a new multiscale Gabor based texture model is incorporated in the algorithm, reducing the spec kle noise effect and allowing to detect the kidney contours at 978-1-4673-1961-4/14/$31.00 ©2014 IEEE 633"
1678938,15226,9099,Modeling tagged photos for automatic image annotation,2011,"A semantic concept can be a physical object (e.g., 'car', 'zebra-fish'), an activity (e.g., 'demonstration', 'running') or an obscure category (e.g., 'historical', 'autumn'). In automatic image annotation, machines are taught to infer such semantic concepts by training with hundreds of manually selected images that contain those concepts. So far, remarkable progress has been made in the areas of visual feature extraction and machine learning algorithms that relate features to concepts. Now the new challenge is to scale the inference and annotation capacity to thousands of semantic concepts in the real world.   The key to large-scale machine annotation lies in automatic training data selection from user-tagged photos shared on social websites. To learn an annotation model for a concept such as 'car', one can exploit the fact that 'car' is also a common tag associated with photos of day-to-day activities. We may consider the concept to be illustrated by the collection of all photos thus annotated. The photos can be retrieved through a text-search interface and readily used to train the annotation model. While this seems like a straightforward idea, a few ramifications have to be considered:   Substandard images: Tagging is an uncontrolled activity not geared towards scientific computation, but guided by personal motivations and communal influences. Tags may be incorrect or incomplete and some images may be poor examples of the target concept. Such substandard training images may hamper the annotation performance.   Modeling constraints: As tags as well as visual features determine the quality of training data, a joint analysis of visual-textual features is necessary. Additionally, a diverse set of features need to be effectively combined even if some features are incapable of modeling specific concepts in the large annotation vocabulary. Efficient computing techniques and infrastructure is necessary to maintain scalability and adaptability to new concepts and training data.     We recently surveyed the application of tagged photographs for image annotation [2]. The literature was organized to address four main annotation types: (a) general-purpose concepts, (b) names of people, (c) names of locations, and (d) events. An important observation was that a majority of studies resorted to semi-supervised learning to annotate an unlabeled (or partially labeled) image. Specifically, content based retrieval techniques were used to identify similar images and a tag-ranking model was developed to transfer the annotations of the retrieved results on to the query image. Such data-driven techniques can potentially access an arbitrarily large annotation vocabulary. However, they do not conform to the idea of model-based vision and only work if large labeled image sets can be analyzed at run-time.   We adopted a supervised learning approach to large-scale image annotation where training data was selected from Flickr images [3]. The selection process was designed to reject substandard images. Annotation models were trained and stored for 1000 words, so that only pixel information of test images needed to be analyzed at run-time. The time required to select training data for a single concept from 10,000 images (634-dimensional feature) using a single CPU of 2.66 GHz speed and 24.4 GB memory was under 5 minutes. Also, the results as compared to a state-of-the-art annotation system showed marked diversity and accuracy.   The supervised annotation approach can only predict the concepts used in its training. In this sense, the approach does not cater to the preferred vocabularies of users. To address this issue, we developed a personalized tagging extension [1]. We proposed a transfer learning model to translate the set of machine annotations to a user's vocabulary using a Naive Bayes formulation. The highlight of the technique was the computation of the translation model from the collective tagging behavior of the user's local social network.   In conclusion, the proposed research harnesses user-tagged images and social interactions on photo sharing websites to develop a practical image annotation system."
1483872,15226,390,Automated cell junction tracking with modified active contours guided by SIFT flow,2014,"AUTOMATED CELL JUNCTION TRACKING WITH MODIFIED ACTIVE CONTOURS GUIDED BY SIFT FLOW Chen-Yu Lee, Sukryool Kang, Andrew D. Chisholm † , and Pamela C. Cosman Department of Electrical and Computer Engineering Division of Biological Sciences, Section of Cell and Development Biology University of California at San Diego, La Jolla, CA, 92093-0407, USA ABSTRACT We present a novel algorithmic approach to track multiple cell junctions automatically in the developing epidermis of the C. elegans embryo. 3D cell boundaries are projected into 2D for segmentation using active contours with a non-intersection force, and subsequently tracked using SIFT (Scale-Invariant Feature Transform) flow. Our method achieves MAD (Mean Absolute Distance) less than 3 pixels between all tracked cell contours and ground truth data. Using this method we have generated a quantitative description of epidermal cell move- ments and shape changes during the process of epidermal en- closure. Fig. 1: Dataset snapshots. Columns 1 to 6 show Z-stack im- ages for slices 1,7,13,19,25, and 31. Last column shows max- imum projection images. Each row represents acquired data at one time point. Images are inverted for display. quires cell contour signals on the embryo surface. Active con- tours (also called snakes) [5] with a proposed non-intersection force can precisely segment all epidermal cells in 2D maxi- mum projection images. To handle large displacement of cell movement, we conduct experiments using conventional opti- cal flow and SIFT (Scale-Invariant Feature Transform) flow [6]. Experimental results show our modified active contours with SIFT flow can accurately track epithelial junctions. Our methods yield a quantitative description of the dynamics of epithelial shape changes during epidermal enclosure. Index Terms— C. elegans, cell tracking, SIFT flow 1. INTRODUCTION Quantitative analysis of cell shape in live samples is an im- portant goal in developmental biology. The nematode worm Caenorhabditis elegans is an excellent organism for analyz- ing fundamental aspects of development because it is simple, easy to grow in bulk populations, and convenient for genetic analysis. C. elegans has a fixed number of cells in early em- bryogenesis, all of which have been individually identified and characterized. We are interested in epidermal develop- ment as a model for epithelial morphogenesis. Tracking cells or subcellular structures in developing em- bryos is important to understand developmental processes. Computer aided tracking allows quantitative analysis of large numbers of cells or objects. Recently, several automated or semi-automated nuclei tracking algorithms [1][2][3][4] that allow quantitative analysis of nuclear positions in C elegans have been developed. However, nuclear positions do not pro- vide direct information on cell shape, size, or cellular con- tacts. A major remaining challenge is to segment and track cell surfaces or contacts in complex 4D data (Figure 1). The difficulty of tracking cells on surfaces lies in lack of image texture or color information. The recorded data only contains cell boundaries along the surface. In this paper, we present a new method to automatically segment and track epithelial junctions in 4D data that only re- U.S. Government work not protected by U.S. copyright 2. DATA ACQUISITION The C. elegans embryo consists of 24 epidermal cells on the surface. We use a Zeiss LSM700 to acquire Confocal Laser Scanning Microscope (CLSM) cell images over 20 minutes to capture the closing of the epidermal cells. Epidermal junc- tions visualized with DLG-1::GFP form lines at the subapical circumference of differentiated epidermal cells. By changing the focal length, our dataset contains 35 Z-stack image slices that cover the whole C. elegans embryo. Columns 1 to 6 in Figure 1 show the Z-stack snapshots for slices 1, 7, 13, 19, 25, and 31. However, Z-stack images are not informative for visu- alizing and tracking cell contours. We use a maximum projec- tion image as our input data. The pixel intensity value of each pixel is selected from the maximum pixel value through all Z-stack images. The maximum projection image can better visualize epidermal cells of the C. elegans embryo as shown in the last column of Figure 1."
910782,15226,390,Transfer learning of tissue photon interaction in optical coherence tomography towardsin vivo histology of the oral mucosa.,2014,"ABSTRACTOral cancer evolves from different premalignant conditionsand the key to save lives is through diagnosis of early symp-toms. The conventional practice of post biopsy histopathol-ogy reporting is dependent on specicity of sampling site andoptical coherence tomography (OCT) imaging is clinicallyused for guidance. Clinicians infer the tissue constitutionby interpreting intensity images and are challenged by inter-and intra-observer variability. In this paper we propose trans-fer learning of tissue specic photon interaction statisticalphysics in swept-source OCT for characterizing the oral mu-cosa with the aim of reducing this reporting variability. Thesource task models statistical physics of ballistic and near-ballistic photons and its intensity attenuation and target tasklearns the parameters obtained by solving the source taskto identify co-located heterogeneity of tissues. Performanceis compared with conventional histopathology of healthy,premalignant and malignant oral lesions supporting its usetowards in vivo histology of the oral mucosa for pre-biopsyscreening.Index Terms  Optical coherence tomography, tissuecharacterization, transfer learning, in vivo histology, oralcancer.1. INTRODUCTIONMalignancy of the oral cavity is globally the sixth largestcause of cancer related deaths. It is a complex multistageprocess where oral squamous cell carcinoma (OSCC) mayevolve from different premalignant disorders (PMD) like oralsub-mucous brosis (OSF), oral leukoplakia (OLK) and orallichen planus (OLP) and others including the healthy normaloral mucosa (NOM). These PMDs are histopathologicallydistinct among themselves and also distinct from OSCC [1].Histopathological examination is a conclusive practice foridentifying the stage of tissue abnormality. However, re-porting ambiguity arises when different types of PMDs arelocated in a neighborhood and biopsy samples are not col-lected from appropriately representative regions of such aheterogeneous lesion [2]. Image guided sampling is oftenpracticed to address this limitation, where subsurface imag-ing techniques like ultrasonography [3] or optical coherencetomography (OCT) [4] are clinically employed for investigat-ing their pathological diversity in situ prior to biopsy. Sinceboth of these modalities provide only intensity images thatcan be reported by specially skilled clinicians, it restrictstheir seamless inclusion in regular clinical workow sincemost oral pathologists are not experienced to interpret them.The primary challenge is to develop tissue characterizationtechniques for realizing in vivo histology that would enablepre-biopsy screening thus increasing the sampling specicityin heterogeneous lesions.Related Work: Tissue characterization techniques forOCT have been under development in recent past for identi-fyingheterogeneoustissuesinatheroscleroticplaquesimagedwith intravascular OCT [58]. Methods include characteriza-tion using estimated tissue specic optical attenuation [5,6],local texture descriptors and distribution analysis of OCTspeckles [7]. Since these fail to identify co-located tissueheterogeneity, we have recently demonstrated in situ histol-ogy method in mice skin through transfer learning of tissuephoton intereaction statistical physics [8].Challenge: The variation in speckle intensity for eachtype of tissue as visible in OCT is primarily on account of theuncertainty associated with backscattering from co-locatedheterogeneous tissues and the heterogeneous nature of thetissues through which the photons traverse. The challenge isto develop in vivo tissue characterization method for the oral978-1-4673-1961-4/14/$31.00 ©2014 IEEE 1389"
1014083,15226,9099,From multimedia data to situation detection,2011,"We are witnessing a phenomenal increase in multimodal human and device sensing to measure and report parameters such as temperature, vehicle speed, visual experiences, flu cases, and people happiness. Soon we expect these heterogeneous datasets (e.g. images, videos, weather sensors, check-ins and tweets) to become available in real-time in the Cloud for reasoning and decision making.   Important human decisions however cannot be undertaken by piecemeal treatment of these individual data points. Rather we need computational tools to integrate and abstract these data points into higher level actionable representations.   This underscores the need for computational tools to model and detect situations from large heterogeneous spatio-temporal data sets. In this thesis, we computationally define the notion of situations and propose a methodology to bridge the semantic gap between the widely available low level spatio-temporal data and actionable situation inference needed for decision making.   We define a situation as: An actionable abstraction of observed spatio-temporal descriptors.   This definition underscores our viewpoint of computationally defining situations based on statistical descriptors (as opposed to say situation-calculus or recognition-by-parts), a focus on spatio-temporal data (which is indeed the most common connotation associated with situations), scoping of problem only to observable (via human/device sensors) data, and a focus on actionable abstractions (as defined explicitly by human domain experts).   The problem of modeling and detecting situations from (STT) i.e. spatio-temporal-thematic data is relevant in multiple domains like traffic, weather, healthcare, business analysis, emergency response, and political decision making.   Situation Modeling STT data spreads across very disparate application domains as well as data types. However, focusing on the commonalities and not the differences, we realize that there is a core set of operations which is central to defining spatio-temporal situations across different applications. Once a domain expert defines a situation of interest (e.g. a 'flu pandemic', 'hurricane advise') based on data sources, core operations, and user parameters, the same situation model can act as a standing query on realtime data streams and provide 'mass-personalization' to billions of end-users.   Just like E/R modeling, or UML we merely provide the basic building blocks. It is each domain expert's responsibility to define actionable situations by combining these building blocks. These building blocks are designed to be computable, modular and explicit and hence translatable into executable code once the modeling is complete.   Approach Our approach for integrating and characterizing heterogeneous spatio-temporal data is based on the concept of social pixels. We simply organize spatio-temporal values related to any theme on a two dimensional data grid. Such a grid provides heat-map like intuitive visualization, and also an image like computational data structure. Hence multiple spatio-temporal situational descriptors can be implemented as off-the-shelf image and video processing operations (Refer Fig 1).   Current status We have made progress in terms of identifying the generic set of situation detection operations [1]. We have run multiple experiments with STT data sets to answer situational queries like 'what recommendation to give to user indicating flu-like symptoms' [2] and 'where to open a new iphone store'[1]. We are currently implementing the core STT analysis engine which will allow modeling and detection of multiple situation queries across applications. We are also finalizing a methodology to guide domain experts when they model situations in terms of building blocks like data sources, characterizations operators, and user parameters."
773926,15226,390,GENERALIZED HARDI INVARIANTS BY METHOD OF TENSOR CONTRACTION.,2014,"Invariants play an important role in diffusion MRI (dMRI). They represent tissue properties, such as diffusion anisotropy, and are used for registration, tissue segmentation and classification, as well as white matter integrity measures in clinical studies of debilitating brain diseases. Importantly, these features need to be rotation invariant to capture orientation independent tissue properties, as well as to enable comparisons across images that are not completely aligned. Modelling diffusion using second-order tensors, as in DTI, enable construction of diverse anatomically meaningful scalars, such as fractional anisotropy (FA), mean diffusivity (MD), and relative anisotropy (RA), which capture tissue microstructure, and are used to indicate anatomical changes [1]. Most of these scalars are derived using the tensor eigenvalues, hence, they are naturally rotation invariant as the eigenvalues remain intact under rotation. In HARDI, however, the situation is more complicated as the diffusion profile is described by higher-order tensors or orientation distribution functions (ODFs), and there is no straightforward generalization of DTI invariants to these models. The generalized FA (GFA), for example, aims to represent anisotropy in HARDI models, but has limited classification power, and is sensitive to noise being directly computed from a discrete representation of orientation distribution function (ODF) [2].#R##N##R##N#In order to take advantage of the enhanced modelling capabilities of HARDI compared to DTI, it is important to derive new invariants that capture tissue properties, and can be used as white matter biomarkers. Over the last years, researchers have created new invariants for HARDI-based models, for example, the generalized anisotropy (GA) and scaled anisotropy (SE) [3], as well as several approaches that are based on second and fourth-order tensor representations [4, 5]. A recent approach uses the Gaunt coefficients to construct invariants for ODFs and HARDI signals using the more general SH representation [2].#R##N##R##N#Our proposed technique follows a similar path of using the SH representation. We then leverage the idea of invariants constructed by tensor contraction used in computer vision for 3D pattern recognition [6]. The original idea emerged from the theory of angular momentum addition in quantum physics, and although it relies on deep and complex theoretical foundations, the formulation enables systematic construction of invariants in an elegant and simple way. This method is general, as it enables extraction of invariants from any 3D object represented as a SH series. Therefore, it can be used to construct invariants from the dMRI signal, or from any diffusion modelling object, such as ODF or FOD. In addition, it enables direct construction of invariants for any expansion order, thus, it is not limited to the common second or fourth order expansions. This method generalizes the SH descriptors used in [7] to classify autism spectrum disorder (ASD) patients and controls, and to segment brain tissue [8, 9]. It is based on constructing contravariant rank-1 tensors (vectors) using the SH and Clebsch-Gordan (CG) coefficients, and contracting them with covariant vectors to obtain rank-0 tensors (invariants). This process can be continued repeatedly to build as many invariants as desired regardless of the SH order, therefore, enables the construction of long feature vectors with strong classification capabilities. This is an advantage over the method presented in [2] in which the maximal number of invariants is bounded by the rank of the Toeplitz-like matrix.#R##N##R##N#We demonstrate the strengths of our approach in both synthetic and in vivo experiments. Using simulated data we show that these invariants are robust to noise and can classify voxels based on the number of fiber compartments and their diffusivities. Using in vivo brain data, we show that they capture anatomically meaningful information, and may be used as white matter integrity measures."
2363012,15226,9099,Building low-latency remote rendering systems for interactive 3D graphics rendering on mobile devices,2011,"The recent explosion of mobile devices is changing people's computing behaviors and more and more applications are ported to mobile platforms. However, some applications, such as 3D video tele-immersion and 3D video gaming that require intensive computation or network bandwidth are not capable of running on mobile devices yet. Remote rendering is a simple but effective solution. A workstation with enough computation and network bandwidth resources (e.g., cloud server) is served as the rendering server. It receives and renders the source contents (e.g., 3D graphics or 3D video), and sends the rendering results (2D images) to one or multiple clients. The client simply receives and displays the result images. Using remote rendering for 3D game or 3D video rendering on mobile devices can solve both computation and bandwidth problems.   However, there are some issues with remote rendering 3D video/3D games for mobile devices: (1) Interaction Latency: The interaction latency is defined as the time from the generation of user interaction request till the appearance of the first updated rendering frame on the mobile client. In our application scenario, the interaction latency is mainly determined by the round trip time of wireless networks. (2) Bandwidth: Since we are considering the mobile devices using wireless mobile networks as the client in our remote rendering system, the available network bandwidth is very limited compared with the wired networks. The system design should consider the bandwidth limitation and try to minimize the network bandwidth usage. (3) Real Time: There are real-time requirements in our system design. The rendering rate of 3D video is determined by the recording 3D camera and the frame rate of 3D gaming depends on the game motion. The processing operations of every frame should be completed before the start of next frame.   My researches propose novel remote rendering designs to enhance the interactive experience of 3D graphics rendering on mobile devices by reducing the interaction latency. Different from conventional approaches, the proposed system applies no restriction on the network latency to provide low latency rendering services. Instead, the rendering server sends an image-based representation of the current 3D scene/model to the client. Once any viewpoint change interaction happens, the mobile client can directly synthesize the appropriate image using image-based rendering techniques. The research problems can be summarized as the follows:   What image-based representation of the 3D scene/model should the rendering server generate in order to reduce interaction latency?   Given the real-time requirement, how does the rendering server generate the image-based representation of every frame efficiently?   What encoding scheme can be applied to compress the generated image-based representation to meet the limited wireless bandwidth requirement?   How can the remote rendering system with latency reduction enhancements be evaluated effectively?     This research started in 2009 and progresses of all topics have been made. For problem (1), our full paper in MM'091 proposed to generate two depth images as the image-based representation and use 3D image warping algorithm to synthesize the view at the new rendering viewpoint on the mobile client. For problem (2), our full paper in MM'10 introduced several computation efficient algorithms in how to select reference frames for 3D image warping. For problem (3), we have studied a novel video coding method by integrating H.264/AVC together with 3D image warping algorithm and this new coding method can potentially beat the state of art x264 in terms of compression efficiency in the scenario of real-time 3D game video encoding. The full paper with the latest research results has been accepted by MM'11. For the last problem, we developed a new metric: DOL (Distortion Over Latency) to evaluate the interactive performance of remote rendering systems by combining both latency and rendering quality in one score and the paper has been presented in ICME'11."
883358,15226,22130,Using Richer Models for Articulated Pose Estimation of Footballers,2012,"Many computer vision tasks such as object detection, pose estimation,and alignment are directly related to the estimation of correspondences overinstances of an object class. Other tasks such as image classification andverification if not completely solved can largely benefit from correspondenceestimation. This thesis presents practical approaches for tackling the corre-spondence estimation problem with an emphasis on deformable objects.Different methods presented in this thesis greatly vary in details but theyall use a combination of generative and discriminative modeling to estimatethe correspondences from input images in an efficient manner. While themethods described in this work are generic and can be applied to any object,two classes of objects of high importance namely human body and faces arethe subjects of our experimentations.When dealing with human body, we are mostly interested in estimating asparse set of landmarks – specifically we are interested in locating the bodyjoints. We use pictorial structures to model the articulation of the body partsgeneratively and learn efficient discriminative models to localize the parts inthe image. This is a common approach explored by many previous works. Wefurther extend this hybrid approach by introducing higher order terms to dealwith the double-counting problem and provide an algorithm for solving theresulting non-convex problem efficiently. In another work we explore the areaof multi-view pose estimation where we have multiple calibrated cameras andwe are interested in determining the pose of a person in 3D by aggregating2D information. This is done efficiently by discretizing the 3D search spaceand use the 3D pictorial structures model to perform the inference.In contrast to the human body, faces have a much more rigid structureand it is relatively easy to detect the major parts of the face such as eyes,nose and mouth, but performing dense correspondence estimation on facesunder various poses and lighting conditions is still challenging. In a first workwe deal with this variation by partitioning the face into multiple parts andlearning separate regressors for each part. In another work we take a fullydiscriminative approach and learn a global regressor from image to landmarksbut to deal with insufficiency of training data we augment it by a large numberof synthetic images. While we have shown great performance on the standardface datasets for performing correspondence estimation, in many scenariosthe RGB signal gets distorted as a result of poor lighting conditions andbecomes almost unusable. This problem is addressed in another work wherewe explore use of depth signal for dense correspondence estimation. Hereagain a hybrid generative/discriminative approach is used to perform accuratecorrespondence estimation in real-time."
1336478,15226,22130,Multi-view body part recognition with random forests,2013,"Many computer vision tasks such as object detection, pose estimation,and alignment are directly related to the estimation of correspondences overinstances of an object class. Other tasks such as image classification andverification if not completely solved can largely benefit from correspondenceestimation. This thesis presents practical approaches for tackling the corre-spondence estimation problem with an emphasis on deformable objects.Different methods presented in this thesis greatly vary in details but theyall use a combination of generative and discriminative modeling to estimatethe correspondences from input images in an efficient manner. While themethods described in this work are generic and can be applied to any object,two classes of objects of high importance namely human body and faces arethe subjects of our experimentations.When dealing with human body, we are mostly interested in estimating asparse set of landmarks – specifically we are interested in locating the bodyjoints. We use pictorial structures to model the articulation of the body partsgeneratively and learn efficient discriminative models to localize the parts inthe image. This is a common approach explored by many previous works. Wefurther extend this hybrid approach by introducing higher order terms to dealwith the double-counting problem and provide an algorithm for solving theresulting non-convex problem efficiently. In another work we explore the areaof multi-view pose estimation where we have multiple calibrated cameras andwe are interested in determining the pose of a person in 3D by aggregating2D information. This is done efficiently by discretizing the 3D search spaceand use the 3D pictorial structures model to perform the inference.In contrast to the human body, faces have a much more rigid structureand it is relatively easy to detect the major parts of the face such as eyes,nose and mouth, but performing dense correspondence estimation on facesunder various poses and lighting conditions is still challenging. In a first workwe deal with this variation by partitioning the face into multiple parts andlearning separate regressors for each part. In another work we take a fullydiscriminative approach and learn a global regressor from image to landmarksbut to deal with insufficiency of training data we augment it by a large numberof synthetic images. While we have shown great performance on the standardface datasets for performing correspondence estimation, in many scenariosthe RGB signal gets distorted as a result of poor lighting conditions andbecomes almost unusable. This problem is addressed in another work wherewe explore use of depth signal for dense correspondence estimation. Hereagain a hybrid generative/discriminative approach is used to perform accuratecorrespondence estimation in real-time."
1658854,15226,9616,Ranking images based on aesthetic qualities.,2014,"The qualitative assessment of image content and aesthetic impression is affected by various image attributes and relations between the attributes. Modelling of such assessments in the form of objective rankings and learning image representations based on them is not a straightforward problem. The criteria can be varied with different levels of complexity for various applications. A highly-complex problem could involve a large number of interrelated attributes and features alongside varied rules. An example of such an application is fashion-interpretation. In this case one can use attribute recognition to label different parts such as clothing and body shape automatically. Thus, the presence or absence of objects in the image is not ambiguous and a similarity measure can be established between images. It is however not clear how to establish such measure between the aesthetic impressions the images make. #R##N##R##N#As a first contribution an approach for ranking images by pooling from the knowledge and experience of crowdsourced annotators is presented. Specifically, the highly subjective and complex problem of fashion interpretation and assessment of aesthetic qualities of images is addressed. To utilize the visual judgements, a novel dataset complete with labellings of various attributes of clothing and body shapes is introduced. Large scale pairwise comparisons of the order of tens of thousands are performed by annotators who follow fashion. Various consistency measures are then applied to verify agreement and correlation between the annotators to rule out inconsistencies amongst them. Based on the annotations, reliable rankings to automatically compare images according to fashion rules are established. #R##N##R##N#Then, Bag of Visual Words object recognition is used to perform classification of the attributes. By incorporating annotator rankings from the first stage and these classification estimates in a lookup model for automatic assessment of images, pairwise comparisons can be automatically performed. Each visual attribute of clothing and body shape is represented within the rankings. #R##N##R##N#Next, rankings obtained from the crowdsourcing procedure are included within several matching approaches to achieve a matching-based ranking. Nearest neighbour matches can be found for a pair of test images which can be compared with their rankings from the annotators. This can be utilized to establish which configuration is ranked better in an image pair. In particular, two prominent approaches of Bag of Visual Words and Local Descriptor Matching are employed to facilitate an evaluation. Several random splits of the dataset proposed in the first stage are used to form the training and test sets. Matches obtained are incorporated within an approach introduced to generate a global ranking. Evaluation from this stage is used as a comparative basis for the approach proposed next in which a learning procedure based on graphical modelling captures the annotator rankings.#R##N##R##N#Finally, a novel approach for learning image representation based on qualitative assessments of visual aesthetics is proposed. It relies on a multi node multi-state model that represents image attributes and their relations. The model is learnt from pairwise image preferences provided by annotators. To demonstrate the effectiveness the approach is applied to fashion image rating, i.e., comparative assessment of aesthetic qualities. The attributes and their relations are assigned learnt potentials which are used to rate the images. Evaluation of the representation model has demonstrated a high performance rate in ranking fashion images."
1528097,15226,390,MRI biomarkers in neurodegenerative disorders,2012,"The neurodegenerative disorders all share a broad common mechanism: accumulation of abnormal protein within the nervous system leads to excess neuronal loss. Alzheimer's disease (AD), the prototypic neurodegenerative disease, is associated with accumulation of amyloid plaques and neurofibrillary tangles composed of phosphorylated tau which preceded excess neuronal loss within specific brain regions, notably the medial temporal lobes. Other neurodegenerative disorders (including frontotemporal dementia and dementia with Lewy bodies) are associated with deposition of different proteins, and different patterns of regional brain loss. Cross-sectional neuroimaging is widely used for diagnostic purposes and research, with MRI (and more recently PET) being used as inclusion/exclusion criteria and safety outcome measure for clinical trials. Serial MRI is widely used to assess the extent and pattern of progression in neurodegenerative diseases both for observational and therapeutic studies. In observational studies, longitudinal MRI-based measures of brain volume change (reflecting cerebral atrophy) are used to provide insights into the timing, extent and correlates of neurodegeneration. For clinical trials serial MRI based measures of atrophy are widely used as outcome measures, both to distinguish symptomatic from disease-modifying drug effects — under the premise that the latter but not the former will lead to attenuation of atrophy rate in the treated group compared to placebo — and to reduce sample size requirement. If atrophy based measures are to be used as outcome measures, it is essential that image acquisition is carefully planned and undertaken to provide appropriate tissue classification and stability over time. Thereafter, it is over to the image analyst. In this talk I will discuss the enduser requirements of an image-analysis tool, focusing on the need for clinicians and image analysts to work closely together to ensure that any such techniques produce not only robust but also clinically relevant results. In the absence of a gold standard means of assessing the validity of any methods of determining brain volume change over time, safeguards to help provide reassurance will be discussed, including the use of simulated atrophy; comparison with manual techniques; and ensuring techniques are commutative/symmetric and transitive, reproducible and biological plausible. The importance of striving to reduce variance — the key factor influencing sample size — will be emphasized, with the caveat that the most that can be asked of any image analytical tool is to reduce within subject variability. The advantages and pitfalls of using sample size reduction per se as a criterion for comparing techniques will also be covered, with particular regard to data produced from the Alzheimer's disease Neuroimaging Initiative."
764099,15226,390,Quantitative imaging biomarkers in neurologic disease: Population study perspective,2012,"The capacity of recognizing the first signs of disease has enormous socio-economic benefits. Population studies have the potential to see disease develop before your eyes, and when including advanced imaging techniques in these studies, literally so. Population imaging studies, especially when complemented with other biomedical and genetic data, provide unique databases that can be exploited with advanced analysis and search techniques for discovering methods for early detection and prediction of disease. This new way of medical research will have considerable impact in the practice of medicine at large. In this presentation we will focus on the development of quantitative imaging biomarkers in neurology using imaging data acquired in a population setting. Currently, effective treatment strategies are lacking in e.g. dementia and stroke. In order to develop such strategies, improved understanding of the early, preclinical stages, of disease, is essential. Quantitative imaging biomarkers for neurologic disease are developed within the context of the Rotterdam Study, a prospective population based study of the causes and determinants of chronic diseases in the elderly that was initiated in 1995. MR brain imaging was performed during this study in random subsets in 1995 and 1999, and since 2005, MR brain imaging is part of the core protocol of the Rotterdam Study. The large scale acquisition of MR brain imaging within the Rotterdam Study allows us to study whether morphologic brain pathology is already present years before clinical onset of neurologic disease, and whether MRI based measurements may be used for prognosis. More information on the Rotterdam Scan Study can be found in [1]. Within the context of the Rotterdam Scan Study, a standardized and validated image analysis workflow is being developed to enable the objective, accurate, and reproducible extraction of relevant parameters describing brain anatomy, possible brain pathologies, and brain connectivity from multispectral MRI data. Image processing in the Rotterdam Scan Study has four main goals: First, owing to the sheer size and complexity of the imaging database being generated, automation of the tedious task of manual analysis is required. Second, qualitative image assessment should be replaced by objective quantitative analyses as much as possible. Third, we aim to limit or avoid altogether inter- and intraobserver variability. Fourth, image processing allows the extraction of relevant image-derived parameters that would not be feasible manually or cannot be assessed visually. This presentation will provide an overview of different quantitative imaging biomarkers that have been developed, or are currently developed as part of the Rotterdam Scan studies. These include brain tissue quantification (grey matter, white matter, also quantified per lobe), quantification of cerebrospinal fluid, volume and shape of neurostructures such as the hippocampus, ventricles and cerebellum, brain connectivity based on diffusion tensor MRI, and vascular brain pathologies such as white matter lesions and microbleeds."
932120,15226,22035,Beauty is in the scale of the beholder: Comparison of methodologies for the subjective assessment of image aesthetic appeal,2014,"ABSTRACT A first step towards creating automatic measures of image aesthetic appeal is understanding its appreciation via subjective testing. Nevertheless, reliably setting up such tests appears to be challenging, as aesthetic appeal is proven to be influenced by a number of subjective factors. In this paper we investigate four scale types for aesthetic appeal rating and assess their ability to provide general quantification of aesthetic appeal, as well as repeatable judgment across experiments. We asked 24 users to assess a representative image set constructed to uniformly cover a wide range of aesthetic appeal. Our experiments show that the Absolute Category Rating (ACR) 5-point scale provides the most consistent ratings across participants, which are also repeatable across different experiments. Index Terms— Image Aesthetic appeal, subjective rating, subjective quality assessment, Quality of Experience 1. INTRODUCTION Understanding how users judge image beauty has increasingly become of interest to researchers, since aesthetic appeal is viewed as one of the key aspect contributing to a user’s overall Quality of Experience (QoE) [1]. Being able to automatically predict the aesthetic appeal of images would be beneficial for a number of applications, such as content management, recommendation and retrieval in multimedia systems [2, 3], as well as visual quality optimization [1]. Studies in computational aesthetics typically approach the problem of predicting image aesthetic appeal from the machine learning viewpoint, by selecting features of images that characterize their aesthetic properties and then mapping them into scores representing the degree to which an image is considered to be beautiful [4-7]. In order to train and assess the performance of these predictors, ground truth scores need to be generated first. They are either collected from ratings on photo sharing websites [4, 5], or from user tests where participants are asked to quantify the aesthetic appeal of the images under investigation [6, 7]. Generating the ground-truth scores is, however, not a straightforward task. The scores should not only reliably represent the perceived appeal by test users, but also be robust to variations in the test user group, image data serving as stimuli and the context in which the scores are generated. When it comes to aesthetic appeal, obtaining a general agreement between users in the assessment of images is challenging. Studies suggest that a person’s appreciation of an image is affected by the individual itself, e.g. by the person’s implicit experiences and situational demands [8]. Nevertheless, research on empirical aesthetics shows that in several cases consistent intra- and inter-rater judgments can be obtained [9]: despite the subjectivity of aesthetic appeal, there is still accord on what is generally considered beautiful and what is not. For collecting ground truth aesthetic appeal scores, it would be preferable to use scoring methodologies that emphasize this agreement and favor the minimization of individual differences and maximize user agreement. Furthermore, subjective assessment of aesthetic appeal should be repeatable: the judgment of a same image should be similar across user samples and experimental conditions (e.g., when performed in a controlled lab or in a crowdsourcing environment). So far, a wide range of methods was used to subjectively measure aesthetic appeal [3,10,11]. In some cases, these have been shown to yield consistent results [10]; in other cases, it was found that scores collected in a controlled lab environment for a set of images, did not correlate well with the corresponding ratings collected from a crowdsourcing platform [11]. This brings forth a question of whether these different methods of data collection actually yield repeatable judgments of image aesthetic appeal. In the past, a great deal of work has looked into scoring methodologies from various angles: for example, the effect on ratings of discrete and continuous scales [12], and the impact of different environment variables [13] in subjective video quality assessment. To the best of our knowledge, though, no similar research has been conducted with respect to the scoring of aesthetic appeal. Therefore, the following questions are still open: (1) which scoring methodology can yield aesthetic appeal assessments where individual differences are minimized?, and (2) to what extent these aesthetic appeal assessments are repeatable, if the experimental conditions (e.g., population sample, environmental conditions) change? To investigate the above research questions, we conduct a controlled lab experiment with 24 users scoring a set of images constructed to uniformly cover a wide range of"
19748,15226,9004,Multi-organ Localization Combining Global-to-Local Regression and Confidence Maps,2014,"We propose a method for fast, accurate and robust localiza- tion of several organs in medical images. We generalize global-to-local cascades of regression forests (1) to multiple organs. A first regressor en- codes global relationships between organs. Subsequent regressors refine the localization of each organ locally and independently for improved accuracy. We introduce confidence maps, which incorporate information about both the regression vote distribution and the organ shape through probabilistic atlases. They are used within the cascade itself, to better select the test voxels for the second set of regressors, and to provide richer information than the classical bounding boxes thanks to the shape prior. We demonstrate the robustness and accuracy of our approach through a quantitative evaluation on a large database of 130 CT volumes. 1 Medical Motivation and Overview With the ever growing size and complexity of 3D medical acquisitions, auto- matic, robust and accurate anatomy localization is of prime interest. First, it enables faster data navigation and visualization of target structures. Secondly, organ localization is a key initialization step for tasks such as segmentation. It is, overall, a crucial component to complex workflows such as treatment follow-up. General object detection has been deeply studied in computer vision. However algorithms proposed for natural 2D scenes are usually not efficient enough (ex- haustive scanning of the image) or not even applicable (from 2D to 3D )t o the case of anatomical objects. Moreover, medical images often hold specific contex- tual information, which entails to design specific methods. In the literature we can mostly find three types of approaches for multi-organ localization: classifi- cation, regression and atlas-based approaches. As shown in (2), regression-based methods are computationally less expensive (about 25 times less) than atlas- based ones, and then more adapted to clinical contexts. A good overview of the different classification and regression approaches proposed so far can be found in (3). In this paper we focus on regression-based methods, as their speed and accuracy (2) make them well adapted to clinical contexts. The idea of these ap- proaches is to learn a regression function which relates a voxel and its associated image features to a set of parameters that we want to predict (e.g. organ bound- ing box). We say that a voxel votes for a set of parameters. The votes from several voxels form a distribution from which we can infer the final result. In (2)"
2212232,15226,22279,PTZ network configuration for optimal 3D coverage,2011,"During the last years, the need for security-oriented surveillance systems has grown higher and higher. Nowadays many public environments, such as airports, train stations, etc. are monitored by some sort of video-surveillance system in order to detect or prevent security issues. The involved technology ranges from the use of plain closed-circuit cameras (CCTV) to sophisticated computer-based video processing systems. The CCTV approach has been the only feasible choice in the past, and it is still widely used, however its limits are more and more evident: the increase of the number of sensors (modern surveillance systems can use hundreds of cameras) is often not matched by an adequate number of human operators, whose attention is spread on many different tasks and quickly decreases over time. Modern computer-based systems try to face these problems using automatic video analysis and understanding techniques, in order to cover wide areas and simultaneously highlight only the potential security issues and thus requiring the attention of a human operator only in a limited number of cases (e.g. [6, 5]). The research in this field has been very active and produced many techniques for video analysis and interpretation, but many works are limited to the use of static cameras. Only recently the research community started focusing on more sophisticated sensors like Pan-Tilt-Zoom (PTZ) cameras, and the research on dynamic, active networks of PTZ cameras is still limited (for an example of some recent works in this field, see [1]). Many of these works focus on exploiting the dynamic features of a network of PTZ cameras to improve tracking performance [3, 4, 13, 10, 12], while relatively few works address the problem of optimizing the camera coverage of the monitored area according to specific criteria. Angella et al. [2] propose a method to maximize the area coverage by using a 3D model of the observed zone, but their work only aims at finding a good initial camera displacement, which cannot be dynamically modified according to the observed data. Mittal and Davis [8, 7] also consider the presence of dynamic occluding objects in order to evaluate the visibility of the scene. Piciarelli et al. [11] propose a method to automatically and dynamically reconfigure the camera orientations and zoom levels using an Expectation-Maximization-based approach."
1461073,15226,9099,Robust and accurate mobile visual localization and its applications,2013,"Mobile applications are becoming increasingly popular. More and more people are using their phones to enjoy ubiquitous location-based services (LBS). The increasing popularity of LBS creates a fundamental problem: mobile localization. Besides traditional localization methods that use GPS or wireless signals, using phone-captured images for localization has drawn significant interest from researchers. Photos contain more scene context information than the embedded sensors, leading to a more precise location description. With the goal being to accurately sense real geographic scene contexts, this article presents a novel approach to mobile visual localization according to a given image (typically associated with a rough GPS position). The proposed approach is capable of providing a complete set of more accurate parameters about the scene geo-context including the real locations of both the mobile user and perhaps more importantly the captured scene, as well as the viewing direction. To figure out how to make image localization quick and accurate, we investigate various techniques for large-scale image retrieval and 2D-to-3D matching. Specifically, we first generate scene clusters using joint geo-visual clustering, with each scene being represented by a reconstructed 3D model from a set of images. The 3D models are then indexed using a visual vocabulary tree structure. Taking geo-tags of the database image as prior knowledge, a novel location-based codebook weighting scheme proposed to embed this additional information into the codebook. The discriminative power of the codebook is enhanced, thus leading to better image retrieval performance. The query image is aligned with the models obtained from the image retrieval results, and eventually registered to a real-world map. We evaluate the effectiveness of our approach using several large-scale datasets and achieving estimation accuracy of a user's location within 13 meters, viewing direction within 12 degrees, and viewing distance within 26 meters. Of particular note is our showcase of three novel applications based on localization results: (1) an on-the-spot tour guide, (2) collaborative routing, and (3) a sight-seeing guide. The evaluations through user studies demonstrate that these applications are effective in facilitating the ideal rendezvous for mobile users."
257329,15226,9004,Label Inference with Registration and Patch Priors,2014,"In this paper, we present a novel label inference method that integrates registration and patch priors, and serves as a remedy for la- belling errors around structural boundaries. With the initial label map provided by nonrigid registration methods, its corresponding signed dis- tance function can be estimated and used to evaluate the segmentation confidence. The pixels with less confident labels are selected as candidate nodes to be refined and those with relatively confident results are settled as seeds. The affinity between seeds and candidate nodes, which consists of regular image lattice connections, registration prior based on signed distance and patch prior from the warped atlas, is encoded to guide the label inference procedure. For method evaluation, experiments have been carried out on two publicly available data sets and it only takes several seconds for our method to improve the segmentation quality significantly. Due to poor contrast condition and intensity inhomogeneity in brain magnetic resonance (MR) images, it is challenging to provide a reliable segmentation re- sult. Manual labelling is tedious and time-consuming, which also suffers from inter- and intra-labeler variability (13). Various automatic labelling methods have been proposed and atlas-based segmentation approaches become widely used owing to the relatively high accuracy. With manually labelled atlas, the label map for the target image can be propagated from the atlas based on non- rigid registration (9). To obtain a reasonable deformation field, smoothness or regularization term is conventionally enforced during registration (6,14). How- ever, because of the anatomical variability among subjects, the enforcement of regularization can lead to labelling errors near the object surfaces. Patch-based method is first introduced for image denoising (4) and later em- ployed in medical image segmentation. In (11), with the similarity values calcu- lated from a kernel function as weights, the small patches inside a region weighted vote for the labels of the target image. In this process, no nonrigid registration is required and all information provided by the small patches will be utilized. To reduce the adverse impact from dissimilar patches, an extension is proposed in (3), by first ranking these small patches based on structure similarity and then combining the selected ones together for the final labelling."
210946,15226,11052,Match Selection and Refinement for Highly Accurate Two-View Structure from Motion,2014,"We present an approach to enhance the accuracy of structure from motion (SfM) in the two-view case. We first answer the question: fewer data with higher accuracy, or more data with less accuracy? For this, we establish a relation between SfM errors and a function of the number of matches and their epipolar errors. Using an accuracy estima- tor of individual matches, we then propose a method to select a subset of matches that has a good quality vs. quantity compromise. We also pro- pose a variant of least squares matching to refine match locations based on a focused grid and a multi-scale exploration. Experiments show that both selection and refinement contribute independently to a better ac- curacy. Their combination reduces errors by a factor of 1.1 to 2.0 for rotation, and 1.6 to 3.8 for translation. 3D reconstructions from pictures are increasingly being used to model real scenes or objects. For some applications such as video games or virtual film sets, cap- turing the general shape and appearance is enough. The reconstruction method does not have to be particularly accurate. However, in industrial settings, where 3D models are used for measurement, accuracy is crucial. Moreover, even for less demanding tasks, accurate reconstruction reduces the quantity of required images, thus reducing the costs and increasing the applicability. Better estimates also lessen the impact of outliers. In this paper, we propose a method to greatly enhance the accuracy of two- view structure from motion (SfM), i.e., the estimation of the camera poses (posi- tions and orientations) and of the basic structure of the scene (3D point cloud). As 3D reconstruction methods strongly rely on the quality of the estimated cal- ibration, this is a crucial initial step. In most cases, being wrong at calibration time cannot be recovered later. Match Selection. Most SfM approaches are based on the detection and match- ing of interest points (features) in image pairs (11). Given point matches between two images, we can estimate a fundamental matrix F relating them. If internal calibration parameters are known (calibration matrix K), this also provides an estimate of the camera motion (rotation R ,t ranslationt) and 3D position of matched points. As feature detection and matching is not perfect, two main things can go wrong in the SfM process: the matches can be either incorrect"
219002,15226,9004,Do We Need Annotation Experts? A Case Study in Celiac Disease Classification,2014,"Inference of clinically-relevant findings from the visual ap- pearance of images has become an essential part of processing pipelines for many problems in medical imaging. Typically, a sufficient amount labeled training data is assumed to be available, provided by domain experts. However, acquisition of this data is usually a time-consuming and expensive endeavor. In this work, we ask the question if, for certain problems, expert knowledge is actually required. In fact, we investigate the impact of letting non-expert volunteers annotate a database of en- doscopy images which are then used to assess the absence/presence of celiac disease. Contrary to previous approaches, we are not interested in algorithms that can handle the label noise. Instead, we present compelling empirical evidence that label noise can be compensated by a sufficiently large corpus of training data, labeled by the non-experts. 1 Motivation Many problems in medical imaging involve some sort of decision-making process based on the visual appearance of images acquired by some modality. Typical examples include, but are not limited to, computer-aided assessment of various types of cancer, or the classification of tissue types for subsequent segmenta- tion. The prevalent paradigm of these approaches is to assume the existence of expert-annotated data to train a classification system which is then used to make predictions for new data instances. For segmentation tasks, predictions are typ- ically made on a pixel level, whereas for computer-aided diagnosis, predictions are made on suitable representations of images regions or even the full images. While many approaches demonstrate fairly good performance for the respec- tive task, classifier training inherently depends on the pristine expert annota- tions. In practice, though, such annotations are typically hard to obtain, since the annotation task is often time-consuming and thus expensive. Consequently, the amount of available training data tends to be rather limited which can lead to non-conclusive statements about the generalization ability of a system. This is in contrast to many computer vision problems, where annotation tasks can typically be crowd-sourced easily."
1473468,15226,22130,Incremental Surface Extraction from Sparse Structure-from-Motion Point Clouds,2013,"In this paper we propose a new method to incrementally extract a surface from a consecutively growing Structure-from-Motion (SfM) point cloud in real-time. Our method is based on a Delaunay triangulation (DT) on the 3D points. The core idea is to robustly label all tetrahedra into freeand occupied space using a random field formulation and to extract the surface as the interface between differently labeled tetrahedra. For this reason, we propose a new energy function that achieves the same accuracy as state-of-the-art methods but reduces the computational effort significantly. Furthermore, our new formulation allows us to extract the surface in an incremental manner, i. e. whenever the point cloud is updated we adapt our energy function. Instead of minimizing the updated energy with a standard graph cut, we employ the dynamic graph cut of Kohli et al. [1] which enables efficient minimization of a series of similar random fields by re-using the previous solution. In such a way we are able to extract the surface from an increasingly growing point cloud nearly independent of the overall scene size. Energy Function for Surface Extraction Our method formulates surface extraction as a binary labeling problem, with the goal of assigning each tetrahedron either a free or occupied label. For this reason, we model the probabilities that a tetrahedron is free- or occupied space analyzing the set of rays that connect all 3D points to image features. Following the idea of the truncated signed distance function (TSDF), which is known from voxel-based surface reconstructions, a tetrahedron in front of a 3D point X has a high probability to be free space, whereas a tetrahedron behind X is presumably occupied space. We further assume that it is very unlikely that neighboring tetrahedra obtain different labels, except for pairs of tetrahedra that have a ray through the face connecting both. Such a labeling problem can be elegantly formulated as a pairwise random field and since our priors are submodular, we can efficiently find a global optimal labeling solution e. g. using graph cuts. In contrast to existing methods like [2], our energy depends only on the visibility information that is directly connected to the four 3D points that span the tetrahedraVi. Hence a modification of the tetrahedral structure by inserting new points has only limited effect on the energy function. This property enables us to easily adopt the energy function to a modified tetrahedral structure. Incremental Surface Extraction To enable efficient incremental surface reconstruction, our method has to consecutively integrate new scene information (3D points as well as visibility information) in the energy function and to minimize the modified energy efficiently. Integrating new visibility information, i. e. adding rays for newly available 3D points, affects only those terms of the energy function that relate"
2182821,15226,10994,Planar Structures from Line Correspondences in a Manhattan World,2014,"Planar Structures from Line Correspondences in a Manhattan World Chelhwon Kim 1 , Roberto Manduchi 2 Electrical Engineering Department Computer Engineering Department University of California, Santa Cruz Santa Cruz, CA, US Abstract. Traditional structure from motion is hard in indoor environ- ments with only a few detectable point features. These environments, however, have other useful characteristics: they often contain severable visible lines, and their layout typically conforms to a Manhattan world geometry. We introduce a new algorithm to cluster visible lines in a Man- hattan world, seen from two different viewpoints, into coplanar bundles. This algorithm is based on the notion of “characteristic line”, which is an invariant of a set of parallel coplanar lines. Finding coplanar sets of lines becomes a problem of clustering characteristic lines, which can be accomplished using a modified mean shift procedure. The algorithm is computationally light and produces good results in real world situations. Introduction This paper addresses the problem of reconstructing the scene geometry from pictures taken from different viewpoints. Structure from motion (SFM) has a long history in computer vision [1, 2], and SFM (or visual SLAM) algorithms have been ported on mobile phones [3, 4]. Traditional SFM relies on the ability of detecting and matching across views a substantial number of point features. Unfortunately, robust point detection and matching in indoor environments can be challenging, as the density of detectable points (e.g. corners) may be low. At the same time, indoor environments are typically characterized by (1) the presence of multiple line segments (due to plane intersections and other linear structures), and (2)“Manhattan world” layouts, with a relatively small number of planes at mutually orthogonal orientations. This paper introduces a new algorithm for the detection and localization of planar structures and relative camera pose in a Manhattan world, using line matches from two images taken from different viewpoints. As in previous ap- proaches [5–7], the orientation (but not the position) of the two cameras with respect to the environment is computed using vanishing lines and inertial sensors (available in all new smartphones). The main novelty of our algorithm is in the criterion used to check whether groups of lines matched in the two images may be coplanar. Specifically, we introduce a new invariant feature (~n -characteristic line) of the image of a bundle of coplanar parallel lines, and show how this"
642675,15226,9004,"A Deep Learning Architecture for Image Representation, Visual Interpretability and Automated Basal-Cell Carcinoma Cancer Detection",2013,"This paper presents and evaluates a deep learning architecture for automated basal cell carcinoma cancer detection that integrates (1) image repre- sentation learning, (2) image classification and (3) result interpretability. A novel characteristic of this approach is that it extends the deep learning architecture to also include an interpretable layer that highlights the visual patterns that con- tribute to discriminate between cancerous and normal tissues patterns, working akin to a digital staining which spotlights image regions important for diagnos- tic decisions. Experimental evaluation was performed on set of 1,417 images from 308 regions of interest of skin histopathology slides, where the presence of absence of basal cell carcinoma needs to be determined. Different image rep- resentation strategies, including bag of features (BOF), canonical (discrete cosine transform (DCT) and Haar-based wavelet transform (Haar)) and proposed learned- from-data representations, were evaluated for comparison. Experimental results show that the representation learned from a large histology image data set has the best overall performance (89.4% in F-measure and 91.4% in balanced accuracy), which represents an improvement of around 7% over canonical representations and 3% over the best equivalent BOF representation. This paper presents a unified method for histopathology image representation learning, visual analysis interpretation, and automatic classification of skin histopathology im- ages as either having basal cell carcinoma or not. The novel approach is inspired by ideas from image feature representation learning and deep learning (10) and yields a deep learning architecture that combines an autoencoder learning layer, a convolutional layer, and a softmax classifier for cancer detection and visual analysis interpretation. Deep learning (DL) architectures are formed by the composition of multiple linear and non-linear transformations of the data, with the goal of yielding more abstract - and ultimately more useful - representations (10). These methods have recently become popular since they have shown outstanding performance in different computer vision and pattern recognition tasks (2,8,10). DL architectures are an evolution of multilayer neural networks (NN), involving different design and training strategies to make them competitive. These strategies include spatial invariance, hierarchical feature learning"
1700573,15226,23735,Real-time Pose Estimation of Deformable Objects Using a Volumetric Approach,2014,"Pose estimation of deformable objects is a funda- mental and challenging problem in robotics. We present a novel solution to this problem by first reconstructing a 3D model of the object from a low-cost depth sensor such as Kinect, and then searching a database of simulated models in different poses to predict the pose. Given noisy depth images from 360- degree views of the target object acquired from the Kinect sensor, we reconstruct a smooth 3D model of the object using depth image segmentation and volumetric fusion. Then with an efficient feature extraction and matching scheme, we search the database, which contains a large number of deformable objects in different poses, to obtain the most similar model, whose pose is then adopted as the prediction. Extensive experiments demonstrate better accuracy and orders of magnitude speed- up compared to our previous work. An additional benefit of our method is that it produces a high-quality mesh model and camera pose, which is necessary for other tasks such as regrasping and object manipulation. I. INTRODUCTION In robotics and computer vision, recognition and ma- nipulation of deformable objects such as garments, are well-known challenging tasks. Recently, mature solutions to manipulating rigid objects have emerged and been applied in industry (3). However, in the fabric and food industry, which involve a large number of deformable objects, there is still a large gap between the high demand for automatic operations, and the lack of reliable solutions. Compared with rigid objects, deformable objects are much harder to recognize and manipulate, especially because of the large variance of appearance in materials and the way they deform. This variance subsequently makes it difficult to establish a robust recognition pipeline to predict the pose of the deformable objects based on traditional visual sensors, such as regular cameras. However, newly emerged low-cost depth sensors such as Microsoft Kinect can provide accurate depth measurements. With this depth information, a robotic system is able to resolve the ambiguity of visual appearance better, and thus provide higher performance on recognition tasks. Our interests are in detecting the pose of deformable objects such as garments as a part of a larger pipeline for manipulating these objects. Once the robot has identified the pose of the objects, it can then proceed to manipulate those objects, for tasks such as regarsping and garment folding. y indicates equal contribution"
29413,15226,9004,Construction of a coronary artery atlas from CT angiography.,2014,"Describing the detailed statistical anatomy of the coronary artery tree is important for determining the aetiology of heart disease. A number of studies have investigated geometrical features and have found that these correlate with clinical outcomes, e.g. bifurcation angle with major adverse cardiac events. These methodologies were mainly two- dimensional, manual and prone to inter-observer variability, and the data commonly relates to cases already with pathology. We propose a hybrid atlasing methodology to build a population of computational models of the coronary arteries to comprehensively and accurately assess anatomy including 3D size, geometry and shape descriptors. A random sample of 122 cardiac CT scans with a calcium score of zero was segmented and analysed using a standardised protocol. The resulting atlas includes, but is not limited to, the distributions of the coronary tree in terms of angles, diameters, centrelines, principal component shape analysis and cross- sectional contours. This novel resource will facilitate the improvement of stent design and provide a reference for hemodynamic simulations, and provides a basis for large normal and pathological databases. 1 Background Blockage of the coronary arteries causes chest pain, heart attack and sudden death. The prevalence of coronary heart disease (CHD) in the United States is 6.4%, where a myocardial infarction approximately every 44 seconds leads to 1 in 6 deaths being attributable to CHD (6). The coronary arteries supply oxygen to the heart muscle. Lipid accumulation in the vessel wall (atheroma) narrows the artery, with stenoses particularly likely to develop at sites of altered flow and shear stress such as at the outer flanks of bifurcations (15). Percutaneous coronary intervention (PCI) is an important treatment for stable atheroma causing chest pain on exertion (angina) and for acute coronary syndromes. This involves the insertion of a guide wire into the artery, balloon expansion, and deployment of a metal or polymer stent to hold the artery open. Each year two million stents"
1739447,15226,22130,Properties of Datasets Predict the Performance of Classifiers,2013,"This thesis is mostly about supervised visual recognition problems. Based on a general definition of categories, the contents are divided into two parts: one which models categories and one which is not category based. We are interested in data driven solutions for both kinds of problems.In the category-free part, we study novelty detection in temporal and spatial domains as a category-free recognition problem. Using data driven models, we demonstrate that based on a few reference exemplars, our methods are able to detect novelties in ego-motions of people, and changes in the static environments surrounding them.In the category level part, we study object recognition. We consider both object category classification and localization, and propose scalable data driven approaches for both problems. A mixture of parametric classifiers, initialized with a sophisticated clustering of the training data, is demonstrated to adapt to the data better than various baselines such as the same model initialized with less subtly designed procedures. A nonparametric large margin classifier is introduced and demonstrated to have a multitude of advantages in comparison to its competitors: better training and testing time costs, the ability to make use of indefinite/invariant and deformable similarity measures, and adaptive complexity are the main features of the proposed model.We also propose a rather realistic model of recognition problems, which quantifies the interplay between representations, classifiers, and recognition performances. Based on data-describing measures which are aggregates of pairwise similarities of the training data, our model characterizes and describes the distributions of training exemplars. The measures are shown to capture many aspects of the difficulty of categorization problems and correlate significantly to the observed recognition performances. Utilizing these measures, the model predicts the performance of particular classifiers on distributions similar to the training data. These predictions, when compared to the test performance of the classifiers on the test sets, are reasonably accurate.We discuss various aspects of visual recognition problems: what is the interplay between representations and classification tasks, how can different models better adapt to the training data, etc. We describe and analyze the aforementioned methods that are designed to tackle different visual recognition problems, but share one common characteristic: being data driven."
2112793,15226,390,CADOnc ⓒ: An integrated toolkit for evaluating radiation therapy related changes in the prostate using multiparametric MRI,2011,"The use of multi-parametric Magnetic Resonance Imaging (T2-weighted, MR Spectroscopy (MRS), Diffusion-weighted (DWI)) has recently shown great promise for diagnosing and staging prostate cancer (CaP) in vivo. Such imaging has also been utilized for evaluating the early effects of radiotherapy (RT) (e.g. intensity-modulated radiation therapy (IMRT), proton beam therapy, brachytherapy) in the prostate with the overarching goal being to successfully predict short- and long-term patient outcome. Qualitative examination of post-RT changes in the prostate using MRI is subject to high inter- and intra-observer variability. Consequently, there is a clear need for quantitative image segmentation, registration, and classification tools for assessing RT changes via multi-parametric MRI to identify (a) residual disease, and (b) new foci of cancer (local recurrence) within the prostate. In this paper, we present a computerized image segmentation, registration, and classification toolkit called CADOnc ⓒ, and leverage it for evaluating (a) spatial extent of disease pre-RT, and (b) post-RT related changes within the prostate. We demonstrate the applicability of CADOnc ⓒ in studying IMRT-related changes using a cohort of 7 multi-parametric (T2w, MRS, DWI) prostate MRI patient datasets. First, the different MRI protocols from pre- and post-IMRT MRI scans are affinely registered (accounting for gland shrinkage), followed by automated segmentation of the prostate capsule using an active shape model. A number of feature extraction schemes are then applied to extract multiple textural, metabolic, and functional MRI attributes on a per-voxel basis. An AUC of 0.7132 was achieved for automated detection of CaP on pre-IMRT MRI (via integration of T2w, DWI, MRS features); evaluated on a per-voxel basis against radiologist-derived annotations. CADOnc ⓒ also successfully identified a total of 40 out of 46 areas where disease-related changes (both absence and recurrence) occurred post-IMRT, based on changes in the expression of quantitative MR imaging biomarkers. CADOnc ⓒ thus provides an integrated platform of quantitative analysis tools to evaluate treatment response in vivo, based on multi-parametric MRI data."
205675,15226,11052,Learning High-Level Judgments of Urban Perception,2014,"Human observers make a variety of perceptual inferences about pictures of places based on prior knowledge and experience. In this paper we apply computational vision techniques to the task of pre- dicting the perceptual characteristics of places by leveraging recent work on visual features along with a geo-tagged dataset of images associated with crowd-sourced urban perception judgments for wealth, uniqueness, and safety. We perform extensive evaluations of our models, training and testing on images of the same city as well as training and testing on im- ages of different cities to demonstrate generalizability. In addition, we collect a new densely sampled dataset of streetview images for 4 cities and explore joint models to collectively predict perceptual judgments at city scale. Finally, we show that our predictions correlate well with ground truth statistics of wealth and crime. In this paper we apply computer vision techniques to predict human percep- tions of place. In particular we show that - perhaps surprisingly - it is possi- ble to predict human judgments of safety, uniqueness, and wealth of locations with remarkable accuracy. We also find that predictors learned for one place are applicable to predicting perceptions of other unseen locations, indicating the generalizability of our models. Additionally, we explore models to jointly predict perceptions coherently across an entire city. Finally, we also find good correla- tions with ground truth statistics of crime and wealth when predicting on a more densely sampled set of images. The world, or even a single city, is a large continuous evolving space that can not be experienced at once. The seminal work of Lynch, The Image of the City (19) was influential in urban design and the approach of social scientists to urban studies. Of course, collecting human judgments is a time consuming and costly process. With accurate computational prediction tools, we could ex- tend human labeled data of a place to nearby locations or potentially the entire world, thus enabling social scientists to better understand and analyze public"
85064,15226,10994,Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling,2014,"We frame the task of predicting a semantic labeling as a sparse reconstruction procedure that applies a target-specific learned transfer function to a generic deep sparse code representation of an image. This strategy partitions training into two distinct stages. First, in an unsupervised manner, we learn a set of dictionaries optimized for sparse coding of image patches. These generic dictionaries minimize error with respect to representing image appearance and are independent of any particular target task. We train a multilayer representation via recursive sparse dictionary learning on pooled codes output by earlier layers. Second, we encode all training images with the generic dictionaries and learn a transfer function that optimizes reconstruction of patches extracted from annotated ground-truth given the sparse codes of their corresponding image patches. At test time, we encode a novel image using the generic dictionaries and then reconstruct using the transfer function. The output reconstruction is a semantic labeling of the test image. #R##N##R##N#Applying this strategy to the task of contour detection, we demonstrate performance competitive with state-of-the-art systems. Unlike almost all prior work, our approach obviates the need for any form of hand-designed features or filters. Our model is entirely learned from image and ground-truth patches, with only patch sizes, dictionary sizes and sparsity levels, and depth of the network as chosen parameters. To illustrate the general applicability of our approach, we also show initial results on the task of semantic part labeling of human faces. #R##N##R##N#The effectiveness of our data-driven approach opens new avenues for research on deep sparse representations. Our classifiers utilize this representation in a novel manner. Rather than acting on nodes in the deepest layer, they attach to nodes along a slice through multiple layers of the network in order to make predictions about local patches. Our flexible combination of a generatively learned sparse representation with discriminatively trained transfer classifiers extends the notion of sparse reconstruction to encompass arbitrary semantic labeling tasks."
1570919,15226,390,Quantitative parameters derived from FDG and amyloid PET scans for clinical trials,2012,"Parameters from FDG PET are derived from those brain areas that show impaired metabolism in AD. They are either expressed as ratios relative to least affected reference areas or as measures that reflect the severity of abnormality (e.g., z-scores) relative to normal controls. FDG PET scans are typically taken at resting state conditions (no sensory or cognitive stimulation in a low ambient noise and light environment). Blood glucose levels should be within the normal fasting range. With amyloid PET, the increase of neocortical binding shows little regional differences and a global average is typically expressed as a ratio relative to a reference region (usually cerebellum or pons). Because of nonspecific white matter uptake in controls, techniques for distinction between grey and white matter are required (atlas-based or by coregistration with segmented MR scans). In addition to standard quality assurance of tracer production and scanner, including homogeneity and correction for scatter and attenuation, the following factors can influence quantitative parameters substantially and need to be controlled: • scanner resolution, which is usually dealt with by smoothing of high-resolution scans to a common low resolution • time interval of data acquisition after tracer injection • choice, definition, and reproducible placement of target and reference regions (typically as 3D volumes of interest), typically requiring spatial normalisation and rigid-body 3D coregistration of all data sets from each individual to reduce intra-and inter-subject variability Other factors that can influence results but do not necessarily need to be homogenous across participating centres include image reconstruction methods (iterative or by filtered back projection). Statistical models for data analysis (typically ANOVA with regional values and follow-up measurements as within-subject factors) should be specified in the study protocol."
1951483,15226,9004,Simulation of Lipofilling Reconstructive Surgery Using Coupled Eulerian Fluid and Deformable Solid Models,2013,"We present a method to simulate the outcome of reconstruc- tive facial surgery based on fat-filling. Facial anatomy is complex: the fat is constrained between layers of tissues which behave as walls along the face; in addition, connective tissues that are present between these dif- ferent layers also influence the fat-filling procedure. To simulate the end result, we propose a method which couples a 2.5D Eulerian fluid model for the fat and a finite element model for the soft tissues. The two models are coupled using the computation of the mechanical compliance matrix. Two contributions are presented in this paper: a solver for fluids which couples properties of solid tissues and fluid pressure, and an application of this solver to fat-filling surgery procedure simulation. The Parry-Romberg syndrome is a progressive hemifacial atrophy. It is due to disorders of central nervous system and is characterized by a degeneration of tissues beneath the skin. The syndrome affects generally one side of the face and distorts the nose and the mouth. Causes are still unknown, an autoimmune mechanism is suspected. The reconstructive surgery is the only way to restore the face with the adding of fat. In addition, there are less severe cases of facial dystrophy due to side effect of drugs or insulin-resistance for which drug solutions are not considered sufficient compared with reconstructive surgery (1). The fat-filling procedure consists of injecting fat in subcutaneous areas. The main literature concerning about the fat-filling surgery are the result analyses of the operation. Surgeons test different methods of injection, or different products, and they compare measures before the operation, short-term and long-term to define the viability of their method (2). As it is difficult to predict the results, most of the time surgeons rely on their experience to plan the operation and calculate the required volume of fat. So the surgeon must become familiar with the possible depths of injection (subdermal, intramuscular, supraperiosteal), and the amounts of fat to get the desired change (3). The facial anatomy is complex: there are many layers of tissues (skin, Surper- ficial Musculo-Aponeurotic System ...) with different properties (4). Futhermore, ligaments, nerves and blood vessels connect the layers, change the stiffness of"
1888476,15226,390,Multi-modal data fusion schemes for integrated classification of imaging and non-imaging biomedical data,2011,"With a wide array of multi-modal, multi-protocol, and multi-scale biomedical data available for disease diagnosis and prognosis, there is a need for quantitative tools to combine such varied channels of information, especially imaging and non-imaging data (e.g. spectroscopy, proteomics). The major problem in such quantitative data integration lies in reconciling the large spread in the range of dimensionalities and scales across the different modalities. The primary goal of quantitative data integration is to build combined meta-classifiers; however these efforts are thwarted by challenges in (1) homogeneous representation of the data channels, (2) fusing the attributes to construct an integrated feature vector, and (3) the choice of learning strategy for training the integrated classifier. In this paper, we seek to (a) define the characteristics that guide the 4 independent methods for quantitative data fusion that use the idea of a meta-space for building integrated multi-modal, multi-scale meta-classifiers, and (b) attempt to understand the key components which allowed each method to succeed. These methods include (1) Generalized Embedding Concatenation (GEC), (2) Consensus Embedding (CE), (3) Semi-Supervised Multi-Kernel Graph Embedding (SeSMiK), and (4) Boosted Embedding Combination (BEC). In order to evaluate the optimal scheme for fusing imaging and non-imaging data, we compared these 4 schemes for the problems of combining (a) multi-parametric MRI with spectroscopy for prostate cancer (CaP) diagnosis in vivo, and (b) histological image with proteomic signatures (obtained via mass spectrometry) for predicting prognosis in CaP patients. The kernel combination approach (SeSMiK) marginally outperformed the embedding combination schemes. Additionally, intelligent weighting of the data channels (based on their relative importance) appeared to outperform unweighted strategies. All 4 strategies easily outperformed a naive decision fusion approach, suggesting that data integration methods will play an important role in the rapidly emerging field of integrated diagnostics and personalized healthcare."
601723,15226,9004,Combining DTI and MRI for the automated detection of alzheimer’s disease using a large european multicenter dataset,2012,"Diffusion tensor imaging (DTI) allows assessing neuronal fiber tract integrity in vivo to support the diagnosis of Alzheimer's disease (AD). It is an open research question to which extent combinations of different neuroimaging techniques increase the detection of AD. In this study we examined different methods to combine DTI data and structural T1-weighted magnetic resonance imaging (MRI) data. Further, we applied machine learning techniques for automated detection of AD. We used a sample of 137 patients with clinically probable AD (MMSE 20.6 ±5.3) and 143 healthy elderly controls, scanned in nine different scanners, obtained from the recently created framework of the European DTI study on Dementia (EDSD). For diagnostic classification we used the DTI derived indices fractional anisotropy (FA) and mean diffusivity (MD) as well as grey matter density (GMD) and white matter density (WMD) maps from anatomical MRI. We performed voxel-based classification using a Support Vector Machine (SVM) classifier with tenfold cross validation. We compared the results from each single modality with those from different approaches to combine the modalities. For our sample, combining modalities did not increase the detection rates of AD. An accuracy of approximately 89% was reached for GMD data alone and for multimodal classification when GMD was included. This high accuracy remained stable across each of the approaches. As our sample consisted of mildly to moderately affected patients, cortical atrophy may be far progressed so that the decline in structural network connectivity derived from DTI may not add additional information relevant for the SVM classification. This may be different for predementia stages of AD. Further research will focus on multimodal detection of AD in predementia stages of AD, e.g. in amnestic mild cognitive impairment (aMCI), and on evaluating the classification performance when adding other modalities, e.g. functional MRI or FDG-PET."
236524,15226,9616,RGB and depth intra-frame Cross-Compression for low bandwidth 3D video,2012,"With the recent explosion in the development of multimedia hardware capable of 3D display, 3D Picture Coding Sytems have assumed a pivotal role. While encoding techniques for stereo-scopic images is a well researched topic and compression standards such as MPEG provide variants to support it, compression of RGB-D data such as from the Microsoft Kinect sensor offers a number of unsolved challenges. Projected texture based active sensors such as the Kinect offer a number of advantages in comparison with traditional 3D capture systems. While not affecting the visible spectrum of the scene these sensors are capable of producing highly accurate 3D reconstructions of complex scenes (as well as novel viewpoints) — even those with homogeneous surfaces lacking textural features that form the foundation of stereoscopic range measurement systems. Conventional approaches to compressing the RGB and D images separately are suboptimal in terms of compression efficiency, bandwidth usage and scalability. On the other hand, state-of-the-art methods in the field are not suitable for low bandwidth applications, typical of mobile phone devices, on-field civilian and defense robotic systems, especially those operating on unreliable or high-loss wireless networks. In order to address these concerns, we present a novel RGB-D Cross-Compression algorithm that can be used for static 3D scene reconstruction as well as intra-frame coding of 3D videos. The algorithm detects salient edge-like structures in RGB and D images and perform cross-coding across the modalities to yield a scalable system for 3D video coding. Results presented using the Microsoft Ballet and Breakdancers test sequences demonstrate the efficiency of the system in terms of compression rate, reconstruction quality and ratedistortion characteristics. The scalability of the approach also makes it well suited for mobile and wireless applications."
1684009,15226,390,Spatial intensity prior correction for tissue segmentation in the developing human brain,2011,"The degree of white matter (WM) myelination is rather inhomogeneous across the brain. As a consequence, white matter appears differently across the cortical lobes in MR images acquired during early postnatal development. At 1 year old specifically, the gray/white matter contrast of MR images in prefrontal and temporal lobes is limited and thus tissue segmentation results show commonly reduce accuracy in these lobes. In this novel work, we propose the use of spatial intensity growth maps (IGM) for T1 and T2 weighted image to compensate for local appearance inhomogeneity. The IGM captures expected intensity changes from 1 to 2 years of age, as appearance inhomogeneity is highly reduced by the age of 24 months. For that purpose, we employ MRI data from a large dataset of longitudinal (12 and 24 month old subjects) MR study of Autism. The IGM creation is based on automatically co-registered images at 12 months, corresponding registered 24 months images, and a final registration of all image to a prior average template. In template space, voxelwise correspondence is thus achieved and the IGM is computed as the coefficient of a voxelwise linear regression model between corresponding intensities at 1-year and 2-years. The proposed IGM shows low regression values of 1-10% in GM and CSF regions, as well as in WM regions at advanced stage of myelination at 1-year. However, in the prefrontal and temporal lobe we observed regression values of 20-25%, indicating that the IGM appropriately captures the expected large intensity change in these lobes due to myelination.The IGM is applied to cross-sectional MRI datasets of 1-year old subjects via registration, correction and tissue segmentation of the corrected dataset. We validated our approach in a small study of images with known, manual “ground truth” segmentations. We furthermore present an EM-like optimization of adapting existing non-optimal prior atlas probability maps to fit known expert rater segmentations."
1483859,15226,23735,Probabilistic surface classification for rover instrument targeting,2013,"Communication blackouts and latency are significant bottlenecks for planetary surface exploration; rovers cannot typically communicate during long traverses, so human operators cannot respond to unanticipated science targets discovered along the route. Targeted data collection by point spectrometers or high-resolution imagery requires precise aim, so it typically happens under human supervision during the start of each command cycle, directed at known targets in the local field of view. Spacecraft can overcome this limitation using onboard science data analysis to perform autonomous instrument targeting. Two critical target selection capabilities are the ability to target priority features of a known geologic class, and the ability to target anomalous surfaces that are unlike anything seen before. This work addresses both challenges using probabilistic surface classification in traverse images. We first describe a method for targeting known classes in the presence of high measurement cost that is typical for power- and time-constrained rover operations. We demonstrate a Bayesian approach that abstains from uncertain classifications to significantly improve the precision of geologic surface classifications. Our results show a significant increase in classification performance, including a seven-fold decrease in misclassification rate for our random forest classifier. We then take advantage of these classifications and learned scene context in order to train a semi-supervised novelty detector. Operators can train the novelty detection to ignore known content from previous scenes, a critical requirement for multi-day rover operations. By making use of prior scene knowledge we find nearly double the number of abnormal features detected over comparable algorithms. We evaluate both of these techniques on a set of images acquired during field expeditions in the Mojave Desert."
1607901,15226,9099,Ground truth generation in medical imaging: a crowdsourcing-based iterative approach,2012,"As in many other scientific domains where computer--based tools need to be evaluated, also medical imaging often requires the expensive generation of manual ground truth. For some specific tasks medical doctors can be required to guarantee high quality and valid results, whereas other tasks such as the image modality classification described in this text can in sufficiently high quality be performed with simple domain experts. Crowdsourcing has received much attention in many domains recently as volunteers perform so--called human intelligence tasks for often small amounts of money, allowing to reduce the cost of creating manually annotated data sets and ground truth in evaluation tasks. On the other hand there has often been a discussion on the quality when using unknown experts. Controlling task quality has remained one of the main challenges in crowdsourcing approaches as potentially the persons performing the tasks may not be interested in results quality but rather their payment.   On the other hand several crowdsourcing platforms such as Crowdflower that we used allow creating interfaces and sharing them with only a limited number of known persons. The text describes the interfaces developed and the quality obtained through manual annotation of several domain experts and one medical doctor. Particularly the feedback loop of semi--automatic tools is explained. The results of an initial crowdsourcing round classifying medical images into a set of image categories were manually controlled by domain experts and then used to train an automatic system that visually classified these images. The automatic classification results were then used to manually confirm or refuse the automatic classes, reducing the time for the initial tasks.   Crowdsourcing platforms allow creating a large variety of interfaces for judgements. Whether used among known experts or paying for unknown persons, they allow increasing the speed of ground truth creation and limit the amount of money to be paid."
1664142,15226,20338,Understanding couch potatoes: measurement and modeling of interactive usage of IPTV at large scale,2011,"We investigate how consumers view content using Video on Demand (VoD) in the context of an IP-based video distribution environment. Users today can use interactive stream control functions such as skip, replay, fast-forward, pause, and rewind to control their viewing. The use of these functions can place additional demands on the distribution infrastructure (servers, network, and set top boxes) and can be challenging to manage with a large subscriber base. A model of user interaction provides insight into the impact of stream control on server and bandwidth requirements, client responsiveness, etc.   We capture the activity users in a natural setting, viewing video at home. We first develop a model for the arrival process of requests for content. We then develop two stream control models that accurately capture user interaction. We show that stream control events can be characterized by a finite state machine and a sojourn time model, parametrized for major periods of usage (weekend and weekday). Our semi-Markov (SM) model for the sojourn time in each stream control state uses a novel technique based on a polynomial fit to the logarithm of the Inverse CDF. A second constrained model(CM) uses a stick-breaking approach familiar in machine learning to model the individual state sojourn time distributions. The SM model seeks to preserve the sojourn time distribution for each state while the CM model puts a greater emphasis on preserving the overall session duration distribution. Using traces across a period of 2 years from a large-scale operational IPTV environment, we validate the proposed model and show that we are able to faithfully predict the workload presented to a video server. We also provide a synthetic trace developed from the model enabling researchers to also study other problems of interest. We also use the techniques to model consumer viewing of video content recorded on their personal Digital Video Recorder (DVR)."
872282,15226,9078,Rate-distortion optimized merge frame using piecewise constant functions,2013,"The ability to efficiently switch from one pre-encoded video stream to another is a valuable attribute for a variety of interactive streaming applications, such as switching among streams of the same video encoded in different bit-rates for real-time bandwidth adaptation, or view-switching among videos capturing the same dynamic 3D scene but from different viewpoints. It is well known that intra-coded I-frames can be used at switch boundaries to facilitate stream-switching. However, the size of an I-frame is large, making frequent insertion impractical. A recent proposal towards a more efficient stream-switching mechanism is distributed source coding (D-SC), which exploits worst-case correlation between a set of potential predictor frames in the decoder buffer (called side information (SI) frames) and a target frame to lower encoding rate. However, the conventional use of bit-plane and channel coding means the encoding and decoding complexity of DSC frames is large. In this paper, we pursue a novel approach to the stream-switching problem based on the concept of “signal merging”, using piecewise constant (p-wc) function as the merge operator. Specifically, we propose a new merge mode for a code block, where for each k-th transform coefficient in the block, we encode appropriate step size and horizontal shift parameters at the encoder, so that the resulting floor function at the decoder can map corresponding coefficients from any SI frame to the same reconstructed value, resulting in an identically merged signal. The selection of shift parameter per coefficient, as well as coding modes between intra and merge per block, are optimized in a rate-distortion (RD) optimal manner. Experiments show encouraging coding gain over a previous implementation of DSC frame at low-to mid-bitrates at reduced computation complexity."
1188712,15226,9773,Performance Evaluation of Algorithms for Newspaper Article Identification,2011,"A typical modern newspaper recognition system operates in distinct phases: i) page segmentation (also called page decomposition or zoning), that is the process of decomposing a page into its structural and logical units (called regions or zones), ii) region (or zone) labeling, where the previously identified units are labeled according to their types (title, text, images, and lines), iii) article identification (or tracking or clustering), in which all the units that belong to a single article are clustered together, and iv) read order identification, in which each item in an article is assigned its reading order inside the article. So far, in the literature, several works appeared describing algorithms and metrics for the first two phases, i.e. page segmentation and region labeling, that indeed play a crucial role in the whole process, however, few results focused on article identification, that is a difficult task mainly due to the rich and complex variety of newspapers layouts. In this paper we propose a methodology to evaluate news-papers article identification algorithms, our approach is based on well-established tools from graph theory: in particular, we reduce the newspaper article clustering problem to a specific graph clustering problem, that is therefore evaluated using the appropriate coverage and performance measures. The advantages of our approach are twofold: on one side, the proposed measures correctly detects that not all the errors are equals, i.e. some errors are worse than others, and the scores are assigned properly. On the other side, we show how to reverse the reduction, in order to exploit the large number of graph clustering algorithm available: indeed, given a graph clustering algorithm, to obtain a full working newspaper article identification algorithm we only need to define a similarity measure between units in the article. We provide some examples, using a specifically designed dataset. Finally, we would like to point out that both our dataset, together with its ground-truth base, and the software tool, that implements the proposed approach, are freely available."
1262673,15226,9099,The space between the images,2013,"Multimedia content has become a ubiquitous presence on all our computing devices, spanning the gamut from live content captured by device sensors such as smartphone cameras to immense databases of images, audio and video stored in the cloud. As we try to maximize the utility and value of all these petabytes of content, we often do so by analyzing each piece of data individually and foregoing a deeper analysis of the relationships between the media. Yet with more and more data, there will be more and more connections and correlations, because the data captured comes from the same or similar objects, or because of particular repetitions, symmetries or other relations and self-relations that the data sources satisfy. This is particularly true for media of a geometric character, such as GPS traces, images, videos, 3D scans, 3D models, etc.   In this talk we focus on the space between the images, that is on expressing the relationships between different mutlimedia data items. We aim to make such relationships explicit, tangible, first-class objects that themselves can be analyzed, stored, and queried -- irrespective of the media they originate from. We discuss mathematical and algorithmic issues on how to represent and compute relationships or mappings between media data sets at multiple levels of detail. We also show how to analyze and leverage networks of maps and relationships, small and large, between inter-related data. The network can act as a regularizer, allowing us to to benefit from the wisdom of the collection in performing operations on individual data sets or in map inference between them.   We will illustrate these ideas using examples from the realm of 2D images and 3D scans/shapes -- but these notions are more generally applicable to the analysis of videos, graphs, acoustic data, biological data such as microarrays, homeworks in MOOCs, etc. This is an overview of joint work with multiple collaborators, as will be discussed in the talk."
30833,15226,11052,Dynamic eye movement datasets and learnt saliency models for visual action recognition,2012,"Systems based on bag-of-words models operating on image features collected at maxima of sparse interest point operators have been extremely successful for both computer-based visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in saccade and fixate regimes, the knowledge, methodology, and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large-scale dynamic computer vision datasets like Hollywood-2[1] and UCF Sports[2] with human eye movements collected under the ecological constraints of the visual action recognition task. To our knowledge these are the first massive human eye tracking datasets of significant size to be collected for video (497,107 frames, each viewed by 16 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as opposed to free-viewing. Second, we introduce novel dynamic consistency and alignment models, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the massive amounts of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the most advanced computer vision practice, can lead to state of the art results."
222898,15226,10994,A Non-invasive Facial Visual-Infrared Stereo Vision Based Measurement as an Alternative for Physiological Measurement,2014,"Our main aim is to propose a vision-based measurement as an alternative to physiological measurement for recognizing mental stress. The development of this emotion recognition system involved three stages: experimental setup for vision and physiological sensing, facial feature extraction in visual-thermal domain, mental stress stimulus experiment and data analysis and classification based on Support Vector Machine. In this research, 3 vision-based measurement and 2 physiological measurement were implemented in the system. Vision based measurement in facial vision domain consists of eyes blinking and in facial thermal domain consists 3 ROI’s temperature value and blood vessel volume at Supraorbital area. Two physiological measurement were done to measure the ground truth value which is heart rate and salivary amylase level. We also propose a new calibration chessboard attach with fever plaster to locate calibration point in stereo view. A new method of integration of two different sensors for detecting facial feature in both thermal and visual is also presented by applying nostril mask, which allows one to find facial feature namely nose area in thermal and visual domain. Extraction of thermal-visual feature images was done by using SIFT feature detector and extractor to verify the method of using nostril mask. Based on the experiment conducted, 88.6 % of correct matching was detected. In the eyes blinking experiment, almost 98 % match was detected successfully for without glasses and 89 % with glasses. Graph cut algorithm was applied to remove unwanted ROI. The recognition rate of 3 ROI’s was about 90 %–96 %. We also presented new method of automatic detection of blood vessel volume at Supraorbital monitored by LWIR camera. The recognition rate of correctly detected pixel was about 93 %. An experiment to measure mental stress by using the proposed system based on Support Vector Machine classification had been proposed and conducted and showed promising results."
1392398,15226,30,"Knowledge Discovery in Medical Systems Using Differential Diagnosis, LAMSTAR, and $k$ -NN",2012,"Medical data are an ever-growing source of information generated from the hospitals in the form of patient records. When mined properly, the information hidden in these records is a huge resource bank for medical research. As of now, these data are mostly used only for clinical work. These data often contain hidden patterns and relationships, which can lead to better diagnosis, better medicines, better treatment, and overall, a platform to better understand the mechanisms governing almost all aspects of the medical domain. Unfortunately, discovery of these hidden patterns and relationships often goes unexploited. However, there is on-going research in medical diagnosis which can predict the diseases of the heart, lungs, and various tumours based on the past data collected from the patients. They are mostly limited to domain-specific systems that predict diseases restricted to their area of operation like heart, brain, and various other domains. These are not applicable to the whole medical dataset. The system proposed in this paper uses this vast storage of information so that diagnosis based on these historical data can be made. It focuses on computing the probability of occurrence of a particular ailment from the medical data by mining it using a unique algorithm which increases accuracy of such diagnosis by combining the key points of neural networks, Large Memory Storage, and Retrieval,  k -NN, and differential diagnosis all integrated into one single algorithm. The system uses a service-oriented architecture wherein the system components of diagnosis, information portal, and other miscellaneous services are provided. This algorithm can be used in solving a few common problems that are encountered in automated diagnosis these days, which include diagnosis of multiple diseases showing similar symptoms, diagnosis of a person suffering from multiple diseases, receiving faster and more accurate second opinion, and faster identification of trends present in the medical records."
1270378,15226,21056,A 3D tele-immersion system based on live captured mesh geometry,2013,"3D Tele-immersion enables participants in remote locations to share, in real-time, an activity. It offers users natural interactivity and immersive experiences, but it challenges current networking solutions. Work in the past has mainly focused on the efficient delivery of image-based 3D videos and on the realistic rendering and reconstruction of geometry-based 3D objects. The contribution of this paper is a complete media pipeline that allows for geometry-based 3D tele-immersion. Unlike previous approaches, that stream videos or video plus depth estimate, our streaming module can transmit the live-reconstructed 3D representations (triangle meshes). Based on a set of comparative experiments, this paper details the architecture and describes a novel component that can efficiently stream geometry in real-time. This component includes both a novel fast local compression algorithm and a rateless packet protection scheme geared towards the requirements imposed by real-time transmission of live-capture mesh geometry. Tests on a large dataset show an encoding and decoding speed-up of over 10 times at similar compression and quality rates, when compared to the high-end MPEG-4 SC3DMC mesh encoder. The implemented rateless code ensures complete packet loss protection of the triangle mesh object and avoids delay introduced by retransmissions. This approach is compared to a streaming mechanism over TCP and outperforms it at packet loss rates over 2% and/or latencies over 9 ms in terms of end-to-end transmission delay. As reported in this paper, the component has been successfully integrated into a larger tele-immersive environment that includes beyond state of the art 3D reconstruction and rendering modules. This resulted in a prototype that can capture, compress transmit and render triangle mesh geometry in real-time over the internet."
2588234,15226,22113,Information fusion based learning for frugal traffic state sensing,2013,"Traffic sensing is a key baseline input for sustainable cities to plan and administer demand-supply management through better road networks, public transportation, urban policies etc., Humans sense the environment frugally using a combination of complementary information signals from different sensors. For example, by viewing and/or hearing traffic one could identify the state of traffic on the road. In this paper, we demonstrate a fusion based learning approach to classify the traffic states using low cost audio and image data analysis using real world dataset. Road side collected traffic acoustic signals and traffic image snapshots obtained from fixed camera are used to classify the traffic condition into three broad classes viz., Jam, Medium and Free. The classification is done on {10sec audio, image snapshot in that 10sec} data tuple. We extract traffic relevant features from audio and image data to form a composite feature vector. In particular, we extract the audio features comprising MFCC (Mel-Frequency Cepstral Coefficients) classifier based features, honk events and energy peaks. A simple heuristic based image classifier is used, where vehicular density and number of corner points within the road segment are estimated and are used as features for traffic sensing. Finally the composite vector is tested for its ability to discriminate the traffic classes using Decision tree classifier, SVM classifier, Discriminant classifier and Logistic regression based classifier. Information fusion at multiple levels (audio, image, overall) shows consistently better performance than individual level decision making. Low cost sensor fusion based on complementary weak classifiers and noisy features still generates high quality results with an overall accuracy of 93-96%."
1664723,15226,9078,Graph-based joint denoising and super-resolution of generalized piecewise smooth images,2014,"Images are often decoded with noise at receiver due to capturing errors and/or signal quantization during compression. Further, it is often necessary to display a decoded image at a higher resolution than the captured one, given available high-resolution (HR) display or a need to zoom-in for detailed examination. In this paper, we address the problems of image denoising and super-resolution (SR) jointly in one unified graph-based framework, focusing on a special class of signals called generalized piecewise smooth (GPWS) images. GPWS images are composed mostly of smooth regions connected by transition regions, and represent an important subclass of images, including cartoon, sub-regions of video frames with captions, graphics images in video games, etc. Like our previous work on piecewise smooth (PWS) images, GPWS images also imply simple-enough graph representations in the pixel domain, so that suitable graph-based filtering techniques can be readily applied. Specifically, leveraging on previous work on graph spectral analysis, for a given pixel block in low-resolution (LR) we first use the second eigenvector of a computed graph Laplacian matrix to identify a hard boundary, and then use the third eigenvector to identify two piecewise smooth regions and a transition region that separates them. The LR hard boundary is then super-resolved into HR via a procedure based on local self-similarity, while graph weights of the LR transition region is mapped to those of the HR transition region via polynomial fitting. Using the computed HR boundary and weights in the transition region, we construct a suitable HR graph corresponding to the LR counterpart, and perform joint denoising / SR using a graph smoothness prior. Experimental results show that our proposed algorithm outperforms two representative separable denoising / SR schemes in both subjective and objective quality."
2452199,15226,11321,Automated nuclear segmentation of coherent anti-stokes Raman scattering microscopy images by coupling superpixel context information with artificial neural networks,2011,"Coherent anti-Stokes Raman scattering (CARS) microscopy is attracting major scientific attention because its high-resolution, label-free properties have great potential for real time cancer diagnosis during an image-guided-therapy process. In this study, we develop a nuclear segmentation technique which is essential for the automated analysis of CARS images in differential diagnosis of lung cancer subtypes. Thus far, no existing automated approaches could effectively segment CARS images due to their low signal-to-noise ratio (SNR) and uneven background. Naturally, manual delineation of cellular structures is time-consuming, subject to individual bias, and restricts the ability to process large datasets. Herein we propose a fully automated nuclear segmentation strategy by coupling superpixel context information and an artificial neural network (ANN), which is, to the best of our knowledge, the first automated nuclear segmentation approach for CARS images. The superpixel technique for local clustering divides an image into small patches by integrating the local intensity and position information. It can accurately separate nuclear pixels even when they possess subtly lower contrast with the background. The resulting patches either correspond to cell nuclei or background. To separate cell nuclei patches from background ones, we introduce the rayburst shape descriptors, and define a superpixel context index that combines information from a given superpixel and it's immediate neighbors, some of which are background superpixels with higher intensity. Finally we train an ANN to identify the nuclear superpixels from those corresponding to background. Experimental validation on three subtypes of lung cancers demonstrates that the proposed approach is fast, stable, and accurate for segmentation of CARS images, the first step in the clinical use of CARS for differential cancer analysis."
1968129,15226,9616,GPU-Based PostgreSQL Extensions for Scalable High-Throughput Pattern Matching,2014,"Numerous fields require large-scale pattern matching to achieve a variety of computational goals. Herein, we present novel graphics processing unit (GPU) extensions that facilitate high-throughput pattern matching in a PostgreSQL database. We have developed an extension framework to perform data block processing of large pattern data sets, using a stream processing design that results in global k-nearest neighbor matches. This framework was specifically designed to support pattern matching on GPU from within the database environment. This approach avoids the necessity of storing an entire data set onto GPU hardware, which facilitates significant scale-up of pattern databases. This provides enormous potential to incorporate or exploit auxiliary (meta)data as part of the pattern matching process, as well as pipelining the results into traditional relational algebra expressions. By pipelining pattern matching results into a relational expression, the power of the database can be leveraged to build result sets based on various parameterized correlations between the query pattern(s) and the results. In this preliminary work, we have integrated GPU-based high-throughput p-norm metric functions into the database server. This allows one to design heterogeneous data processing techniques that combine large-scale content-based image retrieval (CBIR) with traditional data processing capabilities of the database such as relational, spatial, or text search. We present timing characteristics for various pattern sizes and metric combinations, as well as address the balancing of database and GPU parameterization. Our feature vector datasets range from 18 to 85 GB in database table storage size, reaching 100 million 128 dimensional vectors. We are able to efficiently execute global top k searches from within the database."
1617784,15226,390,Which registration method for high resolution fMRI to explore hand movement cortical representation,2012,"Progress in our understanding of brain functions relies on our capability to explore the human cortical surface at a fine scale (typically 1.5 mm isotropic at 3T). For this purpose, high accuracy is required for all processing steps from image acquisition to data analysis. For group studies, the high intersubject variability of the human cortices hampers their precise registration. Based on the hypothesis that function follows anatomy, accurate inter-subject sulci registration should result in precise alignment of corresponding functional regions and then improve the statistical significance of parametric maps. Converging evidence from intra-operative mapping, cytoarchitectony, and functional neuroimaging conclusively rely primary cortical regions to morphological landmarks. Thus, the so-called ”hand knob” landmark, a specific feature of the central sulcus (CS), consistently separates the primary motor cortex (M1), on its anterior bank, from the primary sensory cortex (S1) on its posterior bank. In an effort to define a dedicated processing pipeline for a fine non-invasive exploration of human M1, we compared four nonlinear registration methods applied on high resolution fMRI of basic hand movements. First, based on anatomical measures, we show how recent local or global diffeomorphic techniques improve the alignment of sulcal folds in M1. Second, with functional measures, we quantitatively evaluate their effect on the detection and localization of activation clusters at the population level. Based on such measures, we show that two diffeomorphic methods working globally (DARTEL) or including sulcal constraints (DISCO+DARTEL) improve activation detection and localization opening the way to a non-invasive exploration of the human hand motor cortex."
1600881,15226,390,Tractography density and network measures in Alzheimer'S disease,2013,"Brain connectivity declines in Alzheimer's disease (AD), both functionally and structurally. Connectivity maps and networks derived from diffusion-based tractography offer new ways to track disease progression and to understand how AD affects the brain. Here we set out to identify (1) which fiber network measures show greatest differences between AD patients and controls, and (2) how these effects depend on the density of fibers extracted by the tractography algorithm. We computed brain networks from diffusion-weighted images (DWI) of the brain, in 110 subjects (28 normal elderly, 56 with early and 11 with late mild cognitive impairment, and 15 with AD). We derived connectivity matrices and network topology measures, for each subject, from whole-brain tractography and cortical parcellations. We used an ODF lookup table to speed up fiber extraction, and to exploit the full information in the orientation distribution function (ODF). This made it feasible to compute high density connectivity maps. We used accelerated tractography to compute a large number of fibers to understand what effect fiber density has on network measures and in distinguishing different disease groups in our data. We focused on global efficiency, transitivity, path length, mean degree, density, modularity, small world, and assortativity measures computed from weighted and binary undirected connectivity matrices. Of all these measures, the mean nodal degree best distinguished diagnostic groups. High-density fiber matrices were most helpful for picking up the more subtle clinical differences, e.g. between mild cognitively impaired (MCI) and normals, or for distinguishing subtypes of MCI (early versus late). Care is needed in clinical analyses of brain connectivity, as the density of extracted fibers may affect how well a network measure can pick up differences between patients and controls."
840295,15226,9099,Finding perfect rendezvous on the go: accurate mobile visual localization and its applications to routing,2012,"While on the go, more and more people are using their phones to enjoy ubiquitous location-based services (LBS). One of the fundamental problems of LBS is localization. Researchers are now investigating ways to use a phone-captured image for localization as it contains more scene context information than the embedded sensors. In this paper, we present a novel approach to mobile visual localization that accurately senses geographic scene context according to the current image (typically associated with a rough GPS position). Unlike most existing visual localization methods, the proposed approach is capable of providing a complete set of more accurate parameters about the scene geo---including the actual locations of both the mobile user and perhaps more importantly the captured scene along with the viewing direction. Our approach takes advantage of advanced techniques for large-scale image retrieval and 3D model reconstruction from photos. Specifically, we first perform joint geo-visual clustering in the cloud to generate scene clusters, with each scene represented by a 3D model. The 3D scene models are then indexed using a visual vocabulary tree structure. The phone-captured image is used to retrieve the relevant scene models, then aligned with the models, and further registered to the real-world map. Our approach achieves an estimation accuracy of user location within 14 meters, viewing direction within 9 degrees, and scene location within 21 meters. Such a complete set of accurate geo-parameters can lead to various LBS applications for routing that cannot be achieved with most existing methods. In particular, we showcase three novel applications: 1) accurate self-localization, 2) collaborative localization for rendezvous routing, and 3) routing for photographing. The evaluations through user studies indicate these applications are effective for facilitating the perfect rendezvous for mobile users."
1774529,15226,11052,Part Bricolage: Flow-Assisted Part-Based Graphs for Detecting Activities in Videos,2014,"Space-time detection of human activities in videos can significantly enhance visual search. To handle such tasks, while solely using low-level fea- tures has been found somewhat insufficient for complex datasets; mid-level fea- tures (like body parts) that are normally considered, are not robustly accounted for their inaccuracy. Moreover, the activity detection mechanisms do not con- structively utilize the importance and trustworthiness of the features. This paper addresses these problems and introduces a unified formulation for robustly detecting activities in videos. Our first contribution is the formulation of the detection task as an undirected node- and edge-weighted graphical struc- ture called Part Bricolage (PB), where the node weights represent the type of features along with their importance, and edge weights incorporate the probabil- ity of the features belonging to a known activity class, while also accounting for the trustworthiness of the features connecting the edge. Prize-Collecting-Steiner- Tree (PCST) problem (19) is solved for such a graph that gives the best connected subgraph comprising the activity of interest. Our second contribution is a novel technique for robust body part estimation, which uses two types of state-of-the-art pose detectors, and resolves the plausible detection ambiguities with pre-trained classifiers that predict the trustworthiness of the pose detectors. Our third con- tribution is the proposal of fusing the low-level descriptors with the mid-level ones, while maintaining the spatial structure between the features. For a quantitative evaluation of the detection power of PB ,w e runPB on Hollywood and MSR-Actions datasets and outperform the state-of-the-art by a significant margin for various detection paradigms."
89257,15226,9004,Variable Importance in Nonlinear Kernels (VINK): Classification of Digitized Histopathology,2013,"Quantitative histomorphometry is the process of modeling appearance of disease morphology on digitized histopathology images via image-based features (e.g., texture, graphs). Due to the curse of di- mensionality, building classifiers with large numbers of features requires feature selection (which may require a large training set) or dimension- ality reduction (DR). DR methods map the original high-dimensional features in terms of eigenvectors and eigenvalues, which limits the poten- tial for feature transparency or interpretability. Although methods exist for variable selection and ranking on embeddings obtained via linear DR schemes (e.g., principal components analysis (PCA)), similar methods do not yet exist for nonlinear DR (NLDR) methods. In this work we present a simple yet elegant method for approximating the mapping between the data in the original feature space and the transformed data in the ker- nel PCA (KPCA) embedding space; this mapping provides the basis for quantification of variable importance in nonlinear kernels (VINK). We show how VINK can be implemented in conjunction with the popular Isomap and Laplacian eigenmap algorithms. VINK is evaluated in the contexts of three different problems in digital pathology: (1) predict- ing five year PSA failure following radical prostatectomy, (2) predicting Oncotype DX recurrence risk scores for ER+ breast cancers, and (3) distinguishing good and poor outcome p16+ oropharyngeal tumors. We demonstrate that subsets of features identified by VINK provide sim- ilar or better classification or regression performance compared to the original high dimensional feature sets."
2431699,15226,9616,Hybrid Aggregation of Sparse Coded Descriptors for Food Recognition,2014,"Recent year, with the increasing of unhealthy diets which will threaten people's life due to the various resulted risks such as heart stroke, liver trouble and so on, the maintaining for healthy life has attracted much attention and then how to manage the dietary life is becoming more and more important. In this research, we aim to construct an auto-recognition system of food images and keep the daily food-log records which will contribute to manage dietary life. With the easily available food images taken by mobile phone, it prospects to give the insight about the daily dietary of users with our constructed food recognition system. In order to achieve the acceptable recognition performance of the food images, we propose to apply a sparse model for coding local descriptors extracted from the food images and various pooling methods for aggregating the xoded descriptors. Sparse coding: an extension of vector quantization for local descriptors, which is popularly used in Bag-of-Features (BoF) for image representation, can reconstruct the local descriptors more effective, and then obtain more discriminated feature for food image representation. However, in order to emphasize the strongest activated pattern, the widely applied aggregation strategy of the sparse coded vector is only to retain the maximum coefficient in all (named as Max-pooling), which would completely ignore the frequency: an important signature for identifying different types of images, of the activated patterns. Therefore, we explore a hybrid aggregation strategy named as top-ranked average pooling (TRAP), which integrates not only the maximum activated magnitude but also the stronger activated number for image representation. Experiments validate that the proposed hybrid aggregation strategy combined with sparse model can greatly improve the recognition rates compared with the conventional BOF model and the state-of-the-art methods on two databases: our constructed RFID and the public PFID."
1429723,15226,9616,Watch-List Screening Using Ensembles Based on Multiple Face Representations,2014,"Still-to-video face recognition (FR) is an important function in watch list screening, where faces captured over a network of video surveillance cameras are matched against reference stills of target individuals. Recognizing faces in a watch list is a challenging problem in semi --and unconstrained surveillance environments due to the lack of control over capture and operational conditions, and to the limited number of reference stills. This paper provides a performance baseline and guidelines for ensemble-based systems using a single high-quality reference still per individual, as found in many watch list screening applications. In particular, modular systems are considered, where an ensemble of template matchers based on multiple face representations is assigned to each individual of interest. During enrollment, multiple feature extraction (FE) techniques are applied to patches isolated in the reference still to generate diverse face-part representations that are robust to various nuisance factors (e.g., illumination and pose) encountered in video surveillance. The selection of relevant feature subsets, decision thresholds, and fusion functions of ensembles are achieved using faces of non-target individuals selected from reference videos (forming a universal background model). During operations, a face tracker gradually regroups faces captured from different people appearing in a scene, while each user-specific ensemble generates a decision per face capture. This leads to robust spatio-temporal FR when accumulated ensemble predictions surpass a detection threshold. Simulation results obtained with the Chokepoint video dataset show a significant improvement to accuracy, (1) when performing score-level fusion of matchers, where patches-based and FE techniques generate ensemble diversity, (2) when defining feature subsets and decision thresholds for each individual matcher of an ensemble using non-target videos, and (3) when accumulating positive detections over multiple frames."
444694,15226,9616,Semantic saliency using k-TR theory of visual perception,2012,"Saliency in 2D imagery has been receiving increasing attention over the last few years owing to the need to minimize computation requirements through visual search space reduction, especially in the field of domestic robotics. Saliency and pre-attention mechanisms such as the Itti-Koch model have largely been focused on multi-scale local features mimicking low level attention processes in visual system, without any regard for the semantic content of the scene and therefore any cognitive grounding in visual processing. The ‘k-TR’ theory presents the first attempt at a true cognitive understanding of scenes by explaining visual perception and object recognition, in terms of Recognition of Component Affordances (RBCA). The k-TR model, presents a bi-layer recognition process through a combination of local, global, semantic and affordance features. The k-TR theory provides psychophysical, neurobiological, linguistic and evolutionary studies to support the theory and explains recognition of over 250 categories of common household objects. The features used by k-TR for object representation, termed as k-TRONs are available from the publicly available Affordance Network database (AfNet). In this paper, we use the k-TRON features, in particular the 35+ affordance features, in order to incorporate semantic context into saliency models. Saliency or surprise for pre-attention is modeled in the form of affordance aberrations. By using affordance aberration features for conspicuity map generation, we show that the resulting saliency and attention points more closely resemble the salient regions or surprise regions generated by the human visual system, hence providing superior performance in comparison to the Itti framework. Furthermore, by learning of affordance affinities from test subjects, the degree of influence of each affordance aberration towards visual saliency is estimated and incorporated into the overall saliency model."
1603940,15226,9078,Geodesic methods for biomedical image segmentation,2014,"Tubular and tree structures appear very commonly in biomedical images like vessels, microtubules or neuron cells. Minimal paths have been used for long as an interactive tool to segment these structures as cost minimizing curves. The user usually provides start and end points on the image and gets the minimal path as output. These minimal paths correspond to minimal geodesics according to some adapted metric. They are a way to find a (set of) curve(s) globally minimizing the geodesic active contours energy. Finding a geodesic distance can be solved by the Eikonal equation using the fast and efficient Fast Marching method. In the past years we have introduced different extensions of these minimal paths that improve either the interactive aspects or the results. For example, the metric can take into account both scale and orientation of the path. This leads to solving an anisotropic minimal path in a 2D or 3D+radius space. On a different level, the user interaction can be minimized by adding iteratively what we called the keypoints, for example to obtain a closed curve from a single initial point. The result is then a set of minimal paths between pairs of keypoints. This can also be applied to branching structures in both 2D and 3D images. We also proposed different criteria to obtain automatically a set of end points of a tree structure by giving only one starting point. More recently, we introduced a new general idea that we called Geodesic Voting or Geodesic Density. The approach consists in computing geodesics between a given source point and a set of points scattered in the image. The geodesic density is defined at each pixel of the image as the number of geodesics that pass over this pixel. The target structure corresponds to image points with a high geodesic density. We will illustrate different possible applications of this approach. The work we will present involved as well F. Benmansour, Y. Rouchdy and J. Mille at CEREMADE."
804866,15226,8502,Multiple foreground recognition and cosegmentation: An object-oriented CRF model with robust higher-order potentials,2014,"Localizing, recognizing, and segmenting multiple foreground objects jointly from a general user's photo stream that records a specific event is an important task with many useful applications. As argued in recent Multiple Foreground Cosegmentation (MFC) work by Kim and Xing, this task is very challenging in that it contrasts substantially from the classical cosegmentation problem, and aims to parse a set of realistic event photos but each containing irregularly occurring multiple foregrounds with high appearance and scene configuration variations. Inspired by the impressive advance in scene understanding and object recognition, this paper casts the multiple foreground recognition and cosegmentation (MFRC) problem within a conditional random fields (CRFs) framework in a principled manner. We capitalize centrally on the key objective that MFRC is to segment out and annotate foreground objects or “things” rather than “stuff”. To this end, we exploit a few complementary objectness cues (e.g. contours, object detectors and layout) and propose novel and efficient methods to capture object-level information. Integrating object potentials as soft constraints (e.g. robust higher-order potentials defined over detected object regions) with low-level unary and pairwise terms holistically, we solve the MFRC task with a probabilistic CRF model. The inference for such a CRF model is performed efficiently with graph cut based move making algorithms. With a minimal amount of user annotations on just a few example photos, the proposed approach produces spatially coherent, boundary-aligned segmentation results with correct and consistent object labeling. Experiments on the FlickrMFC dataset justify that our method achieves state-of-the-art performance."
761894,15226,8502,Small Hand-held Object Recognition Test (SHORT),2014,"The ubiquity of smartphones with high quality cameras and fast network connections will spawn many new applications. One of these is visual object recognition, an emerging smartphone feature which could play roles in high-street shopping, price comparisons and similar uses. There are also potential roles for such technology in assistive applications, such as for people who have visual impairment. We introduce the Small Hand-held Object Recognition Test (SHORT), a new dataset that aims to benchmark the performance of algorithms for recognising hand-held objects from either snapshots or videos acquired using hand-held or wearable cameras. We show that SHORT provides a set of images and ground truth that help assess the many factors that affect recognition performance. SHORT is designed to be focused on the assistive systems context, though it can provide useful information on more general aspects of recognition performance for hand-held objects. We describe the present state of the dataset, comprised of a small set of high quality training images and a large set of nearly 135,000 smartphone-captured test images of 30 grocery products. In this version, SHORT addresses another context not covered by traditional datasets, in which high quality catalogue images are being compared with variable quality user-captured images; this makes the matching more challenging in SHORT than other datasets. Images of similar quality are often not present in “database” and “query” datasets, a situation being increasingly encountered in commercial applications. Finally, we compare the results of popular object recognition algorithms of different levels of complexity when tested against SHORT and discuss the research challenges arising from the particularities of visual object recognition from objects that are being held by users."
2259788,15226,11470,Hidden Markov Model for eye gaze prediction in networked video streaming,2011,"With the advent of eye gaze tracking technology, eye gaze is increasingly being used as a media interaction trigger in a variety of applications, such as eye typing, video content customization, and network video streaming based on region-of-interest (ROI). The reaction time of a gaze-based networked system, however, is in practice lower-bounded by the round trip time (RTT) of today's networks, which can be large. To improve the efficacy of gaze-based networked systems, in the paper we propose a Hidden Markov Model (HMM)-based gaze prediction strategy to predict future gaze locations to lower end-to-end reaction delay. We first design an HMM with three states corresponding to human's three major types of intrinsic eye movements. HMM parameters are obtained offline on a per-video basis during training phase. During testing phase, a window of noisy gaze observations are collected in real-time as input to a forward algorithm, which computes the most likely HMM state. Given the deduced HMM state, linear prediction is used to predict gaze location RTT seconds into the future. We demonstrate the applicability of our gaze prediction strategy by focusing on ROI-based bit allocation for network video streaming. To reduce transmission rate of a video stream without degrading viewer's perceived visual quality, we allocate more bits to encode the viewer's current spatial ROI, while devoting fewer bits in other spatial regions. The challenge lies in overcoming the delay between the time a viewer's ROI is detected by gaze tracking, to the time the effected video is encoded, delivered and displayed at the viewer's terminal. To this end, we use our proposed gaze-prediction strategy to predict future eye gaze locations, so that optimized bit allocation can be performed for future frames. Our experiments show that bit rate can be reduced by 21% without noticeable visual quality degradation when end-to-end network delay is as high as 200ms."
1538475,15226,23735,A back-end L 1 norm based solution for factor graph SLAM,2013,"Graphical models jointly with non linear optimization have become the most popular approaches for solving SLAM and Bundle Adjustment problems: using a non linear least squares (NLSQs) description of the problem, these math tools serve to formalize the minimization of an error cost function that relates state variables through relative sensor observations. The simplest case just considers as state variables the locations of the sensor/robot in the environment deriving in a pose graph subproblem. In general, the cost function is based on the L 2  norm whose principal iterative solutions exploit the sparse connectivity of the corresponding Gaussian Markov Field (GMRF) or the Factor Graph, whose adjacency matrices are given by the fill-in of the Hessian and Jacobian of the cost function respectively. In this paper we propose a novel solution based on the L 1  norm as a back-end to the pose graph subproblem. In contrast to other NLSQs approaches, we formulate an iterative algorithm inspired directly on the Factor Graph structure to solve for the linearized residual ∥Ax - b∥ 1 . Under the presence of spurious measurements the L 1  based solution can achieve similar results to the robust Huber norm. Indeed, our main interest in L 1  optimization is that it opens the door to the set of more robust non-convex L p  norms where p ≤ 1. Since our approach depends on the minimization of a non differentiable function, we provide the theoretical insights to solve for the L 1  norm. Our optimization is based on a primal-dual formulation successfully applied for solving variational convex problems in computer vision. We show the effectiveness of the L 1  norm to produce both a robust initial seed and a final optimized solution on challenging and well known datasets widely used in other state of the art works."
1280846,15226,9616,Nonparametric Discovery of Learning Patterns and Autism Subgroups from Therapeutic Data,2014,"Autism Spectrum Disorder (ASD) is growing at a staggering rate, but, little is known about the cause of this condition. Inferring learning patterns from therapeutic performance data, and subsequently clustering ASD children into subgroups, is important to understand this domain, and more importantly to inform evidence-based intervention. However, this data-driven task was difficult in the past due to insufficiency of data to perform reliable analysis. For the first time, using data from a recent application for early intervention in autism (TOBY Play pad), whose download count is now exceeding 4500, we present in this paper the automatic discovery of learning patterns across 32 skills in sensory, imitation and language. We use unsupervised learning methods for this task, but a notorious problem with existing methods is the correct specification of number of patterns in advance, which in our case is even more difficult due to complexity of the data. To this end, we appeal to recent Bayesian nonparametric methods, in particular the use of Bayesian Nonparametric Factor Analysis. This model uses Indian Buffet Process (IBP) as prior on a binary matrix of infinite columns to allocate groups of intervention skills to children. The optimal number of learning patterns as well as subgroup assignments are inferred automatically from data. Our experimental results follow an exploratory approach, present different newly discovered learning patterns. To provide quantitative results, we also report the clustering evaluation against K-means and Nonnegative matrix factorization (NMF). In addition to the novelty of this new problem, we were able to demonstrate the suitability of Bayesian nonparametric models over parametric rivals."
2353692,15226,8502,Historical comparison of vehicles using scanned x-ray images,2011,"X-ray scanners are increasingly used for scanning vehicles crossing international borders or entering critical infrastructure installations. The ability to penetrate through steel and other opaque materials and the nondestructive nature of x-ray radiation make them ideal for finding drugs, explosives and other contraband. In many situations, the same vehicles cross the checkpoint repeatedly, such as the employee vehicles entering a high-risk facility or cargo vehicles crossing international borders back and forth. Manual analysis of these images puts extra burden on the operator and results in slow throughput. In this paper we report an integrated and fully automated system to solve this problem. In the first stage of the algorithm, a model-based segmentation approach is used to find the vehicle outline. It proceeds by first using background subtraction to find the overall body of the vehicle. Next, we find the outlines of tires by using rotating edge detection kernels. The lower outline of the vehicle is found using active contours. We then use a deformable registration approach to align the vehicles which is specifically designed for the requirements of this problem. An intensity normalization step is then performed to account for the intensity variations between the scans at two time points. We use a histogram-based approach that scales and shifts the histogram of one image to match that of the other. The differences between the two inspection results are computed next. We then apply knowledge-based rules to remove false alarms such as lights and driver's body. The system is specifically designed for back-scatter x-ray imaging which is a powerful modality for detecting organic materials such as drugs and explosives. We have applied this system to images scanned by a deployed x-ray scanner and have achieved satisfactory results."
1155931,15226,9773,Document Recognition without Strong Models,2011,"Can a high-performance document image recognition system be built without detailed knowledge of the application? Having benefited from the statistical machine learning revolution of the last twenty years, our architectures rely less on hand-crafted special-case rules and more on models trained on labeled-sample data sets. But urgent questions remain. When we can't collect (and label) enough real training data, does it help to complement them with data synthesized using generative models? Is it ever completely safe to rely on synthetic data? If we can't manage to train (or craft) a single complete, near-perfect, application-specific strong model to drive recognition, can we make progress by combining several imperfect or incomplete weak models? Can recognition that is carried out jointly over weak models perform optimally while still running fast? Can a recognizer automatically pick a strong model of its input? Must we always pre-train models for every kind (style) of input expected, or can a recognizer adapt to unknown styles? Can weak models adapt autonomously, growing stronger and so driving accuracy higher, without any human intervention? Can one model criticize -- and then proceed to correct -- other models, even while it is being criticized and corrected in turn by them? After twenty-five years of research on these questions we have partial answers, many in the affirmative: in addition to promising laboratory demonstrations, we can take pride in successful applications. I'll illustrate the evolution of the state of the art with concrete examples, and point out open problems."
950873,15226,9015,FOCUS: clustering crowdsourced videos by line-of-sight,2013,"We present a demonstration of FOCUS [1], a system to appear in the  SenSys 2013  main conference. FOCUS is a video-clustering service for live user video streams, indexed automatically and in realtime by shared content. FOCUS uniquely leverages visual, 3D model reconstruction and multimodal sensing to decipher and continuously track a video's line-of-sight. Through spatial reasoning on the relative geometry of multiple video streams, FOCUS recognizes shared content even when viewed from diverse angles and distances. We believe FOCUS can enable a new family of applications, such as instant replay, augmented reality, citizen journalism, security breach detection, and disaster assessment.   In the demonstration, we will show 325 video clips taken at Duke University Wallace Wade Stadium being processed in real-time via FOCUS pipeline. The recorded video clips contain one of three spots in the stadium: East Stand, Scoreboard, and West Stand. The demo shall be shown in the form of a web interface, first showing randomly clustered video clips. Later, on a button click, FOCUS shall process displayed videos in real-time, outputting clusters of videos, containing the common shared subject in each of them. For each successfully processed video clip in a cluster, we will further show similar clips from near, medium, and wide angle. To display performance and accuracy of FOCUS in indoor environments, a similar demonstration will be shown for an office space. FOCUS shall run on multi-node Hadoop cluster built on top of IBM SmartCloud platform."
2594467,15226,9004,Multi-part left atrium modeling and segmentation in C-arm CT volumes for atrial fibrillation ablation,2011,"As a minimally invasive surgery to treat left atrial (LA) fibrillation, catheter based ablation uses high radio-frequency energy to eliminate potential sources of the abnormal electrical events, especially around the ostia of pulmonary veins (PV). Due to large structural variations of the PV drainage pattern, a personalized LA model is helpful to translate a generic ablation strategy to a specific patient's anatomy. Overlaying the LA model onto 2D fluoroscopic images provides valuable visual guidance during surgery. A holistic shape model is not accurate enough to represent the whole shape population of the LA. In this paper, we propose a part based LA model (including the chamber, appendage, and four major PVs) and each part is a much simpler anatomical structure compared to the holistic one. Our approach works on un-gated C-arm CT, where thin boundaries between the LA blood pool and surrounding tissues are often blurred due to the cardiac motion artifacts (which presents a big challenge compared to the highly contrasted gated CT/MRI). To avoid segmentation leakage, the shape prior is exploited in a model based approach to segment the LA parts. However, independent detection of each part is not optimal and its robustness needs further improvement (especially for the appendage and PVs). We propose to enforce a statistical shape constraint during the estimation of pose parameters (position, orientation, and size) of different parts. Our approach is computationally efficient, taking about 1.5 s to process a volume with 256×256×250 voxels. Experiments on 469 C-arm CT datasets demonstrate its robustness."
853405,15226,30,GPU-Based Visualization and Synchronization of 4-D Cardiac MR and Ultrasound Images,2012,"In minimally invasive image-guided interventions, different imaging modalities, such as magnetic resonance imaging (MRI), computed tomography (CT), and 3-D ultrasound (US), can provide complementary, multispectral image information. Dynamic image registration is a well-established approach that permits real-time diagnostic information to be enhanced by placing lower-quality real-time images within a high quality anatomical context. For the guidance of cardiac interventions, it would be valuable to register dynamic MRI or CT with intra-operative US. However, in practice, either the high computational cost prohibits such real-time visualization, or else the resulting image quality is not satisfactory for accurate interventional guidance. Modern graphics processing units (GPUs) provide the programmability, parallelism and increased computational precision to address this problem. In this paper, we first outline our research on dynamic 3-D cardiac MR and US image acquisition, real-time dual-modality registration and US tracking. Next, we describe our contributions on image processing and optimization techniques for 4-D (3-D + time) cardiac image rendering, and our GPU-accelerated methodologies for multimodality 4-D medical image visualization and optical blending, along with real-time synchronization of dual-modality dynamic cardiac images. Finally, multiple transfer functions, various image composition schemes, and an extended window-level setting and adjustment approach are proposed and applied to facilitate the dynamic volumetric MR and US cardiac data exploration and enhance the feature of interest of US image that is usually restricted to a narrow voxel intensity range."
778775,15226,9078,Saliency-cognizant robust view synthesis in free viewpoint video streaming,2013,"In free viewpoint video, texture and depth maps from two camera-captured viewpoints are transmitted, so that at receiver, a novel virtual view chosen by the client can be synthesized via depth-image-based rendering (DIBR). When irrecoverable packet losses occur during transmission-typically affecting less important spatial regions in the video given unequal error protection (UEP) is deployed-appropriate error concealment strategies must be used at decoder to minimize resulting visual degradation in the synthesized view. Towards this goal, we propose a new optimization framework based on visual saliency to combine two different concealment techniques. First, given a pixel in the virtual view is typically constructed as a convex combination of corresponding pixels in the left and right captured views, weighted pixel blending (WPB) readjusts the weights in the linear sum to reflect the expected error in code blocks that contain the corresponding pixels. Second, exemplar-based patch matching (EPM) finds the most similar patches in the known spatial region to complete missing pixels in the unknown region. To choose between candidates constructed using the two techniques when filling a given pixel patch in the synthesized view, we first compute a weighted sum of expected error and visual saliency for each candidate patch. The candidate with the smaller sum (one with small expected error and visual saliency, so that even if errors do occur, they do not stand out visually) is selected for pixel completion. Experimental results show that our scheme can outperform the use of co-located blocks from a previous frame by up to 0.7dB in PSNR and improve subjective visual quality."
786079,15226,390,Reconstruction and quantification of neuronal morphology,2012,"The human brain with all its faculties and intricacies has fascinated many generations of researchers [1] and will likely be the final frontier of science. Understanding the principles underlying the brain's higher-order cognitive functions is indeed a major challenge and will profoundly impact our views on what defines a human being. On a more down-to-earth level, knowledge of the structure, function, and development of neuronal cells and networks is of crucial importance in investigating potential causes of neurological and psychiatric disorders and developing effective drugs and therapies for treating them. Research in this area is increasingly relying on imaging and gives rise to large amounts of image data. The need for advanced bioimage informatics [2] and neuroinformatics [3] solutions for analyzing these data is therefore rising rapidly. One of the key challenges here is the development of computational methods and tools for the study of neuronal anatomy [4]. Studying the morphological properties of neuronal arborizations first requires converting the usually large and sparse image data into a more parsimonious representation that captures the essential image information and is easier to archive, exchange, analyze, and compare. In the past decades, quite a number of methods have been developed for this purpose [5, 6, 7], but the quest for more robust, fully automatic, and generally applicable tools continues. The goal of this presentation is to survey the state of the art in the field for anyone interested in taking up the challenge. Relevant aspects addressed in the presentation include common image segmentation approaches for neuronal reconstruction, quantitative measures of neuronal morphology, currently available software tools, and morphology databases."
781562,15226,390,PET in drug development,2012,"Positron emission tomography (PET) is a tomographic imaging technique that allows for accurate non-invasive in vivo measurements of regional tissue function in man. It is the most selective and sensitive imaging modality able to measure molecular pathways and interactions in vivo at a picomolar level. PET is an important tool in drug development and can be used in different ways. Firstly, it is possible to label the drug itself with a positron emitter. This enables direct measurements of tissue drug concentrations, allowing for an assessment whether these concentrations are high enough for therapeutic purposes. An example is the uptake of labelled anti-cancer drugs in tumours, which can be compared with uptake in normal tissue (toxicity). Secondly, it is possible to measure perfusion or metabolism following or during treatment. Examples are measurements of tumour perfusion following anti-angiogenesis treatment and tumour glucose metabolism as a marker of response to chemotherapy. The latter is now accepted as a surrogate endpoint in trials with new anticancer drugs. Finally, receptor occupancy and enzyme inhibition can be measured using established radioligands for the molecular targets under investigation. The advantage of this approach is that it does not require novel radiochemistry or kinetic models. Receptor occupancy or enzyme activity can be measured as function of administered drug dose and biological clearance of the drug can be measured by performing scans at various times after drug administration. Both optimal dose and dosing regimen can be determined from studies with less than ten subjects. PET studies can be performed not only in patients, but also in animal models of disease. Since PET is non-invasive, it is possible to perform repeat measurements in the same animal. This is a major advantage as an experimental animal can be used as its own control."
2446874,15226,390,Validation of a non-rigid registration method for motion compensation in 4D ultrasound of the liver,2013,"Future therapy using focused ultrasound (FUS) to treat tumors in abdominal organs, such as the liver, must incorporate motion tracking of these organs due to breathing and drift caused by gravity and intestines (peristalsis). Motion tracking of the target (e.g. tumor) is needed to ensure accurately located sonications. We have performed a quantitative validation of a methodology for motion tracking of the liver with 4D (3D+time) ultrasound. The offline analysis was done using a recently published non-rigid registration algorithm that was specifically designed for motion estimation from dynamic imaging data. The method registers the entire 4D sequence in a group-wise optimization fashion, thus avoiding a bias towards a specifically chosen reference time point. Both spatial and temporal smoothness of the transformations are enforced by using a 4D free-form B-spline deformation model. For our evaluation, three healthy volunteers were scanned over several breath cycles from three different positions and angles on the abdomen (totally nine 4D scans). A skilled physician performed the scanning and manually annotated well-defined anatomic landmarks for assessment of the automatic algorithm. Four engineers each annotated these points in all time frames, the mean of which was taken as a gold standard. The error of the automatic motion estimation method was compared with inter-observer variability. The registration method estimated liver motion better than the observers and had an error (75% percentile over all datasets) of 1 mm. We conclude that the methodology was able to accurately track the motion of the liver in the 4D ultrasound data."
2138280,15226,8960,Image Parsing with Stochastic Scene Grammar,2011,"This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classifiers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative + relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive - relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efficient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene configurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to find the most probable configuration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree."
2374845,15226,8960,Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction,2014,"Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation leaking, optical flow bleeding etc. In this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and NRSfM for 3D reconstruction of objects in videos. For each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3D shape under different rank bounds of the trajectory matrix. We show that dense 3D shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera. We achieve competitive results on a public NRSfM benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches. We further test our approach on popular video segmentation datasets. To the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in Youtube or Hollywood movies, without object-specific priors."
825784,15226,9078,Hybrid parametric-nonparametric modeling with application to natural image upsampling,2011,"Linear autoregressive (AR) model is widely used in signal processing. Usually the AR models are solved by classical least square (LS) method. An important issue with the LS solution of the AR model, which has been seemingly overlooked, is its numerical stability. The issue is related to the rank condition of the design matrix. We observed, in case of natural images, that the probability of numerical rank deficiency is rather high, roughly thirty-five per cent, due to discrete nature and structures of the digital images. Without care numerical rank deficiency can adversely affect the parameter estimation of the AR model. In this paper we use the rank revealing QR (RRQR) factorization to select optimal subset from the design matrix so as to effectively lower the condition number of the system. By removing the ill conditioned part of the right orthogonal matrix of the RRQR decomposition, we obtain a robust truncated solution to the linear system. On the other hand, for natural images, the unselected data tend to highly correlate with the pixel being modeled, and their exclusion from the modeling process waste valuable information. To avoid this loss we recycle the data including those discard by the parametric AR estimator into a nonparametrgic model of nonlocal type. Interestingly, the data that cause ill condition to the parametric AR model are of high quality for the non-local nonparametric modeling. Therefore, an approach of hybrid parametric-nonparametric modeling can make the best use of data and improve the model performance. The hybrid modeling approach is applied to image resolution upconversion, and it greatly improves the performance of the state-of-the-art image interpolator, achieving a gain of 3dB or more in PSNR in some cases."
2477543,15226,11052,As-Rigid-As-Possible Stereo under Second Order Smoothness Priors,2014,"Imposing smoothness priors is a key idea of the top-ranked global stereo models. Recent progresses demonstrated the power of sec- ond order priors which are usually defined by either explicitly consid- ering three-pixel neighborhoods, or implicitly using a so-called 3D-label for each pixel. In contrast to the traditional first-order priors which only prefer fronto-parallel surfaces, second-order priors encourage arbitrary collinear structures. However, we still can find defective regions in match- ing results even under such powerful priors, e.g., large textureless regions. One reason is that most of the stereo models are non-convex, where pixel- wise smoothness priors, i.e., local constraints, are too flexible to prevent the solution from trapping in bad local minimums. On the other hand, long-range spatial constraints, especially the segment-based priors, have advantages on this problem. However, segment-based priors are too rigid to handle curved surfaces. We present a mixture model to combine the benefits of these two kinds of priors, whose energy function consists of two terms 1) a Laplacian operator on the disparity map which imposes pixel-wise second-order smoothness; 2) a segment-wise matching cost as a function of quadratic surface, which encourages as-rigid-as-possible smoothness. To effectively solve the problem, we introduce an interme- diate term to decouple the two subenergies, which enables an alternated optimization algorithm that is about an order of magnitude faster than PatchMatch (1). Our approach is one of the top ranked models on the Middlebury benchmark at sub-pixel accuracy."
1192961,15226,21106,iGroup: Weakly supervised image and video grouping,2011,"We present a generic, efficient and iterative algorithm for interactively clustering classes of images and videos. The approach moves away from the use of large hand labelled training datasets, instead allowing the user to find natural groups of similar content based upon a handful of “seed” examples. Two efficient data mining tools originally developed for text analysis; min-Hash and APriori are used and extended to achieve both speed and scalability on large image and video datasets. Inspired by the Bag-of-Words (BoW) architecture, the idea of an image signature is introduced as a simple descriptor on which nearest neighbour classification can be performed. The image signature is then dynamically expanded to identify common features amongst samples of the same class. The iterative approach uses APriori to identify common and distinctive elements of a small set of labelled true and false positive signatures. These elements are then accentuated in the signature to increase similarity between examples and “pull” positive classes together. By repeating this process, the accuracy of similarity increases dramatically despite only a few training examples, only 10% of the labelled groundtruth is needed, compared to other approaches. It is tested on two image datasets including the caltech101 [9] dataset and on three state-of-the-art action recognition datasets. On the YouTube [18] video dataset the accuracy increases from 72% to 97% using only 44 labelled examples from a dataset of over 1200 videos. The approach is both scalable and efficient, with an iteration on the full YouTube dataset taking around 1 minute on a standard desktop machine."
1506607,15226,390,A new shape-based framework for the left ventricle wall segmentation from cardiac first-pass perfusion mri,2013,"We propose a shape-based approach for the segmentation of the left ventricle (LV) wall on cardiac first-pass magnetic resonance imaging (FP-MRI) using level sets. To reduce the variabilities of the LV wall in FP-MRI, it is first imperative to co-align the time series images to account for the global and local motions of the heart. Therefore, we developed a two-step registration methodology that includes an affine-based registration followed by a local B-splines based alignment to maximize a similarity function that accounts for the first- and second-order normalized mutual information (NMI). Additionally, myocardial signal intensity varies with the agent transit, which makes it difficult to control the level set evolution using image intensities alone. Thus, we constrained the level set evolution using three features: a weighted probabilistic shape prior, the first-order pixel-wise image intensities, and a second-order Markov-Gibbs random field (MGRF) spatial interaction model. We tested our approach on 24 data sets in 8 infarction patients using the Dice similarity coefficient (DSC), comparing our approach to other shape-based segmentation approaches. We also tested the performance of our segmentation approach using the receiver operating characteristics (ROC). Our approach achieved a mean DSC value of 0.910±0.037 compared to other shape-based methods that achieved 0.862±0.045 and 0.844±0.047. Finally, the ROC analysis for our segmentation method showed the best performance, with area under the ROC curve of 0.92, while that for intensity showed the worst performance, with area under the ROC curve of 0.69."
669088,15226,9078,Adaptive frame and QP selection for temporally super-resolved full-exposure-time video,2011,"In order to allow sufficient amount of light into the image sensor, videos captured in poor lighting conditions typically have low frame rate and frame exposure time equals to inter-frame period — commonly called full exposure time (FET). FET low-frame-rate videos are common in situations where lighting cannot be improved a priori due to practical (e.g., large physical distance between camera and captured objects) or economical (e.g., long duration of nighttime surveillance) reasons. Previous computer vision work has shown that content at a desired higher frame rate can be recovered (to some degree of precision) from the captured FET video using self-similarity-based temporal super-resolution. For a network streaming scenario, where a client receives a FET video stream from a server and plays back in real-time, the following practical question remains, however: what is the most suitable representation of the captured FET video at encoder, given that a video at higher frame rate must be constructed at the decoder at low complexity? In this paper, we present an adaptive frame and quantization parameter (QP) selection strategy, where, for a given targeted rate-distortion (RD) tradeoff, FET video frames at appropriate temporal resolutions and QP are selected for encoding using standard H.264 tools at encoder. At the decoder, temporal super-resolution is performed at low complexity on the decoded frames to synthesize the desired high frame rate video for display in real-time. We formulate the selection of individual FET frames at different temporal resolutions and QP as a shortest path problem to minimize Lagrangian cost of the encoded sequence. Then, we propose a computation-efficient algorithm based on monotonicity in predictor's temporal resolution and QP to find the shortest path. Experiments show that our strategy outperforms alternative na¨ive non-adaptive approaches by up to 1.3dB at the same bitrate."
1830883,15226,30,A Learning-Based Similarity Fusion and Filtering Approach for Biomedical Image Retrieval Using SVM Classification and Relevance Feedback,2011,"This paper presents a classification-driven biomedical image retrieval framework based on image filtering and similarity fusion by employing supervised learning techniques. In this framework, the probabilistic outputs of a multiclass support vector machine (SVM) classifier as category prediction of query and database images are exploited at first to filter out irrelevant images, thereby reducing the search space for similarity matching. Images are classified at a global level according to their modalities based on different low-level, concept, and keypoint-based features. It is difficult to find a unique feature to compare images effectively for all types of queries. Hence, a query-specific adaptive linear combination of similarity matching approach is proposed by relying on the image classification and feedback information from users. Based on the prediction of a query image category, individual precomputed weights of different features are adjusted online. The prediction of the classifier may be inaccurate in some cases and a user might have a different semantic interpretation about retrieved images. Hence, the weights are finally determined by considering both precision and rank order information of each individual feature representation by considering top retrieved relevant images as judged by the users. As a result, the system can adapt itself to individual searches to produce query-specific results. Experiment is performed in a diverse collection of 5 000 biomedical images of different modalities, body parts, and orientations. It demonstrates the efficiency (about half computation time compared to search on entire collection) and effectiveness (about 10%-15% improvement in precision at each recall level) of the retrieval approach."
350451,15226,20515,Model interpolation for eye localization using the Discriminative Generalized Hough Transform,2012,"The Discriminative Generalized Hough Transform (DGHT) is a general method for the localization of arbitrary objects with well-defined shape, which has been successfully applied in medical image processing. In this contribution, the framework is used for eye localization in the public PUT face database. The DGHT combines the Generalized Hough Transform (GHT) with a discriminative training procedure to generate GHT shape models with individual positive and negative model point weights. Based on a set of training images with annotated target points, the individual votes of model points in the Hough space are combined in a maximum-entropy probability distribution and the free parameters are optimized with respect to the training error rate. The estimated model point specific weights reflect the important model structures to distinguish the target object from other confusable image parts. Additionally, the point weights allow for a determination of irrelevant parts in the model, which can be eliminated to make space for new model point candidates from training images with high localization error. The iterative training procedure of weight estimation, point elimination, testing on training images, and incorporation of new model point candidates is repeated until a stopping criterion is reached. Furthermore, the DGHT framework incorporates a multi-level approach, in which the searched region is reduced in 6 zooming steps, using individually trained shape models. In order to further enhance the robustness of the method, the DGHT framework is, for the first time, extended by a linear model interpolation for the trained left and right eye model. An evaluation on the PUT face database has shown a success rate of 99% for iris detection in frontal-view images and 97% if the test set contains a large head pose variability."
727867,15226,9078,Joint gaze-correction and beautification of DIBR-synthesized human face via dual sparse coding,2014,"Gaze mismatch is a common problem in video conferencing, where the viewpoint captured by a camera (usually located above or below a display monitor) is not aligned with the gaze direction of the human subject, who typically looks at his counterpart in the center of the screen. This means that the two parties cannot converse eye-to-eye, hampering the quality of visual communication. One conventional approach to the gaze mismatch problem is to synthesize a gaze-corrected face image as viewed from center of the screen via depth-image-based rendering (DIBR), assuming texture and depth maps are available at the camera-captured viewpoint(s). Due to self-occlusion, however, there will be missing pixels in the DIBR-synthesized view image that require satisfactory filling. In this paper, we propose to jointly solve the hole-filling problem and the face beautification problem (subtle modifications of facial features to enhance attractiveness of the rendered face) via a unified dual sparse coding framework. Specifically, we first train two dictionaries separately: one for face images of the intended conference subject, one for images of “beautiful” human faces. During synthesis, we simultaneously seek two code vectors — one is sparse in the first dictionary and explains the available DIBR-synthesized pixels, the other is sparse in the second dictionary and matches well with the first vector up to a restricted linear transform. This ensures a good match with the intended target face, while increasing proximity to “beautiful” facial features to improve attractiveness. Experimental results show naturally rendered human faces with noticeably improved attractiveness."
622810,15226,11052,Space-variant descriptor sampling for action recognition based on saliency and eye movements,2012,"Algorithms using bag of features-style video representations currently achieve state-of-the-art performance on action recognition tasks, such as the challenging Hollywood2 benchmark [1,2,3]. These algorithms are based on local spatiotemporal descriptors that can be extracted either sparsely (at interest points) or densely (on regular grids), with dense sampling typically leading to the best performance [1]. Here, we investigate the benefit of space-variant processing of inputs, inspired by attentional mechanisms in the human visual system. We employ saliency-mapping algorithms to find informative regions and descriptors corresponding to these regions are either used exclusively, or are given greater representational weight (additional codebook vectors). This approach is evaluated with three state-of-the-art action recognition algorithms [1,2,3], and using several saliency algorithms. We also use saliency maps derived from human eye movements to probe the limits of the approach. Saliency-based pruning allows up to 70% of descriptors to be discarded, while maintaining high performance on Hollywood2. Meanwhile, pruning of 20-50% (depending on model) can even improve recognition. Further improvements can be obtained by combining representations learned separately on salience-pruned and unpruned descriptor sets. Not surprisingly, using the human eye movement data gives the best mean Average Precision (mAP; 61.9%), providing an upper bound on what is possible with a high-quality saliency map. Even without such external data, the Dense Trajectories model [1] enhanced by automated saliency-based descriptor sampling achieves the best mAP (60.0%) reported on Hollywood2 to date."
2313870,15226,11491,Learning to Rank Similar Apparel Styles with Economically-Efficient Rule-Based Active Learning,2014,"Increasingly, people define and express themselves in online social networks, such as Facebook and Instagram, by uploading photos showing the clothes they wear. As a result, such online social networks are becoming major sources of inspiration, with users looking for others with similar clothing style. In this paper, we propose a novel learning to rank (L2R) algorithm for finding similar apparel style given a query image. L2R algorithms use a labeled training set to generate a ranking model that can later be used to rank new query results. These training sets, however, are costly and laborious to produce, requiring human annotators to assess the relevance of candidate images in relation to a query. Active learning algorithms are able to reduce the labeling effort by selectively sampling an unlabeled set of images and choosing the subset that maximizes a learning function's effectiveness. Specifically, our proposed L2R algorithm employs an association rule active sampling algorithm to select very small but effective training sets. Further, our algorithm operates on visual (e.g., image descriptors) and textual (e.g., comments associated with the image) elements, in a way that makes it able (i) to expand the query image (for which only visual elements are available) with textual elements, and (ii) to combine multiple elements, being visual or textual, using basic economic efficiency concepts. We conducted a systematic evaluation of the proposed algorithm using every-day photos collected from Instagram, and we show that our L2R algorithm reduces by two orders of magnitude the need for labeled images, and still improves upon the state-of-the-art models by 4-8% in terms of mean average precision."
1328960,15226,23735,A multi-AUV state estimator for determining the 3D position of tagged fish,2014,"This paper presents a multi-AUV state-estimator that can determine the 3D position of a tagged fish. In addition to angle measurements, the state-estimator also incorporates distance and depth measurements. These additional sensor measurements allow for greater accuracy in the position es- timates. A newly developed motion model that better accounts for multiple hypotheses of the motion of a tagged fish is used to increase the robustness of the state-estimator. A series of multi-AUV shark tracks were conducted at Santa Catalina Island, California over the span of four days to demonstrate the ability of the state-estimator to determine the 3D position of a tagged leopard shark. Additional experiments in which the AUVs tracked a tagged boat of known location were conducted to quantify the performance of the presented state-estimator. Experimental results demonstrate a three-fold decrease in mean state-estimation error compared to previous works. I. INTRODUCTION Studying the spatial movement of sharks and other fishes is an important tool for monitoring habitat and maintaining fish populations. Typical methods for tracking fish include tagging individuals with acoustic transmitters, and then using hydrophone-receiver systems to detect and measure the signals transmitted. Often, the hydrophone-receivers are placed at fixed locations around an environment of interest to passively track tagged individuals that move through the static array (1). Alternatively, active tracking can be done manually by mounting a directional hydrophone on a boat and continuously following the tagged individual from the surface for periods up to 96h (2). To enable active tracking without the need for human operators, the authors have demonstrated in previous works that a multi-AUV system using only low resolution angle measurements is able to autonomously track and follow tagged leopard sharks (3). A key component of this system is a state-estimator which determines the 2D position of the tagged shark (4). In order to provide more sensor"
410429,15226,9616,A novel CAD system for analyzing cardiac first-pass MR images,2012,"We propose a novel framework for the analysis of myocar-dial transit of contrast agent on cardiac first-pass magnetic resonance imaging (FP-MRI). Significant shape changes of the left ventricle (LV) due to heart contraction and respiratory motion limit accurate quantification of perfusion parameters on time series data. To account for the rigid and nonrigid deformations of the heart, we developed a registration methodology for the segmented LV that includes: (i) an initial affine-based registration, (ii) a local B-splines based alignment to maximize a similarity metric that accounts for the 1 st - and 2 nd -order normalized mutual information (NMI) between the globally aligned frames, followed by (iii) a refinement registration step that deforms each pixel of the target wall over evolving iso-contours to closely match the reference wall based on solving Laplace equation. Finally, from contrast agent kinetic curves, obtained from the co-aligned frames, four perfusion indexes — peak signal intensity, time-to-peak, initial up-slope, and the average signal change of the slowly varying agent delivery phase — are determined. To depict regional perfusion, we computed pixel-by-pixel parametric maps for the derived indexes. We have tested our framework on 24 FP-MRI data sets that have been collected from 8 patients with ischemic damage from heart attacks, who are undergoing a novel myoregeneration therapy and have documented an improvement in the visualization and display of treatment effects."
2369773,15226,9616,Automatic Object Segmentation by Quantum Cuts,2014,"In this study, the link between quantum mechanics and graph-cuts is exploited and a novel saliency map generation and salient object segmentation method is proposed based on the ground state solution of a modified Hamiltonian. First, the graph representation of certain quantum mechanical operators is studied. This reveals strong connections with widely used graph-cut algorithms while quantum mechanical constraints exhibit crucial advantages over the existing graph-cut algorithms. Furhtermore, concepts such as potential field helps solving a particular singularity problem related to laplacian matrices. In the proposed approach, the ground state (wave function) corresponding to a sub-atomic particle of a modified Hamiltonian operator corresponds to a particular optimization problem, the solution of which yields the salient object segmentation in a digital image. This approach provides a parameter-free -hence dataset independent-, unsupervised and fully automatic saliency map generation, which outperforms many existing state-of-the-art algorithms. The results of the proposed salient object extraction method exhibit such a promising accuracy that pushes the frontier in this field to the borders of the input-driven processing only - without the use of object knowledge aided by long-term human memory and intelligence. Furthermore, with the novel technologies for measuring a quantum wave function, the proposed method has a unique potential: Salient object segmentation in an actual physical setup in nano-scale. Such an unprece-dendent property will not only produce segmentation results instantaneously, but may be a unique opportunity to achieve accurate object segmentation in real-time for the massive visual repositories of today's Big Data."
1508988,15226,9099,Semi-automated magazine layout using content-based image features,2012,"We present a system for automating magazine layout process and data on its performance in a user evaluation test. The purpose of the study is to find the feasibility of the key system variables on the end-user. This semi-automatic system is based on content-based image feature algorithms to automate the layout process. The image related automation includes image cropping, overlaying text on top of images, image color palette creation, and image alignment. The algorithms rely on principles of photography and are used here together in the context of graphic design. For example the rule-of-space and leading line concepts can be extended to layout contexts for improved alignment of text and images. The computation is based on automatic analysis, such as face detection, color saliency, and textureness. We used the automation to create a functional prototype, an iPad magazine, using an open HTML5 eBook framework, which relies on CSS3 media queries for the layout adaptation. User experiments (40 participants) were conducted where the system was compared against two commercial iPad magazine systems in terms of overall usability with emphasis on visual aspects. Questionnaires and free commenting was used in a task-based usage scenario. The experiments were video recorded and the user comments were transcribed and coded into attributes. Usability results mixed with qualitative observation are reported. Results show that usability, read- ability and visuality are important to the users. Our system was considered more usable than the other two systems with some of the defining aspects being simplicity in terms of usability, subjective readability and visual clarity."
832676,15226,9099,The silent power: applications of research in medical x-ray combining with photography and digital graphic design,2012,"X-ray is a kind of secret power, it is usually used on the medicine, and it can be perspective human body and some items which are not metals. In the graphic art and design filed, digital photography is the common method for creating image, go through this method and the after effect editing, so many alluring images are presented for the audience; however, these images which are created by digital photography only can present the externals. The internal world is always as a secret region, we maybe know what it is, but we couldn't comprehend how it is like, thus, x-ray is the best way to understand what the internals is. In this research, medical contrast is used with lily flower, let the flower absorb the contrast for 12 hours, and the contrast will winding through the whole flower, then exposure the flower by x-ray, therefore, because of wavelength of radiation and contrast, we can see the internal of stem and leaves. The second method is used on human body. One of the authors is model in this part, and takes photo with same view and angle by digital camera and medical x-ray; consequently, the audience could see the internal and external view of models' body at the same time. On the basis of this research project, the intended outcome is going to bring authors some new vision, and after go through the whole project, authors wish some new imaginative ideas would be created in the testing procedure, therefore we could obtain new thoughts and experience from this project, and the x-ray combining design could let us know this silent power could be so different and it can be used not just in medicine, and delivering more ideas and methods to the other designers; artists and researchers who are interested on the same topic in design."
1687644,15226,9099,Towards efficient sparse coding for scalable image annotation,2013,"Nowadays, content-based retrieval methods are still the development trend of the traditional retrieval systems. Image labels, as one of the most popular approaches for the semantic representation of images, can fully capture the representative information of images. To achieve the high performance of retrieval systems, the precise annotation for images becomes inevitable. However, as the massive number of images in the Internet, one cannot annotate all the images without a scalable and flexible (i.e., training-free) annotation method. In this paper, we particularly investigate the problem of accelerating sparse coding based scalable image annotation, whose off-the-shelf solvers are generally inefficient on large-scale dataset. By leveraging the prior that most reconstruction coefficients should be zero, we develop a general and efficient framework to derive an accurate solution to the large-scale sparse coding problem through solving a series of much smaller-scale subproblems. In this framework, an active variable set, which expands and shrinks iteratively, is maintained, with each snapshot of the active variable set corresponding to a subproblem. Meanwhile, the convergence of our proposed framework to global optimum is theoretically provable. To further accelerate the proposed framework, a sub-linear time complexity hashing strategy, e.g.  Locality-Sensitive Hashing , is seamlessly integrated into our framework. Extensive empirical experiments on NUS-WIDE and IMAGENET datasets demonstrate that the orders-of-magnitude acceleration is achieved by the proposed framework for large-scale image annotation, along with zero/negligible accuracy loss for the cases without/with hashing speed-up, compared to the expensive off-the-shelf solvers."
694656,15226,11491,Consumer video understanding: a benchmark database and an evaluation of human and machine performance,2011,"Recognizing visual content in unconstrained videos has become a very important problem for many applications. Existing corpora for video analysis lack scale and/or content diversity, and thus limited the needed progress in this critical area. In this paper, we describe and release a new database called CCV, containing 9,317 web videos over 20 semantic categories, including events like baseball and parade, scenes like beach, and objects like cat. The database was collected with extra care to ensure relevance to consumer interest and originality of video content without post-editing. Such videos typically have very little textual annotation and thus can benefit from the development of automatic content analysis techniques.   We used Amazon MTurk platform to perform manual annotation, and studied the behaviors and performance of human annotators on MTurk. We also compared the abilities in understanding consumer video content by humans and machines. For the latter, we implemented automatic classifiers using state-of-the-art multi-modal approach that achieved top performance in recent TRECVID multimedia event detection task. Results confirmed classifiers fusing audio and video features significantly outperform single-modality solutions. We also found that humans are much better at understanding categories of nonrigid objects such as cat, while current automatic techniques are relatively close to humans in recognizing categories that have distinctive background scenes or audio patterns."
2129433,15226,23735,RANSAC for motion-distorted 3D visual sensors,2013,"Visual odometry (VO) is a highly efficient and powerful 6D motion estimation technique; state-of-the-art bundle adjustment algorithms now optimize over several frames of temporally tracked, appearance-based features in real time. It is well known that the temporal feature correspondence process is highly prone to mismatches. The standard technique used for outlier rejection in this process is random sample consensus (RANSAC), which is an iterative and non-deterministic process used to find the parameters of a mathematical model that best describe a likely set of inliers. The traditional model used for RANSAC in the visual odometry pipeline is a rigid transformation between two camera poses; this model has long assumed the use of an imaging sensor with a global shutter. In order to use imaging sensors that do not operate with a global shutter, it is proposed that the RANSAC algorithm be modified to use a constant-camera-velocity model. Specifically, this paper investigates the use of a two-axis scanning lidar in the visual-odometry pipeline. Images are formed using lidar intensity data, and due to the scanning-while-moving nature of the lidar, the behaviour of the sensor resembles that of a slow rolling-shutter camera. We formulate a Motion-Compensated RANSAC algorithm that uses a constant-velocity model and the individual timestamp of each extracted feature. The algorithm is validated using 6880 lidar frames with a resolution of 480 × 360, captured at 2 Hz, over a 1.1 km traversal. Our results show that the new algorithm results in far more inlying feature tracks for rolling-shutter-type images and ultimately higher-accuracy VO results."
115844,15226,10994,Registration of pre-operative CT and non-contrast-enhanced c-arm CT: an application to trans-catheter aortic valve implantation (TAVI),2012,"Trans-catheter Aortic Valve Implantation (TAVI) has proven to be an effective minimal-invasive alternative to traditional open-heart valve replacement surgery. Despite the success of contrast enhanced C-arm CT for intra-operative guidance during TAVI, utilization of pre-operative CT in the Hybrid Operating Room provides additional advantages of an improved workflow and minimized usage of contrast agent. In this paper, we propose a framework for CT/non-contrast-enhanced C-arm CT volume registration so that pre-operative CT can be used intra-operatively without additional contrast medium. The proposed method consists of two steps, rigid-body coarse alignment followed by deformable fine registration. Our contribution is twofold. First, robust heart center detection on both image modalities is used to boost the success rate of rigid-body registration. Second, a structural encoded similarity measure and anatomical correlation-regularized deformation fields are proposed to improve the performance of intensity-based deformable registration using the variational framework. Experiments were performed on ten sets of TAVI patient data, and the results have shown that the proposed method provides a highly robust and accurate registration. The resulting accuracy of 1.83 mm mean mesh-to-mesh error at the aortic root and the high efficiency of an average running time of 2 minutes on a common computer make it potentially feasible for clinical usage in TAVI. The proposed heart registration method is generic and hence can be easily applied to other cardiac applications."
2383032,15226,23749,Separating Performance Anomalies from Workload-Explained Failures in Streaming Servers,2012,"Video-streaming services are dominating the Internet, delivering content for video-on-demand, TV, education and collaborative work. Service parameters addressing quality and continuity of video content have a special importance due to the human sensitiveness to variations on video quality and decades of quality patterns absorbed by traditional TV users. Thus, the performance analysis and repair lifecycle at server and network levels is mandatory to avoid degradation of user experience. At the network level, there are several effective techniques based on temporal and spatial data redundancy, though they deeply depend on healthy servers with enough resources to afford both the client and recovery workloads. Excess of streaming workloads and performance anomalies (i.e., server resources exhaustion not explained by client requests) are typical causes of server performance failures. The former is often caused by memory caching of popular videos, which impacts the number of requests accepted by the server and consequently blurs load admittance mechanisms when the workload changes. The latter is caused by server internal factors independent of client workloads (e.g., memory leaks and maintenance activities). Separating client workload related failures from performance anomalies is mandatory for selection of immediate repair actions, capacity planning and to support fault repair. We evaluated the performance of Naive Bayes and C4.5 Trees algorithms for classification of these failure states using client and server performance metrics. Results shown that it is possible to predict the type of failure with levels of recall and accuracy higher than 90% for workload types with different popularity levels."
2601175,15226,9004,A Heat Kernel Based Cortical Thickness Estimation Algorithm,2013,"Cortical thickness estimation in magnetic resonance imaging (MRI) is an important technique for research on brain development and neurodegenerative diseases. This paper presents a heat kernel based cortical thickness estimation algorithm, which is driven by the graph spectrum and the heat kernel theory, to capture the grey matter geometry information in the in vivo brain MR images. First, we use the harmonic energy function to establish the tetrahedral mesh matching with the MR images and generate the Laplace-Beltrami operator matrix which includes the inherent geometric characteristics of the tetrahedral mesh. Second, the isothermal surfaces are computed by the finite element method with the volumetric Laplace-Beltrami operator and the direction of the steamline is obtained by tracing the maximum heat transfer probability based on the heat kernel diffusion. Thereby we can calculate the cerebral cortex thickness information between the point on the outer surface and the corresponding point on the inner surface. The method relies on intrinsic brain geometry structure and the computation is robust and accurate. To validate our algorithm, we apply it to study the thickness differences associated with Alzheimer's disease (AD) and mild cognitive impairment (MCI) on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our preliminary experimental results in 151 subjects (51 AD, 45 MCI, 55 controls) show that the new algorithm successfully detects statistically significant difference among patients of AD, MCI and healthy control subjects. The results also indicate that the new method may have better performance than the Freesurfer software."
1863095,15226,22130,Improved Initialization and Gaussian Mixture Pairwise Terms for Dense Random Fields with Mean-field Inference.,2012,"Recently, Krahenbuhl and Koltun proposed an efficient inference method for densely connected pairwise random fields using the mean-field approximation for a Conditional Random Field (CRF). However, they restrict their pairwise weights to take the form of a weighted combination of Gaussian kernels where each Gaussian component is allowed to take only zero mean, and can only be rescaled by a single value for each label pair. Further, their method is sensitive to initialization. In this paper, we propose methods to alleviate these issues. First, we propose a hierarchical mean-field approach where labelling from the coarser level is propagated to the finer level for better initialisation. Further, we use SIFT-flow based label transfer to provide a good initial condition at the coarsest level. Second, we allow our approach to take general Gaussian pairwise weights, where we learn the mean, the co-variance matrix, and the mixing co-efficient for every mixture component. We propose a variation of Expectation Maximization (EM) for piecewise learning of the parameters of the mixture model determined by the maximum likelihood function. Finally, we demonstrate the efficiency and accuracy offered by our method for object class segmentation problems on two challenging datasets: PascalVOC-10 segmentation and CamVid datasets. We show that we are able to achieve state of the art performance on the CamVid dataset, and an almost 3% improvement on the PascalVOC10 dataset compared to baseline graph-cut and mean-field methods, while also reducing the inference time by almost a factor of 3 compared to graph-cuts based methods."
732651,15226,11470,HDR2014 - A high dynamic range image quality database,2014,"High dynamic range (HDR) imaging has attracted a lot of attention and enthusiasm in the last decades. With the quick advances of sensor technologies, even consumer level digital cameras are capable of capturing HDR images. However, a vast majority of nowadays displays still only have 8-bit color depth, and this leads to the widely studied topic of displaying HDR images on low dynamic range (LDR) devices or tone mapping. On the other hand, the recent emergence of 10-bit display devices brings the possibility of direct visualization of HDR images. So a natural question to ask is whether existing popular image quality metrics designed for and validated on LDR (8-bit) images perform equally well for HDR (10-bit) images. In this paper we propose a new and dedicated High Dynamic Range image quality database (HDR2014). That database is composed of 192 images with four kinds of distortions applied on 6 reference images. More specifically, we use 8 distortion levels for the artifacts of JPEG/JPEG2000 compression, white noise injection and Gaussian blurring. Twenty-five inexperienced viewers were involved in the subjective viewing test. Images were displayed on a pair of carefully calibrated 8-bit LDR and 10-bit HDR monitors and the subjective scores on both of which were recorded. We then tested some ubiquitous and state-of-the-art IQA metrics on the HDR2014 database. Experimental results show that HDR monitor indeed improved perceptual quality of the visual stimuli, as compared to LDR ones. And several existing IQA metrics are still doing well on HDR images, yet performance of some metrics drop significantly."
512309,15226,22113,Classification of emerging extreme event tracks in multivariate spatio-temporal physical systems using dynamic network structures: application to hurricane track prediction,2011,"Understanding extreme events, such as hurricanes or forest fires, is of paramount importance because of their adverse impacts on human beings. Such events often propagate in space and time. Predicting--even a few days in advance--what locations will get affected by the event tracks could benefit our society in many ways. Arguably, simulations from first principles, where underlying physics-based models are described by a system of equations, provide least reliable predictions for variables characterizing the dynamics of these extreme events. Data-driven model building has been recently emerging as a complementary approach that could learn the relationships between historically observed or simulated multiple, spatio-temporal ancillary variables and the dynamic behavior of extreme events of interest. While promising, the methodology for predictive learning from such complex data is still in its infancy. In this paper, we propose a dynamic networks-based methodology for in-advance prediction of the dynamic tracks of emerging extreme events. By associating a network model of the system with the known tracks, our method is capable of learning the recurrent network motifs that could be used as discriminatory signatures for the event's behavioral class. When applied to classifying the behavior of the hurricane tracks at their early formation stages inWestern Africa region, our method is able to predict whether hurricane tracks will hit the land of the North Atlantic region at least 10-15 days lead lag time in advance with more than 90% accuracy using 10-fold cross-validation. To the best of our knowledge, no comparable methodology exists for solving this problem using data-driven models"
10527,15226,21106,Traffic observation and situation assessment,2011,"Utilization of camera systems for surveillance tasks (e. g. traffic monitoring) has become a standard procedure and has been in use for over 20 years. However, most of the cameras are operated locally and data analyzed manually. Locally means here a limited field of view and that the image sequences are processed independently from other cameras. For the enlargement of the observation area and to avoid occlusions and non-accessible areas multiple camera systems with overlapping and non-overlapping cameras are used. The joint processing of image sequences of a multi-camera system is a scientific and technical challenge. The processing is divided traditionally into camera calibration, object detection, tracking and interpretation. The fusion of information from different cameras is carried out in the world coordinate system. To reduce the network load, a distributed processing concept can be implemented.#R##N##R##N#Object detection and tracking are fundamental image processing tasks for scene evaluation. Situation assessments are based mainly on characteristic local movement patterns (e.g. directions and speed), from which trajectories are derived. It is possible to recognize atypical movement patterns of each detected object by comparing local properties of the trajectories. Interaction of different objects can also be predicted with an additional classification algorithm.#R##N##R##N#This presentation discusses trajectory based recognition algorithms for atypical event detection in multi object scenes to obtain area based types of information (e.g. maps of speed patterns, trajectory curvatures or erratic movements) and shows that two-dimensional areal data analysis of moving objects with multiple cameras offers new possibilities for situational analysis."
1804014,15226,9078,Transform domain sparsification of depth maps using iterative quadratic programming,2011,"Compression of depth maps is important for “texture plus depth” format of multiview images, which enables synthesis of novel intermediate views via depth-image-based rendering (DIBR) at decoder. Previous depth map coding schemes exploit unique depth data characteristics to compactly and faithfully reproduce the original signal. In contrast, since depth map is only a means to the end of view synthesis and not itself viewed, in this paper we explicitly manipulate depth values, without causing severe synthesized view distortion, in order to maximize representation sparsity in the transform domain for compression gain — we call this process transform domain spar-sification (TDS). Specifically, for each pixel in the depth map, we first define a quadratic penalty function, with minimum at ground truth depth value, based on synthesized view's distortion sensitivity to the pixel's depth value during DIBR. We then define an objective for a depth signal in a block as a weighted sum of: i) signal's sparsity in the transform domain, and ii) per-pixel synthesized view distortion penalties for the chosen signal. Given that sparsity (70-norm) is non-convex and difficult to optimize, we replace the Zo-norm in the objective with a computationally inexpensive weighted 12-norm; the optimization is then an unconstrained quadratic program, solvable via a set of linear equations. For the weighted /2-norm to promote sparsity, we solve the optimization iteratively, where at each iteration weights are readjusted to mimic sparsity-promoting Z T -norm, 0 < r < 1. Using JPEG as an example transform codec, we show that our TDS approach gained up to 1.7dB in rate-distortion performance for the interpolated view over compression of unaltered depth maps."
1491501,15226,21106,Corrected-Moment Illuminant Estimation,2013,"Image colors are biased by the color of the prevailing illumination. As such the color at pixel cannot always be used directly in solving vision tasks from recognition, to tracking to general scene understanding. Illuminant estimation algorithms attempt to infer the color of the light incident in a scene and then a color cast removal step discounts the color bias due to illumination. However, despite sustained research since almost the inception of computer vision, progress has been modest. The best algorithms - now often built on top of expensive feature extraction and machine learning - are only about twice as good as the simplest approaches. This paper, in effect, will show how simple moment based algorithms - such as Gray-World - can, with the addition of a simple correction step, deliver much improved illuminant estimation performance. The corrected Gray-World algorithm maps the mean image color using a fixed (per camera) 3x3 matrix transform. More generally, our moment approach employs 1st, 2nd and higher order moments - of colors or features such as color derivatives - and these again are linearly corrected to give an illuminant estimate. The question of how to correct the moments is an important one yet we will show a simple alternating least-squares training procedure suffices. Remarkably, across the major datasets - evaluated using a 3-fold cross validation procedure - our simple corrected moment approach always delivers the best results (and the performance increment is often large compared with the prior art). Significantly, outlier performance was found to be much improved."
2025865,15226,30,Multiscale Amplitude-Modulation Frequency-Modulation (AM–FM) Texture Analysis of Multiple Sclerosis in Brain MRI Images,2011,"This study introduces the use of multiscale amplitude modulation-frequency modulation (AM-FM) texture analysis of multiple sclerosis (MS) using magnetic resonance (MR) images from brain. Clinically, there is interest in identifying potential associations between lesion texture and disease progression, and in relating texture features with relevant clinical indexes, such as the expanded disability status scale (EDSS). This longitudinal study explores the application of 2-D AM-FM analysis of brain white matter MS lesions to quantify and monitor disease load. To this end, MS lesions and normal-appearing white matter (NAWM) from MS patients, as well as normal white matter (NWM) from healthy volunteers, were segmented on transverse T2-weighted images obtained from serial brain MR imaging (MRI) scans (0 and 6-12 months). The instantaneous amplitude (IA), the magnitude of the instantaneous frequency (IF), and the IF angle were extracted from each segmented region at different scales. The findings suggest that AM-FM characteristics succeed in differentiating 1) between NWM and lesions; 2) between NAWM and lesions; and 3) between NWM and NAWM. A support vector machine (SVM) classifier succeeded in differentiating between patients that, two years after the initial MRI scan, acquired an EDSS ≤ 2 from those with EDSS >; 2 (correct classification rate = 86%). The best classification results were obtained from including the combination of the low-scale IA and IF magnitude with the medium-scale IA. The AM-FM features provide complementary information to classical texture analysis features like the gray-scale median, contrast, and coarseness. The findings of this study provide evidence that AM-FM features may have a potential role as surrogate markers of lesion load in MS."
1291646,15226,9078,Interactive multiview video system with low decoding complexity,2011,"Research in multimedia is always investigating new ways of improving the immersive experience of the users. One current solution consists in designing systems which offer a high level of interactivity, such as multiview content navigation where the point of view can be changed while watching at a video sequence (e.g., free viewpoint television, gaming, etc.). The coding algorithm designed for the transmission of such media streams must be adapted to these novel decoder needs. However, video plus depth data transmission is usually performed by considering the information flows as two sequences encoded with MVC schemes. Whereas it achieves good compression performance, this coding approach is not appropriate for interactive applications since the decoding of a frame often requires the prior transmission and decoding of several reference frames. Moreover, the techniques recently developed to improve interactivity are generally implemented at the decoder, whose computational complexity requirements are augmented. In this paper, we propose a novel coding scheme for video plus depth sequences that is adapted to user navigation; contrarily to several common approaches, the additional complexity is added on the encoder side so that the decoder stays simple. We further propose to limit the additional bandwidth imposed by interactivity requirements by designing a rate allocation algorithm that builds on a model of the user behavior. A first version of our novel coding architecture is evaluated in terms of rate-distortion performance, where it is shown to offer a high interactivity at a reasonable bandwidth cost."
1745324,15226,21106,An asymmetric real-time dense visual localisation and mapping system,2011,"This paper describes a dense tracking system (both monocular and multi-camera) which each perform in real-time (45Hz). The proposed approach combines a prior dense photometric model with online visual odometry which enables handling dynamic changes in the scene. In particular it will be shown how the technique takes into account large illumination variations and subsequently improves direct tracking techniques which are highly prone to illumination change. This is achieved by exploiting the relative advantages of both model-based and visual odometry techniques for tracking. In the case of direct model-based tracking, photometric models are usually acquired under significantly greater lighting differences than those observed by the current camera view, however, model-based approaches avoid drift. Incremental visual odometry, on the other hand, has relatively less lighting variation but integrates drift. To solve this problem a hybrid approach is proposed to simultaneously minimise drift via a 3D model whilst using locally consistent illumination to correct large photometric differences. Direct 6 dof tracking is performed by an accurate method, which directly minimizes dense image measurements iteratively, using non-linear optimisation. A stereo technique for automatically acquiring the 3D photometric model has also been optimised for the purpose of this paper. Real experiments are shown on complex 3D scenes for a hand-held camera undergoing fast 3D movement and various illumination changes including daylight, artificial-lights, significant shadows, non-Lambertian reflections, occlusions and saturations."
1349590,15226,11491,Event-Driven Semantic Concept Discovery by Exploiting Weakly Tagged Internet Images,2014,"Analysis and detection of complex events in videos require a semantic representation of the video content. Existing video semantic representation methods typically require users to pre-define an exhaustive concept lexicon and manually annotate the presence of the concepts in each video, which is infeasible for real-world video event detection problems. In this paper, we propose an automatic semantic concept discovery scheme by exploiting Internet images and their associated tags. Given a target event and its textual descriptions, we crawl a collection of images and their associated tags by performing text based image search using the noun and verb pairs extracted from the event textual descriptions. The system first identifies the candidate concepts for an event by measuring whether a tag is a meaningful word and visually detectable. Then a concept visual model is built for each candidate concept using a SVM classifier with probabilistic output. Finally, the concept models are applied to generate concept based video representations. We use the TRECVID Multimedia Event Detection (MED) 2013 as our video test set and crawl 400K Flickr images to automatically discover 2, 000 visual concepts. We show significant performance gains of the proposed concept discovery method over different video event detection tasks including supervised event modeling over concept space and semantic based zero-shot retrieval without training examples. Importantly, we show the proposed method of automatic concept discovery outperforms other well-known concept library construction approaches such as Classemes and ImageNet by a large margin (228%) in zero-shot event retrieval. Finally, subjective evaluation by humans also confirms clear superiority of the proposed method in discovering concepts for event representation."
1039487,15226,22279,Sound source localization for video surveillance camera,2013,"While video analytics used in surveillance applications performs well in normal conditions, it may not work as accurately under adverse circumstances. Taking advantage of the complementary aspects of video and audio can lead to a more effective analytics framework resulting in increased system robustness. For example, sound scene analysis may indicate potential security risks outside field-of-view, pointing the camera in that direction. This paper presents a robust low-complexity method for two-microphone estimation of sound direction. While the source localization problem has been studied extensively, a reliable low-complexity solution remains elusive. The proposed direction estimation is based on the Generalized Cross-Correlation with Phase Transform (GCC-PHAT) method. The novel aspects of our approach include band-selective processing and inter-frame filtering of the GCC-PHAT objective function prior to peak detection. The audio bandwidth, microphone spacing, angle resolution, processing delay and complexity can all be adjusted depending on the application requirements. The described algorithm can be used in a multi-microphone configuration for spatial sound localization by combining estimates from microphone pairs. It has been implemented as a real-time demo on a modified TI DM8127 IP camera. The default 16 kHz audio sampling frequency requires about 5 MIPS processing power in our fixed-point implementation. The test results show robust sound direction estimation under a variety of background noise conditions."
2324537,15226,11052,Beyond spatial pyramids: a new feature extraction framework with dense spatial sampling for image classification,2012,"We introduce a new framework for image classification that extends beyond the window sampling of fixed spatial pyramids to include a comprehensive set of windows densely sampled over location, size and aspect ratio. To effectively deal with this large set of windows, we derive a concise high-level image feature using a two-level extraction method. At the first level, window-based features are computed from local descriptors (e.g., SIFT, spatial HOG, LBP) in a process similar to standard feature extractors. Then at the second level, the new image feature is determined from the window-based features in a manner analogous to the first level. This higher level of abstraction offers both efficient handling of dense samples and reduced sensitivity to misalignment. More importantly, our simple yet effective framework can readily accommodate a large number of existing pooling/coding methods, allowing them to extract features beyond the spatial pyramid representation.#R##N##R##N#To effectively fuse the second level feature with a standard first level image feature for classification, we additionally propose a new learning algorithm, called Generalized Adaptive lp-norm Multiple Kernel Learning (GA-MKL), to learn an adapted robust classifier based on multiple base kernels constructed from image features and multiple sets of pre-learned classifiers of all the classes. Extensive evaluation on the object recognition (Caltech256) and scene recognition (15Scenes) benchmark datasets demonstrates that the proposed method outperforms state-of-the-art image classification algorithms under a broad range of settings."
1962464,15226,23735,Exploiting motion priors in visual odometry for vehicle-mounted cameras with non-holonomic constraints,2011,"This paper presents a new method to estimate the relative motion of a vehicle from images of a single camera. The biggest problem in visual motion estimation is data association; matched points contain many outliers that must be detected and removed so that the motion can be estimated accurately. A very established method for robust motion estimation in the presence of outliers is the five-point RANSAC algorithm. Five-point RANSAC operates by generating motion hypotheses from randomly-sampled minimal sets of five-point correspondences. These hypotheses are then tested against all data points and the motion hypothesis that after a given number of iterations returns the largest number of inliers is taken as the solution to the problem. A typical drawback of RANSAC is that the number of iterations required to find a suitable solution grows exponentially with the number of outliers, often requiring thousands of iterations for typical data from urban environments. Another problem is that - due to its random nature - sometimes the found solution is not the “best” solution to the motion estimation problem. In this paper, we describe an algorithm for relative motion estimation in the presence of outliers, which does not rely on RANSAC. Contrary to RANSAC, motion hypotheses are not generated from randomly-sampled point correspondences, but from a “proposal distribution” that is built by exploiting the vehicle non-holonomic constraints. We show that not only is the proposed algorithm significantly faster than RANSAC, but that the returned solution may also be better in that it favors the underlying motion model of the vehicle, thus overcoming the typical limitations of RANSAC. Additionally, the proposed algorithm provides the likelihood of the motion estimate, which can be very useful in all those applications where a probability distribution of the position of the vehicle is required (e.g., SLAM). Finally, the performance of the proposed method is compared to that of the standard five-point RANSAC on real images collected from a vehicle moving in a cluttered, urban environment."
18496,15226,9004,Digital imaging for the education of proper surgical hand disinfection,2011,"Nosocomial infections are the undesirable result of a treatment in a hospital, or a health care service unit, not related to the patient's original condition. Despite the evolution of medicine, fundamental problems with hand hygiene remain existent, leading to the spread of nosocomial infections. Our group has been working on a generic solution to provide a method and apparatus to teach and verify proper hand disinfection. The general idea is to mark the skin surfaces that were sufficiently treated with alcoholic hand rub. Digital image processing is employed to determine the location of these areas and overlay it on the segmented hand, visualizing the results in an intuitive form. A non-disruptive ultraviolet marker is mixed to a commercially available hand rub, therefore leaving the original hand washing workflow intact. Digital images are taken in an enclosed device we developed for this purpose. First, robust hand contour segmentation is performed, then a histogram-based formulation of the fuzzy c-means algorithm is employed for the classification of clean versus dirty regions, minimizing the processing time of the images. The method and device have been tested in 3 hospitals in Hungary, Romania and Singapore, on surgeons, residents, medical students and nurses. A health care professional verified the results of the segmentation, since no gold standard is available for the recorded human cases. We were able to identify the hand boundaries correctly in 99.2% of the cases. The device can give objective feedback to medical students and staff to develop and maintain proper hand disinfection practice."
2122977,15226,11052,"Patch complexity, finite pixel correlations and optimal denoising",2012,"Image restoration tasks are ill-posed problems, typically solved with priors. Since the optimal prior is the exact unknown density of natural images, actual priors are only approximate and typically restricted to small patches. This raises several questions: How much may we hope to improve current restoration results with future sophisticated algorithms? And more fundamentally, even with perfect knowledge of natural image statistics, what is the inherent ambiguity of the problem? In addition, since most current methods are limited to finite support patches or kernels, what is the relation between the patch complexity of natural images, patch size, and restoration errors? Focusing on image denoising, we make several contributions. First, in light of computational constraints, we study the relation between denoising gain and sample size requirements in a non parametric approach. We present a law of diminishing return, namely that with increasing patch size, rare patches not only require a much larger dataset, but also gain little from it. This result suggests novel adaptive variable-sized patch schemes for denoising. Second, we study absolute denoising limits, regardless of the algorithm used, and the converge rate to them as a function of patch size. Scale invariance of natural images plays a key role here and implies both a strictly positive lower bound on denoising and a power law convergence. Extrapolating this parametric law gives a ballpark estimate of the best achievable denoising, suggesting that some improvement, although modest, is still possible."
359447,15226,20332,Visual saliency estimation through manifold learning,2012,"Saliency detection has been a desirable way for robotic vision to find the most noticeable objects in a scene. In this paper, a robust manifold based saliency estimation method has been developed to help capture the most salient objects in front of robotic eyes, namely cameras. In the proposed approach, an image is considered as a manifold of visual signals (stimuli) spreading over a connected grid, and local visual stimuli are compared against the global image variation to model the visual saliency. With this model, manifold learning is then applied to minimize the local variation while keeping the global contrast, and turns the RGB image into a multi channel image. After the projection through manifold learning, histogram based contrast is then computed for saliency modeling of all channels of the projected images, and mutual information is introduced to evaluate each single channel saliency map against prior knowledge to provide cues for the fusion of multiple channels. In the last step, the fusion procedure combines all single channel saliency maps according to their mutual information score, and generates the final saliency map. In our experiment, the proposed method is evaluated using one of the largest publicly available image datasets. The experimental results validated that our algorithm consistently outperforms the state of the art unsupervised saliency detection methods, yielding higher precision and better recall rates. Furthermore, the proposed method is tested on a video where a moving camera is trying to catch up with the walking person a salient object in the video sequence. Our experimental results demonstrated that the proposed approach can successful accomplish this task, revealing its potential use for similar robotic applications."
1410936,15226,21056,GPS-aided recognition-based user tracking system with augmented reality in extreme large-scale areas,2011,"We present a recognition-based user tracking and augmented reality system that works in extreme large scale areas. The system will provide a user who captures an image of a building facade with precise location of the building and augmented information about the building. While GPS cannot provide information about camera poses, it is needed to aid reducing the searching ranges in image database. A patch-retrieval method is used for efficient computations and real-time camera pose recovery. With the patch matching as the prior information, the whole image matching can be done through propagations in an efficient way so that a more stable camera pose can be generated. Augmented information such as building names and locations are then delivered to the user. The proposed system mainly contains two parts, offline database building and online user tracking. The database is composed of images for different locations of interests. The locations are clustered into groups according to their UTM coordinates. An overlapped clustering method is used to cluster these locations in order to restrict the retrieval range and avoid ping pong effects. For each cluster, a vocabulary tree is built for searching the most similar view. On the tracking part, the rough location of the user is obtained from the GPS and the exact location and camera pose are calculated by querying patches of the captured image. The patch property makes the tracking robust to occlusions and dynamics in the scenes. Moreover, due to the overlapped clusters, the system simulates the soft handoff feature and avoid frequent swaps in memory resource. Experiments show that the proposed tracking and augmented reality system is efficient and robust in many cases."
1626280,15226,22279,Dana36: A Multi-camera Image Dataset for Object Identification in Surveillance Scenarios,2012,"We present a novel dataset for evaluation of object matching and recognition methods in surveillance scenarios. Dataset consists of more than 23,000 images, depicting 15 persons and nine vehicles. A ground truth data -- the identity of each person or vehicle -- is provided, along with the coordinates of the bounding box in the full camera image. The dataset was acquired from 36 stationary camera views using a variety of surveillance cameras with resolutions ranging from standard VGA to three megapixel. 27 cameras observed the persons and vehicles in an outdoor environment, while the remaining nine observed the same persons indoors. The activity of persons was planned in advance, they drive the cars to the parking lot, exit the cars and walk around the building, through the main entrance, and up the stairs, towards the first floor of the building. The intended use of the dataset is performance evaluation of computer vision methods that aim to (re)identify people and objects from many different viewpoints in different environments and under variable conditions. Due to variety of camera locations, vantage points and resolutions, the dataset provides means to adjust the difficulty of the identification task in a controlled and documented manner. An interface for easy use of dataset within Matlab is provided as well, and the data is complemented by baseline results using a basic color histogram-based descriptor. While the cropped images of persons and vehicles represent the primary data in our dataset, we also provide full-frame images and a set of tracklets for each object as a courtesy to the dataset users."
1648079,15226,23735,Voxel planes: Rapid visualization and meshification of point cloud ensembles,2013,"Conversion of unorganized point clouds to surface reconstructions is increasingly required in the mobile robotics perception processing pipeline, particularly with the rapid adoption of RGB-D (color and depth) image sensors. Many contemporary methods stem from the work in the computer graphics community in order to handle the point clouds generated by tabletop scanners in a batch-like manner. The requirements for mobile robotics are different and include support for real-time processing, incremental update, localization, mapping, path planning, obstacle avoidance, ray-tracing, terrain traversability assessment, grasping/manipulation and visualization for effective human-robot interaction. We carry out a quantitative comparison of Greedy Projection and Marching cubes along with our voxel planes method. The execution speed, error, compression and visualization appearance of these are assessed. Our voxel planes approach first computes the PCA over the points inside a voxel, combining these PCA results across 2×2×2 voxel neighborhoods in a sliding window. Second, the smallest eigenvector and voxel centroid define a plane which is intersected with the voxel to reconstruct the surface patch (3-6 sided convex polygon) within that voxel. By nature of their construction these surface patches tessellate to produce a surface representation of the underlying points. In experiments on public datasets the voxel planes method is 3 times faster than marching cubes, offers 300 times better compression than Greedy Projection, 10 fold lower error than marching cubes whilst allowing incremental map updates."
480376,15226,10994,AfNet: the affordance network,2012,"There has been a growing need to build an object recognition system that can successfully characterize object constancy, irrespective of lighting, shading, occlusions, viewpoint variations and most importantly, deal with the multitude of shapes, colors and sizes in which objects are found. Affordances on the other hand, provide symbolic grounding mechanisms that enable linking features obtained from visual perception with the functionality of the objects, which provides the most consistent and holistic characterization of an object. Recognition by Component Affordances (RBCA) is a recent theory that builds affordance features for recognition. As an extension of the psychophysical theory of Recognition by Components (RBC) to generic visual perception, RBCA is well suited for cognitive visual processing systems which are required to perform implicit cognitive tasks. A common task is to substitute a cup for a mug, bottle, jug, pitcher, pilsner, beaker, chalice, goblet or any other unlabeled object, but with a physical part affording the ability to hold liquid and a part affording grasping by a human hand, given the goal of 'finding an empty cup' and no cups are available in the work environment of interest. In this paper, we present affordance features for recognition of objects. Using a set of 25 structural and 10 material affordances we define a database of over 250 common household objects. This database called the Affordance Network or AfNet is available as community development framework and is well suited for deployment on domestic robots. Sample object recognition results using AfNet and the associated inference engine that grounds the affordances through visual perception features demonstrate the effectiveness of the approach."
2105205,15226,8228,Measurements and Analysis of an Unconstrained User Generated Content System,2011,"User-Generated Content (UGC) is overwhelming the Internet with its interactivity and various contents. However, traditional UGC still have constrains on videos' length and size, which block out a wide variety of potential popular contents. In this paper, we present specific measurements and analysis of an Unconstrained User-Generated Content (UUGC) system---a test site of PPLive. This test site is a video-sharing portal just like YouTube (traditional UGC), while its contents are not constrained by video's length or size, similar to traditional Video-on-Demand (VoD). As an UUGC system, its most special characteristics are the various types of contents offered (movie, TV episode, TV show, music, documentary, sports, etc) and the wide range of providers, which make it an interesting case study. By searching relative key words in video's index, we classify the contents into several basic types and analyze the statistics of three major types-movie, TV episode and TV show (MVI, TV-E and TV-S). For further study of various contents, we demonstrate the patterns of correlated videos by three typical cases-versions of different definitions, preview version&formal version and different episodes in a series. Flash crowd triggering of MVI, TV-E and TV-S are also demonstrated by this approach. To find out the viewers' consumption pattern, we investigate daily&weekly circles and the publish time, as well as grouping the videos by age and exhibiting the popularity evolution. By performing graph fitting with multiple known distributions to actual video views, we show that power law with exponential cutoff best fits the videos' popularity distribution."
1677526,15226,390,Quantifying regional growth patterns through longitudinal analysis of distances between multimodal MR intensity distributions,2012,"Quantitative analysis of early brain development through imaging is critical for identifying pathological development, which may in turn affect treatment procedures. We propose a framework for analyzing spatiotemporal patterns of brain maturation by quantifying intensity changes in longitudinal MR images. We use a measure of divergence between a pair of intensity distributions to study the changes that occur within specific regions, as well as between a pair of anatomical regions, over time. The change within a specific region is measured as the contrast between white matter and gray matter tissue belonging to that region. The change between a pair of regions is measured as the divergence between regional image appearances, summed over all tissue classes. We use kernel regression to integrate the temporal information across different subjects in a consistent manner. We applied our method on multimodal MRI data with T1-weighted (T1W) and T2-weighted (T2W) scans of each subject at the approximate ages of 6 months, 12 months, and 24 months. The results demonstrate that brain maturation begins at posterior regions and that frontal regions develop later, which matches previously published histological, qualitative and morphometric studies. Our multimodal analysis also confirms that T1W and T2W modalities capture different properties of the maturation process, a phenomena referred to as T2 time lag compared to T1. The proposed method has potential for analyzing regional growth patterns across different populations and for isolating specific critical maturation phases in different MR modalities."
231294,15226,10994,Hierarchical space tiling for scene modeling,2012,"A typical scene category, e.g., street and beach, contains an enormous number (e.g., in the order of 104 to 105) of distinct scene configurations that are composed of objects and regions of varying shapes in different layouts. A well-known representation that can effectively address such complexity is the family of compositional models; however, learning the structures of the hierarchical compositional models remains a challenging task in vision. The objective of this paper is to present an efficient method for learning such models from a set of scene configurations. We start with an over-complete representation called Hierarchical Space Tiling (HST), which quantizes the huge and continuous scene configuration space in an And-Or tree (AOT). This hierarchical AOT can generate a combinatorial number of configurations (in the order of 1031) through a small dictionary of elements. Then we estimate the HST/AOT model through a learning-by-parsing strategy, which iteratively updates the HST/AOT parameters while constructing the optimal parse trees for each training configuration. Finally we prune out the branches with zero or low probability to obtain a much smaller HST/AOT. The HST quantization allows us to transfer the challenging structure-learning problem to a tractable parameter-learning problem. We evaluate the representation in three aspects. (i) Coding efficiency. We show the learned representation can approximate valid configurations with less errors using smaller number of primitives than other popular representations. (ii) Semantic power of learning. The learned representation is less ambiguous in parsing configuration and has semantically meaningful inner concepts. It captures both the diversity and the frequency (prior) of the scene configurations. (iii) Scene classification. The model is not only fully generative but also yields discriminative scene classification performance which outperforms the state-of-the-art methods."
128987,15226,11052,Active Deformable Part Models Inference,2014,"This paper presents an active approach for part-based ob- ject detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to for- malize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets. Part-based models such as deformable part models (DPM) (7) have become the state of the art in today's object detection methods. They offer powerful repre- sentations which can be learned from annotated datasets and capture both the appearance and the configuration of the parts. DPM-based detectors achieve un- rivaled accuracy on standard datasets but their computational demand is high since it is proportional to the number of parts in the model and the number of locations at which to evaluate the part filters. Approaches for speeding-up the DPM inference such as cascades, branch-and-bound, and multi-resolution schemes, use the responses obtained from initial part-location evaluations to re- duce the future computation. This paper introduces two novel ideas, which are missing in the state-of-the-art methods for speeding up DPM inference. First, at each location in the image pyramid, a part-based detector has to make a decision: whether to evaluate more parts and in what order or to stop and predict a label. This decision can be treated as a planning problem ,w hose"
83515,15226,9004,A Generative Model for Resolution Enhancement of Diffusion MRI Data,2013,"The advent of diffusion magnetic resonance imaging (DMRI) presents unique opportunities for the exploration of white matter connec- tivity in vivo and non-invasively. However, DMRI suffers from insufficient spatial resolution, often limiting its utility to the studying of only ma- jor white matter structures. Many image enhancement techniques rely on expensive scanner upgrades and complex time-consuming sequences. We will instead take a post-processing approach in this paper for res- olution enhancement of DMRI data. This will allow the enhancement of existing data without re-acquisition. Our method uses a generative model that reflects the image generation process and, after the param- eters of the model have been estimated, we can effectively sample high- resolution images from this model. More specifically, we assume that the diffusion-weighted signal at each voxel is an agglomeration of signals from an ensemble of fiber segments that can be oriented and located freely within the voxel. Our model for each voxel therefore consists of an arbitrary number of signal generating fiber segments, and the model parameters that need to be determined are the locations and orienta- tions of these fiber segments. Solving for these parameters is an ill-posed problem. However, by borrowing information from neighboring voxels, we show that this can be solved by using Markov chain Monte Carlo (MCMC) methods such as the Metropolis-Hastings algorithm. Prelimi- nary results indicate that out method substantially increases structural visibility in both subcortical and cortical regions."
1295839,15226,9099,SocialTransfer: cross-domain transfer learning from social streams for media applications,2012,"The usage and applications of social media have become pervasive. This has enabled an innovative paradigm to solve multimedia problems (e.g., recommendation and popularity prediction), which are otherwise hard to address purely by traditional approaches. In this paper, we investigate how to build a mutual connection among the disparate social media on the Internet, using which cross-domain media recommendation can be realized. We accomplish this goal through  SocialTransfer ---a novel cross-domain real-time transfer learning framework. While existing transfer learning methods do not address how to utilize the real time social streams, our proposed  SocialTransfer  is able to effectively learn from social streams to help multimedia applications, assuming an intermediate topic space can be built across domains. It is characterized by two key components: 1) a topic space learned in real time from social streams via Online Streaming Latent Dirichlet Allocation (OSLDA), and 2) a real-time cross-domain graph spectra analysis based transfer learning method that seamlessly incorporates learned topic models from social streams into the transfer learning framework. We present as use cases of \emph{SocialTransfer} two video recommendation applications that otherwise can hardly be achieved by conventional media analysis techniques: 1) socialized query suggestion for video search, and 2) socialized video recommendation that features socially trending topical videos. We conduct experiments on a real-world large-scale dataset, including 10.2 million tweets and 5.7 million YouTube videos and show that \emph{SocialTransfer} outperforms traditional learners significantly, and plays a natural and interoperable connection across video and social domains, leading to a wide variety of cross-domain applications."
70750,15226,9004,Using Multiparametric Data with Missing Features for Learning Patterns of Pathology,2012,"The paper presents a method for learning multimodal clas- sifiers from datasets in which not all subjects have data from all modal- ities. Usually, subjects with a severe form of pathology are the ones failing to satisfactorily complete the study, especially when it consists of multiple imaging modalities. A classifier capable of handling subjects with unequal numbers of modalities prevents discarding any subjects, as is traditionally done, thereby broadening the scope of the classifier to more severe pathology. It also allows design of the classifier to include as much of the available information as possible and facilitates testing of subjects with missing modalities over the constructed classifier. The pre- sented method employs an ensemble based approach where several sub- sets of complete data are formed and trained using individual classifiers. The output from these classifiers is fused using a weighted aggregation step giving an optimal probabilistic score for each subject. The method is applied to a spatio-temporal dataset for autism spectrum disorders (ASD)(96 patients with ASD and 42 typically developing controls) that consists of functional features from magnetoencephalography (MEG) and structural connectivity features from diffusion tensor imaging (DTI). A clear distinction between ASD and controls is obtained with an aver- age 5-fold accuracy of 83.3% and testing accuracy of 88.4%. The fusion classifier performance is superior to the classification achieved using sin- gle modalities as well as multimodal classifier using only complete data (78.3%). The presented multimodal classifier framework is applicable to all modality combinations."
535726,15226,9004,Estimating the 4d respiratory lung motion by spatiotemporal registration and building super-resolution image,2011,"The estimation of lung motion in 4D-CT with respect to the respiratory phase becomes more and more important for radiation therapy of lung cancer. Modern CT scanner can only scan a limited region of body at each couch table position. Thus, motion artifacts due to the patient's free breathing during scan are often observable in 4D-CT, which could undermine the procedure of correspondence detection in the registration. Another challenge of motion estimation in 4D-CT is how to keep the lung motion consistent over time. However, the current approaches fail to meet this requirement since they usually register each phase image to a pre-defined phase image independently, without considering the temporal coherence in 4D-CT. To overcome these limitations, we present a unified approach to estimate the respiratory lung motion with two iterative steps. First, we propose a new spatiotemporal registration algorithm to align all phase images of 4D-CT (in low-resolution) onto a high-resolution group-mean image in the common space. The temporal consistency is persevered by introducing the concept of temporal fibers for delineating the spatiotemporal behavior of lung motion along the respiratory phase. Second, the idea of super resolution is utilized to build the group-mean image with more details, by integrating the highly-redundant image information contained in the multiple respiratory phases. Accordingly, by establishing the correspondence of each phase image w.r.t. the high-resolution group-mean image, the difficulty of detecting correspondences between original phase images with missing structures is greatly alleviated, thus more accurate registration results can be achieved. The performance of our proposed 4D motion estimation method has been extensively evaluated on a public lung dataset. In all experiments, our method achieves more accurate and consistent results in lung motion estimation than all other state-of-the-art approaches."
1349926,15226,390,Structural and functional imaging biomarkers to assess cancer treatment response,2012,"The development of anti-cancer therapies demands new biomarkers in order to assess efficacy. Established measurements that rely on changes in tumor size from a structural acquisition may not be the most appropriate. There is also the desire to move away from manual techniques in favor or more automated analysis methods. The variety of MRI sequences and availability of PET radiotracers provides scientists with a rich collection of imaging biomarkers. A brief background on the broader collection of imaging approaches will be provided before the focus shifts to acquisition techniques that capture functional and/or structural properties of the tumor. It should be noted that the variety of image acquisition techniques are not mutually exclusive, but complementary, and care is required when selecting a set of imaging biomarkers. Criteria include but are not limited to the organ system, dosing schedule, downstream treatment effects, patient burden and cost. When anti-angiogenic or anti-vascular therapies must be assessed in-vivo measurements of tissue perfusion, using dynamic contrast-enhanced MRI, have been widely used. The application of diffusion-weighted MRI has also seen a dramatic rise in the last five years. Of course, this is not an exhaustive list. With the use of more advanced acquisition techniques comes the need for standardization and validation, especially when considering their implementation in a multi-center clinical trial. Efforts are ongoing in order to provide practitioners with advice on the design, application and analysis of advanced imaging biomarkers for therapeutic response in oncology."
903711,15226,9099,Towards multi-semantic image annotation with graph regularized exclusive group lasso,2011,"To bridge the semantic gap between low level feature and human perception, most of the existing algorithms aim mainly at annotating images with concepts coming from only one semantic space, e.g. cognitive or affective. The naive combination of the outputs from these spaces will implicitly force the conditional independence and ignore the correlations among the spaces. In this paper, to exploit the comprehensive semantic of images, we propose a general framework for harmoniously integrating the above multiple semantics, and investigating the problem of learning to annotate images with training images labeled in two or more correlated semantic spaces, such as fascinating nighttime, or exciting cat. This kind of semantic annotation is more oriented to real world search scenario. Our proposed approach outperforms the baseline algorithms by making the following contributions. 1) Unlike previous methods that annotate images within only one semantic space, our proposed multi-semantic annotation associates each image with labels from multiple semantic spaces. 2) We develop a multi-task linear discriminative model to learn a linear mapping from features to labels. The tasks are correlated by imposing the exclusive group lasso regularization for competitive feature selection, and the graph Laplacian regularization to deal with insufficient training sample issue. 3) A Nesterov-type smoothing approximation algorithm is presented for efficient optimization of our model. Extensive experiments on NUS-WIDEEmotive dataset (56k images) with 8×81 emotive cognitive concepts and Object&Scene datasets from NUS-WIDE well validate the effectiveness of the proposed approach."
2251519,15226,11052,Canonical Correlation Analysis on Riemannian Manifolds and Its Applications,2014,"Canonical correlation analysis (CCA) is a widely used statis- tical technique to capture correlations between two sets of multi-variate random variables and has found a multitude of applications in computer vision, medical imaging and machine learning. The classical formulation assumes that the data live in a pair of vector spaces which makes its use in certain important scientific domains problematic. For instance, the set of symmetric positive definite matrices (SPD), rotations and proba- bility distributions, all belong to certain curved Riemannian manifolds where vector-space operations are in general not applicable. Analyzing the space of such data via the classical versions of inference models is rather sub-optimal. But perhaps more importantly, since the algorithms do not respect the underlying geometry of the data space, it is hard to provide statistical guarantees (if any) on the results. Using the space of SPD matrices as a concrete example, this paper gives a principled gen- eralization of the well known CCA to the Riemannian setting. Our CCA algorithm operates on the product Riemannian manifold representing SPD matrix-valued fields to identify meaningful statistical relationships on the product Riemannian manifold. As a proof of principle, we present results on an Alzheimer's disease (AD) study where the analysis task in- volves identifying correlations across diffusion tensor images (DTI) and Cauchy deformation tensor fields derived from T1-weighted magnetic resonance (MR) images."
857874,15226,9099,3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks,2014,"Human activity understanding with 3D/depth sensors has received increasing attention in multimedia processing and interactions. This work targets on developing a novel deep model for automatic activity recognition from RGB-D videos. We represent each human activity as an ensemble of cubic-like video segments, and learn to discover the temporal structures for a category of activities, i.e. how the activities to be decomposed in terms of classification. Our model can be regarded as a structured deep architecture, as it extends the convolutional neural networks (CNNs) by incorporating structure alternatives. Specifically, we build the network consisting of 3D convolutions and max-pooling operators over the video segments, and introduce the latent variables in each convolutional layer manipulating the activation of neurons. Our model thus advances existing approaches in two aspects: (i) it acts directly on the raw inputs (grayscale-depth data) to conduct recognition instead of relying on hand-crafted features, and (ii) the model structure can be dynamically adjusted accounting for the temporal variations of human activities, i.e. the network configuration is allowed to be partially activated during inference. For model training, we propose an EM-type optimization method that iteratively (i) discovers the latent structure by determining the decomposed actions for each training example, and (ii) learns the network parameters by using the back-propagation algorithm. Our approach is validated in challenging scenarios, and outperforms state-of-the-art methods. A large human activity database of RGB-D videos is presented in addition."
1368588,15226,9099,Predicting domain adaptivity: redo or recycle?,2012,"Over the years, the academic researchers have contributed various visual concept classifiers. Nevertheless, given a new dataset, most researchers still prefer to develop large number of classifiers from scratch despite expensive labeling efforts and limited computing resources. A valid question is why not multimedia community ``embrace the green and recycle off-the-shelf classifiers for new dataset. The difficulty originates from the domain gap that there are many different factors that govern the development of a classifier and eventually drive its performance to emphasize certain aspects of dataset. Reapplying a classifier to an unseen dataset may end up GIGO (garbage in, garbage out) and the performance could be much worse than re-developing a new classifier with very few training examples. In this paper, we explore different parameters, including shift of data distribution, visual and context diversities, that may hinder or otherwise encourage the recycling of old classifiers for new dataset. Particularly, we give empirical insights of when to recycle available resources, and when to redo from scratch by completely forgetting the past and train a brand new classifier. Based on these analysis, we further propose an approach for predicting the negative transfer of a concept classifier to a different domain given the observed parameters. Experimental results show that the prediction accuracy of over 75\% can be achieved when transferring concept classifiers learnt from LSCOM (news video domain), ImageNet (Web image domain) and Flickr-SF (weakly tagged Web image domain), respectively, to TRECVID 2011 dataset (Web video domain)."
1344168,15226,9773,Bayesian Network Structure Learning and Inference Methods for Handwriting,2013,"Probabilistic models of characteristics of handwritten words are useful in forensic document examination since they can be used to answer queries such as: determine the rarity of a given style of writing of the word, find the probability of observing those characteristics in a representative database of given size, etc. The task considered here is to use a training set of samples of a word written by a representative population of individuals (with each individual's writing of the word being described by a fixed set of discrete categorical variables), to construct directed probabilistic graphical models (Bayesian networks or BNs) and then use such models to answer probabilistic queries. However, since the BN structure learning problem is NP-hard, we propose an approximate method and analyze its performance and complexity. The proposed algorithm uses a local measure of deviance from independence (chi-squared tests between pairs of variables) and a global score (log-loss). The method builds the BN structure incrementally, by adding directed edges with high deviance and choosing the edge direction to minimize log-loss. The method is evaluated with samples of the word and obtained from a representative population of the United States with descriptive characteristic sets that are different for cursive writing and for hand-printing. For several samples obtained from the BN, the probability of random correspondence (PRC) is inferred. A measure of the discriminatory power of the characteristic set (conditional PRC) is also determined. The computational complexity of determining the probability of finding a similar one to a given sample, within a tolerance, in a database of given size, is discussed."
684568,15226,9078,Depth map compression using multi-resolution graph-based transform for depth-image-based rendering,2012,"Depth map compression is important for efficient network transmission of 3D visual data in texture-plus-depth format, where the observer can synthesize an image of a freely chosen viewpoint via depth-image-based rendering (DIBR) using received neighboring texture and depth maps as anchors. Unlike texture maps, depth maps exhibit unique characteristics like smooth interior surfaces and sharp edges that can be exploited for coding gain. In this paper, we propose a multi-resolution approach to depth map compression using previously proposed graph-based transform (GBT). The key idea is to treat smooth surfaces and sharp edges of large code blocks separately and encode them in different resolutions: encode edges in original high resolution (HR) to preserve sharpness, and encode smooth surfaces in low-pass-filtered and down-sampled low resolution (LR) to save coding bits. Because GBT does not filter across edges, it produces small or zero high-frequency components when coding smooth-surface depth maps and leads to a compact representation in the transform domain. By encoding down-sampled surface regions in LR GBT, we achieve representation compactness for a large block without the high computation complexity associated with an adaptive large-block GBT. At the decoder, encoded LR surfaces are up-sampled and interpolated while preserving encoded HR edges. Experimental results show that our proposed multi-resolution approach using GBT reduced bitrate by 68% compared to native H.264 intra with DCT encoding original HR depth maps, and by 55% compared to single-resolution GBT encoding small blocks."
1606171,15226,11491,Accurate content-based video copy detection with efficient feature indexing,2011,"We describe an accurate content-based copy detection system that uses both local and global visual features to ensure robustness. Our system advances state-of-the-art techniques in four key directions. (1) Multiple-codebook-based product quantization: conventional product quantization methods encode feature vectors using a single codebook, resulting in large quantization error. We propose a novel codebook generation method for an arbitrary number of codebooks. (2) Handling of temporal burstiness: for a stationary scene, once a query feature matches incorrectly, the match continues in successive frames, resulting in a high false-alarm rate. We present a temporal-burstiness-aware scoring method that reduces the impact from similar features, thereby reducing false alarms. (3) Densely sampled SIFT descriptors: conventional global features suffer from a lack of distinctiveness and invariance to non-photometric transformations. Our densely sampled global SIFT features are more discriminative and robust against logo or pattern insertions. (4) Bigram- and multiple-assignment-based indexing for global features: we extract two SIFT descriptors from each location, which makes them more distinctive. To improve recall, we propose multiple assignments on both the query and reference sides. Performance evaluation on the TRECVID 2009 dataset indicates that both local and global approaches outperform conventional schemes. Furthermore, the integration of these two approaches achieves a three-fold reduction in the error rate when compared with the best performance reported in the TRECVID 2009 workshop."
2074757,15226,21106,Fast Face Detector Training Using Tailored Views,2013,"Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core implementation of Viola Jones' AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset."
1158892,15226,9099,Discovering the City by Mining Diverse and Multimodal Data Streams,2014,"This work attempts to tackle the IBM grand challenge - seeing the daily life of New York City (NYC) in various perspectives by exploring rich and diverse social media content. Most existing works address this problem relying on single media source and covering limited life aspects. Because different social media are usually chosen for specific purposes, multiple social media mining and integration are essential to understand a city comprehensively. In this work, we first discover the similar and unique natures (e.g., attractions, topics) across social media in terms of visual and semantic perceptions. For example, Instagram users share more food and travel photos while Twitter users discuss more about sports and news. Based on these characteristics, we analyze a broad spectrum of life aspects - trends, events, food, wearing and transportation in NYC by mining a huge amount of diverse and freely available media (e.g., 1.6M Instagram photos, 5.3M Twitter posts). Because transportation logs are hardly available in social media, the NYC Open Data (e.g., 6.5B subway station transactions) is leveraged to visualize temporal traffic patterns. Furthermore, the experiments demonstrate that our approaches can effectively overview urban life with considerable technical improvement, e.g., having 16% relative gains in food recognition accuracy by a hierarchy cross-media learning strategy, reducing the feature dimensions of sentiment analysis by 10 times without sacrificing precision."
141831,15226,9004,Diffeomorphism invariant riemannian framework for ensemble average propagator computing,2011,"In Diffusion Tensor Imaging (DTI), Riemannian framework based on Information Geometry theory has been proposed for processing tensors on estimation, interpolation, smoothing, regularization, segmentation, statistical test and so on. Recently Riemannian framework has been generalized to Orientation Distribution Function (ODF) and it is applicable to any Probability Density Function (PDF) under orthonormal basis representation. Spherical Polar Fourier Imaging (SPFI) was proposed for ODF and Ensemble Average Propagator (EAP) estimation from arbitrary sampled signals without any assumption. Purpose: Tensors only can represent Gaussian EAP and ODF is the radial integration of EAP, while EAP has full information for diffusion process. To our knowledge, so far there is no work on how to process EAP data. In this paper, we present a Riemannian framework as a mathematical tool for such task. Method: We propose a state-of-the-art Riemannian framework for EAPs by representing the square root of EAP, called wavefunction based on quantum mechanics, with the Fourier dual Spherical Polar Fourier (dSPF) basis. In this framework, the exponential map, logarithmic map and geodesic have closed forms, and weighted Riemannian mean and median uniquely exist. We analyze theoretically the similarities and differences between Riemannian frameworks for EAPs and for ODFs and tensors. The Riemannian metric for EAPs is diffeomorphism invariant, which is the natural extension of the affine-invariant metric for tensors. We propose Log-Euclidean framework to fast process EAPs, and Geodesic Anisotropy (GA) to measure the anisotropy of EAPs. With this framework, many important data processing operations, such as interpolation, smoothing, atlas estimation, Principal Geodesic Analysis (PGA), can be performed on EAP data. Results and Conclusions: The proposed Riemannian framework was validated in synthetic data for interpolation, smoothing, PGA and in real data for GA and atlas estimation. Riemannian median is much robust for atlas estimation."
2222573,15226,390,Out-of-sample extrapolation using semi-supervised manifold learning (OSE-SSL): Content-based image retrieval for prostate histology grading,2011,"In this paper, we present an out-of-sample extrapolation (OSE) scheme in the context of semi-supervised manifold learning (OSESSL). Manifold learning (ML) takes samples with high dimensionality and learns a set of low dimensional embeddings. Embeddings generated by ML preserve nonlinear relationships between samples allowing dataset visualization, classification, or evaluation of object similarity. Semi-supervised ML (SSL), a recent ML extension, exploits known class labels to learn embeddings, which may result in greater separation between samples of different classes compared to unsupervised ML schemes. Most ML schemes utilize the eigenvalue decomposition (EVD) to learn embeddings. For instance, Graph Embedding (GE) learns embeddings by EVD on a similarity matrix that models high dimensional feature vector similarity between samples. In datasets where new samples are acquired, such as a content-based image retrieval (CBIR) system, recalculating EVD is infeasible. OSE schemes obtain new embeddings without recalculating EVD. The Nystrom method (NM) is an OSE algorithm where new embeddings are estimated as a weighted sum of known embeddings. Known embeddings must describe the embedding space for NM to accurately estimate new embeddings. In this paper, NM and semi-supervised GE (SSGE) are combined to learn embeddings which cluster samples by class and rapidly calculate embeddings for new samples without recalculating EVD. OSE-SSL is compared to (i) NM paired with GE (NM-GE), and (ii) SSGE obtained for the full database, where SSGE results represent ground truth embeddings. OSE-SSL, NM-GE, and SSGE are evaluated in their ability to: (1) cluster samples by label, measured by Silhouette Index (SI); (2) CBIR accuracy, measured by area under the precision-recall curve (AUPRC). In a synthetic Swiss roll dataset of 2000 samples, OSE-SSL requires training on 50% of the dataset to achieve SI and AUPRC similar to SSGE while NM-GE requires 70% of dataset to achieve SI and AUPRC similar to GE. For a prostate histology dataset of 888 glands, a CBIR system was evaluated on its ability to retrieve images according to Gleason Grade. OSE-SSL had AUPRC of 0.6 while NM-GE had AUPRC of 0.3."
41860,15226,9616,Identification of Biomarkers for Prostate Cancer Prognosis Using a Novel Two-Step Cluster Analysis,2011,"Prognosis of Prostate cancer is challenging due to incom- plete assessment by clinical variables such as Gleason score, metastasis stage, surgical margin status, seminal vesicle invasion status and pre- operative prostate-specific antigen level. The whole-genome gene expres- sion assay provides us with opportunities to identify molecular indicators for predicting disease outcomes. However, cell composition heterogene- ity of the tissue samples usually generates inconsistent results for cancer profile studies. We developed a two-step strategy to identify prognos- tic biomarkers for prostate cancer by taking into account the variation due to mixed tissue samples. In the first step, an unsupervised EM clus- tering analysis was applied to each gene to cluster patient samples into subgroups based on the expression values of the gene. In the second step, genes were selected based on χ 2 correlation analysis between the cluster indicators obtained in the first step and the observed clinical outcomes. Two simulation studies showed that the proposed method identified 30% more prognostic genes than the traditional differential expression analy- sis methods such as SAM and LIMMA. We also analyzed a real prostate cancer expression data set using the new method and the traditional methods. The pathway assay showed that the genes identified with the new method are significantly enriched by prostate cancer relevant path- ways such as the wnt signaling pathway and TGF-β signaling pathway. Nevertheless, these genes were not detected by the traditional methods."
1395586,15226,9099,"Listen, look, and gotcha: instant video search with mobile phones by layered audio-video indexing",2013,"Mobile video is quickly becoming a mass consumer phenomenon. More and more people are using their smartphones to search and browse video content while on the move. In this paper, we have developed an innovative instant mobile video search system through which users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. The system is able to index large-scale video data using a new layered audio-video indexing approach in the cloud, as well as extract light-weight joint audio-video signatures in real time and perform progressive search on mobile devices. Unlike most existing mobile video search applications that simply send the original video query to the cloud, the proposed mobile system is one of the first attempts at instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is characterized by four unique properties: 1) a joint audio-video signature to deal with the large aural and visual variances associated with the query video captured by the mobile phone, 2) layered audio-video indexing to holistically exploit the complementary nature of audio and video signals, 3) light-weight fingerprinting to comply with mobile processing capacity, and 4) a progressive query process to significantly reduce computational costs and improve the user experience---the search process can stop anytime once a confident result is achieved. We have collected 1,400 query videos captured by 25 mobile users from a dataset of 600 hours of video. The experiments show that our system outperforms state-of-the-art methods by achieving 90.79% precision when the query video is less than 10 seconds and 70.07% even when the query video is less than 5 seconds."
17046,15226,9004,Low-Rank and Sparse Matrix Decomposition for Compressed Sensing Reconstruction of Magnetic Resonance 4D Phase Contrast Blood Flow Imaging (LoSDeCoS 4D-PCI),2013,"Blood flow measurements using 4D Phase Contrast blood flow imaging (PCI) provide an excellent fully non-invasive technique to assess the hemodynamics clinically in-vivo. Iterative reconstruction techniques combined with parallel MRI have been proposed to reduce the data acquisition time, which is the biggest drawback of 4D PCI. The novel LoSDeCoS technique combines these ideas with the separation into a low-rank and a sparse component. The high-dimensionality of the PC data renders it ideally suited for this approach. The proposed method is not limited to a single body region, but can be applied to any 4D flow measurement. The benefits of the new method are twofold: It al- lows to significantly accelerate the acquisition; and generates additional images highlighting temporal and directional flow changes. Reduction in acquisition time improves patient comfort and can be used to achieve better temporal or spatial resolution, which in turn allows more precise calculations of clinically important quantitative numbers such as flow rates or the wall shear stress. With LoSDeCoS, acceleration factors of 6-8 were achieved for 16 in-vivo datasets of both the carotid artery (6 datasets) and the aorta (10 datasets), while decreasing the Normalized Root Mean Square Error by over 10 % compared to a standard iterative reconstruction and by achieving similarity values of over 0.93. Inflow- Outflow phantom experiments showed good parabolic profiles and an excellent mass conservation."
1327869,15226,9099,Spatial pooling of heterogeneous features for image applications,2012,"The Bag-of-Features (BoF) model has played an important role for image representation in many multimedia applications. It has been extensively applied to many tasks including image classification, image retrieval, scene understanding, and so on. Despite the advantages of this model such as simplicity, efficiency and generality, there are also notable drawbacks for this model, including poor power of semantic expression of local descriptors, and lack of robust structures upon single visual words. To overcome these problems, various techniques have been proposed, such as multiple descriptors, spatial context modeling and interest region detection. Though they have been proven to improve the BoF model to some extent, there still lacks a coherent scheme to integrate each individual module.   To address the problems above, we propose a novel framework with spatial pooling of heterogeneous features. Our framework differs from the traditional Bag-of-Features model on three aspects. First, we propose a new scheme for combining texture and edge based local features together at the descriptor extraction level. Next, we build geometric visual phrases to model spatial context upon heterogeneous features for mid-level representation of images. Finally, based on a smoothed edgemap, a simple and effective spatial weighting scheme is performed on our mid-level image representation. We test our integrated framework on several benchmark datasets for image classification and retrieval applications. The extensive results show the superior performance of our algorithm over state-of-the-art methods."
1963266,15226,21106,Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms,2013,"Evaluating visual tracking algorithms, or trackers for short, is of great importance in computer vision. However, it is hard to fairly compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the second best ones in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rankings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo's and Glicko's rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research."
1842953,15226,22130,Gradient Edge Map Features for Frontal Face Recognition under Extreme Illumination Changes.,2012,"Our aim in this paper is to robustly match frontal faces in the presence of extreme illumination changes, using only a single training image per person and a single probe image. In the illumination conditions we consider, which include those with the dominant light source placed behind and to the side of the user, directly above and pointing downwards or indeed below and pointing upwards, this is a most challenging problem. The presence of sharp cast shadows, large poorly illuminated regions of the face, quantum and quantization noise and other nuisance effects, makes it difficult to extract a sufficiently discriminative yet robust representation. We introduce a representation which is based on image gradient directions near robust edges which correspond to characteristic facial features. Robust edges are extracted using a cascade of processing steps, each of which seeks to harness further discriminative information or normalize for a particular source of extra-personal appearance variability. The proposed representation was evaluated on the extremely difficult YaleB data set. Unlike most of the previous work we include all available illuminations, perform training using a single image per person and match these also to a single probe image. In this challenging evaluation setup, the proposed gradient edge map achieved 0.8% error rate, demonstrating a nearly perfect receiver-operator characteristic curve behaviour. This is by far the best performance achieved in this setup reported in the literature, the best performing methods previously proposed attaining error rates of approximately 6–7%."
1240749,15226,21056,User preference-aware music video generation based on modeling scene moods,2014,"Due to technical advances in mobile devices (e.g., smartphones, tablets) and wireless communications, people now can easily capture user-generated videos (UGVs) anywhere, anytime and instantly share their real-life experiences via social web sites. Enjoying videos has become very popular entertainment. One challenge is that many mobile videos do not have very appealing audio that was captured with the video. In this demonstration, to overcome this issue we propose a music video generation/creation system (Android app and backend system) that aims to make UGVs more attractive by generating scene-adaptive and user-preference aware music tracks. In our system, we take geographic categories, visual content and user listening history into account. In particular, the sequences of geographic categories and visual features are integrated into a SVM  hmm   model to predict video scene moods. The music genre, as a user preference is also exploited to personalize the recommended songs. We believe this is the first work that predicts scene moods from a real-world video dataset collected by users' daily outdoor recordings to facilitate user-preference aware music video generation. Our experiments confirm that our system can effectively combine objective scene moods and individual music tastes to recommend appealing soundtracks for videos. Our Android app only sends recorded sensor data and a few keyframes of a UGV to a cloud service (backend system) to retrieve recommended music tracks, therefore it is bandwidth efficient since the transmission of video data is not required for analysis."
1326909,15226,9078,Optimizing peer grouping for live free viewpoint video streaming,2013,"In free viewpoint video, a user can pull texture and depth videos captured from two nearby reference viewpoints to synthesize his chosen intermediate virtual view for observation via depth-image-based rendering (DIBR). For users who are observing the same video at the same time but not necessarily from the same virtual viewpoint, they have incentive to pull the same reference views so that the streaming cost can be shared. On the other hand, in general distortion of a synthesized virtual view increases with its distance to the reference views, and so a user also has incentive to select reference views that tightly “sandwich” his chosen virtual view, minimizing distortion. In a previous work, reference view sharing strategies-ones that optimally trade off shared streaming costs with synthesized view distortions-were investigated for the case when users are first divided into groups, and each user group independently pulls two reference views and shares the resulting streaming cost. In this paper, we generalize the previous notion of user group, so that a user can simultaneously belong to two groups, and each group shares the streaming cost of a single view. We also aim to find a Nash Equilibrium (NE) solution of reference view selection, which is stable and from which no one has incentive to unilaterally deviate. Specifically, we first derive a lemma based on known properties of synthesized view distortion functions. We then design a search algorithm to find a NE solution, leveraging on the derived lemma to reduce search complexity. Experimental results show that the stable NE solution increases the overall cost only slightly when compared to the unstable optimal reference selection that gives the lowest overall cost. Further, a larger network will give a lower average cost for each user, and thus, users tend to join large networks for cooperation."
1828666,15226,21106,Similarity invariant classification of events by KL divergence minimization,2011,"This paper proposes a novel method for recognition and classification of events represented by Mixture distributions of location and flow. The main idea is to classify observed events into semantically meaningful groups even when motion is observed from distinct viewpoints. Events in the proposed framework are modeled as motion patterns, which are represented by mixtures of multivariate Gaussians, and are obtained by hierarchical clustering of optical flow in the four dimensional space (x, y, u, v). Such motion patterns observed from varying viewpoints, and in distinct locations or datasets, can be compared using different families of divergences between statistical distributions, given that a transformation between the views is known. One of the major contributions of this paper is to compare and match two motion pattern mixture distributions by estimating the similarity transformation between them, that minimizes their Kullback-Leibler (KL) divergence. The KL divergence between Gaussian mixtures is approximated by Monte Carlo sampling, and the minimization is accomplished by employing an iterative nonlinear least squares estimation method, which bears close resemblance to the Iterative Closest Point (ICP) algorithm. We present a robust framework for matching of high-dimensional, sampled point sets representing statistical distributions, by defining similarity measures between them, for global energy minimization. The proposed approach is tested for classification of events observed across several datasets, captured from both static and moving cameras, involving real world pedestrian as well as vehicular motion. Encouraging results are obtained which demonstrate the feasibility and validity of the proposed approach."
2022384,15226,22130,Shadow Detection based on Colour Segmentation and Estimated Illumination.,2011,"In this paper we show how to improve the detection of shadows in natural scenes using a novel combination of colour and illumination features. Detecting shadows is useful because they provide information about both light sources and the shapes of objects thereby illuminated. Recent shadow detection methods use supervised machine learning techniques with input from colour and texture features extracted directly from the original images (e.g. Lalonde et al. ECCV 2010, Zhu et al. CVPR 2010). It seems sensible to augment these with estimates of scene illumination, as can be obtained with an intrinsic image extraction algorithm. Intrinsic image extraction separates the illumination and reflectance components in a scene, and the resulting illumination maps contain robust intensity change features at shadow boundaries. In this paper, we make two main contributions. First we improve upon existing methods for extracting illumination maps. Second we show how to use these illumination maps together with colour segmentation to extend the Lalonde’s approach to shadow detection. Illumination maps are extracted using a steerable filter framework based on global and local correlations in low and high frequency bands respectively. The illumination and colour features so extracted are then input to a decision tree trained to detect shadow edges using AdaBoost. We tested variations of our proposed approach on two public databases of natural scenes. This study showed that our approach improves on that of Lalonde both in terms of sensitivity to shadow edges and rejection of false positives. Following Lalonde we show that our detection results are further improved by imposing an edge continuity constraint via a conditional random field (CRF) model."
1367230,15226,11470,Saliency-Cognizant Error Concealment in Loss-Corrupted Streaming Video,2012,"Error concealment in packet-loss-corrupted streaming video is inherently an under-determined problem, as there are insufficient number of well-defined criteria to recover the missing blocks perfectly. When a Region-of-Interest (ROI) based unequal error protection (UEP) scheme is deployed during video streaming -- i.e., more visually salient regions are strongly protected -- a %(e.g., using strong Forward Error Correction (FEC) codes) -- a lost block is likely to be of low saliency in the original frame. In this paper, we propose to add a low-saliency prior to the error concealment problem as a regularization term. It serves two purposes. First, in ROI-based UEP video streaming, low-saliency prior provides the right side information for the client to identify the correct replacement blocks for concealment. Second, in the event that a perfectly matched block cannot be unambiguously identified, the low-saliency prior reduces viewer's visual attention on the loss-stricken region, resulting in higher overall subjective quality. We study the effectiveness of a low-saliency prior in the context of a previously proposed RECAP[1] error concealment system. RECAP transmits a low-resolution (LR) version of an image alongside the original high-resolution (HR) version, so that if blocks in the HR version are lost, the correctly-received LR version can serve as a template for matching of suitable replacement blocks from a previously correctly-decoded HR frame. We add a low-saliency prior to the block identification process, so that only replacement candidate blocks with good match and low saliency can be selected. Further, we design and apply four saliency reduction operators iteratively in a loop, in order to reduce the saliency of candidate blocks. Experimental results show that: i) PSNR of the error-concealed frames can be increased dramatically (up to $3.2$dB over the original RECAP), showing the effectiveness of a low-saliency prior in the under-determined error concealment problem, and ii) subjective quality of the repaired video using our proposal, as confirmed by an extensive user study, is better than the original RECAP."
2057798,15226,22130,Multiple queries for large scale specific object retrieval,2012,"The aim of large scale specific-object image retrieval systems is to instanta neously find images that contain the query object in the image database. Current s ystems, for example Google Goggles, concentrate on querying using a single view of an object, e.g. a photo a user takes with his mobile phone, in order to answer the question “what is this?”. Here we consider the somewhat converse problem of finding all images of an object given that the user knows what he is looking for; so the input modality is text, not a n image. This problem is useful in a number of settings, for example media production teams are interested in searching internal databases for images or video footage to accompany news reports and newspaper articles. Given a textual query (e.g. “coca cola bottle”), our approach is to firs t obtain multiple images of the queried object using textual Google image search. These images are then used to visually query the target database to discover images containing the object of interest. We compare a number of different methods for combining the multiple query images, including discriminative learning. We show that issuing multiple queries significantly improves recall and enables the system to find quite challenging occu rrences of the queried object. The system is evaluated quantitatively on the standard Oxford Buildings benchmark dataset where it achieves very high retrieval performance, and also qualitatively on the TrecVid 2011 known-item search dataset."
110261,15226,9004,Lattice Boltzmann Method For Fast Patient-Specific Simulation of Liver Tumor Ablation from CT Images,2013,"Radio-frequency ablation (RFA), the most widely used minimally invasive ablative therapy of liver cancer, is challenged by a lack of patient-specifi c planning. In particular, the presence of blood vessels and time varying thermal di ffusivity makes the prediction of the extent of the ablated tissue diffi cult. This may result in incomplete treatments and increased risk of recurrence. We propose a new model of the physical mechanisms involved in RFA of abdominal tumors based on Lattice Boltzmann Method to predict the extent of ablation given the probe location and the biological parameters. Our method relies on patient images, from which level set representations of liver geometry, tumor shape and vessels are extracted. Then a computational model of heat diff usion, cellular necrosis and blood flow through vessels and liver is solved to estimate the extent of ablated tissue. After quantitative verifi cations against an analytical solution, we apply our framework to 5 patients datasets which include pre- and post-operative CT images, yielding promising correlation between predicted and actual ablation extent (mean point to mesh errors of 8.7 mm). Implemented on graphics processing units, our method may enable RFA planning in clinical settings as it leads to near real-time computation: 1 minute of ablation is simulated in 1.14 minutes,which is almost 60 faster than standard fi nite element method."
2025115,15226,21106,Visual Reranking through Weakly Supervised Multi-graph Learning,2013,"Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo-labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts."
974382,15226,8228,WE-CARE: A wearable efficient telecardiology system using mobile 7-lead ECG devices,2013,"Cardiovascular disease (CVD), principally heart disease and stroke, has become the worldwide leading killer for people of all racial and ethnic groups. In China, this disease contributes 41% to all deaths each year, and it costs the country more than 400 billion US dollars in last decade, including health expenditures and lost productivity, which continues to grow as the population size increases. Recent research findings indicate that CVD can be effectively prevented to some extent by an interdisciplinary approach that combines ICT (Information and Communication Technology) and healthcare applications. Based on this motivation, we developed WE-CARE, a Wearable Efficient teleCARdiology systEm using mobile 7-lead ECG devices. Our WE-CARE system surpasses existing resting ECG systems that reside in hospitals owing to its improved mobility brought by wearable and mobile ECG devices. Meanwhile, it outperforms those conventional dynamic 1-lead or 3-lead ECG systems for mobile users in terms of the appropriate ECG information required for clinical proposes. We carried out clinical trials by deploying the WE-CARE systems at People Hospital of Peking University. The results showed that our solution achieves a high detection rate over 95% against common types of anomalies in ECG signals, while its risk alert delay is limited around one second, both of which match the criteria of real-time healthcare monitoring very well. As demonstrated by the results, the WE-CARE system is a useful and efficient tool for the cardiovascular disease prevention and daily monitoring."
914373,15226,9099,One of a Kind: User Profiling by Social Curation,2014,"Social Curation Service (SCS) is a new type of emerging social media platform, where users can select, organize and keep track of multimedia contents they like. In this paper, we take advantage of this great opportunity and target at the very starting point in social media: user profiling, which supports fundamental applications such as personalized search and recommendation. As compared to other profiling methods in conventional Social Network Services (SNS), our work benefits from the two distinguishable characteristics of SCS: a) organized multimedia user-generated contents, and b) content-centric social network. Based on these two characteristics, we are able to deploy the state-of-the-art multimedia analysis techniques to establish content-based user profiles by extracting user preferences and their social relations. First, we automatically construct a content-based user preference ontology and learn the ontological models to generate comprehensive user profiles. In particular, we propose a new deep learning strategy called multi-task convolutional neural network (mtCNN) to learn profile models and profile-related visual features simultaneously. Second, we propose to model the multi-level social relations offered by SCS to refine the user profiles in a low-rank recovery framework. To the best of our knowledge, our work is the first that explores how social curation can help in content-based social media technologies, taking user profiling as an example. Extensive experiments on 1,293 users and 1.5 million images collected from Pinterest in fashion domain demonstrate that recommendation methods based on the proposed user profiles are considerably more effective than other state-of-the-art recommendation strategies."
2121413,15226,30,Quasi-Static Modeling of Human Limb for Intra-Body Communications With Experiments,2011,"In recent years, the increasing number of wearable devices on human has been witnessed as a trend. These devices can serve for many purposes: personal entertainment, communication, emergency mission, health care supervision, delivery, etc. Sharing information among the devices scattered across the human body requires a body area network (BAN) and body sensor network (BSN). However, implementation of the BAN/BSN with the conventional wireless technologies cannot give optimal result. It is mainly because the high requirements of light weight, miniature, energy efficiency, security, and less electromagnetic interference greatly limit the resources available for the communication modules. The newly developed intra-body communication (IBC) can alleviate most of the mentioned problems. This technique, which employs the human body as a communication channel, could be an innovative networking method for sensors and devices on the human body. In order to encourage the research and development of the IBC, the authors are favorable to lay a better and more formal theoretical foundation on IBC. They propose a multilayer mathematical model using volume conductor theory for galvanic coupling IBC on a human limb with consideration on the inhomogeneous properties of human tissue. By introducing and checking with quasi-static approximation criteria, Maxwell's equations are decoupled and capacitance effect is included to the governing equation for further improvement. Finally, the accuracy and potential of the model are examined from both in vitro and in vivo experimental results."
1068578,15226,9099,Spatio-temporal fisher vector coding for surveillance event detection,2013,"We present a generic event detection system evaluated in the Surveillance Event Detection (SED) task of TRECVID 2012. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task. This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames. A Gaussian Mixture Model(GMM) is learned to model the distribution of the low level features. Then for each sliding window, the Fisher vector encoding [improvedFV] is used to generate the sample representation. The model is learnt using a Linear SVM for each event. The main novelty of our system is the introduction of Fisher vector encoding into video event detection. Fisher vector encoding has demonstrated great success in image classification. The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features. FV encoding uses higher order statistics in place of histograms in the standard BoW. FV has several good properties: (a) it can naturally separate the video specific information from the noisy local features and (b) we can use a linear model for this representation. We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time. We also take advantage of non-trivial object localization techniques to feed into the video event detection, e.g. multi-scale detection and non-maximum suppression. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types."
2043058,15226,8502,Segmenting color images into surface patches by exploiting sparse depth data,2011,"We present a new method for segmenting color images into their composite surfaces by combining color segmentation with model-based fitting utilizing sparse depth data, acquired using time-of-flight (Swissranger, PMD CamCube) and stereo techniques. The main target of our work is the segmentation of plant structures, i.e., leaves, from color-depth images, and the extraction of color and 3D shape information for automating manipulation tasks. Since segmentation is performed in the dense color space, even sparse, incomplete, or noisy depth information can be used. This kind of data often represents a major challenge for methods operating in the 3D data space directly. To achieve our goal, we construct a three-stage segmentation hierarchy by segmenting the color image with different resolutions-assuming that “true” surface boundaries must appear at some point along the segmentation hierarchy. 3D surfaces are then fitted to the color-segment areas using depth data. Those segments which minimize the fitting error are selected and used to construct a new segmentation. Then, an additional region merging and a growing stage are applied to avoid over-segmentation and label previously unclustered points. Experimental results demonstrate that the method is successful in segmenting a variety of domestic objects and plants into quadratic surfaces. At the end of the procedure, the sparse depth data is completed using the extracted surface models, resulting in dense depth maps. For stereo, the resulting disparity maps are compared with ground truth and the average error is computed."
2520573,15226,21106,3D Scene Understanding by Voxel-CRF,2013,"Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images."
976044,15226,9616,Evaluation of Feature Detectors and Descriptors for Motion Detection from Aerial Videos,2014,"In tasks of motion detection from aerial videos, feature-based image registration is an essential step to compensate ego motion of airborne vehicle between consecutive frames. This paper presents the first performance evaluation of feature detectors and descriptors for image alignment and frame difference to detect pixels with motion from aerial videos. To this end, we design two criteria, namely Position Error Rate(PER) and Correct Match Rate(CMR), to characterize the registration accuracy and frame difference success rate, respectively. To generate the pixel-wise registration ground-truth, we employ sophisticated block-matching method, which is then checked and corrected manually by control-points-based alignment method. Based on the proposed metrics and ground-truth registration parameters, five detectors (Harris, FAST, SUSAN, DoG, and SUSAN_M) and four descriptors (Intensity, BRIEF, HOG and SIFT) are examined. We test detector-descriptor combinations in typical visual light aerial videos and infrared aerial videos. We find that detector plays a more important role in both registration accuracy and efficiency than descriptor does, thus should receive more attention in the area of motion detection from aerial videos. For detectors, DoG performs well in most videos but has the lowest efficiency, and SUSAN_M achieves good performance balance between registration accuracy and efficiency. We also reveal that currently widely used detectors should be tailored to moving object detection tasks in future research on the aspects of feature spatial layout, removing features on moving targets, feature number control, as well as computational efficiency."
2090505,15226,22130,Push and Pull: Iterative grouping of media,2011,"We present an approach to iteratively cluster images and video in an efficient and intuitive manor. While many techniques use the traditional approach of time consuming groundtruthing large amounts of data [10, 16, 20, 23], this is increasingly infeasible as dataset size and complexity increase. Furthermore it is not applicable to the home user, who wants to intuitively group his/her own media without labelling the content. Instead we propose a solution that allows the user to select media that semantically belongs to the same class and use machine learning to pull this and other related content together. We introduce an image signature descriptor and use min-Hash and greedy clustering to efficiently present the user with clusters of the dataset using multi-dimensional scaling. The image signatures of the dataset are then adjusted by APriori data mining identifying the common elements between a small subset of image signatures. This is able to both pull together true positive clusters and push apart false positive examples. The approach is tested on real videos harvested from the web using the state of the art YouTube dataset [18]. The accuracy of correct group label increases from 60.4% to 81.7% using 15 iterations of pulling and pushing the media around. While the process takes only 1 minute to compute the pair wise similarities of the image signatures and visualise the youtube whole dataset. © 2011. The copyright of this document resides with its authors."
1418112,15226,21056,Dynamic scheduling on video transcoding for MPEG DASH in the cloud environment,2014,"The Dynamic Adaptive Streaming over HTTP (referred as MPEG DASH) standard is designed to provide high quality of media content over the Internet delivered from conventional HTTP web servers. The visual content, divided into a sequence of segments, is made available at a number of different bitrates so that an MPEG DASH client can automatically select the next segment to download and play back based on current network conditions. The task of transcoding media content to different qualities and bitrates is computationally expensive, especially in the context of large-scale video hosting systems. Therefore, it is preferably executed in a powerful cloud environment, rather than on the source computer (which may be a mobile device with limited memory, CPU speed and battery life). In order to support the live distribution of media events and to provide a satisfactory user experience, the overall processing delay of videos should be kept to a minimum. In this paper, we propose a novel dynamic scheduling methodology on video transcoding for MPEG DASH in a cloud environment, which can be adapted to different applications. The designed scheduler monitors the workload on each processor in the cloud environment and selects the fastest processors to run high-priority jobs. It also adjusts the video transcoding mode ( VTM ) according to the system load. Experimental results show that the proposed scheduler performs well in terms of the video completion time, system load balance, and video playback smoothness."
453399,15226,9004,T2-Relaxometry for Myelin Water Fraction Extraction Using Wald Distribution and Extended Phase Graph,2014,"Quantitative assessment of myelin density in the white matter is an emerging tool for neurodegenerative disease related studies such as multiple sclerosis and Schizophrenia. For the last two decades, T2 relaxometry based on multi-exponential fitting to a single slice multi-echo sequence has been the most common MRI technique for myelin water fraction (MWF) mapping, where the short T2 is associated with myelin water. However, modeling the spectrum of the relaxations as the sum of large number of impulse functions with unknown amplitudes makes the accuracy and robustness of the estimated MWF's questionable. In this paper, we introduce a novel model with small number of parameters to simultaneously characterize transverse relaxation rate spectrum and B1 inhomogeneity at each voxel. We use mixture of three Wald distributions with unknown mixture weights, mean and shape parameters to represent the distribution of the relative amount of water in between myelin sheets, tissue water, and cerebrospinal fluid. The parameters of the model are estimated using the variable projection method and are used to extract the MWF at each voxel. In addition, we use Extended Phase Graph (EPG) method to compensate for the stimulated echoes caused by B1 inhomogeneity. To validate our model, synthetic and real brain experiments were conducted where we have compared our novel algorithm with the non-negative least squares (NNLS) as the state-of-the-art technique in the literature. Our results indicate that we can estimate MWF map with substantially higher accuracy as compared to the NNLS method."
1573412,15226,20358,Quizz: targeted crowdsourcing with a billion (potential) users,2014,"We describe Quizz, a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions, Quizz estimates the user's competence. To acquire new knowledge, Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns, effectively leveraging the targeting capabilities of existing, publicly available, ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user. The feedback allows the ad targeting mechanism to further optimize ad placement.   Our experiments, which involve over ten thousand users, confirm that we can crowdsource knowledge curation for niche and specialized topics, as the advertising network can automatically identify users with the desired expertise and interest in the given topic. We present controlled experiments that examine the effect of various incentive mechanisms, highlighting the need for having short-term rewards as goals, which incentivize the users to contribute. Finally, our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms, while offering the additional advantage of giving access to billions of potential users all over the planet, and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces."
1934653,15226,390,"Cascaded multi-class pairwise classifier (CascaMPa) for normal, cancerous, and cancer confounder classes in prostate histology",2011,"Gleason grading of prostate cancer is complicated by cancer confounders, or benign tissues closely resembling malignant processes (e.g. atrophy), which account for as much as 34% of misdiagnoses. Thus, it is critical to correctly identify confounders in a computer-aided diagnosis system. In this work, we present a cascaded multi-class pairwise classifier (CascaMPa) to identify the class of regions of interest (ROIs) of prostate tissue biopsies. CascaMPa incorporates domain knowledge to partition the multi-class problem into several binary-class tasks, reducing the intra-class heterogeneity that causes errors in one-versus-all multi-class approaches. Nuclear centroids are detected automatically via a deconvolution and watershed algorithm, and a set of features are calculated from graphs (Voronoi, Delaunay, and minimum spanning tree graphs) constructed on the centroids. The cascaded multi-class algorithm identifies each feature vector as one of six tissue classes: Gleason patterns 3, 4, and 5, normal epithelium and stroma, and non-cancerous atrophy (a cancer confounder) using a decision tree classifier. Performance of CascaMPa is evaluated using positive predictive value (PPV) and area under the receiver operating characteristic curve (AUC). For a database of 50 image patches per class (300 ROIs) from 118 patient studies, the classifier achieves an average PPV of 0.760 and AUC of 0.762 across all classes, while the one-versus-all approach yields an average PPV of 0.633 and AUC of 0.444."
1334026,15226,21056,Exploiting just-noticeable difference of delays for improving quality of experience in video conferencing,2013,"This paper proposes a novel approach for improving the quality of experience (QoE) of real-time video conferencing systems. In these systems, QoE is affected by signal quality as well as interactivity, both depending on the packet loss rate, delay jitters, and mouth-to-ear delay (MED) that measures the sender-receiver delay on audio signals (and will be the same as that of video signals when video and audio is synchronized). We notice in the current Internet that increasing MED as well as reducing packet rate can help reduce the delay-aware loss rate in congested connections. Between the two methods, the former plays a more important role and applies well to a variety of network conditions for improving audiovisual signal quality, although overly increasing the MED will degrade interactivity. Based on a psychophysical concept called just-noticeable difference (JND), we find the extent to which MED can be increased, without humans perceiving the difference from the original conversation. The approach can be applied to improve existing video conferencing systems. Starting from the operating point of an existing system, we increase its MED to within JND in order to have more room for smoothing network delay spikes as well as recovering lost packets, without incurring noticeable degradation in interactivity. We demonstrate the idea on Skype and Windows Live Messenger by designing a traffic interceptor to extend their buffering time and to perform packet scheduling/recovery. Our experimental results show significant improvements in QoE, with much better signal quality while maintaining similar interactivity."
1555510,15226,9713,A GIS-based serious game interface for therapy monitoring,2014,"In this paper, we present a novel idea of a map-based therapy environment for people with Hemiplegia. The therapy environment is designed according to the suggestions of therapists, which consists of a spatial map browsing serious game augmented with our novel multi-sensory natural user interface (NUI). The NUI is based on 3D motion sensors that can recognize different hand and body gestures used for browsing a 3D or 2D map. The 3D motion sensors work in a non-invasive way; hence, they do not require any wearable body attachments and can be used at home without assistance from the therapists. The map-browsing environment provides an immersive experience to the disabled users, which helps in performing therapy in an interesting and entertaining manner. We have developed analytics for measuring certain quality of health improvement metrics from each type of spatial map browsing movements. The 3D motion sensors have been tested with Nokia, Google, ESRI, and a number of other maps that allow a subject to visualize and browse the 3D and 2D maps of the world. The map browsing session data shows the nature of big data; hence, the session data is stored in a cloud environment. Our developed serious game environment is web-based; thus anyone having the appropriate low cost sensor hardware can plug it in and start experiencing a natural way of hands free map browsing. We have deployed our framework in a hospital that treats Hemiplegic patients. Based on the feedback obtained, the developed platform shows a huge potential for use in hospitals that provide physiotherapy services as well as at patients' home as an assistive therapeutic service."
1155369,15226,11470,SSIM-Inspired Perceptual Video Coding for HEVC,2012,"Recent advances in video capturing and display technologies, along with the exponentially increasing demand of video services, challenge the video coding research community to design new algorithms able to significantly improve the compression performance of the current H.264/AVC standard. This target is currently gaining evidence with the standardization activities in the High Efficiency Video Coding (HEVC) project. The distortion models used in HEVC are mean squared error (MSE) and sum of absolute difference (SAD). However, they are widely criticized for not correlating well with perceptual image quality. The structural similarity (SSIM) index has been found to be a good indicator of perceived image quality. Meanwhile, it is computationally simple compared with other state-of-the-art perceptual quality measures and has a number of desirable mathematical properties for optimization tasks. We propose a perceptual video coding method to improve upon the current HEVC based on an SSIM-inspired divisive normalization scheme as an attempt to transform the DCT domain frame prediction residuals to a perceptually uniform space before encoding. Based on the residual divisive normalization process, we define a distortion model for mode selection and show that such a divisive normalization strategy largely simplifies the subsequent perceptual rate-distortion optimization procedure. We further adjust the divisive normalization factors based on local content of the video frame. Experiments show that the proposed scheme can achieve significant gain in terms of rate-SSIM performance when compared with HEVC."
2544635,15226,20338,Video stream quality impacts viewer behavior: inferring causality using quasi-experimental designs,2012,"The distribution of videos over the Internet is drastically transforming how media is consumed and monetized. Content providers, such as media outlets and video subscription services, would like to ensure that their videos do not fail, startup quickly, and play without interruptions. In return for their investment in video stream quality, content providers expect less viewer abandonment, more viewer engagement, and a greater fraction of repeat viewers, resulting in greater revenues. The key question for a content provider or a CDN is whether and to what extent changes in video quality can cause changes in viewer behavior. Our work is the first to establish a causal relationship between video quality and viewer behavior, taking a step beyond purely correlational studies. To establish causality, we use Quasi-Experimental Designs, a novel technique adapted from the medical and social sciences.   We study the impact of video stream quality on viewer behavior in a scientific data-driven manner by using extensive traces from Akamai's streaming network that include 23 million views from 6.7 million unique viewers. We show that viewers start to abandon a video if it takes more than 2 seconds to start up, with each incremental delay of 1 second resulting in a 5.8%increase in the abandonment rate. Further, we show that a moderate amount of interruptions can decrease the average play time of a viewer by a significant amount. A viewer who experiences a rebuffer delay equal to 1% of the video duration plays 5% less of the video in comparison to a similar viewer who experienced no rebuffering. Finally, we show that a viewer who experienced failure is 2.32% less likely to revisit the same site within a week than a similar viewer who did not experience a failure."
1294743,15226,9616,A Preliminary Study of Lower Leg Geometry as a Soft Biometric Trait for Forensic Investigation,2014,"Criminal and victim identification is always vital in forensic investigation. Many biometric traits, such as DNA, fingerprint, face and palm print, have been regularly used by law enforcement agencies. However, they are not applicable to legal cases where only non-facial body sites of criminals or victims in evidence images are available for identification. These cases include but are not limited to violent protests, masked gunmen and child pornography. To address this challenging identification problem, skin marks, blood vessels hidden in color images, androgenic hair patterns and tattoos have been considered. Tattoos are not always available. Skin marks and blood vessels are suitable for high resolution images. Androgenic hair patterns provide useful identification information even in low resolution images, but their performance is still far from perfect. Thus, new biometric traits are still demanded especially for low resolution evidence images. This paper evaluates lower leg geometry as a soft biometric trait for criminal and victim identification. Lower legs are considered in this study because they are often observable in evidence images. The algorithm utilized in this evaluation first aligns two lower leg shapes from input images and extracts geometric features, including the partial sum of squared difference, the polynomial coefficients and the number of intersection points of the aligned leg shapes. Support vector machines, neural networks and decision trees are used to perform the classification. The algorithm is applied to 1,138 images from 283 subjects. The experimental results indicate that lower leg geometry is an effective soft biometric trait. This study provides a foundation for further research on criminal and victim identification based on body geometry."
2620195,15226,11052,Graduated Consistency-Regularized Optimization for Multi-graph Matching,2014,"Graph matching has a wide spectrum of computer vision ap- plications such as finding feature point correspondences across images. The problem of graph matching is generally NP-hard, so most exist- ing work pursues suboptimal solutions between two graphs. This paper investigates a more general problem of matching N attributed graphs to each other, i.e. labeling their common node correspondences such that a certain compatibility/affinity objective is optimized. This multi- graph matching problem involves two key ingredients affecting the over- all accuracy: a) the pairwise affinity matching score between two local graphs, and b) global matching consistency that measures the uniqueness and consistency of the pairwise matching results by different sequential matching orders. Previous work typically either enforces the matching consistency constraints in the beginning of iterative optimization, which may propagate matching error both over iterations and across different graph pairs; or separates score optimizing and consistency synchroniza- tion in two steps. This paper is motivated by the observation that affinity score and consistency are mutually affected and shall be tackled jointly to capture their correlation behavior. As such, we propose a novel multi- graph matching algorithm to incorporate the two aspects by iteratively approximating the global-optimal affinity score, meanwhile gradually in- fusing the consistency as a regularizer, which improves the performance of the initial solutions obtained by existing pairwise graph matching solvers. The proposed algorithm with a theoretically proven convergence shows notable efficacy on both synthetic and public image datasets."
2388735,15226,21106,Tracking visual and infrared objects using joint Riemannian manifold appearance and affine shape modeling,2011,"This paper addresses the problem of object tracking from visual and infrared videos captured either by a dynamic or stationary camera where objects contain large pose changes. We propose a novel object tracking scheme that exploits the geometrical structure of Riemannian manifold and piecewise geodesics under a Bayesian framework. Two particle filters are alternatingly employed for tracking dynamic objects. One for online learning object appearances on Riemannian manifolds using tracked candidates, another for tracking object bounding box parameters with appearances on the manifold embedded. The rationale for obtaining this enhanced manifold tracker as compared with existing ones is to introduce an additional state variable, such that not only the manifold point representing the object is updated, but also the velocity of dynamic manifold point is estimated. Main contributions of the paper include: (a) propose an online appearance learning strategy by a particle filter on the manifold; (b) an object tracker that incorporates the manifold appearance for prediction under a particle filter framework; (c) use partitioned sub-regions of object bounding box that incorporates the spatial information in the appearance; (d) use Gabor features in different frequencies and orientations in partitioned sub-regions for IR (infrared) video objects. Hence, the proposed tracking scheme is applicable to both visual and IR videos. Experiments on videos where objects contain significant pose changes show very robust tracking results. The proposed scheme is also compared with two most relevant manifold tracking methods, results have shown much improved tracking performance in terms of tracking drift and tightness and accuracy of tracked boxes."
1654135,15226,9078,"Harris, SIFT and SURF features comparison for vehicle localization based on virtual 3D model and camera",2012,"This paper proposed a new vehicle geo-localization method in urban environment integrating a new source of information that is a virtual 3D city model. This 3D model provides a realistic representation of the navigation environment of the vehicle. To optimize the performance of vehicle geo-localization system, several sources of information are integrated for their complementarity and redundancy: a GPS receiver, proprioceptive sensors (odometers and gyrometer), a video camera and a virtual 3D city model. The pose estimation algorithm used to fuse the different sensors data is an IMM-UKF (Interacting Multiple Model - Unscented Kalman Filter). The proprioceptive sensors allow to continuously estimating the dead-reckoning position and orientation of the vehicle. This dead-reckoning estimation of the pose is corrected by GPS measurements. Moreover, a 3D model/camera based observation of the vehicle pose is constructed to compensate the drift of the dead-reckoning localization when GPS measurements are unavailable for a long time. This pose observation is based on the matching between the virtual image extracted from the 3D city model and the real image acquired by the camera. The observation construction is composed of two major parts. The first part consists in detecting and matching the feature points of the real and virtual images. Three features are compared: Harris corner, SIFT (Scale Invariant Feature Transform) and SURF (Speed Up Robust Features). The second part is the pose computation using POSIT algorithm and the previously matched features set. The developed approach has been tested on a real sequence and the obtained results proved the feasibility and robustness of the approach."
1333398,15226,422,Assessing team strategy using spatiotemporal data,2013,"The Moneyball revolution coincided with a shift in the way professional sporting organizations handle and utilize data in terms of decision making processes. Due to the demand for better sports analytics and the improvement in sensor technology, there has been a plethora of ball and player tracking information generated within professional sports for analytical purposes. However, due to the continuous nature of the data and the lack of associated high-level labels to describe it - this rich set of information has had very limited use especially in the analysis of a team's tactics and strategy. In this paper, we give an overview of the types of analysis currently performed mostly with hand-labeled event data and highlight the problems associated with the influx of spatiotemporal data. By way of example, we present an approach which uses an entire season of ball tracking data from the English Premier League (2010-2011 season) to reinforce the common held belief that teams should aim to win home games and draw away ones. We do this by: i) forming a representation of team behavior by chunking the incoming spatiotemporal signal into a series of quantized bins, and ii) generate an expectation model of team behavior based on a code-book of past performances. We show that home advantage in soccer is partly due to the conservative strategy of the away team. We also show that our approach can flag anomalous team behavior which has many potential applications."
34599,15226,8228,Feature selection by user specific feature mask on a biometric hash algorithm for dynamic handwriting,2011,"One of the most important requirements on a biometric verification system, beside others (e.g. biometric template protection), is a high user authentication performance. During the last years a lot of research is done in different domains to improve user authentication performance. In this work we suggest a user specific feature mask vector MV applied on a biometric hash algorithm for dynamic handwriting to improve user authentication and hash generation performance. MV is generated using an additional set of reference data in order to select/deselect certain features used during the verification process. Therefore, this method is considered as a simple feature selection strategy and is applied for every user within the system. In our first experiments we evaluate 5850 raw data samples captured from 39 users for five different semantics. Semantics are alternative written content to conceal the real identity of a user. First results show a noticeable decrease of the equal error rate by approximately three percentage points for each semantic. Lowest equal error rate (5.77%) is achieved by semantic symbol. In the context of biometric hash generation, the reproduction rates (RR) increases by an average of approx. 26%, whereas the highest RR (88.46%) is obtained by semantic symbol along with a collision rate (CR) of 5.11%. The minimal amount of selected features during the evaluation is 81 and the maximum amount is 131 (all available features)."
1941892,15226,9616,Self-Updating with Facial Trajectories for Video-to-Video Face Recognition,2014,"For applications of face recognition (FR) in video surveillance, it is often costly or unfeasible to collect several high quality reference samples a priori to design representative facial models. Moreover, changes in capture conditions and human physiology create divergence between facial models and input captures. Multiple classifier systems (MCS) have been successfully applied to video-to-video FR, where the face of each individual of interest is modeled using an ensemble of 2-class classifiers (trained on target vs. non-target samples). However, the reliable self-update of these individual-specific ensembles with relevant target and non-target samples raises several challenges. In this paper, an adaptive MCS is proposed that allows for self-updating facial models given face trajectories captured during operations. Different faces appearing in a camera viewpoint are tracked, and ensemble predictions for facial captures are accumulated along each track for robust video-to-video FR. When the number of positive predictions over time surpasses an update threshold, the target face samples extracted from the trajectory are combined with non-target samples selected from the cohort and universal models for efficient self-update the corresponding face model. A learn-and-combine strategy is then employed to avoid knowledge corruption during self-update of an ensemble. At a transaction level, the adaptive MCS outperforms the reference systems that do not allow self-updating on Face in Action videos. Analysis at a trajectory level indicates that the proposed system allows for robust spatio-temporal recognition, which translates to enhanced security and situation analysis."
2678835,15226,20332,Integrating digital pens in breast imaging for instant knowledge acquisition,2013,"Future radiology practices assume that the radiology reports should be uniform, comprehensive, and easily managed. This means that reports must be readable to humans and machines alike. In order to improve reporting practices in breast imaging, we allow the radiologist to write structured reports with a special pen on paper with an invisible dot pattern. In this way, we provide a knowledge acquisition system for printed mammography patient forms for the combined work with printed and digital documents. In this domain, printed documents cannot be easily replaced by computer systems because they contain free-form sketches and textual annotations, and the acceptance of traditional PC reporting tools is rather low among the doctors. This is due to the fact that current electronic reporting systems significantly add to the amount of time it takes to complete the reports. We describe our real-time digital paper application and focus on the use case study of our deployed application. We think that our results motivate the design and implementation of intuitive pen-based user interfaces for the medical reporting process and similar knowledge work domains. Our system imposes only minimal overhead on traditional form-filling processes and provides for a direct, ontology-based structuring of the user input for semantic search and retrieval applications, as well as other applied artificial intelligence scenarios which involve manual form-based data acquisition."
1138538,15226,11470,Information systems and management in media industries a first problem evaluation from a business perspective,2014,"Media industry is undergoing a very intensive transformation from traditional Media towards fully digital media environments. Within that market space, the environment as well as consumer behavior is changing very rapidly. In particular inside media houses various changes are taking place. One of these changes is the introduction or improvement of IT infrastructure to support business activities - in short, introducing or improving information systems and management in eMedia and creativity industry. In view a focus on advanced information management has a vital role of having a major impact in creating competitive advantage in media companies. In this respect, one of the biggest challenges is how to efficiently manage the ever changing massive amount of mostly unstructured and diverse data. This data consists of traditional business information (e.g. Customer Relationship Management (CRM), Supplier Relationship Management (SRM), Financial Supply Chain Management (FSCM)), information directly related to media content (e.g. media information, metadata, content information), as well as consumer data (e.g. social media, customer databases). Traditional media companies will need to invest in latest IT technology which is able to manage flexibly, quickly and efficiently the large amounts of structured and unstructured data. On the other hand the challenge is to deliver structured data in form of business intelligence throughout the organizational levels, between business partners, and the complete media eco-system. Within the scope of this paper we present a first problem definition of information systems and management in eMedia industries."
1611459,15226,22035,A study on perception of mobile video with surrounding contextual influences,2012,"Contemporary users have been viewing video contents via mobile devices from virtually anywhere and at any time. One common experience these mobile users feel is the significant difference in visual perception when viewing context changes, e.g. from indoor to outdoor. Conventional video quality assessment (VQA) defined under ITU-T recommendations outlines a set of evaluation condi-tions that need to be strictly followed. These conditions, including viewing distance, room illumination, display brightness and chro-maticity of background are historically tuned to simulate the living room television viewing scenario. In this paper, we first present a set of contextual factors that are unique for mobile video and are substantially different from conventional living room evaluation conditions. We design and perform a series of subjective tests to evaluate the influence of these contextual factors that frequently encountered in the case of mobile video. These evaluation results show that (1) perceptual quality is highly correlated with the con-text factors; (2) with the presence of the contextual visual interfer-ence, the viewers shall spot less video signal distortion and have lower expectations on the quality of mobile video. We then pro-pose a VQA model based on the Just Noticeable Distortion (JND) theory and we prove that this model is able to provide context aware prediction of perceived mobile video quality. Finally, we also discuss its application of this VQA model in the design of video transmission systems."
2249332,15226,9004,Regional heart motion abnormality detection via multiview fusion,2012,"This study investigates regional heart motion abnormality detection via multiview fusion in cine cardiac MR images. In contrast to previous methods which rely only on short-axis image sequences, the proposed approach exploits the information from several other long-axis image sequences, namely, 2-chamber, 3-chamber and 4-chamber MR images. Our analysis follows the standard issued by American Heart Association to identify 17 standardized left ventricular segments. The proposed method first computes an initial sequence of corresponding myocardial points using a nonrigid image registration algorithm within each sequence. Then, these points were mapped to 3D space and tracked using Unscented Kalman Filter (UKS). We propose a maximum likelihood based track-to-track fusion approach to combine UKS tracks from multiple image views. Finally, we use a Shannon's differential entropy of distributions of potential classifiers obtained from multiview fusion estimates, and a naive Bayes classifier algorithm to automatically detect abnormal functional regions of the myocardium. We proved the benefits of the proposed method by comparing the classification results with and without fusion over 480 regional myocardial segments obtained from 30 subjects. The evaluations in comparisons to the ground truth classifications by radiologists showed that the proposed fusion yielded an area-under-the-curve (AUC) of 95.9%, bringing a significant improvement of 3.8 % in comparisons to previous methods that use only short-axis images."
690020,15226,20338,YouTube everywhere: impact of device and infrastructure synergies on user experience,2011,"In this paper we present a complete measurement study that compares YouTube traffic generated by mobile devices (smart-phones,tablets) with traffic generated by common PCs (desktops, notebooks, netbooks). We investigate the users' behavior and correlate it with the system performance. Our measurements are performed using unique data sets which are collected from vantage points in nation-wide ISPs and University campuses from two countries in Europe and the U.S.   Our results show that the user access patterns are similar across a wide range of user locations, access technologies and user devices. Users stick with default player configurations, e.g., not changing video resolution or rarely enabling full screen playback. Furthermore it is very common that users abort video playback, with 60% of videos watched for no more than 20% of their duration.   We show that the YouTube system is highly optimized for PC access and leverages aggressive buffering policies to guarantee excellent video playback. This however causes 25%-39% of data to be unnecessarily transferred, since users abort the playback very early. This waste of data transferred is even higher when mobile devices are considered. The limited storage offered by those devices makes the video download more complicated and overall less efficient, so that clients typically download more data than the actual video size. Overall, this result calls for better system optimization for both, PC and mobile accesses."
2200227,15226,9748,Accelerating Boosting-Based Face Detection on GPUs,2012,"The goal of face detection is to determine the presence of faces in arbitrary images, along with their locations and dimensions. As it happens with any graphics workloads, these algorithms benefit from data-level parallelism. Existing parallelization efforts strictly focus on mapping different divide and conquer strategies into multicore CPUs and GPUs. However, even the most advanced single-chip many-core processors to date are still struggling to effectively handle real-time face detection under high-definition video workloads. To address this challenge, face detection algorithms typically avoid computations by dynamically evaluating a boosted cascade of classifiers. Unfortunately, this technique yields a low ALU occupancy in architectures such as GPUs, which heavily rely on large SIMD widths for maximizing data-level parallelism. In this paper we present several techniques to increase the performance of the cascade evaluation kernel, which is the most resource-intensive part of the face detection pipeline. Particularly, the usage of concurrent kernel execution in combination with cascades generated with the Gentle Boost algorithm solves the problem of GPU underutilization, and achieves a 5X speedup in 1080p videos on average over the fastest known implementations, while slightly improving the accuracy. Finally, we also studied the parallelization of the cascade training process and its scalability under SMP platforms. The proposed parallelization strategy exploits both task and data-level parallelism and achieves a 3.5X speedup over single-threaded implementations."
678338,15226,11166,Transform Residual K-Means Trees for Scalable Clustering,2013,"The K-means problem, i.e., to partition a dataset into K clusters is a fundamental problem common to numerous data mining applications. As it is an NP-hard problem, iterative optimizations are typically used such as the K-means algorithm to compute cluster centers. As the K-means algorithm is exponential both in computation and storage in terms of required bits to encode cluster centers, approaches with low computation and storage complexity have been actively studied both for signal compression under real-time constraints and for clustering of large scale high-dimensional datasets. By generating cluster centers via Cartesian products of cluster centers in multiple groups or multiple stages, product and residual K-means trees reduce computation and storage complexity, making it possible to cluster large scale datasets. As residual K-means trees do not require assumed statistical independence among groups that are required by product K-means trees, they generally give better clustering quality. A known limitation of residual K-means trees is that the gain diminishes with added stages. In this paper, by identifying increased intrinsic dimensions of residual vectors with more stages as a limiting factor of performance due to the curse of dimensionality, we propose transform residual K-means trees as a generalization by applying cluster specific transforms to increase the correlations of residual vectors. Our methods substantially reduce the increase of intrinsic dimensions and therefore increase the effectiveness of multiple-stage residual Kmeans trees. Experimental and comparative results using widely used datasets show our methods give the best performance among all scalable methods. Furthermore, our methods enable effective super-clustering learning and provide scalable solutions to duplicate detection and other data mining problems."
791698,15226,11470,MASK: Robust Local Features for Audio Fingerprinting,2012,"This paper presents a novel local audio fingerprint called MASK (Masked Audio Spectral Key points) that can effectively encode the acoustic information existent in audio documents and discriminate between transformed versions of the same acoustic documents and other unrelated documents. The fingerprint has been designed to be resilient to strong transformations of the original signal and to be usable for generic audio, including music and speech. Its main characteristics are its locality, binary encoding, robustness and compactness. The proposed audio fingerprint encodes the local spectral energies around salient points selected among the main spectral peaks in a given signal. Such encoding is done by centering on each point a carefully designed mask defining regions of the spectrogram whose average energies are compared with each other. From each comparison we obtain a single bit depending on which region has more energy, and group all bits into a final binary fingerprint. In addition, the fingerprint also stores the frequency of each peak, quantized using a Mel filter bank. The length of the fingerprint is solely defined by the number of compared regions being used, and can be adapted to the requirements of any particular application. In addition, the number of salient points encoded per second can be also easily modified. In the experimental section we show the suitability of such fingerprint to find matching segments by using the NIST-TRECVID benchmarking evaluation datasets by comparing it with a well known fingerprint, obtaining up to $26$\% relative improvement in NDCR score."
1427475,15226,22130,Reproduction angular error: An improved performance metric for illuminant estimation,2014,"Only if we can estimate the colour of the prevailing light - and discount it from the image - can image colour be used as a stable cue for indexing, recognition and tracking (amongst other tasks). Almost all illumination estimation research uses the angle between the RGB of the actual measured illuminant colour and that estimated one as the recovery error. However here we identify a problem with this metric. We observe that the same scene, viewed under two different coloured lights for the same algorithm, leads to different recovery errors despite the fact that when we remove the colour bias due to illuminant (we divide out by light) exactly the same reproduction is produced. We begin this paper by quantifying the scale of this problem. For a given scene and algorithm, we solve for the range of recovery angular errors that can be observed given all colours of light. We also show that the lowest errors are for red, green and blue lights and the largest for cyans, magentas and yellows. Next, we propose a new reproduction angular error which is defined as the angle between the image RGB of a white surface when the actual and estimated illuminations are 'divided out'. Reassuringly, this reproduction error metric, by construction, gives the same error for the same algorithm-scene pair. For many algorithms and many benchmark datasets we recompute the illuminant estimation performance of a range of algorithms for the new reproduction error and then compare against the algorithm rankings for the old recovery error. We find that the overall rankings of algorithms remains, broadly, unchanged - though there can be local switches in rank - and the algorithm parameters provide that the best illuminant estimation performance depend on the error metric used."
1339602,15226,9713,Visually-complete aerial LiDAR point cloud rendering,2012,"Aerial LiDAR (Light Detection and Ranging) point clouds are gathered by a downward scanning laser on a low-flying aircraft. Due to the imaging process, vertical surface features such as building walls, and ground areas under tree canopies are totally or partially occluded, resulting in gaps and sparsely sampled areas. These gaps produce unwanted holes and uneven point distributions that often produce artifacts when visualized using point-based rendering (PBR) techniques. We show how to extend PBR by inferring the physical nature of LiDAR points for visual realism and added comprehension. More specifically, the class of object a point is related to augments the point cloud in pre-processing and/or adapts the online rendering, to produce visualizations that are more complete and realistic. We provide examples of point cloud augmentation for building walls and ground areas under tree canopies. We show how different types of procedurally generated geometry can be used to recover building walls. These methods are generic and can be applied to any aerial LiDAR data set with buildings and trees. Our work also incorporates an out-of-core strategy for hierarchical data management and GPU-accelerated PBR with extended deferred shading. The combined system provides interactive visually-complete rendering of virtually unlimited-size LiDAR point clouds. Experimental results show that our rendering approach adds only a slight overhead to PBR and provides comparable visual cues to visualizations generated by off-line pre-computation of 3D polygonal urban models."
1084295,15226,21106,Face authentication using graph-based low-rank representation of facial local structures for mobile vision applications,2011,"Mobile vision systems involve more challenges than ordinary vision systems. In this paper, we propose a novel face authentication approach that considers the difficulties, which exist in mobile vision systems, e.g. lower resolution and quality acquisition, lower storage capabilities, lower computation power, and poor imaging conditions. The proposed authentication approach uses specific fiducial components for authentication purposes. A facial graph model, which involves both of appearance and geometric facial information, is built for face representation. The graph of an image in the gallery set is composed of the low rank matrices of those components as nodes and the mean Euclidean distances between these components as edges. The probe image is represented in a similar way, except that the nodes are represented by the intensity vectors of the components. A weighted matching procedure is performed between the probe and the gallery graphs to make an authentication decision. The proposed system was evaluated using a locally-designed challenging dataset, which is acquired using a cell phone camera. The quality of the captured images of the evaluation dataset is intentionally very low, in order to mimic severe imaging conditions, which a mobile authentication system may encounter. For comparison purposes and as a benchmark test, we evaluate the proposed approach using the FRGC 2.0 dataset. The evaluation results show the potential of the proposed approach in making correct authentication decisions, given very low quality-images. The effectiveness of the proposed approach, in terms of accuracy and memory requirements, versus relevant approaches is demonstrated."
22409,15226,9004,Data-Driven breast decompression and lesion mapping from digital breast tomosynthesis,2012,"Digital Breast Tomosynthesis (DBT) emerges as a new 3D modality for breast cancer screening and diagnosis. Like in conventional 2D mammography the breast is scanned in a compressed state. For orientation during surgical planning, e.g., during presurgical ultrasound-guided anchor-wire marking, as well as for improving communication between radiologists and surgeons it is desirable to estimate an uncompressed model of the acquired breast along with a spatial mapping that allows localizing lesions marked in DBT in the uncompressed model. We therefore propose a method for 3D breast decompression and associated lesion mapping from 3D DBT data. The method is entirely data-driven and employs machine learning methods to predict the shape of the uncompressed breast from a DBT input volume. For this purpose a shape space has been constructed from manually annotated uncompressed breast surfaces and shape parameters are predicted by multiple multi-variate Random Forest regression. By exploiting point correspondences between the compressed and uncompressed breasts, lesions identified in DBT can be mapped to approximately corresponding locations in the uncompressed breast model. To this end, a thin-plate spline mapping is employed. Our method features a novel completely data-driven approach to breast shape prediction that does not necessitate prior knowledge about biomechanical properties and parameters of the breast tissue. Instead, a particular deformation behavior (decompression) is learned from annotated shape pairs, compressed and uncompressed, which are obtained from DBT and magnetic resonance image volumes, respectively. On average, shape prediction takes 26 s and achieves a surface distance of 15.80±4.70 mm. The mean localization error for lesion mapping is 22.48±8.67 mm."
1597449,15226,9099,Multiple Features But Few Labels?: A Symbiotic Solution Exemplified for Video Analysis,2014,"Video analysis has been attracting increasing research due to the proliferation of internet videos. In this paper, we investigate how to improve the performance on internet quality video analysis. Particularly, we work on the scenario of few labeled training videos being provided, which is less focused in multimedia. To being with, we consider how to more effectively harness the evidences from the low-level features. Researchers have developed several promising features to represent videos to capture the semantic information. However, as videos usually characterize rich semantic contents, the analysis performance by using one single feature is potentially limited. Simply combining multiple features through early fusion or late fusion to incorporate more informative cues is doable but not optimal due to the heterogeneity and different predicting capability of these features. For better exploitation of multiple features, we propose to mine the importance of different features and cast it into the learning of the classification model. Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance. On the other hand, to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way. The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos. We perform extensive experiments on video action recognition and multimedia event recognition and the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework."
52282,15226,9004,Image segmentation with implicit color standardization using spatially constrained expectation maximization: detection of nuclei,2012,"Color nonstandardness -- the propensity for similar objects to exhibit different color properties across images -- poses a significant problem in the computerized analysis of histopathology. Though many papers propose means for improving color constancy, the vast majority assume image formation via reflective light instead of light transmission as in microscopy, and thus are inappropriate for histological analysis. Previously, we presented a novel Bayesian color segmentation algorithm for histological images that is highly robust to color nonstandardness; this algorithm employed the expectation maximization (EM) algorithm to dynamically estimate -- for each individual image -- the probability density functions that describe the colors of salient objects. However, our approach, like most EM-based algorithms, ignored important spatial constraints, such as those modeled by Markov random field (MRFs). Addressing this deficiency, we now present spatially-constrained EM (SCEM), a novel approach for incorporating Markov priors into the EM framework. With respect to our segmentation system, we replace EM with SCEM and then assess its improved ability to segment nuclei in H&E stained histopathology. Segmentation performance is evaluated over seven (nearly) identical sections of gastrointestinal tissue stained using different protocols (simulating severe color nonstandardness). Over this dataset, our system identifies nuclear regions with an area under the receiver operator characteristic curve (AUC) of 0.838. If we disregard spatial constraints, the AUC drops to 0.748."
726886,15226,8502,Depth-based patch scaling for content-aware stereo image completion,2014,"A number of recent algorithms have been proposed for working with stereo image pairs in ways that are already familiar to users of single-image editing tools. In particular, Morse, et al. (2012) have proposed a method for performing image completion in stereo images so as to maintain stereoscopic consistency. Like prior work in stereo completion, this method drew source texture only from regions at the same depth as the target region, which while helping the result can sometimes overly limit the pool of suitable source textures. Other methods such as the Generalized PatchMatch approach of Barnes, et al. (2010) have used scaled (and otherwise transformed) source texture to improve the quality of the completed target region, but these methods rely on randomly sampling the scale (or transformation) space without knowledge of scene geometry. This paper extends stereo image completion to include source textures scaled according to the relative differences in depth between image regions. Limited random sampling is used to make the method robust to minor errors in the stereo disparities and to provide for non-uniform aspect ratios, but with far fewer random samples than prior unrestrained sampling of scale. A preference for unscaled or downsampled source textures rather than upsampled ones is incorporated into the objective function and avoids an inherent matching bias towards low-frequency regions. Results demonstrate that using scene geometry to drive scale selection results in improved image completion compared to either single-image completion or prior methods for stereo completion."
301114,15226,11052,Interactively Guiding Semi-Supervised Clustering via Attribute-Based Explanations,2014,"Unsupervised image clustering is a challenging and often ill- posed problem. Existing image descriptors fail to capture the clustering criterion well, and more importantly, the criterion itself may depend on (unknown) user preferences. Semi-supervised approaches such as distance metric learning and constrained clustering thus leverage user-provided annotations indicating which pairs of images belong to the same clus- ter (must-link) and which ones do not (cannot-link). These approaches require many such constraints before achieving good clustering perfor- mance because each constraint only provides weak cues about the de- sired clustering. In this paper, we propose to use image attributes as a modality for the user to provide more informative cues. In particular, the clustering algorithm iteratively and actively queries a user with an image pair. Instead of the user simply providing a must-link/cannot-link constraint for the pair, the user also provides an attribute-based reason- ing e.g. these two images are similar because both are natural and have still water or these two people are dissimilar because one is way older than the other. Under the guidance of this explanation, and equipped with attribute predictors, many additional constraints are automatically generated. We demonstrate the effectiveness of our approach by incorpo- rating the proposed attribute-based explanations in three standard semi- supervised clustering algorithms: Constrained K-Means, MPCK-Means, and Spectral Clustering, on three domains: scenes, shoes, and faces, using both binary and relative attributes."
2572547,15226,21106,Semantic structure from motion: a novel framework for joint object recognition and 3d reconstruction,2011,"Conventional rigid structure from motion (SFM) addresses the problem of recovering the camera parameters (motion) and the 3D locations (structure) of scene points, given observed 2D image feature points. In this chapter, we propose a new formulation called Semantic Structure From Motion (SSFM). In addition to the geometrical constraints provided by SFM, SSFM takes advantage of both semantic and geometrical properties associated with objects in a scene. These properties allow to jointly estimate the structure of the scene, the camera parameters as well as the 3D locations, poses, and categories of objects in a scene. We cast this problem as a max-likelihood problem where geometry (cameras, points, objects) and semantic information (object classes) are simultaneously estimated. The key intuition is that, in addition to image features, the measurements of objects across views provide additional geometrical constraints that relate cameras and scene parameters. These constraints make the geometry estimation process more robust and, in turn, make object detection more accurate. Our framework has the unique ability to: i) estimate camera poses only from object detections, ii) enhance camera pose estimation, compared to feature-point-based SFM algorithms, iii) improve object detections given multiple uncalibrated images, compared to independently detecting objects in single images. Extensive quantitative results on three datasets --- LiDAR cars, street-view pedestrians, and Kinect office desktop --- verify our theoretical claims."
825518,15226,21056,CamMark: a camcorder copy simulation as watermarking benchmark for digital video,2014,"In 1998, Petitcolas et al. proposed StirMark [14] as a benchmark for image watermarking schemes. The main idea was to introduce a re-sampling process that mimics the analog process of printing and scanning a watermarked image. For digital video, the corresponding concept is a camcorder copy, where a video displayed on a screen is (digitally) recorded using a video camera. As most commercial video streaming systems (VOD, IPTV) and offline distribution (Blu-ray, HDDs for cinemas) are strongly protected by means of DRM, filming a display is actually a relevant use case and a requirement for robust video watermarking systems to survive.   We therefore present a tool to simulate content re-acquisition with a camcorder. Our goal is to support watermark development by enabling automated test cases for such camcorder copy attacks, as well as to provide a benchmark for robust video watermarking. Manually creating camcorder copies is a cumbersome process, and even more problematic, it is hardly reproducible with the same setup.   By re-sampling each video frame, we simulate the typical artifacts of a camcorder copy: geometric modifications (aspect ratio changes, cropping, perspective and lens distortion), temporal modifications (unsynchronized frame rates and the resulting frame blending), sub-sampling (rescaling, filtering, Bayer color array filter), and histogram changes (AGC, AWB). We also support simulating camera movement (e.g., a hand-held camera) and background insertion."
1970909,15226,10994,Two-Image perspective photometric stereo using shape-from-shading,2012,"Shape-from-Shading and photometric stereo are two fundamental problems in Computer Vision aimed at reconstructing surface depth given either a single image taken under a known light source or multiple images taken under different illuminations, respectively. Whereas the former utilizes partial differential equation (PDE) techniques to solve the image irradiance equation, the latter can be expressed as a linear system of equations in surface derivatives when 3 or more images are given. It therefore seems that current photometric stereo techniques do not extract all possible depth information from each image by itself. This paper utilizes PDE techniques for the solution of the combined Shape-from-Shading and photometric stereo problem when only 2 images are available. Extending our previous results on this problem, we consider the more realistic perspective projection of surfaces during the photographic process. Under these assumptions, there is a unique weak (Lipschitz continuous) solution to the problem at hand, solving the well known convex/concave ambiguity of the Shape-from-Shading problem. We propose two approximation schemes for the numerical solution of this problem, an up-wind finite difference scheme and a Semi-Lagrangian scheme, and analyze their properties. We show that both schemes converge linearly and accurately reconstruct the original surfaces. In comparison with a similar method for the orthographic 2-image photometric stereo, the proposed perspective one outperforms the orthographic one. We also demonstrate the method on real-life images. Our results thus show that using methodologies common in the field of Shape-from-Shading it is possible to recover more depth information for the photometric stereo problem under the more realistic perspective projection assumption."
1747730,15226,9078,Accurate modeling of tagged CMR 3D image appearance characteristics to improve cardiac cycle strain estimation,2012,"To reduce noise within a tag line, unsharpen the tag edges in spatial domain, and amplify the tag-to-background contrast, a 3D energy minimization framework for the enhancement of tagged Cardiac Magnetic Resonance (CMR) image sequences, based on learning first- and second-order visual appearance models, is proposed. The first-order appearance modeling uses adaptive Linear Combinations of Discrete Gaussians (LCDG) to accurately approximate the empirical marginal probability distribution of CMR signals for a given sequence, and separates tag and background submodels. It is also used to classify the tag lines and the background. The second-order model considers image sequences as samples of a translation- and rotation-invariant 3D Markov-Gibbs Random Field (MGRF) with multiple pairwise voxel interactions. A 3D energy function for this model is built by using the analytical estimation of the spatio-temporal geometry and Gibbs potentials of interaction. To improve the strain estimation, by enhancing the tag and background homogeneity and contrast, the given sequence is adjusted using comparisons to the energy minimizer. Special 3D geometric phantoms, motivated by statistical analysis of the tagged CMR data, have been designed to validate the accuracy of our approach. Experiments with the phantoms and eight real data sets have confirmed the high accuracy of the functional parameters that are estimated for the enhanced tagged sequences when using popular spectral techniques, such as spectral Harmonic Phase (HARP)."
1394402,15226,23712,In-Network Compute Extensions for Rate-Adaptive Content Delivery in Mobile Networks,2014,"Traffic from mobile wireless networks has been growing at a fast pace in recent years and is expected to surpass wired traffic very soon. Service providers face significant challenges at such scales including providing seamless mobility, efficient data delivery, security, and provisioning capacity at the wireless edge. In the Mobility First project, we have been exploring clean slate enhancements to the network protocols that can inherently provide support for at-scale mobility and trustworthiness in the Internet. An extensible data plane using pluggable compute-layer services is a key component of this architecture. We believe these extensions can be used to implement in-network services to enhance mobile end-user experience by either off-loading work and/or traffic from mobile devices, or by enabling en-route service-adaptation through context-awareness (e.g., Knowing contemporary access bandwidth). In this work we present details of the architectural support for in-network services within Mobility First, and propose protocol and service-API extensions to flexibly address these pluggable services from end-points. As a demonstrative example, we implement an in network service that does rate adaptation when delivering video streams to mobile devices that experience variable connection quality. We present details of our deployment and evaluation of the non-IP protocols along with compute-layer extensions on the GENI test bed, where we used a set of programmable nodes across 7 distributed sites to configure a Mobility First network with hosts, routers, and in-network compute services."
2178177,15226,9616,Decision Level Fusion of Domain Specific Regions for Facial Action Recognition,2014,"In this paper we propose a new method for the detection of action units that relies on a novel region-based face representation and a mid-level decision layer that combines region-specific information. Different from other approaches, we do not represent the face as a regular grid based on the face location alone (holistic representation), nor by using small patches centred at iducial facial point locations (local representation). Instead, we propose to use domain knowledge regarding AU-specific facial muscle contractions to define a set of face regions covering the whole face. Therefore, as opposed to local appearance models, our face representation makes use of the full facial appearance, while the use of facial point locations to define the regions means that we obtain better-registered descriptors compared to holistic representations. Finally, we propose an AU-specific weighted sum model is used as a decision-level fusion layer in charge of combining region-specific probabilistic information. This configuration allows each classier to learning the typical appearance changes for a specific face part and reduces the dimensionality of the problem thus proving to be more robust. Our approach is evaluated on the DISFA and GEMEP-FERA datasets using two histogram-based appearance features, Local Binary Pattern and Local Phase Quantisation. We show superior performance for both the domain-specific region definition and the decision-level fusion respect to the standard approaches when it comes to automatic facial action unit detection."
898410,15226,22279,AVSS 2011 demo session: A systems level approach to perimeter protection,2011,"Summary form only given. The rapid evolution of tools and software systems to design experiments, automatically monitor, collect and warehouse large amounts of data, from applications such as life sciences and industrial processes has resulted in a new paradigm shift. This change of paradigm is so fast that some of the practices for optimization and management of these processes that were valid only 5–10 years ago may no longer be fully acceptable or sufficient for today's business optimization and management. This has a direct influence on the best practices for knowledge discovery and management of the discovered knowledge in real-world data mining applications. Establishing and managing a real-world data mining project in any domain, in particular in today's life science industry, is not a trivial task. A few approaches have been proposed in the literature. However, initiation and successful management of such efforts may depend on where a given case study fits in the overall classification of data mining approaches. Today's knowledge discovery from data can be classified in several ways: (i) data mining on engineered systems (e.g. complex equipment) or systems designed by nature (e.g. life sciences), (ii) explanatory or predictive data mining, (iii) data mining from static data (e.g. data warehouse) or dynamic data (e.g. data streams), (iv) user operated or automated data mining. There could still be other ways to classify data mining applications. This talk provides an overview of the above listed knowledge discovery applications. We provide examples where we demonstrate how small or large amounts of data, when understood from a real-world data mining point of view and the required data is properly integrated, can result in novel knowledge discovery case studies. We explain motivations and challenges of establishing real-world dat"
1540421,15226,9099,Scalar quantization for large scale image search,2012,"Bag-of-Words (BoW) model based on SIFT has been widely used in large scale image retrieval applications. Feature quantization plays a crucial role in BoW model, which generates visual words from the high dimensional SIFT features, so as to adapt to the inverted file structure for indexing. Traditional feature quantization approaches suffer several problems: 1) high computational cost---visual words generation (codebook construction) is time consuming especially with large amount of features; 2) limited reliability---different collections of images may produce totally different codebooks and quantization error is hard to be controlled; 3) update inefficiency--once the codebook is constructed, it is not easy to be updated. In this paper, a novel feature quantization algorithm,  scalar quantization , is proposed. With scalar quantization, a SIFT feature is quantized to a descriptive and discriminative bit-vector, of which the first tens of bits are taken out as  code word . Our quantizer is independent of collections of images. In addition, the result of scalar quantization naturally lends itself to adapt to the classic inverted file structure for image indexing. Moreover, the quantization error can be flexibly reduced and controlled by efficiently enumerating nearest neighbors of code words.   The performance of scalar quantization has been evaluated in partial-duplicate Web image search on a database of one million images. Experiments reveal that the proposed scalar quantization achieves a relatively 42% improvement in mean average precision over the baseline (hierarchical visual vocabulary tree approach), and also outperforms the state-of-the-art Hamming Embedding approach and soft assignment method."
2082799,15226,30,"Atherosclerotic Plaque Ultrasound Video Encoding, Wireless Transmission, and Quality Assessment Using H.264",2011,"We propose a unifying framework for efficient encoding, transmission, and quality assessment of atherosclerotic plaque ultrasound video. The approach is based on a spatially varying encoding scheme, where video-slice quantization parameters are varied as a function of diagnostic significance. Video slices are automatically set based on a segmentation algorithm. They are then encoded using a modified version of H.264/AVC flexible macroblock ordering (FMO) technique that allows variable quality slice encoding and redundant slices (RSs) for resilience over error-prone transmission channels. We evaluate our scheme on a representative collection of ten ultrasound videos of the carotid artery for packet loss rates up to 30%. Extensive simulations incorporating three FMO encoding methods, different quantization parameters, and different packet loss scenarios are investigated. Quality assessment is based on a new clinical rating system that provides independent evaluations of the different parts of the video (subjective). We also use objective video-quality assessment metrics and estimate their correlation to the clinical quality assessment of plaque type. We find that some objective quality assessment measures computed over the plaque video slices gave very good correlations to mean opinion scores (MOSs). Here, MOSs were computed using two medical experts. Experimental results show that the proposed method achieves enhanced performance in noisy environments, while at the same time achieving significant bandwidth demands reductions, providing transmission over 3G (and beyond) wireless networks."
1636400,15226,11491,Automatic Detection of CSA Media by Multi-modal Feature Fusion for Law Enforcement Support,2014,"The growing amounts of multimedia data being made available and shared via the Internet pose an increasing problem for law enforcement to investigate the distribution and possession of child sexual abuse (CSA) media. In this paper we address the automatic detection of CSA material in image and video data by multi-modal feature description. Instead of analyzing hash sums or file names, we propose the content-based analysis on visual and, in case of videos, also audio features. To this end, we apply multiple low level features as well as SentiBank, a novel mid-level representation of visual content. In collaboration with police partners and European cyber crime units, we conducted experiments on several datasets, including real world CSA media. Our quantitative evaluation reveals the challenging nature of child pornography detection, especially in the joint presence of non-illegal pornographic data, rendering skin detection, a popular feature for detecting pornography, less discriminative. Further, the utilization of SentiBank features shows high potential for detection and explainability of such content. Overall, multi-modal feature fusion can achieve an improved detection accuracy, reducing equal error rate from 17% to 10% for images and from 16% to 8% for videos as compared to best single feature performance for the challenging task of classifying CSA content from adult media."
784337,15226,11470,Human Gesture Analysis Using Multimodal Features,2012,"Human gesture as a natural interface plays an utmost important role for achieving intelligent Human Computer Interaction (HCI). Human gestures include different components of visual actions such as motion of hands, facial expression, and torso, to convey meaning. So far, in the field of gesture recognition, most previous works have focused on the manual component of gestures. In this paper, we present an appearance-based multimodal gesture recognition framework, which combines the different groups of features such as facial expression features and hand motion features which are extracted from image frames captured by a single web camera. We refer 12 classes of human gestures with facial expression including neutral, negative and positive meanings from American Sign Languages (ASL). We combine the features in two levels by employing two fusion strategies. At the feature level, an early feature combination can be performed by concatenating and weighting different feature groups, and PLS is used to choose the most discriminative elements by projecting the feature on a discriminative expression space. The second strategy is applied on decision level. Weighted decisions from single modalities are fused in a later stage. A condensation-based algorithm is adopted for classification. We collected a data set with three to seven recording sessions and conducted experiments with the combination techniques. Experimental results showed that facial analysis improve hand gesture recognition, decision level fusion performs better than feature level fusion."
2554826,15226,9099,Towards a comprehensive computational model foraesthetic assessment of videos,2013,"In this paper we propose a novel aesthetic model emphasizing psycho-visual statistics extracted from multiple levels in contrast to earlier approaches that rely only on descriptors suited for image recognition or based on photographic principles. At the lowest level, we determine dark-channel, sharpness and eye-sensitivity statistics over rectangular cells within a frame. At the next level, we extract Sentibank features (1,200 pre-trained visual classifiers) on a given frame, that invoke specific sentiments such as colorful clouds, smiling face etc. and collect the classifier responses as frame-level statistics. At the topmost level, we extract trajectories from video shots. Using viewer's fixation priors, the trajectories are labeled as foreground, and background/camera on which statistics are computed. Additionally, spatio-temporal local binary patterns are computed that capture texture variations in a given shot. Classifiers are trained on individual feature representations independently. On thorough evaluation of 9 different types of features, we select the best features from each level -- dark channel, affect and camera motion statistics. Next, corresponding classifier scores are integrated in a sophisticated low-rank fusion framework to improve the final prediction scores. Our approach demonstrates strong correlation with human prediction on 1,000 broadcast quality videos released by NHK as an aesthetic evaluation dataset."
753055,15226,8228,ccnSim: An highly scalable CCN simulator,2013,"Research interest about Information Centric Networking (ICN) has grown at a very fast pace over the last few years, especially after the 2009 seminal paper of Van Jacobson et al. describing a Content Centric Network (CCN) architecture. While significant research effort has been produced in terms of architectures, algorithms, and models, the scientific community currently lacks common tools and scenarios to allow a fair cross-comparison among the different proposals. The situation is particularly complex as the commonly used general-purpose simulators cannot cope with the expected system scale: thus, many proposals are currently evaluated over small and unrealistic scale, especially in terms of dominant factors like catalog and cache sizes. As such, there is need of a scalable tool under which different algorithms can be tested and compared. Over the last years, we have developed and optimized ccnSim, an highly scalable chunk-level simulator especially suitable for the analysis of caching performance of CCN network. In this paper, we briefly describe the tool, and present an extensive benchmark of its performance. To give an idea of ccnSim scalability, a common off-the-shelf PC equipped with 8GB of RAM memory is able to simulate 2-hours of a 50-nodes CCN network, where each nodes is equipped with 10GB caches, serving a 1PB catalog in about 20 min CPU time."
1430731,15226,9078,Frame structure optimization for interactive multiview video streaming with bounded network delay,2011,"Interactive multiview video streaming (IMVS) is an application that streams to a client one out of N available video views for observation, but client can periodically request switches to neighboring views as the video is played back uninterrupted in time. Previous IMVS works focused on the design of a frame structure at encoding time, trading off expected transmission rate with storage, without knowing the exact view trajectory a client may select at stream time. None of the existing IMVS schemes, however, explicitly addressed the network delay problem, and so a client will suffer a round trip time (RTT) delay for each requested view-switch. In this paper, we optimize frame structure for a bounded RTT, so that a client can switch to neighboring views as the video is played back without view-switching delay. The key idea is to send additional views likely to be requested by a client within one RTT beyond the current requested view. Each required set of contiguous views (corresponding to a given current requested single view) are pre-encoded using frames of previously transmitted set of views as predictors to lower transmission rate. Using I-, P- and distributed source coding (DSC) frames, we first formulate the structure design problem as a Lagrangian minimization for a desired bandwidth/storage tradeoff. We then develop a low-complexity greedy algorithm to automatically generate a good structure. Experimental results show that for the same storage cost, the transmission rate of the proposed structure can be 42% lower than that of I-frame-only structure, and 8% lower than that of the structure without DSC frames."
15779,15226,9004,Optimally Stabilized PET Image Denoising Using Trilateral Filtering,2014,"Low-resolution and signal-dependent noise distribution in positron emission tomography (PET) images makes denoising process an inevitable step prior to qualitative and quantitative image analysis tasks. Conventional PET de- noising methods either over-smooth small-sized structures due to resolution lim- itation or make incorrect assumptions about the noise characteristics. Therefore, clinically important quantitative information may be corrupted. To address these challenges, we introduced a novel approach to remove signal-dependent noise in the PET images where the noise distribution was considered as Poisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation (GAT) was used to stabilize varying nature of the PET noise. Other than noise stabilization, it is also desirable for the noise removal filter to preserve the boundaries of the struc- tures while smoothing the noisy regions. Indeed, it is important to avoid signifi- cant loss of quantitative information such as standard uptake value (SUV)-based metrics as well as metabolic lesion volume. To satisfy all these properties, we ex- tended bilateral filtering method into trilateral filtering through multiscaling and optimal Gaussianization process. The proposed method was tested on more than 50 PET-CT images from various patients having different cancers and achieved the superior performance compared to the widely used denoising techniques in the literature."
1311878,15226,422,Unsupervised clustering of multidimensional distributions using earth mover distance,2011,"Multidimensional distributions are often used in data mining to describe and summarize different features of large datasets. It is natural to look for distinct classes in such datasets by clustering the data. A common approach entails the use of methods like  k -means clustering. However, the  k -means method inherently relies on the Euclidean metric in the embedded space and does not account for additional topology underlying the distribution.   In this paper, we propose using Earth Mover Distance (EMD) to compare multidimensional distributions. For a  n -bin histogram, the EMD is based on a solution to the transportation problem with time complexity O(n 3  log  n ). To mitigate the high computational cost of EMD, we propose an approximation that reduces the cost to linear time. Given the large size of our dataset a fast approximation is crucial for this application.   Other notions of distances such as the information theoretic Kullback-Leibler divergence and statistical χ 2  distance, account only for the correspondence between bins with the same index, and do not use information across bins, and are sensitive to bin size. A cross-bin distance measure like EMD is not affected by binning differences and meaningfully matches the perceptual notion of nearness.   Our technique is simple, efficient and practical for clustering distributions. We demonstrate the use of EMD on a real-world application of analyzing 411,550 anonymous mobility usage patterns which are defined as distributions over a manifold. EMD allows us to represent inherent relationships in this space, and enables us to successfully cluster even sparse signatures."
1934061,15226,21106,Regionlets for Generic Object Detection,2013,"Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as region lets. A region let is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These region lets are organized in small groups with stable relative positions to delineate fine grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detection mean average precision of 41.7% on the PASCAL VOC 2007 dataset and 39.7% on the VOC 2010 for 20 object categories. It achieves 14.7% mean average precision on the Image Net dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4.7%."
1979789,15226,9616,Scale Coding Bag-of-Words for Action Recognition,2014,"Recognizing human actions in still images is a challenging problem in computer vision due to significant amount of scale, illumination and pose variation. Given the bounding box of a person both at training and test time, the task is to classify the action associated with each bounding box in an image. Most state-of-the-art methods use the bag-of-words paradigm for action recognition. The bag-of-words framework employing a dense multi-scale grid sampling strategy is the de facto standard for feature detection. This results in a scale invariant image representation where all the features at multiple-scales are binned in a single histogram. We argue that such a scale invariant strategy is sub-optimal since it ignores the multi-scale information available with each bounding box of a person. This paper investigates alternative approaches to scale coding for action recognition in still images. We encode multi-scale information explicitly in three different histograms for small, medium and large scale visual-words. Our first approach exploits multi-scale information with respect to the image size. In our second approach, we encode multi-scale information relative to the size of the bounding box of a person instance. In each approach, the multi-scale histograms are then concatenated into a single representation for action classification. We validate our approaches on the Willow dataset which contains seven action categories: interacting with computer, photography, playing music, riding bike, riding horse, running and walking. Our results clearly suggest that the proposed scale coding approaches outperform the conventional scale invariant technique. Moreover, we show that our approach obtains promising results compared to more complex state-of-the-art methods."
1780803,15226,21106,A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data,2013,"To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrix-based linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with high-dimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with k-means, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensionality reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging - a variant of merging function which allows the subtraction operation - into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level."
2332417,15226,256,Deep and Wide Multiscale Recursive Networks for Robust Image Labeling,2014,"Abstract: Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a `wide' structure with thousands of features, (2) a large field of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these Deep And Wide Multiscale Recursive (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157,464 voxels ($54^3$) to make a prediction at each image location. We present an associated open source software package that enables the simple and flexible creation of DAWMR networks."
933807,15226,21106,Structured Learning of Sum-of-Submodular Higher Order Energy Functions,2013,"Sub modular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow [19] has had significant impact in computer vision [5, 21, 28]. In this paper we address the important class of sum-of-sub modular (SoS) functions [2, 18], which can be efficiently minimized via a variant of max flow called sub modular flow [6]. SoS functions can naturally express higher order priors involving, e.g., local image patches, however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach [15, 34] and formulate the training problem in terms of quadratic programming, as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems [11] can be modified to efficiently solve the sub modular flow problem. Experimental comparisons are made against the OpenCV implementation of the Grab Cut interactive segmentation technique [28], which uses hand-tuned parameters instead of machine learning. On a standard dataset [12] our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels."
2281483,15226,9099,Towards Efficient Privacy-preserving Image Feature Extraction in Cloud Computing,2014,"As the image data produced by individuals and enterprises is rapidly increasing, Scalar Invariant Feature Transform (SIFT), as a local feature detection algorithm, has been heavily employed in various areas, including object recognition, robotic mapping, etc. In this context, there is a growing need to outsource such image computation with high complexity to cloud for its economic computing resources and on-demand ubiquitous access. However, how to protect the private image data while enabling image computation becomes a major concern. To address this fundamental challenge, we study the privacy requirements in outsourcing SIFT computation and propose SecSIFT, a high performance privacy-preserving SIFT feature detection system. In previous private image computation works, one common approach is to encrypt the private image in a public key based homomorphic scheme that enables the original processing algorithms designed for plaintext domain to be performed over ciphertext domain. In contrast to these works, our system is not restricted by the efficiency limitations of homomorphic encryption scheme. The proposed system distributes the computation procedures of SIFT to a set of independent, co-operative cloud servers, and keeps the outsourced computation procedures as simple as possible to avoid utilizing homomorphic encryption scheme. Thus, it enables implementation with practical computation and communication complexity. Extensive experimental results demonstrate that SecSIFT performs comparably to original SIFT on image benchmarks while capable of preserving the privacy in an efficient way."
1475831,15226,9099,Session management of correlated multi-stream 3D tele-immersive environments,2011,"Quality control and resource optimization are challenging problems in 3D tele-immersive (3DTI) environments due to their large scale, multi-stream dependencies and dynamic peer (viewer) behavior. Such systems are also prone to performance degradation due to undesired behavior in the event of drastic demand changes, such as view change and large-scale simultaneous viewer arrivals or departures. Therefore, it is crucial to localize undesired behavior inside the system and re-organize the streaming overlay structures accordingly. Doing this accurately for a large scale is even more challenging and it requires to capture all events effecting the data plan and control plan of the system. Moreover, to do this, we need to understand the desired behavior of the application first, which is defined by the dependency patterns of performance and configuration metadata at each participating peers. To assist that, we propose a learning framework that discovers metadata dependency patterns from the time series metadata and uses an online profiler to detect undesired behavior of the system during run-time. Such universal protocol also enables the prediction of large scale performance degradation due to irregular dependencies. Finally an adaptation is proposed that reallocates the resources and rearranges overlay structures to overcome the undesired behavior. In summary, our goal is to provide a universal session monitoring and management framework for complex multi-stream 3DTI environments to support large number of concurrent viewers. We consider the difficulty in overlay construction, collecting metadata, answering queries, learning patterns, detecting undesired behavior at the participating peers and finally overlay adaptation considering multi-stream dependencies."
2045253,15226,8502,A graph-based algorithm for multi-target tracking with occlusion,2013,"Multi-target tracking plays a key role in many computer vision applications including robotics, human-computer interaction, event recognition, etc., and has received increasing attention in past several years. Starting with an object detector is one of many approaches used by existing multi-target tracking methods to create initial short tracks called tracklets. These tracklets are then gradually grouped into longer final tracks in a heirarchical framework. Although object detectors have greatly improved in recent years, these detectors are far from perfect and can fail to detect the object of interest or identify a false positive as the desired object. Due to the presence of false positives or mis-detections from the object detector, these tracking methods can suffer from track fragmentations and identity switches. To address this problem, we formulate multi-target tracking as a min-cost flow graph problem which we call the average shortest path. This average shortest path is designed to be less biased towards the track length. In our average shortest path framework, object misdetection is treated as an occlusion and is represented by the edges between track-let nodes across non consecutive frames. We evaluate our method on the publicly available ETH dataset. Camera motion and long occlusions in a busy street scene make ETH a challenging dataset. We achieve competitive results with lower identity switches on this dataset as compared to the state of the art methods."
577016,15226,10994,Parameterized variety based view synthesis scheme for multi-view 3DTV,2012,"This paper presents a novel parameterized variety based view synthesis scheme for 3DTV and multi-view systems. We have generalized the parameterized image variety approach to image based rendering proposed in [1] to handle full perspective cameras. An algebraic geometry framework is proposed for the parameterization of the variety associated with full perspective images, by image positions of three reference scene points. A complete parameterization of the 3D scene is constructed. This allows to generate realistic novel views from arbitrary viewpoints without explicit 3D reconstruction, taking few multi-view images as input from uncalibrated cameras.#R##N##R##N#Another contribution of this paper is to provide a generalised and flexible architecture based on this variety model for multi-view 3DTV. The novelty of the architecture lies in merging this variety based approach with standard depth image based view synthesis pipeline, without explicitly obtaining sparse or dense 3D points. This integrated framework subsequently overcomes the problems associated with existing depth based representations. The key aspects of this joint framework are: 1) Synthesis of artifacts free novel views from arbitrary camera positions for wide angle viewing. 2) Generation of signal representation compatible with standard multi-view systems. 3) Extraction of reliable view dependent depth maps from arbitrary virtual viewpoints without recovering exact 3D points. 4) Intuitive interface for virtual view specification based on scene content. Experimental results on standard multi-view sequences are presented to demonstrate the effectiveness of the proposed scheme."
147029,15226,21106,An accurate method for line detection and manhattan frame estimation,2012,"We address the problem of estimating the rotation of a camera relative to the canonical frame of an urban scene, from a single image. Solutions generally rely on the so-called 'Manhattan World' assumption [1] that the major structures in the scene conform to three orthogonal principal directions. This can be expressed as a generative model in which the dense gradient map of the image is explained by a mixture of the three principal directions and a background process [2]. It has recently been shown that using sparse oriented edges rather than the dense gradient map leads to substantial gains in both accuracy and speed [3]. Here we explore whether further gains can be made by basing inference on even sparser extended lines. Standard Houghing techniques suffer from quantization errors and noise that make line extraction unreliable. Here we introduce a probabilistic line extraction technique that eliminates these problems through two innovations. First, we accurately propagate edge uncertainty from the image to the Hough map through a bivariate normal kernel that uses natural image statistics, resulting in a non-stationary 'soft-voting' technique. Second, we eliminate multiple responses to the same line by updating the Hough map dynamically as each line is extracted. We evaluate the method on a standard benchmark dataset [3], showing that the resulting line representation supports reliable estimation of the Manhattan frame, bettering the accuracy of previous edge-based methods by a factor of 2 and the gradient-based Manhattan World method by a factor of 5."
2191836,15226,23735,Practical 3-D Object detection using category and instance-level appearance models,2011,"Effective robotic interaction with household objects requires the ability to recognize both object instances and object categories. The former are often characterized by locally discriminative texture cues (e.g., instances with prominent brand names and logos), and the latter by salient global shape properties (plates, bowls, pots). We describe experiments with both types of cues, combining a template-and-deformable-parts detector to capture overall shape properties with a local feature Naive-Bayes nearest neighbor model to capture local texture properties. We base our implementation on the recently introduced Kinect sensor, which provides reliable depth estimates of indoor scenes. Depth cues provide segmentation and size constraints to our method. Depth affinity is used to modify the appearance term in a segmentation-based proposal step, and size priors are imposed on object classes to prune false positives. We address the complexity of scanning window HOG search using multi-class pruning schemes, first applying a generic object detection scheme to prune unlikely windows, and then focusing only on the most likely class per remaining window. Our method is able to handle relatively cluttered scenes involving multiple objects with varying levels of surface texture, and can efficiently employ multi-class scanning window search."
1446097,15226,390,Compressed beamforming applied to B-mode ultrasound imaging,2012,"Emerging sonography techniques often imply increasing in the number of transducer elements involved in the imaging process. Consequently, larger amounts of data must be acquired and processed by the beamformer. The significant growth in the amounts of data effects both machinery size and power consumption. Within the classical sampling framework, state of the art systems reduce processing rates by exploiting the bandpass bandwidth of the detected signals. It has been recently shown, that a much more significant sample-rate reduction may be obtained, by treating ultrasound signals within the Finite Rate of Innovation framework. These ideas follow the spirit of Xampling, which combines classic methods from sampling theory with recent developments in Compressed Sensing. Applying such low-rate sampling schemes to individual transducer elements, which detect energy reflected from biological tissues, is limited by the noisy nature of the signals. This often results in erroneous parameter extraction, bringing forward the need to enhance the SNR of the low-rate samples. In our work, we manage to achieve such SNR enhancement, by beamforming the sub-Nyquist samples obtained from multiple elements. We refer to this process as “compressed beamforming”. Applying it to cardiac ultrasound data, we successfully image macroscopic perturbations, while achieving a nearly eightfold reduction in sample-rate, compared to standard techniques."
1346245,15226,20561,"Persuasive and pervasive sensing: A new frontier to monitor, track and assist older adults suffering from type-2 diabetes",2013,"California like the entire nation is aging. There are 4.3 million Californians 65 and older accounting for about 11% of the total state population. We also find that 58% of older adults have high blood pressure; about 21% have been told that they have diabetes. California in particular has the highest incidence of new diabetes cases and nearly 4 million people (diagnosed and undiagnosed) are estimated to be suffering from the disease. Diabetes is a chronic disease, which if unchecked leads to acute and long-term complications and ultimately death. Our older adult population often lacks the cognitive resources to deal with the daily self-management regimens. Many unpaid family members are caring for them today but this is unsustainable. In this paper, we discuss the design and implementation of a wireless sensor network system within the home environment that captures activity of daily living. We mine the data and provide feedback via SMS/text (daily) and tailored newsletter (weekly). We introduce a novel idea called “persuasive sensing” and report results from two home implementations that are showing exciting promise. Moreover we show that with the help of an Artificial Neural Network, we can predict bloodglucose levels for the next day from accumulated data with an accuracy of 94%. The predictive model presented here is a break-through in at-home sensing research."
117435,15226,11052,Real-Time camera tracking: when is high frame-rate best?,2012,"Higher frame-rates promise better tracking of rapid motion, but advanced real-time vision systems rarely exceed the standard 10–60Hz range, arguing that the computation required would be too great. Actually, increasing frame-rate is mitigated by reduced computational cost per frame in trackers which take advantage of prediction. Additionally, when we consider the physics of image formation, high frame-rate implies that the upper bound on shutter time is reduced, leading to less motion blur but more noise. So, putting these factors together, how are application-dependent performance requirements of accuracy, robustness and computational cost optimised as frame-rate varies? Using 3D camera tracking as our test problem, and analysing a fundamental dense whole image alignment approach, we open up a route to a systematic investigation via the careful synthesis of photorealistic video using ray-tracing of a detailed 3D scene, experimentally obtained photometric response and noise models, and rapid camera motions. Our multi-frame-rate, multi-resolution, multi-light-level dataset is based on tens of thousands of hours of CPU rendering time. Our experiments lead to quantitative conclusions about frame-rate selection and highlight the crucial role of full consideration of physical image formation in pushing tracking performance."
717894,15226,9704,A novel partially connected cooperative parallel PSO-SVM algorithm: Study based on sleep apnea detection,2012,"Sleep disorders are common in a general population. It effect one in 5 adults and has several short term and long term bad side effects on health. Sleep apnea (SA) is the most important and common component of sleep disorders. This paper presents an automatic approach for detecting apnea events by using few bio-singles that are related to breathe defect. This work uses only air flow, thoracic and abdominal respiratory movement as input. The proposed algorithm consists of three main parts which are signal segmentation, feature generation and classification. A new proposed segmentation method intelligently segments the input signals for further classification, then features are generated for each segment by wavelet packet coefficients and also original signals. In classification phase a unique parallel PSO-SVM algorithm is investigated. PSO used to tune SVM parameters, and also data reduction. Proposed parallel structure used to help PSO to search space more efficiently, also avoiding fast convergence and local optimal results that are common problem in similar parallel algorithms. Obtained results demonstrate that the proposed method is effective and robust in sleep apnea detection and statistical tests on the results shown superiority of it versus previous methods even with more input signals, and also versus single PSO-SVM. Using fewer signals means more comfortable to subject and also, reduction of cost during recording the data."
1701671,15226,23735,Locating occupants in preschool classrooms using a multiple RGB-D sensor system,2013,"Presented are results demonstrating that, in developing a system with its first objective being the sustained detection of adults and young children as they move and interact in a normal preschool setting, the direct application of the straightforward RGB-D innovations presented here significantly outperforms even far more algorithmically advanced methods relying solely on images. The use of multiple RGB-D sensors by this project for depth-aware object localization economically resolves numerous issues regularly frustrating earlier vision-only detection and human surveillance methods, issues such as occlusions, illumination changes, unexpected postures, atypical morphologies, erratic or unanticipated motions, reflections, and misleading textures and colorations. This multiple RGB-D installation forms the front-end for a multi-step pipeline, the first portion of which seeks to isolate, in situ, 3D renderings of classroom occupants sufficient for a later analysis of their behaviors and interactions. Towards this end, a voxel-based approach to foreground/background separation and an effective adaptation of supervoxel clustering for 3D were developed, and 3D and image-only methods were tested and compared. The project's setting is highly challenging, but then so are its longer term goals: the automated detection of early childhood precursors, ofttimes very subtle, to a number of increasingly common developmental disorders."
2135946,15226,9616,A Structural Texture Approach for Characterising Malignancy Associated Changes in Pap Smears Based on Mean-Shift and the Watershed Transform,2014,"This paper presents a novel structural approach to quantitatively characterising nuclear chromatin texture in light microscope images of Pap smears. The approach is based on segmenting the chromatin into blob-like primitives and characterising their properties and arrangement. The segmentation approach makes use of multiple focal planes. It comprises two basic steps: (i) mean-shift filtering in the feature space formed by concatenating pixel spatial coordinates and intensity values centred around the best all-in-focus plane, and (ii) hierarchical marker-based watershed segmentation. The paper also presents an empirical evaluation of the approach based on the classification of 43 routine clinical Pap smears. Two variants of the approach were compared to a reference approach (employing extended depth-of-field rather than mean-shift) in a feature selection/classification experiment, involving 138 segmentation-based features, for discriminating normal and abnormal slides. The results demonstrate improved performance over the reference approach. The results of a second feature selection/classification experiment, including additional classes of features from the literature, show that a combination of the proposed structural and conventional features yields a classification performance of 0.919±0.015 (AUC ± Std. Dev.). Overall the results demonstrate the efficacy of the proposed structural approach and confirm that it is indeed possible to detect malignancy associated changes (MACs) in conventional Papanicolaou stain."
786586,15226,21106,A GPU-assisted personal video organizing system,2011,"Video data is increasing rapidly along with the capacity of storage devices owned by a lay user. Users have moderate to large personal collections of videos and would like to keep them in an organized manner based on its content. Video organizing tools for personal users are way behind even the primitive image organizing tools. We present a mechanism in this paper to help ordinary users organize their personal collection of videos based on categories they choose. We cluster the PHOG features extracted from selected key frames to form a representation for each user-selected category during the learning phase. During the organization phase, labels from a K-NN classifier on these cluster centres for each key frame are aggregated to give a label to the video while categorizing. Video processing is computationally intensive. To perform the computationally intensive steps involved, we exploit the CPU as well as the GPU that is common even on personal systems. Effective use of the parallel hardware on the system is the only way to make the tool scale reasonably to large collections that will be available soon. Our tool is able to organize a set of 100 sport videos of total duration of 1375 minutes in about 9.5 minutes. The process of learning the categories from 12 annotated videos of duration 165 minutes took 75 seconds on a GTX 580 card. These were on a standard desktop with an off-the-shelf GPU. The labeling accuracy is about 96% on all videos."
931043,15226,9099,Analysis and forecasting of trending topics in online media streams,2013,"Among the vast information available on the web, social media streams capture what people currently pay attention to and how they feel about certain topics. Awareness of such trending topics plays a crucial role in multimedia systems such as trend aware recommendation and automatic vocabulary selection for video concept detection systems. Correctly utilizing trending topics requires a better understanding of their various characteristics in different social media streams. To this end, we present the first comprehensive study across three major online and social media streams, Twitter, Google, and Wikipedia, covering thousands of trending topics during an observation period of an entire year. Our results indicate that depending on one's requirements one does not necessarily have to turn to Twitter for information about current events and that some media streams strongly emphasize content of specific categories. As our second key contribution, we further present a novel approach for the challenging task of forecasting the life cycle of trending topics in the very moment they emerge. Our fully automated approach is based on a nearest neighbor forecasting technique exploiting our assumption that semantically similar topics exhibit similar behavior.   We demonstrate on a large-scale dataset of Wikipedia page view statistics that forecasts by the proposed approach are about 9-48k views closer to the actual viewing statistics compared to baseline methods and achieve a mean average percentage error of 45-19% for time periods of up to 14 days."
1100897,15226,9748,Adaptive Pipeline Parallelism for Image Feature Extraction Algorithms,2012,"Currently, multimedia data has become one of the major data types processed and transferred on the Internet. With the rapid growth of multimedia data, it is vitally important to find an efficient way to extract useful information from a large amount of data. SIFT and SURF, as the most popular multimedia feature extraction algorithms, have been widely used in many applications. However, the limited processing speed~(about 1.8 and 2.6 images or frames per second for SIFT and SURF respectively on an ordinary CPU) makes it impossible to apply them in many real-world applications with real-time requirements. Therefore, it has become one of the major challenges that how to improve the processing speed of these multimedia feature extraction algorithms. The popularity of multi-core architecture and the increase of computation resources on different platforms provide a new opportunity to accelerate the processing speed of these image feature extraction algorithms. In this paper, we first systematically analyze the major parallel constraints in SIFT and SURF, such as imbalanced workload and indeterminate time distribution. Then, based on these analysis, we design and implement an adaptive pipeline parallel scheme~(AD-PIPE) for both SIFT and SURF to alleviate these limitations. In our scheme, we dynamically check the workload in different pipeline stages and adjust the thread number in different stages to achieve a balanced partition. Experimental results show that our approach is efficient and scalable. It can achieve a speedup of 16.88X and 20.33X respectively for SIFT and SURF on a 16-core machine and a real time processing speed with about 27 and 52 images or frames per second."
27812,15226,11052,"Automatic Segmentation of Unknown Objects, with Application to Baggage Security",2012,"Computed tomography (CT) is used widely to image patients for medical diagnosis and to scan baggage for threatening materials. Automated reading of these images can be used to reduce the costs of a human operator, extract quantitative information from the images or support the judgements of a human operator. Object quantification requires an image segmentation to make measurements about object size, material composition and morphology. Medical applications mostly require the segmentation of prespecified objects, such as specific organs or lesions, which allows the use of customized algorithms that take advantage of training data to provide orientation and anatomical context of the segmentation targets. In contrast, baggage screening requires the segmentation algorithm to provide segmentation of an unspecified number of objects with enormous variability in size, shape, appearance and spatial context. Furthermore, security systems demand 3D segmentation algorithms that can quickly and reliably detect threats. To address this problem, we present a segmentation algorithm for 3D CT images that makes no assumptions on the number of objects in the image or on the composition of these objects. The algorithm features a new Automatic QUality Measure (AQUA) model that measures the segmentation confidence for any single object (from any segmentation method) and uses this confidence measure to both control splitting and to optimize the segmentation parameters at runtime for each dataset. The algorithm is tested on 27 bags that were packed with a large variety of different objects."
1321384,15226,20515,Matching vein patterns from color images for forensic investigation,2012,"Child sexual abuse is a serious global problem and has gained public attention in recent years. Due to the popularity of digital cameras, many perpetrators take images of their sexual activities with child victims. Traditionally, it was difficult to use cutaneous vascular patterns for forensic identification, because they were nearly invisible in color images. Recently, this limitation was overcome using a computational method based on an optical model to uncover vein patterns from color images for forensic verification. This optical-based vein uncovering (OBVU) method is sensitive to the power of the illuminant and does not utilize skin color in images to obtain training parameters to optimize the vein uncovering performance. Prior publications have not included an automatic vein matching algorithm for forensic identification. As a result, the OBVU method only supported manual verification. In this paper, we propose two new schemes to overcome limitations in the OBVU method. Specifically, a color optimization scheme is used to derive the range of biophysical parameters to obtain training parameters and an automatic intensity adjustment scheme is used to enhance the robustness of the vein uncovering algorithm. We also developed an automatic matching algorithm for vein identification. This algorithm can handle rigid and non-rigid deformations and has an explicit pruning function to remove outliers in vein patterns. The proposed algorithms were examined on a database with 300 pairs of color and near infrared (NIR) images collected from the forearms of 150 subjects. The experimental results are encouraging and indicate that the proposed vein uncovering algorithm performs better than the OBVU method and that the uncovered patterns can potentially be used for automatic criminal and victim identification."
132940,15226,9004,Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe): Distinguishing Tumor Confounders and Molecular Subtypes on MRI,2014,"We introduce a novel biologically inspired feature descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe), that captures higher order co-occurrence patterns of local gradient ten- sors at a pixel level to distinguish disease phenotypes that have sim- ilar morphologic appearances. A number of pathologies (e.g. subtypes of breast cancer) have different histologic phenotypes but similar radio- graphic appearances. While texture features have been previously em- ployed for distinguishing subtly different pathologies, they attempt to capture differences in global intensity patterns. In this paper we attempt to model CoLlAGe to identify higher order co-occurrence patterns of gra- dient tensors at a pixel level. The assumption behind this new feature is that different pathologies, even though they may have very similar overall texture and appearance on imaging, at a local scale, will have different co-occurring patterns with respect to gradient orientations. We demonstrate the utility of CoLlAGe in distinguishing two subtly different types of pathologies on MRI in the context of brain tumors and breast cancer. In the first problem, we look at CoLlAGe for distinguishing ra- diation effects from recurrent brain tumors over a cohort of 40 studies, and in the second, discriminating different molecular subtypes of breast cancer over a cohort of 73 studies. For both these challenging cohorts, CoLlAGe was found to have significantly improved classification perfor- mance, as compared to the traditional texture features such as Haralick, Gabor, local binary patterns, and histogram of gradients."
792730,15226,8228,Unified distributed source coding frames for interactive multiview video streaming,2012,Because of differential coding used in standard video compression algorithms to exploit temporal correlation in adjacent frames for coding gain a frame lost in network will cause error propagation in subsequent frames at the decoder Previously proposed distributed source coding (DSC) frames can be periodically inserted to halt this error propagation by overcoming the uncertainty at encoder of which frames will be correctly received at decoder without resorting to large intra-coded I-frames In the case of interactive multiview video streaming (IMVS) where a user watches one of M available captured views at a time but can periodically select and switch to a neighboring view the encoder must encode multiview video to enable this view-switching interactivity without knowing the exact view trajectories taken by viewers at stream time In this paper we propose a unified DSC frame construction for IMVS so that the encoder can overcome both types of uncertainty in a coding-efficient manner; ie halt error propagation in differentially coded multiview video and facilitate periodic interactive view-switching at the same time Having the additional unified DSC frames we design a multiview frame structure to maximize the expected number of correctly decoded frames at decoder for a given bandwidth constraint We develop a fast algorithm to find locally optimal structure parameters and packetization and packet reordering strategies for transmission Experimental results show that our optimized frame structures using unified DSC frames outperform naive structures using I- and P-frames only by up to 49% in fraction of correctly decoded frames under typical network condition
2071604,15226,21106,Monocular visual odometry and dense 3d reconstruction for on-road vehicles,2012,"More and more on-road vehicles are equipped with cameras each day. This paper presents a novel method for estimating the relative motion of a vehicle from a sequence of images obtained using a single vehicle-mounted camera. Recently, several researchers in robotics and computer vision have studied the performance of motion estimation algorithms under non-holonomic constraints and planarity. The successful algorithms typically use the smallest number of feature correspondences with respect to the motion model. It has been strongly established that such minimal algorithms are efficient and robust to outliers when used in a hypothesize-and-test framework such as random sample consensus (RANSAC). In this paper, we show that the planar 2-point motion estimation can be solved analytically using a single quadratic equation, without the need of iterative techniques such as Newton-Raphson method used in existing work. Non-iterative methods are more efficient and do not suffer from local minima problems. Although 2-point motion estimation generates visually accurate on-road vehicle trajectory, the motion is not precise enough to perform dense 3D reconstruction due to the non-planarity of roads. Thus we use a 2-point relative motion algorithm for the initial images followed by 3-point 2D-to-3D camera pose estimation for the subsequent images. Using this hybrid approach, we generate accurate motion estimates for a plane-sweeping algorithm that produces dense depth maps for obstacle detection applications."
1527043,15226,422,Crowdsourced time-sync video tagging using temporal and personalized topic modeling,2014,"Time-sync video tagging aims to automatically generate tags for each video shot. It can improve the user's experience in previewing a video's timeline structure compared to traditional schemes that tag an entire video clip. In this paper, we propose a new application which extracts time-sync video tags by automatically exploiting crowdsourced comments from video websites such as Nico Nico Douga, where videos are commented on by online crowd users in a time-sync manner. The challenge of the proposed application is that users with bias interact with one another frequently and bring noise into the data, while the comments are too sparse to compensate for the noise. Previous techniques are unable to handle this task well as they consider video semantics independently, which may overfit the sparse comments in each shot and thus fail to provide accurate modeling. To resolve these issues, we propose a novel temporal and personalized topic model that jointly considers temporal dependencies between video semantics, users' interaction in commenting, and users' preferences as prior knowledge. Our proposed model shares knowledge across video shots via users to enrich the short comments, and peels off user interaction and user bias to solve the noisy-comment problem. Log-likelihood analyses and user studies on large datasets show that the proposed model outperforms several state-of-the-art baselines in video tagging quality. Case studies also demonstrate our model's capability of extracting tags from the crowdsourced short and noisy comments."
1076512,15226,390,Image segmentation with implicit color standardization using cascaded EM: Detection of myelodysplastic syndromes,2012,"Color nonstandardness — the propensity for similar objects to exhibit different color properties across images — poses a significant problem in the computerized analysis of histopathology. Though many papers propose means for improving color constancy, the vast majority assume image formation via reflective light instead of light transmission as in microscopy, and thus are inappropriate for histological analysis. In this work, we present a novel Bayesian color segmentation algorithm for histological images that is highly robust to color nonstandardness; this algorithm employs a unique instantiation of the expectation maximization (EM) algorithm to dynamically estimate — for each individual image — the probability density functions (mixtures of gamma and von Mises distributions) that describe the colors of salient objects. To validate our segmentation scheme, we employ it as part of a computerized system to detect myelodysplastic syndromes (MDS) on bone marrow specimens. Qualitative anecdotal evidence suggests that biopsies of MDS exhibit abnormalities in the arrangement of erythroid precursors (immature red blood cells). Herein, we confirm and quantify this phenomenon, using it to discriminate MDS from normal tissue: over a dataset of 53 representative regions selected from 18 patients, our classification system correctly discriminates MDS from normal tissue with an accuracy of 85% and an area under the receiver operator characteristic curve of 0.8803."
1141685,15226,9773,A Comprehensive Representation Model for Handwriting Dedicated to Word Spotting,2013,"In this paper, we propose an original representation model for handwriting document images. Most state-of-the-art handwriting representation models only use separately textural properties, selective dominant features (such as stroke orientation or gradient orientation) or structural properties. To avoid the drawbacks of using the properties from a single aspect, we design a comprehensive model that contains both morphological and topological information of handwriting. After interest points (the starting/ending points, branch points and high-curved points) are selected, an adapted version of Shape Context (SC) descriptor built on the interest points is employed to describe the contour of the text. In order to model the structural characteristics of the handwritten text, a graph is constructed based on the interest points and the skeleton of the text. With the graph, loops and specific strokes in the handwriting are detected and analyzed. Based on this model, a coarse-to-fine approach for word spotting application is introduced. Without segmenting texts into words, a group of regions of interest are selected by comparing textural features (orientation, projection profile, upper and lower border projection) using the DTW method. Afterwards, regions of interest and queries are represented by the proposed model. The final similarity measure is a weighted mixture of the SC cost, loop difference, stroke analysis and texture comparison with different weights. The validation of the model shows the significance of combining the various properties of the handwriting envisaged in its different aspects."
161757,15226,9004,Robust large scale prone-supine polyp matching using local features: a metric learning approach,2011,"Computer aided detection (CAD) systems have emerged as noninvasive and effective tools, using 3D CT Colonography (CTC) for early detection of colonic polyps. In this paper, we propose a robust and automatic polyp prone-supine view matching method, to facilitate the regular CTC workflow where radiologists need to manually match the CAD findings in prone and supine CT scans for validation. Apart from previous colon registration approaches based on global geometric information [1-4], this paper presents a feature selection and metric distance learning approach to build a pairwise matching function (where true pairs of polyp detections have smaller distances than false pairs), learned using local polyp classification features [5-7]. Thus our process can seamlessly handle collapsed colon segments or other severe structural artifacts which often exist in CTC, since only local features are used, whereas other global geometry dependent methods may become invalid for collapsed segmentation cases. Our automatic approach is extensively evaluated using a large multi-site dataset of 195 patient cases in training and 223 cases for testing. No external examination on the correctness of colon segmentation topology [2] is needed. The results show that we achieve significantly superior matching accuracy than previous methods [1-4], on at least one order-of-magnitude larger CTC datasets."
101957,15226,11052,Recursive bilateral filtering,2012,"This paper proposes a recursive implementation of the bilateral filter. Unlike previous methods, this implementation yields an bilateral filter whose computational complexity is linear in both input size and dimensionality. The proposed implementation demonstrates that the bilateral filter can be as efficient as the recent edge-preserving filtering methods, especially for high-dimensional images. Let the number of pixels contained in the image be N, and the number of channels be D, the computational complexity of the proposed implementation will be O(ND). It is more efficient than the state-of-the-art bilateral filtering methods that have a computational complexity of O(ND2) [1] (linear in the image size but polynomial in dimensionality) or O(Nlog(N)D) [2] (linear in the dimensionality thus faster than [1] for high-dimensional filtering). Specifically, the proposed implementation takes about 43 ms to process a one megapixel color image (and about 14 ms to process a 1 megapixel grayscale image) which is about 18 × faster than [1] and 86× faster than [2]. The experiments were conducted on a MacBook Air laptop computer with a 1.8 GHz Intel Core i7 CPU and 4 GB memory. The memory complexity of the proposed implementation is also low: as few as the image memory will be required (memory for the images before and after filtering is excluded). This paper also derives a new filter named gradient domain bilateral filter from the proposed recursive implementation. Unlike the bilateral filter, it performs bilateral filtering on the gradient domain. It can be used for edge-preserving filtering but avoids sharp edges that are observed to cause visible artifacts in some computer graphics tasks. The proposed implementations were proved to be effective for a number of computer vision and computer graphics applications, including stylization, tone mapping, detail enhancement and stereo matching."
1304494,15226,23735,Full scaled 3D visual odometry from a single wearable omnidirectional camera,2012,"In the last years monocular SLAM has been widely used to obtain highly accurate maps and trajectory estimations of a moving camera. However, one of the issues of this approach is that, due to the impossibility of the depth being measured in a single image, global scale is not observable and scene and camera motion can only be recovered up to scale. This problem gets aggravated as we deal with larger scenes since it is more likely that scale drift arises between different map portions and their corresponding motion estimates. To compute the absolute scale we need to know some kind of dimension of the scene (e.g., actual size of an element of the scene, velocity of the camera or baseline between two frames) and somehow integrate it in the SLAM estimation. In this paper, we present a method to recover the scale of the scene using an omnidirectional camera mounted on a helmet. The high precision of visual SLAM allows the head vertical oscillation during walking to be perceived in the trajectory estimation. By performing a spectral analysis on the camera vertical displacement, we can measure the step frequency. We relate the step frequency to the speed of the camera by an empirical formula based on biomedical experiments on human walking. This speed measurement is integrated in a particle filter to estimate the current scale factor and the 3D motion estimation with its true scale. We evaluated our approach using image sequences acquired while a person walks. Our experiments show that the proposed approach is able to cope with scale drift."
1654497,15226,390,Improving full-cardiac cycle strain estimation from tagged CMR by accurate modeling of 3D image appearance characteristics,2012,"To reduce noise within a tag line, unsharpen tag edges in the spatial domain, and amplify the tag-to-background contrast, a 3D energy minimization framework for the enhancement of tagged Cardiac Magnetic Resonance (CMR) images, that is based on first- and second-order learned visual appearance models, is proposed. The first-order appearance modeling uses an adaptive Linear Combination of Discrete Gaussians (LCDG) to accurately approximate the empirical marginal probability distribution of CMR signals for a given sequence, and to separate the tag and background submodels. It is also used to classify the tag lines and the background. The second-order model considers image sequences as samples of a translation- and rotation-invariant 3D Markov-Gibbs Random Field (MGRF), with multiple pairwise voxel interactions. A 3D energy function for this model is built by using the analytical estimation of the spatiotemporal geometry and the Gibbs potentials of interaction. To improve the strain estimation, through enhancement of the tag and background homogeneity and contrast, the given sequence is adjusted using comparisons to the energy minimizer. Special 3D geometric phantoms, motivated by the statistical analysis of the tagged CMR data, have been designed to validate the accuracy of our approach. Experiments with the phantoms and eight in-vivo data sets have confirmed the high accuracy of functional parameter estimation for the enhanced CMR images when using popular spectral techniques, such as spectral Harmonic Phase (HARP)."
772329,15226,9099,Automatic tag generation and ranking for sensor-rich outdoor videos,2011,"Video tag annotations have become a useful and powerful feature to facilitate video search in many social media and web applications. The majority of tags assigned to videos are supplied by users - a task which is time consuming and may result in annotations that are subjective and lack precision. A number of studies have utilized content-based extraction techniques to automate tag generation. However, these methods are compute-intensive and challenging to apply across domains. Here, we describe a complementary approach for generating tags based on the geographic properties of videos. With today's sensor-equipped smartphones, the location and orientation of a camera can be continuously acquired in conjunction with the captured video stream. Our novel technique utilizes these sensor meta-data to automatically tag outdoor videos in a two step process. First, we model the viewable scenes of the video as geometric shapes by means of its accompanied sensor data and determine the geographic objects that are visible in the video by querying geo-information databases through the viewable scene descriptions. Subsequently we extract textual information about the visible objects to serve as tags. Second, we define six criteria to score the tag relevance and rank the obtained tags based on these scores. Then we associate the tags with the video and the accurately delimited segments of the video. To evaluate the proposed technique we implemented a prototype tag generator and conducted a user study. The results demonstrate significant benefits of our method in terms of automation and tag utility."
2180697,15226,9099,"repoVizz: a framework for remote storage, browsing, annotation, and exchange of multi-modal data",2013,"In this technical demo we present repoVizz (http://repovizz.upf.edu), an integrated online system capable of structural formatting and remote storage, browsing, exchange, annotation, and visualization of synchronous multi-modal, time-aligned data. Motivated by a growing need for data-driven collaborative research, repoVizz aims to resolve commonly encountered difficulties in sharing or browsing large collections of multi-modal data. At its current state, repoVizz is designed to hold time-aligned streams of heterogeneous data: audio, video, motion capture, physiological signals, extracted descriptors, annotations, et cetera. Most popular formats for audio and video are supported, while Broadcast WAVE or CSV formats are adopted for streams other than audio or video (e.g., motion capture or physiological signals). The data itself is structured via customized XML files, allowing the user to (re-) organize multi-modal data in any hierarchical manner, as the XML structure only holds metadata and pointers to data files. Datasets are stored in an online database, allowing the user to interact with the data remotely through a powerful HTML5 visual interface accessible from any standard web browser; this feature can be considered a key aspect of repoVizz since data can be explored, annotated, or visualized from any location or device. Data exchange and upload/download is made easy and secure via a number of data conversion tools and a user/permission management system."
1675348,15226,22279,Online Learning of Activities from Video,2012,"The present work introduces a new method for activity extraction from video. To achieve this, we focus on the modelling of context by developing an algorithm that automatically learns the main activity zones of the observed scene by taking as input the trajectories of detected mobiles. Automatically learning the context of the scene (activity zones) allows first to extract a knowledge on the occupancy of the different areas of the scene. In a second step, learned zones are employed to extract people activities by relating mobile trajectories to the learned zones, in this way, the activity of a person can be summarised as the series of zones that the person has visited. For the analysis of the trajectory, a multiresolution analysis is set such that a trajectory is segmented into a series of tracklets based on changing speed points thus allowing differentiating when people stop to interact with elements of the scene or other persons. Tracklets allow thus to extract behavioural information. Starting and ending tracklet points are fed to a simple yet advantageous incremental clustering algorithm to create an initial partition of the scene. Similarity relations between resulting clusters are modeled employing fuzzy relations. These can then be aggregated with typical soft-computing algebra. A clustering algorithm based on the transitive closure calculation of the fuzzy relations allows building the final structure of the scene. To allow for incremental learning and update of activity zones (and thus people activities), fuzzy relations are defined with online learning terms. We present results obtained on real videos from different activity domains."
638044,15226,11052,From Low-Cost Depth Sensors to CAD: Cross-Domain 3D Shape Retrieval via Regression Tree Fields,2014,"The recent advances of low-cost and mobile depth sensors dramatically extend the potential of 3D shape retrieval and analysis. While the traditional research of 3D retrieval mainly focused on search- ing by a rough 2D sketch or with a high-quality CAD model, we tackle a novel and challenging problem of cross-domain 3D shape retrieval, in which users can use 3D scans from low-cost depth sensors like Kinect as queries to search CAD models in the database. To cope with the imper- fection of user-captured models such as model noise and occlusion, we propose a cross-domain shape retrieval framework, which minimizes the potential function of a Conditional Random Field to efficiently gener- ate the retrieval scores. In particular, the potential function consists of two critical components: one unary potential term provides robust cross- domain partial matching and the other pairwise potential term embeds spatial structures to alleviate the instability from model noise. Both po- tential components are efficiently estimated using random forests with 3D local features, forming a Regression Tree Field framework. We con- duct extensive experiments on two recently released user-captured 3D shape datasets and compare with several state-of-the-art approaches on the cross-domain shape retrieval task. The experimental results demon- strate that our proposed method outperforms the competing methods with a significant performance gain."
994734,15226,23712,Analysis of adaptive streaming for hybrid CDN/P2P live video systems,2011,"Most commercial video streaming systems rely on Content Distribution Networks (CDNs) to distribute video content. HTTP adaptive streaming has been recently adopted by major video streaming providers and is now considered the standard technique used with CDN-based streaming systems. Despite the success of these systems, cost-effective scalability continues to be of concern in their design and deployment. To address this, recent work has proposed the use of hybrid CDN and Peer-to-peer (P2P) live streaming systems. The design of these systems aims to combine the scalability of P2P systems and the desirable performance properties of CDN-based systems. However, the use of adaptive streaming, has not been explored extensively in such hybrid systems. Designing and operating an adaptive hybrid streaming system is very challenging. Two design decisions are very critical in the operation of any such system. The first one is the bitrate adaptation strategy which specifies how different bitrates are assigned to different users while maximizing user satisfaction. The second is defining the operational guidelines for switching the system between the CDN and the P2P modes while efficiently utilizing the available resources. In this paper we present a model and analysis of a hybrid CDN-P2P adaptive live streaming system with the objective of answering these two design questions. We first present a stochastic fluid model to the hybrid streaming system with a single video bitrate and we obtain theoretical results to guide the system operation as described above. We then extend the analysis to the adaptive streaming case with multiple video bitrates. We model adaptive streaming as a linear optimization problem to obtain the best bitrate adaptation strategy. We validate our analysis using simulations. Our conclusion is that adaptive hybrid streaming can significantly improve the ability of the system to satisfy more users with higher video bitrates over CDN-based systems."
1530894,15226,9616,4D Space-Time Mereotopogeometry-Part Connectivity Calculus for Visual Object Representation,2014,"Region Connectivity Calculus (RCC) can be used to define the formal grammar describing the relationship between image regions. While RCC provides a possible framework for representation of object part constellations leading to object recognition, little has been done in the direction of RCC for 3D images. Almost all prior RCC representations, such as RCC5/ RCC8/ RCC23/RCC62 are oriented towards 2D projections. While the recently introduced RCC-3D does address this limitation to a certain extent, it heavily depends on other 2D RCC frameworks and as such is limited--it provides no representation for orthogonally aligned or staggered object parts. It also does not provide convenient representations for shape related pose information (such as horizontally aligned along principal axis or vertically aligned etc.). While it is possible to use oriented matroids for projected 3D representations using cocircuits and chirotopes, these are again sub-optimal given that there is considerable information loss (through dimensionality reduction) and multiple object models map to the same structures. In this paper, we introduce a new hierarchical graph based 3D Region/ Surface/ Object/ Part Connectivity Calculus (OCC/PCC), given the domain of affordance based equivalence recognition. We call our PCC--4D Space Time Mereo-topo-geometry (4D-STMTG) and the equivalent OCC as 4D Space Time Joint Topo-geometry (4D-STJTG). The modeling of simple objects using the calculus is demonstrated in practical scenarios. Comparisons of the proposed calculus with respect to the state-of-art RCC-3D is also presented, demonstrating the flexibility, suitability and superiority of 4D-STMTG."
976836,15226,9099,Enabling low bitrate mobile visual recognition: a performance versus bandwidth evaluation,2013,"The rapid development of technologies in both hardware and software have made content-based multimedia services feasible on mobile devices such as smartphones and tablets; and the strong needs for mobile visual search and recognition have been emerging. While many real applications of visual recognition require a large scale recognition systems, the same technologies that support server-based scalable visual recognition may not be feasible on mobile devices due to the resource constraints. Although the client-server framework ensures the scalability, the real-time response subjects to the limitation on network bandwidth. Therefore, the main challenge for mobile visual recognition system should be the recognition bitrate, which is the amount of data transmission under the same recognition performance. For this work, we exploit and compare various strategies such as compact features, feature compression, feature signatures by hashing, image scaling, etc., to enable low bitrate mobile visual recognition. We argue that thumbnail image is a competitive candidate for low bitrate visual recognition because it carries multiple features at once and multi-feature fusion is important as the size of semantic space increases. Our evaluations on two subsets of ImageNet, both contain more than 10,000 images with 19 and 137 categories, verify the efficacy of thumbnail images. We further suggest a new strategy that combines single (local) feature signature and the thumbnail image, which achieves significant bitrate reduction from (average) 102,570 to 4,661 bytes with merely (overall) 10% performance degradation."
2061243,15226,11321,Active Detection via Adaptive Submodularity,2014,"Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision. In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives."
828371,15226,22130,Segmentation of Dynamic Scenes with Distributions of Spatiotemporally Oriented Energies.,2014,"In video segmentation, disambiguating appearance cues by grouping similar motions or dynamics is potentially powerful, though non-trivial. Dynamic changes of appearance can occur from rigid or non-rigid motion, as well as complex dynamic textures. While the former are easily captured by optical flow, phenomena such as a dissipating cloud of smoke, or flickering reflections on water, do not satisfy the assumption of brightness constancy, or cannot be modelled with rigid displacements in the image. To tackle this problem, we propose a robust representation of image dynamics as histograms of motion energy (HoME) obtained from convolutions of the video with spatiotemporal filters. They capture a wide range of dynamics and handle problems previously studied separately (motion and dynamic texture segmentation). They thus offer a potential solution for a new class of problems that contain these effects in the same scene. Our representation of image dynamics is integrated in a graph-based segmentation framework and combined with colour histograms to represent the appearance of regions. In the case of translating and occluding segments, the proposed features additionally serve to characterize the motion of the boundary between pairs of segments, to identify the occluder and inferring a local depth ordering. The resulting segmentation method is completely modelfree and unsupervised, and achieves state-of-the-art results on the SynthDB dataset for dynamic texture segmentation, on the MIT dataset for motion segmentation, and reasonable performance on the CMU dataset for occlusion boundaries."
1923333,15226,9004,Robust bone detection in ultrasound using combined strain imaging and envelope signal power detection,2014,"Bone localization in ultrasound (US) remains challenging de- spite encouraging advances. Current methods, e.g. local image phase- based feature analysis, showed promising results but remain reliant on delicate parameter selection processes and prone to errors at confounding soft tissue interfaces of similar appearance to bone interfaces. We propose a different approach combining US strain imaging and envelope power detection at each radio-frequency (RF) sample. After initial estimation of strain and envelope power maps, we modify their dynamic ranges into a modified strain map (MSM) and a modified envelope map (MEM) that we subsequently fuse into a single combined map that we show cor- responds robustly to actual bone boundaries. Our quantitative results demonstrate a marked reduction in false positive responses at soft tissue interfaces and an increase in bone delineation accuracy. Comparisons to the state-of-the-art on a finite-element-modelling (FEM) phantom and fiducial-based experimental phantom show an average improvement in mean absolute error (MAE) between actual and estimated bone bound- aries of 32% and 14%, respectively. We also demonstrate an average reduction in false bone responses of 87% and 56%, respectively. Finally, we qualitatively validate on clinical in vivo data of the human radius and ulna bones, and demonstrate similar improvements to those observed on phantoms."
884078,15226,9099,Perception-Guided Multimodal Feature Fusion for Photo Aesthetics Assessment,2014,"Photo aesthetic quality evaluation is a challenging task in multimedia and computer vision fields. Conventional approaches suffer from the following three drawbacks: 1) the deemphasized role of semantic content that is many times more important than low-level visual features in photo aesthetics; 2) the difficulty to optimally fuse low-level and high-level visual cues in photo aesthetics evaluation; and 3) the absence of a sequential viewing path in the existing models, as humans perceive visually salient regions sequentially when viewing a photo.    To solve these problems, we propose a new aesthetic descriptor that mimics humans sequentially perceiving visually/semantically salient regions in a photo. In particular, a weakly supervised learning paradigm is developed to project the local aesthetic descriptors (graphlets in this work) into a low-dimensional semantic space. Thereafter, each graphlet can be described by multiple types of visual features, both at low-level and in high-level. Since humans usually perceive only a few salient regions in a photo, a sparsity-constrained graphlet ranking algorithm is proposed that seamlessly integrates both the low-level and the high-level visual cues. Top-ranked graphlets are those visually/semantically prominent graphlets in a photo. They are sequentially linked into a path that simulates the process of humans actively viewing. Finally, we learn a probabilistic aesthetic measure based on such actively viewing paths (AVPs) from the training photos that are marked as aesthetically pleasing by multiple users. Experimental results show that: 1) the AVPs are 87.65% consistent with real human gaze shifting paths, as verified by the eye-tracking data; and 2) our photo aesthetic measure outperforms many of its competitors."
1699798,15226,9078,Facial emotion recognition with anisotropic inhibited Gabor energy histograms,2013,"This paper presents a novel image descriptor called Derivative Variation Pattern (DVP) and its application to face and palmprint recognition. DVP captures image variations in both the frequency and the spatial domains. The effects of uncontrolled illumination are compensated in the frequency domain by discarding the illumination affected frequencies. Image pixels are encoded as binary patterns based on the higher-order spatial derivatives computed in the spatial domain. The proposed descriptor was evaluated on the Extended Yale-B and FERET face databases, and the PolyU palmprint database. Experimental results demonstrate the effectiveness of the DVP descriptor in both the face and the palmprint recognition tasks under uncontrolled illuminations. State-of-the-art approaches have yet to deliver a feature representation for facial emotion recognition that can be applied to non-trivial unconstrained, continuous video data sets. Initially, research advanced with the use of Gabor energy filters. However, in recent work more attention has been given to other features. Gabor energy filters lack generalization needed in unconstrained situations. Additionally, they result in an undesirably high feature vector dimensionality. Nontrivial data sets have millions of samples; feature vectors must be as low dimensional as possible. We propose a novel texture feature based on Gabor energy filters that offers generalization with a background texture suppression component and is as compact as possible due to a maximal response representation and local histograms. We improve performance on the non-trivial Audio/Visual Emotion Challenge 2012 grandchallenge data set."
894595,15226,21106,Freehand 3D scanning in a mobile environment using video,2011,"This work is motivated by two important trends in consumer computing: (i) the growing pervasiveness of mobile computing devices, and (ii) the users' desire for increasingly complex but readily acquired and manipulated information content. Specifically, we develop and describe a system for 3D model creation of an object, using only a standard mobile device such as a smart phone. Our approach applies the structured light projection methodology and exploits multiple image input such as frames from a video sequence. In comparison with previous work, a significant further challenge addressed here is that of lower quality input data and limited hardware (processing power and memory, camera and projector quality). Novelties include: (i) a comparison of projection pattern detection approaches in the context of a mobile environment - a robust method combining colour detection and a phase congruency descriptor is evaluated, (ii) a model for single view reconstruction which exploits epipolar, coplanarity and topological constraints, (iii) the use of mobile device sensor data in the iterative closest point algorithm used to register multiple partial 3D reconstructions, and (iv) two heuristics for determining the order in which buffered single view based reconstructions are merged. Our experiments demonstrate that visually appealing results are obtained in a speedy manner which does not require specialist knowledge or expertise from the user."
2040837,15226,9616,Vein Pattern Visualization through Multiple Mapping Models and Local Parameter Estimation for Forensic Investigation,2014,"Forensic investigation methods based on some human traits, including fingerprint, face, and palm print, have been developed significantly, but some major aspects of particular crimes such as child pornography still lack of notable research efforts. Unlike common forensic identification methods, techniques for identifying criminals in child pornographic images should be developed based on partial non-facial skin observable in the images because criminals always hide their faces. Few methods published recently have shown the potential of vein patterns visualized from color images as a criminal and victim identification tool. However, these methods have two weaknesses: 1) they use single model to visualize vein patterns hidden in color images, which neglects the diversity of skin properties and 2) even though their parameters are determined automatically by an optimization, they do not adapt to fit local image characteristics. To address these weaknesses, this paper proposes an algorithm composed of a bank of mapping models which transform color images to near infrared (NIR) images for visualizing vein patterns and a local parameter estimation scheme for handling different image characteristics in different regions. Imbalanced data regression is also used to systematically construct the model bank. The proposed algorithm is examined and compared with the previous methods on a database of 920 thigh images from 230 subjects. It outperforms the previous methods."
72095,15226,21106,Vitality assessment of boar sperm using an adaptive LBP based on oriented deviation,2012,"A new method to describe sperm vitality using a hybrid combination of local and global texture descriptors is proposed in this paper. In this regard, a new adaptive local binary pattern (ALBP) descriptor is presented in order to carry out the local description. It is built by adding oriented standard deviation information to an ALBP descriptor in order to achieve a more complete representation of the images and hence it has been called ALBPS. Regarding semen vitality assessment, ALBPS outperformed previous literature works with an 81.88% of accuracy and it also yielded higher hit rates than the LBP and ALBP base-line methods. Concerning the global description of sperm heads, several classical texture algorithms were tested and a descriptor based on Wavelet transform and Haralick feature extraction (WCF13) obtained the best results. Both local and global descriptors were combined and the classification was carried out with a Support Vector Machine. Therefore, our proposal is novel in three ways. First, a new local feature extraction method ALBPS is introduced. Second, a hybrid method combining the proposed local ALBPS and a global descriptor is presented outperforming our first approach and all other methods evaluated for this problem. Third, vitality classification accuracy is greatly improved with the two former texture descriptors presented. F-Score and accuracy values were computed in order to measure the performance. The best overall result was yielded by combining ALBPS with WCF13 reaching a F-Score equals to 0.886 and an accuracy of 85.63%."
186812,15226,11052,Accelerated large scale optimization by concomitant hashing,2012,"Traditional locality-sensitive hashing (LSH) techniques aim to tackle the curse of explosive data scale by guaranteeing that similar samples are projected onto proximal hash buckets. Despite the success of LSH on numerous vision tasks like image retrieval and object matching, however, its potential in large-scale optimization is only realized recently. In this paper we further advance this nascent area. We first identify two common operations known as the computational bottleneck of numerous optimization algorithms in a large-scale setting, i.e., min/max inner product. We propose a hashing scheme for accelerating min/max inner product, which exploits properties of order statistics of statistically correlated random vectors. Compared with other schemes, our algorithm exhibits improved recall at a lower computational cost. The effectiveness and efficiency of the proposed method are corroborated by theoretic analysis and several important applications. Especially, we use the proposed hashing scheme to perform approximate l1 regularized least squares with dictionaries with millions of elements, a scale which is beyond the capability of currently known exact solvers. Nonetheless, it is highlighted that the focus of this paper is not on a new hashing scheme for approximate nearest neighbor problem. It exploits a new application for the hashing techniques and proposes a general framework for accelerating a large variety of optimization procedures in computer vision."
88129,15226,11321,Automatic human knee cartilage segmentation from multi-contrast MR images using extreme learning machines and discriminative random fields,2011,"Accurate and automatic segmentation of knee cartilage is required for the quantitative cartilage measures and is crucial for the assessment of acute injury or osteoarthritis. Unfortunately, the current works are still unsatisfactory. In this paper, we present a novel solution toward the automatic cartilage segmentation from multi-contrast magnetic resonance (MR) images using a pixel classification approach. Most of the previous classification based works for cartilage segmentation only rely on the labeling by a trained classifier, such as support vector machines (SVM) or k-nearest neighbor, but they do not consider the spatial interaction. Extreme learning machines (ELM) have been proposed as the training algorithm for the generalized single-hidden layer feedforward networks, which can be used in various regression and classification applications. Works on ELM have shown that ELM for classification not only tends to achieve good generalization performance, but also is easy to be implemented since ELM requires less human intervention (only one user-specified parameter needs to be chosen) and can get direct least-square solution. To incorporate spatial dependency in classification, we propose a new segmentation method based on the convex optimization of an ELM-based association potential and a discriminative random fields (DRF) based interaction potential for segmenting cartilage automatically with multi-contrast MR images. Our method not only benefits from the good generalization classification performance of ELM but also incorporates the spatial dependencies in classification. We test the proposed method on multi-contrast MR datasets acquired from 11 subjects. Experimental results show that our method outperforms the classifiers based solely on DRF, SVM or ELM in segmentation accuracy."
2182235,15226,9856,Using automatic speech recognition for attacking acoustic CAPTCHAs: the trade-off between usability and security,2014,"A common method to prevent automated abuses of Internet services is utilizing challenge-response tests that distinguish human users from machines. These tests are known as CAPTCHAs ( Completely Automated Public Turing Tests to Tell Computers and Humans Apart ) and should represent a task that is easy to solve for humans, but difficult for fraudulent programs. To enable access for visually impaired people, an acoustic CAPTCHA is typically provided in addition to the better-known visual CAPTCHAs. Recent security studies show that most acoustic CAPTCHAs, albeit difficult to solve for humans, can be broken via machine learning.   In this work, we suggest using speech recognition rather than generic classification methods for better analyzing the security of acoustic CAPTCHAs. We show that our attack based on an automatic speech recognition system can successfully defeat reCAPTCHA with a significantly higher success rate than reported in previous studies.   A major difficulty in designing CAPTCHAs arises from the trade-off between human usability and robustness against automated attacks. We present and analyze an alternative CAPTCHA design that exploits specific capabilities of the human auditory system, i.e., auditory streaming and tolerance to reverberation. Since state-of-the-art speech recognition technology still does not provide these capabilities, the resulting CAPTCHA is hard to solve automatically. A detailed analysis of the proposed CAPTCHA shows a far better trade-off between usability and security than the current quasi-standard approach of reCAPTCHA."
52471,15226,11052,Two-granularity tracking: mediating trajectory and detection graphs for tracking under occlusions,2012,"We propose a tracking framework that mediates grouping cues from two levels of tracking granularities, detection tracklets and point trajectories, for segmenting objects in crowded scenes. Detection tracklets capture objects when they are mostly visible. They may be sparse in time, may miss partially occluded or deformed objects, or contain false positives. Point trajectories are dense in space and time. Their affinities integrate long range motion and 3D disparity information, useful for segmentation. Affinities may leak though across similarly moving objects, since they lack model knowledge. We establish one trajectory and one detection tracklet graph, encoding grouping affinities in each space and associations across. Two-granularity tracking is cast as simultaneous detection tracklet classification and clustering (cl2) in the joint space of tracklets and trajectories. We solve cl2 by explicitly mediating contradictory affinities in the two graphs: Detection tracklet classification modifies trajectory affinities to reflect object specific dis-associations. Non-accidental grouping alignment between detection tracklets and trajectory clusters boosts or rejects corresponding detection tracklets, changing accordingly their classification.We show our model can track objects through sparse, inaccurate detections and persistent partial occlusions. It adapts to the changing visibility masks of the targets, in contrast to detection based bounding box trackers, by effectively switching between the two granularities according to object occlusions, deformations and background clutter."
1767979,15226,9078,Image perception of vision impaired by epi-retinal electrode stimulation,2014,"This Retinal prostheses, which helps the vision impaired patients with outer retinal degeneration, uses an outside camera to detect image, convert light energy of the image into a pattern electrical stimulation signal which is transmitted to an array of electrodes placed on the retinal surface, stimulate the remaining retinal ganglion cells to elicit electric activity which is pass down to optic nerve for final processing in the brain and synthesis of a visual image. The visual pathway functions as a complex image processor as well as an information conduit. At higher levels, the visual signals arrive with significant processing completed. In reality due to its easier access, simpler processing and the retinotopic organization, the retina has been the primary focus for artificial stimulation. Studies suggest that retinal implants may provide the patients with an acceptable level of visual mobility via a typical implanted electrode array containing tens of electrodes. In this study, a wide field implantable epi-retinal microelectrode array was designed and fabricated with parylene as flexible substrate material and Pt as electrode and route material, feature test was carried out on the array, and electric characteristics of the array was tested. The feature analysis showed that morphological and electrical properties of the array well met the requirements of implantation and electrical stimulation of retina. The pattern stimulation protocols and projected visual field were discussed."
1231062,15226,21106,Automatic Registration of RGB-D Scans via Salient Directions,2013,"We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. We build upon image features which are plenty, localized well and much more discriminative than geometry features, however, they suffer from viewpoint distortions and request for normalization. We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. This ambiguity is naturally handled by 2D features and allows to find stable correspondences among scans. For geometric pose estimation from tentative matches we propose a fast and robust 2 point sample consensus scheme integrating an early rejection phase. We evaluate our approach on different challenging real world scenes."
489428,15226,11052,Statistical inference of motion in the invisible,2012,"This paper focuses on the unexplored problem of inferring motion of objects that are invisible to all cameras in a multiple camera setup. As opposed to methods for learning relationships between disjoint cameras, we take the next step to actually infer the exact spatiotemporal behavior of objects while they are invisible. Given object trajectories within disjoint cameras' FOVs (field-of-view), we introduce constraints on the behavior of objects as they travel through the unobservable areas that lie in between. These constraints include vehicle following (the trajectories of vehicles adjacent to each other at entry and exit are time-shifted relative to each other), collision avoidance (no two trajectories pass through the same location at the same time) and temporal smoothness (restricts the allowable movements of vehicles based on physical limits). The constraints are embedded in a generalized, global cost function for the entire scene, incorporating influences of all objects, followed by a bounded minimization using an interior point algorithm, to obtain trajectory representations of objects that define their exact dynamics and behavior while invisible. Finally, a statistical representation of motion in the entire scene is estimated to obtain a probabilistic distribution representing individual behaviors, such as turns, constant velocity motion, deceleration to a stop, and acceleration from rest for evaluation and visualization. Experiments are reported on real world videos from multiple disjoint cameras in NGSIM data set, and qualitative as well as quantitative analysis confirms the validity of our approach."
1055080,15226,8502,Real-time multi-target tracking at 210 megapixels/second in Wide Area Motion Imagery,2014,"We present a real-time, full-frame, multi-target Wide Area Motion Imagery (WAMI) tracking system that utilizes distributed processing to handle high data rates while maintaining high track quality. The proposed architecture processes the WAMI data as a series of geospatial tiles and implements both process- and thread-level parallelism across multiple compute nodes. Each tile is processed independently, from decoding the image through generating tracks that are finally merged across all tiles by an inter-tile linker (ITL) module. A high performance PostgreSQL database with GIS extensions is used to control the flow of intermediate data between each tracking process. High quality tracks are produced efficiently due to robust, effective algorithmic modules including: multi-frame moving object detection and track initialization; tracking based on the fusion of motion and appearance with a goal of very pure tracks; and online track linking based on multiple features. In addition, we have configured a high-performance compute cluster using high density blade servers, Infiniband networking, and an HPC filesystem. The compute cluster enables full-frame, state-of-the-art tracking of vehicles or dismounts at the WAMI sensor's native 1.25Hz frame-rate, while only taking 7u of rack space and providing 210 megapixels/second throughput."
860436,15226,11470,Uncrowded window inspired information security display,2014,"With the booming of visual media, people pay more and more attention to privacy protection in public environments. Most existing research on information security such as cryptography and steganography is mainly concerned about transmission and yet little has been done to prevent the information displayed on screens from reaching eyes of the bystanders. To deal with the problem, for the application of text-reading, we proposed an eye tracking based solution using the newly revealed concept of uncrowded window from vision research. The theory of uncrowded window suggests that human vision can only effectively recognize objects inside a small window. Object features outside the window may still be detectable but the feature detection results cannot be efficiently combined properly and therefore those objects will not be recognizable. We use eye-tracker to locate fixation points of the authorized reader in real time, and only the area inside the uncrowded window displays the private information we want to protect. A number of dummy windows with fake messages are displayed around the real uncrowded window as diversions. And without the precise knowledge about the fixations of the authorized reader, the chance for bystanders to capture the private message from those surrounding area and the dummy windows is very low. Meanwhile, since the authorized reader can only read within the uncrowded window, detrimental impact of those dummy windows is almost negligible. The proposed prototype system was written in C++ with SDKs of Direct3D, Tobii Gaze SDK, CEGUI, MuPDF, OpenCV and etc. Extended demonstration of the system will be provided to show that the proposed method is an effective solution to the problem of information security and display."
1593574,15226,9099,Clickage: towards bridging semantic and intent gaps via mining click logs of search engines,2013,"The semantic gap between low-level visual features and high-level semantics has been investigated for decades but still remains a big challenge in multimedia. When search became one of the most frequently used applications, intent gap, the gap between query expressions and users' search intents, emerged. Researchers have been focusing on three approaches to bridge the semantic and intent gaps: 1) developing more representative features, 2) exploiting better learning approaches or statistical models to represent the semantics, and 3) collecting more training data with better quality. However, it remains a challenge to close the gaps. In this paper, we argue that the massive amount of click data from commercial search engines provides a data set that is unique in the bridging of the semantic and intent gap. Search engines generate millions of click data (a.k.a. image-query pairs), which provide almost unlimited yet strong connections between semantics and images, as well as connections between users' intents and queries. To study the intrinsic properties of click data and to investigate how to effectively leverage this huge amount of data to bridge semantic and intent gap is a promising direction to advance multimedia research. In the past, the primary obstacle is that there is no such dataset available to the public research community. This changes as Microsoft has released a new large-scale real-world image click data to public. This paper presents preliminary studies on the power of large-scale click data with a variety of experiments, such as building large-scale concept detectors, tag processing, search, definitive tag detection, intent analysis, etc., with the goal to inspire deeper researches based on this dataset."
702918,15226,390,MR prostate segmentation via distributed discriminative dictionary (DDD) learning,2013,"Segmenting prostate from MR images is important yet challenging. Due to non-Gaussian distribution of prostate appearances in MR images, the popular active appearance model (AAM) has its limited performance. Although the newly developed sparse dictionary learning method[1, 2] can model the image appearance in a non-parametric fashion, the learned dictionaries still lack the discriminative power between prostate and non-prostate tissues, which is critical for accurate prostate segmentation. In this paper, we propose to integrate deformable model with a novel learning scheme, namely the Distributed Discriminative Dictionary (DDD) learning, which can capture image appearance in a non-parametric and discriminative fashion. In particular, three strategies are designed to boost the tissue discriminative power of DDD. First, minimum Redundancy Maximum Relevance (mRMR) feature selection is performed to constrain the dictionary learning in a discriminative feature space. Second, linear discriminant analysis (LDA) is employed to assemble residuals from different dictionaries for optimal separation between prostate and non-prostate tissues. Third, instead of learning the global dictionaries, we learn a set of local dictionaries for the local regions (each with small appearance variations) along prostate boundary, thus achieving better tissue differentiation locally. In the application stage, DDDs will provide the appearance cues to robustly drive the deformable model onto the prostate boundary. Experiments on 50 MR prostate images show that our method can yield a Dice Ratio of 88% compared to the manual segmentations, and have 7% improvement over the conventional AAM."
1197913,15226,9099,3D photo browsing for future mobile devices,2012,"By introducing the interactive 3D photo/video browsing and exploration system, we propose novel approaches for handling the limitations of the current 2D mobile technology from two aspects: interaction design and visualization. Our contributions feature an effective interaction that happens in the 3D space behind the mobile device's camera. 3D motion analysis of the user's gesture captured by the device's camera is performed to facilitate the interaction between users and multimedia collections in various applications. This approach will solve a wide range of problems with the current input facilities such as miniature keyboards, tiny joysticks and 2D touch screens. The suggested interactive technology enables users to control, manipulate, organize, and re-arrange their photo/video collections in 3D space using bare-hand, marker-less gesture. Moreover, with the proposed techniques we aim to visualize the 2D photo collection, in 3D, on normal 2D displays. This process is automatically done by retrieving the 3D structure from single images, finding the stereo/multiple views of a scene or using the geo-tagged meta-data from huge photo collections. By using the design and implementation of the contributions of this work, we aim to achieve the following goals: Solving the limitations of the current 2D interaction facilities by 3D gestural interaction; Increasing the usability of the multimedia applications on mobile devices; Enhancing the quality of user experience with the digital collections."
2109573,15226,9004,Reconstruction of fiber trajectories via population-based estimation of local orientations,2011,"White matter fiber tractography plays a key role in the in vivo understanding of brain circuitry. For tract-based comparison of a population of images, a common approach is to first generate an atlas by averaging, after spatial normalization, all images in the population, and then perform tractography using the constructed atlas. The reconstructed fiber trajectories form a common geometry onto which diffusion properties of each individual subject can be projected based on the corresponding locations in the subject native space. However, in the case of High Angular Resolution Diffusion Imaging (HARDI), where modeling fiber crossings is an important goal, the above-mentioned averaging method for generating an atlas results in significant error in the estimation of local fiber orientations and causes a major loss of fiber crossings. These limitatitons have significant impact on the accuracy of the reconstructed fiber trajectories and jeopardize subsequent tract-based analysis. As a remedy, we present in this paper a more effective means of performing tractography at a population level. Our method entails determining a bipolar Watson distribution at each voxel location based on information given by all images in the population, giving us not only the local principal orientations of the fiber pathways, but also confidence levels of how reliable these orientations are across subjects. The distribution field is then fed as an input to a probabilistic tractography framework for reconstructing a set of fiber trajectories that are consistent across all images in the population. We observe that the proposed method, called PopTract, results in significantly better preservation of fiber crossings, and hence yields better trajectory reconstruction in the atlas space."
1248612,15226,9099,Hybrid social media network,2012,"Analysis and recommendation of multimedia information can be greatly improved if we know the interactions between the content, user, and concept, which can be easily observed from the social media networks. However, there are many heterogeneous entities and relations in such networks, making it difficult to fully represent and exploit the diverse array of information. In this paper, we develop a  hybrid social media network , through which the heterogeneous entities and relations are seamlessly integrated and a joint inference procedure across the heterogeneous entities and relations can be developed. The network can be used to generate personalized information recommendation in response to specific targets of interests, e.g., personalized multimedia albums, target advertisement and friend/topic recommendation. In the proposed network, each node denotes an entity and the multiple edges between nodes characterize the diverse relations between the entities (e.g., friends, similar contents, related concepts, favorites, tags, etc). Given a query from a user indicating his/her information needs, a propagation over the hybrid social media network is employed to infer the utility scores of all the entities in the network while learning the edge selection function to activate only a sparse subset of relevant edges, such that the query information can be best propagated along the activated paths. Driven by the intuition that much redundancy exists among the diverse relations, we have developed a robust optimization framework based on several sparsity principles. We show significant performance gains of the proposed method over the state of the art in multimedia retrieval and recommendation using data crawled from social media sites. To the best of our knowledge, this is the first model supporting not only aggregation but also judicious selection of heterogeneous relations in the social media networks."
1955722,15226,390,Joint cardiac and respiratory motion correction and super-resolution reconstruction in coronary PET/CT,2011,"Coronary artery disease is marked by the development of chronic inflammation in the vascular arteries that is associated with coronary plaques. Positron emission tomography (PET) is capable of detecting inflammation through activated macrophage uptake of FDG. Unfortunately, in conventional cardiac PET, respiratory and cardiac motion during acquisition leads to severe blurring of the resulting images and an effective spatial resolution inadequate for plaque detection and localization. In this paper, we extend our previous image-domain approach to a fully integrated, data-domain method that starts from the observed projection data and performs a model-based inversion and motion correction of all the data to create a high-resolution focused cardiac image. We term the new approach Data-domain Cardiac Shape Tracking and Adjustment for Respiration or D-CSTAR. In contrast to existing image domain methods the image reconstruction and motion correction steps are not separated. Unlike current data domain methods both cardiac and respiratory motions are compensated for. In D-CSTAR, cardiac motion parameters are estimated from X-ray CT images acquired in a breath-hold state. This cardiac motion information is incorporated in a unified PET reconstruction functional which jointly estimates and corrects for respiratory motion, compensates for phase aligned cardiac motion, and super-resolves the image. The technique is presented and applied to simulated cardiac PET/CT data corresponding to the XCAT phantom with both cardiac and respiratory cycles. The results show a marked qualitative and quantitative improvement when compared to conventional and existing PET methods."
2501498,15226,20552,Non-Convex Rank Minimization via an Empirical Bayesian Approach,2012,"In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case."
1933361,15226,11052,Point of gaze estimation through corneal surface reflection in an active illumination environment,2012,"Eye gaze tracking (EGT) is a common problem with many applications in various fields. While recent methods have achieved improvements in accuracy and usability, current techniques still share several limitations. A major issue is the need for external calibration between the gaze camera system and the scene, which commonly restricts to static planar surfaces and leads to parallax errors. To overcome these issues, the paper proposes a novel scheme that uses the corneal imaging technique to directly analyze reflections from a scene illuminated with structured light. This comprises two major contributions: First, an analytic solution is developed for the forward projection problem to obtain the gaze reflection point (GRP), where light from the point of gaze (PoG) in the scene reflects at the corneal surface into an eye image. We also develop a method to compensate for the individual offset between the optical axis and true visual axis. Second, introducing active coded illumination enables robust and accurate matching at the GRP to obtain the PoG in a scene image, which is the first use of this technique in EGT and corneal reflection analysis. For this purpose, we designed a special high-power IR LED-array projector. Experimental evaluation with a prototype system shows that the proposed scheme achieves considerable accuracy and successfully supports depth-varying environments."
1136846,15226,22260,Enabling Privacy-Preserving Image-Centric Social Discovery,2014,"The increasing popularity of images at social media sites is posing new opportunities for social discovery applications, i.e., suggesting new friends and discovering new social groups with similar interests via exploring images. To effectively handle the explosive growth of images involved in social discovery, one common trend for many emerging social media sites is to leverage the commercial public cloud as their robust backend data center. While extremely convenient, directly exposing content-rich images and the related social discovery results to the public cloud also raises new acute privacy concerns. In light of the observation, in this paper we propose a privacy-preserving social discovery service architecture based on encrypted images. As the core of such social discovery is to compare and quantify similar images, we first adopt the effective Bag-of-Words model to extract the visual similarity content of users' images into image profile vectors, and then model the problem as similarity retrieval of encrypted high-dimensional image profiles. To support fast and scalable similarity search over hundreds of thousands of encrypted images, we propose a secure and efficient indexing structure. The resulting design enables social media sites to obtain secure, practical, and accurate social discovery from the public cloud, without disclosing the encrypted image content. We formally prove the security and discuss further extensions on user image update and the compatibility with existing image sharing social functionalities. Extensive experiments on a large Flickr image dataset demonstrate the practical performance of the proposed design. Our qualitative social discovery results show consistency with human perception."
907079,15226,9616,Scene Recognition with Naive Bayes Non-linear Learning,2014,"A crucial feature of a good scene recognition algorithm is its ability to generalize. Scene categories, especially those related to human made indoor places or to human activities like sports, do present a high degree of intra-class variability, which in turn requires high robustness and generalization properties. Such features are amongst the distinctive characteristics of the Naive Bayes Nearest Neighbor (NBNN) approach [1], an image classification framework that since its introduction in 2008 has been gaining momentum in the visual recognition community. In this paper we show how with a straightforward modification of the original NBNN scoring function it is possible to use a recently introduced latent locally linear SVM algorithm to discriminatively learn a set of prototype local features for each class. The resulting classification algorithm, that we call Naive Bayes Nonlinear Learning (NBNL) preserves the generality and robustness properties of the original approach, while greatly reducing its memory requirements during testing, and significantly improving its performance. To the best of our knowledge this is the first work to exploit the structure of the local features through the use of a latent locally linear discriminative learning method. Experiments over three different public scene recognition datasets show the effectiveness of the proposed algorithm, which outperforms several existing NBNN-based methods and is competitive with standard Bag-of-Words plus SVM approaches."
1024176,15226,22130,3D Model-based Reconstruction of the Proximal Femur from Low-dose Biplanar X-Ray Images,2011,"The 3D modeling of the proximal femur is a valuable diagnostic tool for orthopedic surgery planning. The use of computed tomography is the most prominent modality to visualize bones both in terms of resolution as well as in term of bone/tissue separation. Towards reducing the impact of radiation to the patient, low-dose X-ray imaging systems have been introduced while still providing partial views with rather low signal-to-noise ratio. In this paper, we focus on automating the 3D proximal femur reconstruction from simultaneously acquired 2D views. A deformable model represented by triangulated mesh surfaces extends to a linear sub-space describing the variations across individuals. Segmentation consists of inferring a global deformation of 3D model followed by a local adaption based on the most prominent combination of the sub-space parameters. The ba-sis of which relies on the minimization of a cost function based on the biplanar projection of this model. To this end, we employ an active region model that aims at optimizing the 3D model parameters such that projection of surface is attracted from edge potentials, while creating an optimal partition between the bone class and the remaining structures. The global parameters of the model and the local ones are optimized through a gradient-free approach. Promising results demonstrate the potentials of our method compared to a supervised reconstruction technique."
2195506,15226,20796,A heterogenous automatic feedback semi-supervised method for image reranking,2013,"Image reranking, which aims at enhancing the quality of keyword-based image search with the help of image features, recently has become attractive in image search community. A major challenging in this task is that image's visual features do not always well reflect image's semantic meaning. Thus, reranking methods only depending on visual features cannot guarantee to obtain good results. In addition, it is well known that the visual features of an image have strong/weak correlations with its surrounding text. Thus, it is expected that a model considering both visual features and its surrounding text can perform better than those only considering visual features. Motivated by this, in this paper, we propose the HAFSRerank--Heterogenous Automatic Feedback Semi-supervised Reranking method which makes use of both visual and textual features simultaneously during reranking. Specifically, in HAFSRerank, a multigraph is firstly constructed in which each node representing an image includes visual and textual features, and the parallel edges between them are weighted by intra-modal similarity and inter-modal similarity. A heterogenous complete graph is further derived from the multigraph. Then, an automatic feedback graph-based semi-supervised learning method is proposed to propagate the reranking scores on the complete graph, which can make use of the inter-modal similarity to update the weights of heterogenous graph automatically. Finally, the result of the semi-supervised learning is used to rerank the images. The experimental results show that HAFSRerank is superior or highly competitive to some state-of-the-art graph-based reranking methods. Moreover, the proposed reranking algorithm can be well interpreted by Bayesian theory, and does not require complex search models for special queries and any additional input from users."
2677098,15226,23620,Symmetric lenses,2011,"Lenses --bidirectional transformations between pairs of connected structures--have been extensively studied and are beginning to find their way into industrial practice. However, some aspects of their foundations remain poorly understood. In particular, most previous work has focused on the special case of  asymmetric lenses , where one of the structures is taken as primary and the other is thought of as a projection, or view. A few studies have considered symmetric variants, where each structure contains information not present in the other, but these all lack the basic operation of  composition . Moreover, while many domain-specific languages based on lenses have been designed, lenses have not been thoroughly explored from an algebraic perspective.   We offer two contributions to the theory of lenses. First, we present a new symmetric formulation, based on  complements , an old idea from the database literature. This formulation generalizes the familiar structure of asymmetric lenses, and it admits a good notion of composition. Second, we explore the algebraic structure of the space of symmetric lenses. We present generalizations of a number of known constructions on asymmetric lenses and settle some longstanding questions about their properties---in particular, we prove the existence of (symmetric monoidal) tensor products and sums and the  non -existence of full categorical products or sums in the category of symmetric lenses. We then show how the methods of universal algebra can be applied to build  iterator lenses  for structured data such as lists and trees, yielding lenses for operations like mapping, filtering, and concatenation from first principles. Finally, we investigate an even more general technique for constructing mapping combinators, based on the theory of  containers ."
2475846,15226,390,A model-based registration approach of preoperative MRI with 3D ultrasound of the liver for Interventional guidance procedures,2012,"In this paper, we present a novel approach to rigidly register intraoperative electromagnetically tracked ultrasound (US) with pre-operative contrast-enhanced magnetic resonance (MR) images. The clinical rationale for this work is to allow accurate needle placement during thermal ablations of liver metastases using multimodal imaging. We adopt a model-based approach that rigidly matches segmented liver surface shapes obtained from the multimodal image volumes. Towards this end, a shape-constrained deformable surface model combining the strengths of both deformable and active shape models is used to segment the liver surface from the MR scan. It incorporates a priori shape information while external forces guide the deformation and adapts the model to a target structure. The liver boundary is extracted from US by merging a dynamic region-growing method with a graph-based segmentation framework anchored on adaptive priors of neighboring surface points. Registration is performed with a weighted ICP algorithm with a physiological penalizing term. The MR segmentation model was trained with 30 datasets and validated on a separate cohort of 10 patients with corresponding ground truth. The accuracy and robustness of the method were assessed by registering four US/MR datasets, yielding accurate landmark registration errors (3.7 ± 0.69mm) and high robustness, and is thus acceptable for radiofrequency clinical applications."
2470617,15226,390,Optimization of the internal noise models for channelized Hotelling observer,2011,"The channelized Hotelling observer (CHO) has become a widely used approach for evaluating medical image quality, acting as a surrogate for human observers in early-stage research on assessment and optimization of imaging devices and algorithms. Its popularity stems from experiments showing that, when an internal-noise model is introduced, the CHO's detection performance can be tuned to correlate well with human observers' detection performance. Typically, this tuning is achieved using example data obtained from human observers. Thus, it can be argued that this tuning step is essentially a model training exercise; therefore, just as in supervised learning, it is essential to test the CHO model on a set of data that is distinct from that used to tune the model. Furthermore, the test data should be significantly different from the training data if the CHO is to provide useful insight about new imaging algorithms or devices. Motivated by these considerations, in this work a train-test approach was proposed, with new models selection criterions, and used to evaluate ten established internal noise models utilizing four different channel models. I also propose a new internal noise model (although not the main intention of this work) which I, incidentally, find outperforms the ten established models. The results show that the proposed train-test approach is necessary, with the newly proposed model selection criterions, for avoiding spurious conclusions. It was also demonstrate that, in some CHO models, the optimal internal-noise parameter is very sensitive to the choice of training data; therefore, these models are prone to overfitting, and will not likely generalize well to new data."
912560,15226,11470,Video coding using a self-adaptive redundant dictionary consisting of spatial and temporal prediction candidates,2014,"All standard video coders are based on the prediction plus transform representation of an image block, which predicts the current block using various intra- and inter-prediction modes and then represents the prediction error using a fixed orthonormal transform. We propose to directly represent a mean-removed block using a redundant dictionary consisting of all possible inter-prediction candidates with integer motion vectors (mean-removed). In general the dictionary may also contain some intra-prediction candidates and some pre-designed fixed dictionary atoms. However, simulation results reported in this papers are obtained by using the inter-prediction candidates only. We determine the coefficients by minimizing the L0 norm of the coefficients subject to a constraint on the sparse approximation error. We show that using such a self-adaptive dictionary can lead to a very sparse representation, with significantly fewer non-zero coefficients than using the DCT transform on the prediction error. We further propose a modified orthogonal matching pursuit (OMP) algorithm which othonormalizes each new chosen atom with respect to all previously chosen and orthonormalized atoms. Each image block is represented by the quantized coefficients corresponding to the othonormalized atoms, to overcome the inefficiency associated with using non-orthonormal atoms. Each image block is represented by its mean, which is predictively coded, the indices of the chosen atoms, and the quantized coefficients. Each variable is coded based on its unconditional distribution. Simulation results show that the proposed coder can achieve significant gain over the H.264 coder (implemented using x264) and achieve similar performance comparing to the HEVC reference encoder (HM)."
1077359,15226,9773,Stabilize Sequence Learning with Recurrent Neural Networks by Forced Alignment,2013,"Cursive handwriting recognition is still a hot topic of research, especially for non-Latin scripts. One of the techniques which yields best recognition results is based on recurrent neural networks: with neurons modeled by long short-term memory (LSTM) cells, and alignment of label sequence to output sequence performed by a connectionist temporal classification (CTC) layer. However, network training is time consuming, unstable, and tends to over-adaptation. One of the reasons is the bootstrap process, which aligns the label data more or less randomly in early training iterations. This also leads to the fact that the emission peak positions within a character are located unpredictably. But positions near the center of a character are more desirable: In theory, they better model the properties of a character. The solution presented here is to guide the back-propagation training in early iterations: Character alignment is enforced by replacing the forward-backward alignment by fixed character positions: either pre-segmented, or equally distributed. After a number of guided iterations, training may be continued by standard dynamic alignment. A series of experiments is performed to answer some of these questions: Can peak positions be controlled in the long run? Can training iterations be reduced, getting results faster? Is training more stable? And finally: Do defined character position lead to better recognition performance?"
568037,15226,21106,MRF guided anisotropic depth diffusion for kinect range image enhancement,2012,"Projected texture based 3D sensing modalities are being increasingly used for a variety of 3D computer vision applications. However, these sensing modalities, exemplified by the Microsoft Kinect Sensor, suffer from severe drawbacks that hamper the quality of the range estimate output from the sensor. It is well known that the quality of reconstruction of the 3D projected texture for range estimation is a function of the material properties of objects in the image. Objects colored black, yellow or deep red often do not reflect the texture in a manner suitable for the detector to estimate the range values. Furthermore, shiny or highly reflective objects can also scatter the projected texture patterns. Objects with skewed surface orientation, occlusions, object self-shadows and intra-object mutual shadows, transparency and other factors also create problems with projected texture reconstruction. In order to alleviate these concerns, depth interpolation techniques have been used in the past. These techniques, however, create loss of depth structures crucial for segmentation and detection processes. In order to alleviate these concerns, we present a novel MRF based color- depth fusion algorithm which uses information from the RGB sensor of the Kinect and couples it with the depth content to produce fine structure, high fidelity depth maps. This algorithm can be implemented in hardware on the Kinect device, thereby improving the depth resolution, fidelity of the sensor while eliminating range errors and shadows."
2347288,15226,21106,Pedestrian Parsing via Deep Decompositional Network,2013,"We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset that includes 3,673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets."
1715761,15226,23735,KidsArm — An image-guided pediatric anastomosis robot,2013,"Minimally invasive surgery (MIS) revolutionized surgery by drastically reducing patient recovery times by allowing surgeons to perform procedures through a series of small incisions. However, MIS has also increased the complexity of the tasks as tools did not have the same degrees of freedom and dexterity compared to open procedures. In particular, pediatric patients pose a unique challenge as they have smaller volumes and different tissue properties. Our group designed KidsArm, an image-guided pediatric surgical robot, to automate anastomosis. KidsArm is single port anastomosis tool that uses a pair of stereo cameras to generate a 3D point cloud to guide the tool tip and apply a series of sutures. The system was designed to be minimally invasive and constrained by standard pediatric trocar sizes while also being automated. An image processing system was created to extract and track surface features on the simulated tissue samples while providing feedback to the robot controller. The system was tested on two scenarios: side-to-side and end-to-end silicone samples. KidsArm successfully applied 3 sutures autonomously on the side-to-side scenario however the end-to-end scenario proved to be more difficult due to greater deformation and workspace restrictions. However, KidsArm demonstrates that it is feasible for a robot to autonomously perform anastomosis. More work will be required to accelerate the process and characterize the behavior with tissue samples."
1638774,15226,9099,Audiovisual Resynthesis in an Augmented Reality,2014,"Resynthesizing Perception immserses participants within an audiovisual augmented reality using goggles and headphones while they explore their environment. What they hear and see is a computationally generative synthesis of what they would normally hear and see. By demonstrating the associations and juxtapositions the synthesis creates, the aim is to bring to light questions of the nature of representations supporting perception. Two modes of operation are possible. In the first model, while a participant is immersed, salient auditory events from the surrounding environment are stored and continually aggregated to a database. Similarly, for vision, using a model of exogenous attention, proto-objects of the ongoing dynamic visual scene are continually stored using a camera mounted to goggles on the participant's head. The aggregated salient auditory events and proto-objects form a set of representations which are used to resynthesize the microphone and camera inputs in real-time. In the second model, instead of extracting representations from the real world, an existing database of representations already extracted from scenes such as images of paintings and natural auditory scenes are used for synthesizing the real world. This work was previously exhibited at the Victoria and Albert Museum in London. Of the 1373 people to participate in the installation, 21 participants agreed to be filmed and fill out a questionnaire. We report their feedback and show that Resynthesizing Perception is an engaging and thought-provoking experience questioning the nature of perceptual representations."
1253416,15226,390,A novel line detection method in space-time images for microvascular blood flow analysis in sublingual microcirculatory videos,2013,"Recent evidence suggests that quantitative assessment of microcirculatory dysfunction may indicate certain disease states [1, 2, 3]. Relevant microcirculatory hemodynamic parameters include total vessel density, density of perfused vessels, proportion of perfused vessels, and perfusion heterogeneity index. In one non-invasive, clinical approach, a handheld video microscope placed under the tongue records images of blood flow in small (<; 20μm) and medium (approximately 20-100μm) diameter vessels. Hemodynamic parameters are computed from measurements of vessel geometry and blood flow rates. Current technology is limited by poor dynamic range, low resolution, poor image stability, and pressure artifacts. Video images are analyzed quantitatively and semi-quantitatively by trained image analysts using a time-consuming, semi-automated techniques for vessel segmentation, and blood flow measurements. Space-time images are generated for quantitative velocity estimation. We propose a novel line detection method to automatically estimate the orientation of red blood cell (RBC) or plasma gap traces in space-time images. Velocities of RBCs can then be calculated based on the estimated orientation. The proposed automated method for velocity estimation was implemented for 80 vessels and compared with visual estimation of reference slope in space-time diagrams by a trained image analyst. Finally, the proposed method is compared with a Hough transform based velocity estimation method."
115156,15226,11052,Scene recognition on the semantic manifold,2012,"A new architecture, denoted spatial pyramid matching on the semantic manifold (SPMSM), is proposed for scene recognition. SPMSM is based on a recent image representation on a semantic probability simplex, which is now augmented with a rough encoding of spatial information. A connection between the semantic simplex and a Riemmanian manifold is established, so as to equip the architecture with a similarity measure that respects the manifold structure of the semantic space. It is then argued that the closed-form geodesic distance between two manifold points is a natural measure of similarity between images. This leads to a conditionally positive definite kernel that can be used with any SVM classifier. An approximation of the geodesic distance reveals connections to the well-known Bhattacharyya kernel, and is explored to derive an explicit feature embedding for this kernel, by simple square-rooting. This enables a low-complexity SVM implementation, using a linear SVM on the embedded features. Several experiments are reported, comparing SPMSM to state-of-the-art recognition methods. SPMSM is shown to achieve the best recognition rates in the literature for two large datasets (MIT Indoor and SUN) and rates equivalent or superior to the state-of-the-art on a number of smaller datasets. In all cases, the resulting SVM also has much smaller dimensionality and requires much fewer support vectors than previous classifiers. This guarantees much smaller complexity and suggests improved generalization beyond the datasets considered."
1563071,15226,9099,Learning latent spatio-temporal compositional model for human action recognition,2013,"Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions \emph{e.g.} triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom,  i.e.  switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration ( e.g.  the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (\emph{e.g.} different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other methods."
1015407,15226,11491,SUPER: towards real-time event recognition in internet videos,2012,"Event recognition in unconstrained Internet videos has great potential in many applications. State-of-the-art systems usually include modules that need extensive computation, such as the extraction of spatial-temporal interest points, which poses a big challenge for large-scale video processing. This paper presents SUPER, a Speeded UP Event Recognition framework for efficient Internet video analysis. We take a multimodal baseline that has produced strong performance on popular benchmarks, and systematically evaluate each component in terms of both computational cost and contribution to recognition accuracy. We show that, by choosing suitable features, classifiers, and fusion strategies, recognition speed can be greatly improved with minor performance degradation. In addition, we also evaluate how many visual and audio frames are needed for event recognition in Internet videos, a question left unanswered in the literature. Results on a rigorously designed dataset indicate that similar recognition accuracy can be attained using only 14 frames per video on average. We also observe that, different from the visual channel, the soundtracks contains little redundant information for video event recognition. Integrating all the findings, our suggested SUPER framework is 220-fold faster than the baseline approach with merely 3.8% drop in recognition accuracy. It classifies an 80-second video sequence using models of 20 classes in just 4.56 seconds."
1498762,15226,8502,Automatic region-of-interest detection and prioritisation for visually optimised coding of low bit rate videos,2013,"The increasing popularity of video consumption from mobile devices requires an effective video coding strategy. To overcome diverse communication networks, video services often need to maintain sustainable quality when the available bandwidth is limited. One of the strategy for a visually-optimised video adaptation is by implementing a region-of-interest (ROI) based scalability, whereby important regions can be encoded at a higher quality while maintaining sufficient quality for the rest of the frame. The result is an improved perceived quality at the same bit rate as normal encoding, which is particularly obvious at the range of lower bit rate. However, because of the difficulties of predicting region-of-interest (ROI) accurately, there is a limited research and development of ROI-based video coding for general videos. In this paper, the phase spectrum quaternion of Fourier Transform (PQFT) method is adopted to determine the ROI. To improve the results of ROI detection, the saliency map from the PQFT is augmented with maps created from high level knowledge of factors that are known to attract human attention. Hence, maps that locate faces and emphasise the centre of the screen are used in combination with the saliency map to determine the ROI. The contribution of this paper lies on the automatic ROI detection technique for coding a low bit rate videos which include the ROI prioritisation technique to give different level of encoding qualities for multiple ROIs, and the evaluation of the proposed automatic ROI detection that is shown to have a close performance to human ROI, based on the eye fixation data."
1479846,15226,9773,Scene Character Detection by an Edge-Ray Filter,2013,"Edge is a type of valuable clues for scene character detection task. Generally, the existing edge-based methods rely on the assumption of straight text line to prune away the non-character candidates. This paper proposes a new edge-based method, called edge-ray filter, to detect the scene character. The main contribution of the proposed method lies in filtering out complex backgrounds by fully utilizing the essential spatial layout of edges instead of the assumption of straight text line. Edges are extracted by a combination of Canny and Edge Preserving Smoothing Filter (EPSF). To effectively boost the filtering strength of the designed edge-ray filter, we employ a new Edge Quasi-Connectivity Analysis (EQCA) to unify complex edges as well as contour of broken character. Label Histogram Analysis (LHA) then filters out non-character edges and redundant rays through setting proper thresholds. Finally, two frequently-used heuristic rules, namely aspect ratio and occupation, are exploited to wipe off distinct false alarms. In addition to have the ability to handle special scenarios, the proposed method can accommodate dark-on-bright and bright-on-dark characters simultaneously, and provides accurate character segmentation masks. We perform experiments on the benchmark ICDAR 2011 Robust Reading Competition dataset as well as scene images with special scenarios. The experimental results demonstrate the validity of our proposal."
1774376,15226,9616,Automatic Segmentation and Recognition of Human Actions in Monocular Sequences,2014,"This paper addresses the problem of silhouette-based human action segmentation and recognition in monocular sequences. Motion History Images (MHIs), used as 2D templates, capture motion information by encoding where and when motion occurred in the images. Inspired by codebook approaches for object and scene categorization, we first construct a codebook of temporal motion templates by clustering all the MHIs of each particular action. These MHIs capture different actors, speeds and a wide range of camera viewpoints. In this paper, we use a Kohonen's Self-Organizing Map (SOM) to simultaneously cluster the MHI templates and represent them in lower dimensional subspaces. To cope with temporal segmentation, and concurrently carry out action recognition, a new architecture is proposed where the obsrvation MHIs are projected onto all these action-specific manifolds and the Euclidean distance between each MHI and the nearest cluster within each action-manifold constitutes the observation vector of a Markov Model. To estimate the state/action at each time step, we introduce a new method based on Observable Markov Models (OMMs) where the Markov model is augmented with a neutral state. The combination of our action-specific manifolds with the augmented OMM allows to automatically segment and recognize long sequences of consecutive actions, without any prior knowledge about initial and ending frames of each action. Importantly, our method allows to interpolate betweeen training viewpoint and recognizes actions, independently of the camera viewpoint, even from unseen viewpoints."
2428628,15226,11470,Optimizing frame structure for interactive multiview video streaming with viewsynthesis,2011,"Traditional multiview video coding schemes compress all captured video frames exploiting all possible inter-view and temporal frame correlation for coding gain, creating complex inter-frame dependencies in the process. In contrast, interactive multiview video streaming (IMVS) demands data navigation flexibility in the frame structure design, so that server can send only a single periodically selected video view for decoding and display at client, saving transmission bandwidth. In this paper, we generalize previous IMVS frame structure optimization to allow a client to request an arbitrary virtual view; i.e., the server sends two adjacent coded views for the client to synthesize the desired virtual view. Since existing IMVS schemes transmit only one view at a time, they employ only cross-time prediction; i.e., the frame of previous time instant from which the client switches is used as predictor for the requested view. In our new scenario, two coded views are transmitted, thus within-time prediction can also be used, where the coded frame of one transmitted view is used to predict the frame of the other view of same time instant. Using I-frames, P-frames and Merge (M-) frames as building blocks, we formulate a Lagrangian problem to find the optimal frame structure for a desired storage/streaming rate tradeoff, with the right mixture of cross-time / within-time prediction types. Experiments show that for the same storage cost, the expected streaming rate of the proposed structure can be 40% lower than that of the I-frame-only structure, and 9% lower than that of the structure using M-frames but with cross-time prediction only."
1046425,15226,9078,Eye movement analysis for depression detection,2013,"Depression is a common and disabling mental health disorder, which impacts not only on the sufferer but also on their families, friends and the economy overall. Despite its high prevalence, current diagnosis relies almost exclusively on patient self-report and clinical opinion, leading to a number of subjective biases. Our aim is to develop an objective affective sensing system that supports clinicians in their diagnosis and monitoring of clinical depression. In this paper, we analyse the performance of eye movement features extracted from face videos using Active Appearance Models for a binary classification task (depressed vs. non-depressed). We find that eye movement low-level features gave 70% accuracy using a hybrid classifier of Gaussian Mixture Models and Support Vector Machines, and 75% accuracy when using statistical measures with SVM classifiers over the entire interview. We also investigate differences while expressing positive and negative emotions, as well as the classification performance in gender-dependent versus gender-independent modes. Interestingly, even though the blinking rate was not significantly different between depressed and healthy controls, we find that the average distance between the eyelids (`eye opening') was significantly smaller and the average duration of blinks significantly longer in depressed subjects, which might be an indication of fatigue or eye contact avoidance."
2624240,15226,9004,Phenotype Detection in Morphological Mutant Mice Using Deformation Features,2013,"Large-scale global efforts are underway to knockout each of the approximately 25, 000 mouse genes and interpret their roles in shaping the mammalian embryo. Given the tremendous amount of data generated by imaging mutated prenatal mice, high-throughput image analysis systems are inevitable to characterize mammalian development and diseases. Current state-of-the-art computational systems offer only differential volumetric analysis of pre-defined anatomical structures between various gene-knockout mice strains. For subtle anatomical phe- notypes, embryo phenotyping still relies on the laborious histological techniques that are clearly unsuitable in such big data environment. This paper presents a system that automatically detects known phenotypes and assists in discovering novel phenotypes in µCT images of mutant mice. Deformation features obtained from non-linear registration of mu- tant embryo to a normal consensus average image are extracted and analyzed to compute phenotypic and candidate phenotypic areas. The presented system is evaluated using C57BL/10 embryo images. All cases of ventricular septum defect and polydactyly, well-known to be present in this strain, are successfully detected. The system predicts potential phe- notypic areas in the liver that are under active histological evaluation for possible phenotype of this mouse line."
789509,15226,9078,An automatic level set based liver segmentation from MRI data sets,2012,"A fast and accurate liver segmentation method is a challenging work in medical image analysis area. Liver segmentation is an important process for computer-assisted diagnosis, pre-evaluation of liver transplantation and therapy planning of liver tumors. There are several advantages of magnetic resonance imaging such as free form ionizing radiation and good contrast visualization of soft tissue. Also, innovations in recent technology and image acquisition techniques have made magnetic resonance imaging a major tool in modern medicine. However, the use of magnetic resonance images for liver segmentation has been slow when we compare applications with the central nervous systems and musculoskeletal. The reasons are irregular shape, size and position of the liver, contrast agent effects and similarities of the gray values of neighbor organs. Therefore, in this study, we present a fully automatic liver segmentation method by using an approximation of the level set based contour evolution from T2 weighted magnetic resonance data sets. The method avoids solving partial differential equations and applies only integer operations with a two-cycle segmentation algorithm. The efficiency of the proposed approach is achieved by applying the algorithm to all slices with a constant number of iteration and performing the contour evolution without any user defined initial contour. The obtained results are evaluated with four different similarity measures and they show that the automatic segmentation approach gives successful results."
1273649,15226,20338,Analyzing the potential benefits of CDN augmentation strategies for internet video workloads,2013,"Video viewership over the Internet is rising rapidly, and market predictions suggest that video will comprise over 90\% of Internet traffic in the next few years. At the same time, there have been signs that the Content Delivery Network (CDN) infrastructure is being stressed by ever-increasing amounts of video traffic. To meet these growing demands, the CDN infrastructure must be designed, provisioned and managed appropriately. Federated telco-CDNs and hybrid P2P-CDNs are two content delivery infrastructure designs that have gained significant industry attention recently. We observed several user access patterns that have important implications to these two designs in our unique dataset consisting of 30 million video sessions spanning around two months of video viewership from two large Internet video providers. These include partial interest in content, regional interests, temporal shift in peak load and patterns in evolution of interest. We analyze the impact of our findings on these two designs by performing a large scale measurement study. Surprisingly, we find significant amount of synchronous viewing behavior for Video On Demand (VOD) content, which makes hybrid P2P-CDN approach feasible for VOD and suggest new strategies for CDNs to reduce their infrastructure costs. We also find that federation can significantly reduce telco-CDN provisioning costs by as much as 95%."
1389535,15226,20411,Where is who: large-scale photo retrieval by facial attributes and canvas layout,2012,"The ubiquitous availability of digital cameras has made it easier than ever to capture moments of life, especially the ones accompanied with friends and family. It is generally believed that most family photos are with faces that are sparsely tagged. Therefore, a better solution to manage and search in the tremendously growing personal or group photos is highly anticipated. In this paper, we propose a novel way to search for face photos by simultaneously considering attributes (e.g., gender, age, and race), positions, and sizes of the target faces. To better match the content and layout of the multiple faces in mind, our system allows the user to graphically specify the face positions and sizes on a query canvas, where each attribute combination is defined as an icon for easier representation. As a secondary feature, the user can even place specific faces from the previous search results for appearance-based retrieval. The scenario has been realized on a tablet device with an intuitive touch interface. Experimenting with a large-scale Flickr dataset of more than 200k faces, the proposed formulation and joint ranking have made us achieve a hit rate of 0.420 at rank 100, significantly improving from 0.036 of the prior search scheme using attributes alone. We have also achieved an average running time of 0.0558 second by the proposed block-based indexing approach."
1186330,15226,9099,Crowdsourcing a Reverberation Descriptor Map,2014,"Audio production is central to every kind of media that involves sound, such as film, television, and music and involves transforming audio into a state ready for consumption by the public. One of the most commonly-used audio production tools is the reverberator. Current interfaces are often complex and hard-to-understand. We seek to simplify these interfaces by letting users communicate their audio production objective with descriptive language (e.g. Make the drums sound bigger.). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin warmer using a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions lead to the goal. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialReverb, a project to crowdsource a vocabulary of audio descriptors that can be mapped onto concrete actions using a parametric reverberator. We deployed SocialReverb, on Mechanical Turk, where 513 unique users described 256 instances of reverberation using 2861 unique words. We used this data to build a concept map showing which words are popular descriptors, which ones map consistently to specific reverberation types, and which ones are synonyms. This promises to enable future interfaces that let the user communicate their production needs using natural language."
1771122,15226,9099,An effective multi-clue fusion approach for web video topic detection,2012,"The efficient organization and navigation of web videos in the topic level could enhance the user experience and boost the user's understanding about the happened events. Due to the potential application prospects, topic detection attracts increasing research interests in the last decade. On one hand, the user concerned real world hot topic always leads to a massive discussion in the video sharing sites, such as YouTube, Youku,  etc.  On the other hand, the search volume of the topic related keywords are growing explosively in the search engine such as Google, Yahoo,  etc . These keywords are the queries formulated by the users to search their concerned topics. They reflect the users' intention and could be used as a clue to find the hot topics. In this paper, different from the traditional topic detection methods, which mainly rely on data clustering, we propose a novel multi-clue fusion approach for web video topic detection. In our approach, firstly by utilizing the video related tag information, a maximum average score and a burstiness degree are proposed to extract the dense-bursty tag groups. Secondly, the near-duplicate keyframes ( NDK ) are extracted from the videos and fused with the extracted tag groups. After that, the hot search keywords from the search engine are used as guidance for topic detection. Finally, these clues are combined together to detect the topics hidden in the web video data. Experiment is conducted on the YouTube video data and the results demonstrate that the proposed method is effective."
1742628,15226,9078,Multi-stage vector quantization towards low bit rate visual search,2012,"While much progress has been made in mobile visual search, user experiences still relate to the query transmission latency, especially over a bandwidth-constrained wireless link. Low bit rate visual search paradigm has been well advocated in both academic and industrial endeavors, which directly extracts and sends compact visual descriptor(s) rather than sending a query image. Recent advances in compact descriptor design have advocated the use of compressed bag-of-words histogram, which has shown superior performance over other alternatives. However, existing works focus on descriptor compactness, regardless of time cost and memory requirements on the extraction pipeline, which in turn is crucial for the mobile end development. In this paper, we investigate the problem of designing a memory-light descriptor extraction scheme based upon the so-called multi-stage vector quantization. Our scheme starts by quantizing local patches with a small codebook, and the resulting quantization residual is subsequently compensated by a product quantizer. The design of both quantizers are based upon improving PSNR, which would drop a lot through quantization. PSNR is quantitatively shown to be highly correlated with retrieval and matching accuracy. Extensive evaluation on MPEG Compact Descriptor for Visual Search (CDVS) dataset, has reported superior performance over the state-of-the-art."
2188029,15226,23735,Orientation descriptors for localization in urban environments,2011,"Accurately determining the position and orientation of an observer (a vehicle or a human) in outdoor urban environments is an important and challenging problem. The standard approach is to use the Global Positioning System (GPS), but this system performs poorly near tall buildings where line of sight to a sufficient number of satellites cannot be obtained. Most previous vision-based approaches for localization register ground imagery to a previously generated ground-level model of the environment. Generating such a model can be difficult and time consuming, and is impractical in some environments. Instead, we propose to perform localization by registering a single omnidirectional ground image to a 2D urban terrain model that is easily generated from aerial imagery. We introduce a novel image descriptor that encodes the position and orientation of a camera relative to buildings in the environment. The descriptor is efficiently generated from edges and vanishing points in an omnidirectional image and is registered to descriptors previously generated for the terrain model. Rather than constructing a local CAD-like model of the environment, which is difficult in cluttered environments, our descriptor measures, at equally spaced intervals over the 360° field of view, the orientation of visible building facades projected onto the ground plane (i.e., the building footprints). We evaluate our approach on an urban data set with significant clutter and demonstrate an accuracy of about 1 m, which is an order of magnitude better than commercial GPS operating in open environments."
15061,15226,9004,Adaptive energy selective active contour with shape priors for nuclear segmentation and gleason grading of prostate cancer,2011,"Shape based active contours have emerged as a natural solution to overlap resolution. However, most of these shape-based methods are computationally expensive. There are instances in an image where no overlapping objects are present and applying these schemes results in significant computational overhead without any accompanying, additional benefit. In this paper we present a novel adaptive active contour scheme (AdACM) that combines boundary and region based energy terms with a shape prior in a multi level set formulation. To reduce the computational overhead, the shape prior term in the variational formulation is only invoked for those instances in the image where overlaps between objects are identified; these overlaps being identified via a contour concavity detection scheme. By not having to invoke all 3 terms (shape, boundary, region) for segmenting every object in the scene, the computational expense of the integrated active contour model is dramatically reduced, a particularly relevant consideration when multiple objects have to be segmented on very large histopathological images. The AdACM was employed for the task of segmenting nuclei on 80 prostate cancer tissue microarray images. Morphological features extracted from these segmentations were found to able to discriminate different Gleason grade patterns with a classification accuracy of 84% via a Support Vector Machine classifier. On average the AdACM model provided 100% savings in computational times compared to a non-optimized hybrid AC model involving a shape prior."
1335037,15226,9099,Constraint-optimized keypoint inhibition/insertion attack: security threat to scale-space image feature extraction,2012,"Scale-space image feature extraction (SSIFE) has been widely adopted in broad areas due to its powerful resilience to attacks. However, the security threat to SSIFE-based applications, which will be addressed in this paper, is relatively unexplored.   The security threat to SSIFT (called ST-SSIFE), composed of a constrained-optimization keypoint inhibition attack (KIHA) and a keypoint insertion attack (KISA), is specifically designed in this paper for scale-space feature extraction methods, such as SIFT and SURF.   In ST-SSIFE, KIHA aims at making a fool of feature extraction protocols in that the detection rules are purposely violated so as to suppress the existence of a local maximum around a local region.   We show that KIHA can be accomplished quickly via Lagrange multiplier but the resultant new keypoint generation (NKG) problem can be solved via Karush Kuhn Tucker (KKT) conditions.   In order to leverage among keypoint removal with minimum distortion, suppression of NKG, and complexity, we further present a hybrid scheme of integrating Lagrange multiplier and KKT conditions.   On the other hand, KISA is designed via an efficient coarse-to-fine descriptor matching strategy to yield fake feature points so as to create false positives.   Experiments, conducted on keypoint removal rate evaluation and an image copy detection method operating on a web-scale image database as a case study, demonstrate the feasibility of our method."
2563235,15226,8884,"DC proposal: enriching unstructured media content about events to enable semi-automated summaries, compilations, and improved search by leveraging social networks",2011,"Mobile devices like smartphones together with social networks enable people to generate, share, and consume enormous amounts of media content. Common search operations, for example searching for a music clip based on artist name and song title on video platforms such as YouTube, can be achieved both based on potentially shallow humangenerated metadata, or based on more profound content analysis, driven by Optical Character Recognition (OCR) or Automatic Speech Recognition (ASR). However, more advanced use cases, such as summaries or compilations of several pieces of media content covering a certain event, are hard, if not impossible to fulfill at large scale. One example of such event can be a keynote speech held at a conference, where, given a stable network connection, media content is published on social networks while the event is still going on.#R##N##R##N#In our thesis, we develop a framework for media content processing, leveraging social networks, utilizing the Web of Data and fine-grained media content addressing schemes like Media Fragments URIs to provide a scalable and sophisticated solution to realize the above use cases: media content summaries and compilations. We evaluate our approach on the entity level against social media platform APIs in conjunction with Linked (Open) Data sources, comparing the current manual approaches against our semi-automated approach. Our proposed framework can be used as an extension for existing video platforms."
2019449,15226,22130,Randomized Support Vector Forest,2014,"Based on the structural risk minimization principle, the linear SVM aiming at finding the linear decision plane with the maximal margin in the input space has gained increasing popularity due to its generalizability, efficiency and acceptable performance. However, rarely training data are evenly distributed in the input space [1], which leads to a high global VC confidence [4], downgrading the performance of the linear SVM classifier. Partitioning the input space in tandem with local learning may alleviate the unevenly data distribution problem. However, the extra model complexity introduced by partitioning frequently leads to overfitting. To solve this problem, we proposed a new supervised learning algorithm, Randomized Support Vector Forest (RSVF): Many partitions of the input space are constructed with partitioning regions amenable to the corresponding linear SVMs. The randomness of the partitions is injected through random feature selection and bagging. This partition randomness prevents the overfitting introduced by the over-complicated partitioning. We extensively evaluate the performance of the RSVF on several benchmark datasets, originated from various vision applications, including the four UCI datasets, the letter dataset, the KTH and the UCF sports dataset, and the Scene-15 dataset. The proposed RSVF outperforms linear SVM , kernel SVM, Random Forests (RF), and a local learning algorithm, SVM-KNN, on all of the evaluated datasets. The classification speed of the RSVF is comparable to linear SVM."
634395,15226,11052,Block-Sparse RPCA for consistent foreground detection,2012,"Recent evaluation of representative background subtraction techniques demonstrated the drawbacks of these methods, with hardly any approach being able to reach more than 50% precision at recall level higher than 90%. Challenges in realistic environment include illumination change causing complex intensity variation, background motions (trees, waves, etc.) whose magnitude can be greater than the foreground, poor image quality under low light, camouflage etc. Existing methods often handle only part of these challenges; we address all these challenges in a unified framework which makes little specific assumption of the background. We regard the observed image sequence as being made up of the sum of a low-rank background matrix and a sparse outlier matrix and solve the decomposition using the Robust Principal Component Analysis method. We dynamically estimate the support of the foreground regions via a motion saliency estimation step, so as to impose spatial coherence on these regions. Unlike smoothness constraint such as MRF, our method is able to obtain crisply defined foreground regions, and in general, handles large dynamic background motion much better. Extensive experiments on benchmark and additional challenging datasets demonstrate that our method significantly outperforms the state-of-the-art approaches and works effectively on a wide range of complex scenarios."
906654,15226,390,Derived input function from dynamic cardiac 18 F-FDG PET images in rodents based on the generalized Gaussian ICA model,2012,"The aim of this study is to develop a new method to extract input function (IF) from dynamic positron emission tomography (PET) data using the well known [ 18 F]-2-Deoxy-2-fluoro-d-glucose ( 18 F-FDG) for the determination of the myocardial metabolic rate of glucose (MMRG). In the case of cardiac studies, the IF can be obtained directly from the data by mean of a region of interest (ROI) drawn over the blood pool. However, in small animal imaging, hearts and arteries are small compared to the scanner spatial resolution. Consequently, vascular radioactivity is spilled over adjacent tissues and vice versa. As a result, curves obtained from ROIs are made up of a mixture of clean blood activity and the surrounding tissues activities. In this work, we assume that the elementary activities of the ventricular blood pool and the myocardium are spatially independent and that dynamic PET image frames are composed of a mixture of them. We use the independent component analysis (ICA) and a Generalized Gaussian Distribution (GGD) model in order to reveal the unknown de-mixing matrix. The derived ICA-IF are then compared to IF derived from invasive arterial blood samples. Results display similar shapes to those obtained by arterial sampling. Moreover, the MMRG calculated using the ICA-IF and two-compartment model correlated well with the results obtained from the sampled IF (2% difference only)."
87854,15226,9004,Hierarchical Ensemble of Multi-level Classifiers for Diagnosis of Alzheimer's Disease *,2012,"Pattern classification methods have been widely studied for analysis of brain images to decode the disease states, such as diagnosis of Alzheimer's dis- ease (AD). Most existing methods aimed to extract discriminative features from neuroimaging data and then build a supervised classifier for classification. How- ever, due to the rich imaging features and small sample size of neuroimaging da- ta, it is still challenging to make use of features to achieve good classification performance. In this paper, we propose a hierarchical ensemble classification algorithm to gradually combine the features and decisions into a unified model for more accurate classification. Specifically, a number of low-level classifiers are first built to transform the rich imaging and correlation-context features of brain image into more compact high-level features with supervised learning. Then, multiple high-level classifiers are generated, with each evaluating the high-level features of different brain regions. Finally, all high-level classifiers are combined to make final decision. Our method is evaluated using MR brain images on 427 subjects (including 198 AD patients and 229 normal controls) from Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experi- mental results show that our method achieves an accuracy of 92.04% and an AUC (area under the ROC curve) of 0.9518 for AD classification, demonstrat- ing very promising classification performance."
191906,15226,11052,Color Barcode Decoding in the Presence of Specular Reflection,2014,"Color Barcode Decoding in the Presence of Specular Reflection Homayoun Bagherinia and Roberto Manduchi University of California, Santa Cruz Abstract. Color barcodes enable higher information density with re- spect to traditional black and white barcodes. Existing technologies use small color palettes and display the colors in the palette in the barcode itself for easy and robust decoding. This solution comes at the cost of reduced information density due to the fact that the displayed reference colors cannot be used to encode information. We introduce a new ap- proach to color barcode decoding that uses a relatively large palettes (up to 24 colors) and a small number of reference colors (2 to 6) to be displayed in a barcode. Our decoding method specifically accounts for specular reflections using a dichromatic model. The experimental results show that our decoding algorithm achieves higher information rate with a very low probability of decoding error compared to previous approaches that use a color palette for decoding. Keywords: Color barcode decoding, dichromatic reflection model, subspace classification Introduction Barcodes can be characterized by their information rate, that is, by the number of bits that can be encoded within a certain barcode size. One way to increase a barcode’s information rate is through the use of color. By using a palette of N colors, a barcode can convey log 2 N times more bits than a traditional black and white barcode. The most successful example of color barcode is Microsoft Tag, which is based on HCCB (High Capacity Color Barcode) technology [1]. HCCB uses a grid of colored triangles with 4 colors to encode data. To ensure robust decoding, HCCB barcodes display the four colors in a set of “reference patches” at known positions in the barcode. Displaying the reference colors in the barcode enables simple decoding strate- gies. For example, one may compare each color patch to the reference colors, and select the reference color that is closest to the color of the patch. At the same time, displaying all colors in the palette may be counterproductive, in terms of information rate, when large palettes are used [2]. In other words, for large palette size N , the savings produced by a large variety of color palette are off- set by the need to display all colors in the palette. Based on this observation, Bagherinia and Manduchi [2] proposed the use of fairly large palettes with a lim- ited number of reference colors displayed in the barcode. Rather than comparing"
1310677,15226,9078,Compressive Distance Classifier Correlation Filter,2013,"Compressed Sensing (CS) is seen as the pathway to increase the efficiency of sensor systems such as MRI, SAR and SAS while avoiding the huge costs and related processing accompanying high-resolution data acquisition. While there has been a surge in the number of sensor systems and related algorithms using CS, target/object recognition in the sensing domain which offers numerous advantages, is a rather nascent field. The state-of-the-art in this field includes the Smashed Filter (SF), which is a reduced dimensionality maximum likelihood classifier. Nevertheless, the accuracy of the filter remains low for practical applications, especially with variations in scale, translation and rotation in the test data. This paper offers a new type of filter - called the Compressive Distance Classifier Correlation Filter (CDCCF), which applies a transformation in the CS domain thereby increasing the distance between intra-class correlation peaks while reducing the distance between inter-class correlation peaks and is based on the Restricted Isometry Property (RIP) of the compressed manifold and the Johnson Lindenstrauss Lemma. Results presented show that the accuracy of the CDCCF filter is about 70% on a 12 class test data set, which is over a two-fold increase in accuracy over the SF. Confusion matrices, measures of ROC, Mean Average Precision and Accuracy demonstrate the robust performance of the algorithm over SF across different compressive sampling resolutions."
2041899,15226,20515,Weighted adaptive Hough and ellipsopolar transforms for real-time iris segmentation,2012,"Efficient and robust segmentation of less intrusively or non-cooperatively captured iris images is still a challenging task in iris biometrics. This paper proposes a novel two-stage algorithm for the localization and mapping of iris texture in images of the human eye into Daugman's doubly dimensionless polar coordinates. Motivated by the growing demand for real-time capable solutions, coarse center detection and fine boundary localization usually combined in traditional approaches are decoupled. Therefore, search space at each stage is reduced without having to stick to simpler models. Another motivation of this work is independence of sensors. A comparison of reference software on different datasets highlights the problem of database-specific optimizations in existing solutions. This paper instead proposes the application of Gaussian weighting functions to incorporate model-specific prior knowledge. An adaptive Hough transform is applied at multiple resolutions to estimate the approximate position of the iris center. Subsequent polar transform detects the first elliptic limbic or pupillary boundary, and an ellipsopolar transform finds the second boundary based on the outcome of the first. This way, both iris images with clear limbic (typical for visible-wavelength) and with clear pupillary boundaries (typical for near infrared) can be processed in a uniform manner."
2199102,15226,21106,Recursive Estimation of the Stein Center of SPD Matrices and Its Applications,2013,"Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negatively-curved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance - which is the square root of the LogDet divergence - that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable comparisons to other competing methods."
1441934,15226,21056,Rate adaptation for adaptive HTTP streaming,2011,"Recently, HTTP has been widely used for the delivery of real-time multimedia content over the Internet, such as in video streaming applications. To combat the varying network resources of the Internet, rate adaptation is used to adapt the transmission rate to the varying network capacity. A key research problem of rate adaptation is to identify network congestion early enough and to probe the spare network capacity. In adaptive HTTP streaming, this problem becomes challenging because of the difficulties in differentiating between the short-term throughput variations, incurred by the TCP congestion control, and the throughput changes due to more persistent bandwidth changes.   In this paper, we propose a novel rate adaptation algorithm for adaptive HTTP streaming that detects bandwidth changes using a smoothed HTTP throughput measured based on the segment fetch time (SFT). The smoothed HTTP throughput instead of the instantaneous TCP transmission rate is used to determine if the bitrate of the current media matches the end-to-end network bandwidth capacity. Based on the smoothed throughput measurement, this paper presents a receiver-driven rate adaptation method for HTTP/TCP streaming that deploys a step-wise increase/ aggressive decrease method to switch up/down between the different representations of the content that are encoded at different bitrates. Our rate adaptation method does not require any transport layer information such as round trip time (RTT) and packet loss rates which are available at the TCP layer. Simulation results show that the proposed rate adaptation algorithm quickly adapts to match the end-to-end network capacity and also effectively controls buffer underflow and overflow."
1383325,15226,23735,RGB-D Fusion: Real-time Robust Tracking and Dense Mapping with RGB-D Data Fusion,2014,"We present RGB-D Fusion, a framework which robustly tracks and reconstructs dense textured surfaces of scenes and objects by integrating both color and depth images streamed from a RGB-D sensor into a global colored volume in real-time. To handle failure of the ICP-based tracking approach, KinectFusion, due to the lack of sufficient geometric information, we propose a novel approach which registers the input RGB-D image with the colored volume by pho- tometric tracking and geometric alignment. We demonstrate the strengths of the proposed approach compared with the ICP-based approach and show superior performance of our algorithm with real-world data. I. INTRODUCTION Real-time photo-realistic 3D reconstruction of real world scenes and objects plays an important role for a variety of applications such as robotics, augmented reality, human computer interaction, and entertainment. One of the main requirements in the robot interaction with the environment is the 3D reconstruction and representation of real world environments. With the invention of the low-cost Microsoft Kinect sensor as an representative of a consumer-grade range sensing device, the high-resolution range images aligned with pho- tometric information become relatively easily available. The Kinect sensor opens up new possibilities to build a low- cost 3D modeling system with well-studied range mapping technologies."
1315228,15226,9099,Hybrid robotic/virtual pan-tilt-zom cameras for autonomous event recording,2013,"We present a method to generate aesthetic video from a robotic camera by incorporating a virtual camera operating on a delay, and a hybrid controller which uses feedback from both the robotic and virtual cameras. Our strategy employs a robotic camera to follow a coarse region-of-interest identified by a realtime computer vision system, and then resamples the captured images to synthesize the video that would have been recorded along a smooth, aesthetic camera trajectory. The smooth motion trajectory is obtained by operating the virtual camera on a short delay so that perfect knowledge of immediate future events is known. Previous autonomous camera installations have employed either robotic cameras or stationary wide-angle cameras with subregion cropping. Robotic cameras track the subject using realtime sensor data, and regulate a smoothness-latency trade-off through control gains. Fixed cameras post-process the data and suffer significant reductions in image resolution when the subject moves freely over a large area.   Our approach provides a solution for broadcasting events from locations where camera operators cannot easily access. We can also offer broadcasters additional actuated camera angles without the overhead of additional human operators. Experiments on our prototype system for college basketball illustrate how our approach better mimics human operators compared to traditional robotic control approaches, while avoiding the loss in resolution that occurs from fixed camera system."
241374,15226,9004,Segmenting Hippocampus from Infant Brains by Sparse Patch Matching with Deep-Learned Features,2014,"Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1- and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the inter-patch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods."
2668725,15226,22113,Learning compact visual descriptor for low bit rate mobile landmark search,2011,"In this paper, we propose to extract a compact yet discriminative visual descriptor directly on the mobile device, which tackles the wireless query transmission latency in mobile landmark search. This descriptor originates from offline learning the location contexts of geo-tagged Web photos from both Flickr and Panoramio with two phrases: First, we segment the landmark photo collections into discrete geographical regions using a Gaussian Mixture Model [Stauffer et al., 2000]. Second, a ranking sensitive vocabulary boosting is introduced to learn a compact codebook within each region. To tackle the locally optimal descriptor learning caused by imprecise geographical segmentation, we further iterate above phrases incorporating the feedback of an entropy based descriptor compactness into a prior distribution to constrain the Gaussian mixture modeling. Consequently, when entering a specific geographical region, the codebook in the mobile device is downstream adapted, which ensures efficient extraction of compact descriptors, its low bit rate transmission, as well as promising discrimination ability. We descriptors to both HTC and iPhone mobile phones, testing landmark search over one million images in typical areas like Beijing, New York, and Barcelona, etc. Our descriptor outperforms alternative compact descriptors [Chen et al., 2009][Chen et al., 2010][Chandrasekhar et al., 2009a][Chandrasekhar et al., 2009b] with a large margin."
1612872,15226,390,Identifying functional connectomics abnormality in attention deficit hyperactivity disorder,2013,"Attention deficit hyperactivity disorder (ADHD) is one of the most common psychiatric, neurodevelopmental and neurobehavioral disorders occurring in the childhood of human. The typical symptoms are characterized as excessive inattention, hyperactivity/impulsiveness or their combination. Traditionally, it has been thought to be a partial dysfunction caused by prefrontal-striatal circuits. Recent studies, however, indicate the involvement of other brain regions, including the occipital cortex and temporal cortex. Though researchers have already realized the importance of evaluation for the whole brain and multiple structural/functional networks, it is still very challenging to achieve consistent and comparable results across different labs. In the present paper, through the predefined cortical landmarks which possess group-wise structural consistency and intrinsic correspondence, we have the opportunity to access the whole brain and to reveal large-scale structural/functional connectomics abnormalities in ADHD. Our results not only confirmed that the major white matter (WM) alterations occurred at the anterior and posterior regions, but also indicate that hyper-interactions mainly exist between the emotion network and memory related networks. Our results also showed that hypo-interactions are found between the emotion and execution/attention networks. Hence, we hypothesize that the abnormal interactions associated with emotion network contribute to the dysfunction within the ADHD brain."
1913728,15226,11470,Optimized frame structure for interactive light field streaming with cooperative caching,2011,"Light field is a large set of spatially correlated images of the same static scene captured using a 2D array of closely spaced cameras. Interactive light field streaming is the application where a client continuously requests successive light field images along a view trajectory of her choosing, and in response the server transmits appropriate data for the client to correctly reconstruct desired images. The technical challenge is how to encode captured light field images into a reasonably sized frame structure a priori (without knowing eventual clients' view trajectories), so that during streaming session, expected server transmission rate can be minimized, while satisfying client's view requests. In this paper, we design efficient frame structures, using I-frames, redundant P-frames and distributed source coding (DSC) frames as building blocks, to optimally trade off storage size of the frame structure with expected server transmission rate. The key novelty is to optimize structures in such a way that decoded images in caches of neighboring cooperative peers, connected together via a secondary network such as ad hoc WLAN for content sharing, can be reused to further decrease the server-to-client transmission rate. We formulate the structure design problem as a Lagrangian minimization, and propose fast heuristics to find near-optimal solutions. Experimental results show that the expected server streaming rate can be reduced by up to 83% compared to an I-frame-only structure, at less than twice the storage required."
14283,15226,9004,Tensor Total-Variation Regularized Deconvolution for Efficient Low-Dose CT Perfusion,2014,"Acute brain diseases such as acute stroke and transit ischemic attacks are the leading causes of mortality and morbidity worldwide, responsible for 9% of total death every year. 'Time is brain' is a widely accepted concept in acute cerebrovascular disease treatment. Efficient and accurate computational frame- work for hemodynamic parameters estimation can save critical time for throm- bolytic therapy. Meanwhile the high level of accumulated radiation dosage due to continuous image acquisition in CT perfusion (CTP) raised concerns on patient safety and public health. However, low-radiation will lead to increased noise and artifacts which require more sophisticated and time-consuming algorithms for robust estimation. We propose a novel efficient framework using tensor total- variation (TTV) regularization to achieve both high efficiency and accuracy in deconvolution for low-dose CTP. The method reduces the necessary radiation dose to only 8% of the original level and outperforms the state-of-art algorithms with estimation error reduced by 40%. It also corrects over-estimation of cerebral blood flow (CBF) and under-estimation of mean transit time (MTT), at both nor- mal and reduced sampling rate. An efficient computational algorithm is proposed to find the solution with fast convergence."
1501541,15226,9078,"Multimodal semi-supervised image classification by combining tag refinement, graph-based learning and support vector regression",2013,"We investigate an image classification task where the training images come along with tags, but only a subset being labeled, and the goal is to predict the class label of test images without tags. This task is crucial for image search engine on photo sharing Web sites. In previous work, it is handled by first learning a multiple kernel learning classifier using both image content and tags to score unlabeled training images, and then building up a least-squares regression (LSR) model on visual features to predict the label of test images. However, there exist three important issues in the task: (1) Image tags on photo sharing Web sites tend to be inaccurate and incomplete, and thus refining them is beneficial; (2) Supervised learning with a limited number of labeled samples may be unreliable to some extent, while a graph-based semi-supervised approach can be adopted by also considering similarities of unlabeled data; (3) LSR is established upon centered visual kernel columns and breaks the symmetry of kernel matrix, whereas support vector regression can readily use the original visual kernel and thus leverage its full power. To handle the task more effectively, we propose to combine tag refinement, graph-based learning and support vector regression together. Experimental results on the PASCAL VOC'07 and MIR Flickr datasets show the superior performance of the proposed approach."
1785045,15226,10228,Adaptive Bit Rate capable video caching and scheduling,2013,"Adaptive Bit Rate Streaming (ABR) has become a popular video delivery technique, credited to improving the quality of delivered video on wireless networks. At the same time, recent research has shown video caching in the Radio Access Network (RAN) can be a promising way to increase capacity of the network while reducing video latency and improving video quality of experience. In this work, we investigate the opportunities and challenges of combining the advantages of ABR streaming and RAN caching to maximize the video capacity of wireless networks. Since with ABR, each video is divided into multiple segments, chunks, and each chunk can be requested at different bit rates, the caching requirements are very different and challenging; a cache hit will require not only the presence of a specific video chunk, but also the availability of the desired bit rate. One way to solve this problem is to cache all variants of a video, but this approach may significantly increase storage and backhaul requirements or reduce the number of unique videos that can be cached. In this paper, we introduce a framework consisting of rate adaptation algorithm, video caching and processing within the wireless cloud, with the aim to improve video capacity of the wireless network and satisfy or exceed QoE of each video request. To achieve this goal, we propose a new ABR algorithm, along with an ABR aware Least Recently Used (LRU) caching policy with Processing (ABRLRU-P) to support caching of video chunks with different bit rates. Using our MATLAB statistical simulation framework, we demonstrate a capacity improvement of up to 83% when ABR is used with RAN caching and processing using the ABR-LRU-P policy compared with using no RAN caching and video processing. Further, using ABR along with ABRLRU-P caching policy can improve the capacity by 68% compared with using ABR and a straightforward static LRU caching policy which fetches all video bit rate versions of a video chunk upon a cache miss."
1549552,15226,9099,Snap & play: auto-generate personalized find-the-difference mobile game,2011,"According to the year 2010 report of the Entertainment Software Association [5], 42% of USA heads of households reported playing games on mobile devices, rising quickly from the 20% in 2002 and bringing huge market for mobile games. In this paper, by taking the popular game, Find-the-Difference (FiDi), as a concrete example, we explore new mobile game design principles and techniques for enhancing player's gaming experience in personalized, automatic, and dynamic aspects. Unlike traditional FiDi game, where image pairs (source image vs. target image) with M different patches are manually produced by game developer and players may feel boring or cheat after practicing all image pairs, our proposed Personalized FiDi (P-FiDi) mobile game may be played under a new Snap & Play mode. The player may first take photos with one s mobile device (or select from one's own albums). Then, these photos serve as source images, and the P-FiDi system automatically generates the counterpart target images by sequential operations of aesthetic image quality enhancement, image patch and differentiating style joint selection, music adaptation, dynamic difficulty level determination, and ultimate automatic image editing with a rich set of popular differentiating styles used in traditional FiDi game. Finally, the player enjoys the unique gaming with one's own (instant) photos and music, and the freedom to have new gaming image pairs any time. The user studies show that the P-FiDi mobile game is satisfying in terms of player experience."
2356783,15226,9099,Recognition of complex events in open-source web-scale videos: a bottom up approach,2013,"Recognition of complex events in unconstrained Internet videos is a challenging research problem. In this symposium proposal, we present a systematic decomposition of complex events into hierarchical components and make an in-depth analysis of how existing research are being used to cater to various levels of this hierarchy. We also identify three key stages where we make novel contributions which are necessary to not only improve the overall recognition performance, but also develop richer understanding of these events. At the lowest level, our contributions include (a) compact covariance descriptors of appearance and motion features used in sparse coding framework to recognize realistic actions and gestures, and (b) a Lie-algebra based representation of dominant camera motion present in video shots which can be used as a complementary feature for video analysis. In the next level, we propose an (c) efficient maximum likelihood estimate based representation from low-level features computed from videos which demonstrates state of the art performance in large scale visual concept detection, and finally, we propose to (d) model temporal interactions between concepts detected in video shots through two new discriminative feature spaces derived from Linear dynamical systems which eventually boosts event recognition performance. In all cases, we conduct thorough experiments to demonstrate promising performance gains over some of the prominent approaches."
2205414,15226,9099,Accelerating SURF detector on mobile devices,2012,"Running a SURF (Speeded Up Robust Features) detector on mobile devices remains too slow to support emerging applications such as mobile augmented reality. Porting it without adapting the algorithm to account for mobile platform limitations could result in significant runtime degradation. In this paper, we identify two mismatches between the SURF algorithm and the mobile hardware that cause substantial slow-down of the point detection process: 1) mismatch between the data access pattern and the small cache size, and 2) mismatch between the huge amount of branches and high pipeline hazard penalty. To address the mismatches, we propose two techniques: tiled SURF and gradient moment based orientation assignment. Tiled SURF improves data locality and greatly reduces memory traffic. A method for determining the optimal tile sizes, named content-aware tiling, is designed to minimize runtime and maximize detection accuracy. To avoid the penalties caused by pipeline hazards, we replace the original orientation operator with branching-free gradient moment computations. The proposed techniques are tested on three mobile platforms. Comparing to the original SURF, the accelerated SURF achieves a 6x~8x speedup without sacrificing recognition accuracy. Meanwhile, it achieves 59%~80% reductions in the runtime ratio of the detector running on mobile platforms compared with on x86-based PCs."
2161275,15226,11052,"Neural Network Fusion of Color, Depth and Location for Object Instance Recognition on a Mobile Robot",2014,"The development of mobile robots for domestic assistance re-quires solving problems integrating ideas from different fields of research like computer vision, robotic manipulation, localization and mapping. Semantic mapping, that is, the enrichment a map with high-level infor-mation like room and object identities, is an example of such a complex robotic task. Solving this task requires taking into account hard software and hardware constraints brought by the context of autonomous mobile robots, where short processing times and low energy consumption are mandatory. We present a light-weight scene segmentation and object in-stance recognition algorithm using an RGB-D camera and demonstrate it in a semantic mapping experiment. Our method uses a feed-forward neural network to fuse texture, color and depth information. Running at 3 Hz on a single laptop computer, our algorithm achieves a recognition rate of 97% in a controlled environment, and 87% in the adversarial con-ditions of a real robotic task. Our results demonstrate that state of the art recognition rates on a database does not guarantee performance in a real world experiment. We also show the benefit in these conditions of fusing several recognition decisions and data from different sources. The database we compiled for the purpose of this study is publicly available."
172324,15226,21106,Fast and adaptive deep fusion learning for detecting visual objects,2012,"Currently, object tracking/detection is based on a shallow learning paradigm; they locally process features to build an object model and then they apply adaptive methodologies to estimate model parameters. However, such an approach presents the drawback of losing the whole picture information required to maintain a stable tracking for long time and high visual changes. To overcome these obstacles, we need a deep information fusion framework. Deep learning is a new emerging research area that simulates the efficiency and robustness by which the humans' brain represents information; it deeply propagates data into complex hierarchies. However, implementing a deep fusion learning paradigm in a machine presents research challenges mainly due to the highly non-linear structures involved and the curse of dimensionality. Another difficulty which is critical in computer vision applications is that learning should be self adapted to guarantee stable object detection over long time spans. In this paper, we propose a novel fast (in real-time) and adaptive information fusion strategy that exploits the deep learning paradigm. The proposed framework integrates optimization strategies able to update in real-time the non-linear model parameters according in a way to trust, as much as possible, the current changes of the environment, while providing a minimal degradation of the previous gained experience."
1680933,15226,20338,Measurement and analysis of a large scale commercial mobile internet TV system,2011,"Large scale, Internet based mobile TV deployment presents both tremendous opportunities and challenges for mobile operators and technology providers. This paper presents a measurement based study on a large scale mobile TV service offering in China. Within the one month measurement period, our dataset captured over 1 million unique mobile devices and more than 49 million video sessions. Analysis showed that mobile viewing patterns are different from that of landline based IPTV and VoD systems. In particular, the average viewing time is significantly shorter, and the channel popularity distribution is more skewed towards top ranked channels than that of landline based systems. For the channel sojourn time, the distribution follows a piecewise model, which combines lognormal and pareto distribution. The lognormal part, which fits the majority of video sessions, more closely resembles the mobile phone call holding time, rather than the power law distribution in the landline IPTV case. In comparing the 3G and WiFi access methods, we found that users exhibit different behaviors when accessing from different networks. In 3G networks, where users are subject to data charge, users tend to have shorter channel sojourn time and prefer lower bit-rate channels. The parameters of the distributions are also different. Understanding these user behaviors and their implications on network traffic are critical for the success of future mobile TV industry."
1847946,15226,390,A new nonrigid registration framework for improved visualization of transmural perfusion gradients on cardiac first-pass perfusion MRI,2012,"A new framework for accurate registration of the segmented left ventricle (LV) on cardiac first-pass magnetic resonance imaging (FP-MRI) to precisely analyze the myocardial transit of contrast agent, especially in the ischemically damaged heart, is proposed. Due to the continuous physiological motion of the heart that causes the LV wall to change shape significantly, within the same scan, at the same cross section, we propose a new registration methodology that involves three steps: (i) global target-to-reference frame-to-frame alignment based on maximizing normalized mutual information (NMI); (ii) local alignment based on using a B-splines transformation model that maximizes a new similarity function that accounts for the 1 st - and 2-order NMI between the globally aligned frames followed by (iii) a refinement step which is based on deforming each pixel of the target wall over evolving closed equi-spaced contours (iso-contours) to closely match the reference wall. Respective iso-contours in both reference and target frames are matched based on solving the Laplace equation. We have tested our framework on 20 FP-MRI datasets that have been collected from patients with ischemic damage resulting from heart attacks and who are undergoing experimental therapy, and have documented an improvement in the visualization and display of perfusion-related indexes."
1493877,15226,9078,Arithmetic edge coding for arbitrarily shaped sub-block motion prediction in depth video compression,2012,"Depth map compression is important for compact representation of 3D visual data in “texture-plus-depth” format, where texture and depth maps of multiple closely spaced viewpoints are encoded and transmitted. A decoder can then freely synthesize any chosen inter-mediate view via depth-image-based rendering (DIBR) using neighboring coded texture and depth maps as anchors. In this work, we leverage on the observation that “pixels of similar depth have similar motion” to efficiently encode depth video. Specifically, we divide a depth block containing two zones of distinct values (e.g., foreground and background) into two sub-blocks along the dividing edge before performing separate motion prediction. While doing such arbitrarily shaped sub-block motion prediction can lead to very small prediction residuals (resulting in few bits required to code them), it incurs an overhead to losslessly encode dividing edges for sub-block identification. To minimize this overhead, we first devise an edge prediction scheme based on linear regression to predict the next edge direction in a contiguous contour. From the predicted edge direction, we assign probabilities to each possible edge direction using the von Mises distribution, which are subsequently inputted to a conditional arithmetic codec for entropy coding. Experimental results show an average overall bitrate reduction of up to 30% over classical H.264 implementation."
465186,15226,22113,Learning discriminative representations from RGB-D video data,2013,"Recently, the low-cost Microsoft Kinect sensor, which can capture real-time high-resolution RGB and depth visual information, has attracted increasing attentions for a wide range of applications in computer vision. Existing techniques extract hand-tuned features from the RGB and the depth data separately and heuristically fuse them, which would not fully exploit the complementarity of both data sources. In this paper, we introduce an adaptive learning methodology to automatically extract (holistic) spatio-temporal features, simultaneously fusing the RGB and depth information, from RGB-D video data for visual recognition tasks. We address this as an optimization problem using our proposed restricted graph-based genetic programming (RGGP) approach, in which a group of primitive 3D operators are first randomly assembled as graph-based combinations and then evolved generation by generation by evaluating on a set of RGB-D video samples. Finally the best-performed combination is selected as the (near-)optimal representation for a pre-defined task.#R##N##R##N#The proposed method is systematically evaluated on a new hand gesture dataset, SKIG, that we collected ourselves and the public MSR Daily Activity 3D dataset, respectively. Extensive experimental results show that our approach leads to significant advantages compared with state-of-the-art hand-crafted and machine-learned features."
1804324,15226,390,Manifold-constrained embeddings for the detection of white matter lesions in brain MRI,2012,"Brain abnormalities such as white matter lesions (WMLs) are not only linked to cerebrovascular disease, but also with normal aging, diabetes and other conditions increasing the risk for cerebrovascular pathologies. Obtaining quantitative measures which assesses the degree or probability of WML in patients is important for evaluating disease burden, and for evaluating its progression and response to interventions. In this paper, we introduce a novel approach for detecting the presence of WMLs in periventricular areas of the brain using manifold-constrained embeddings. The proposed method uses locally linear embedding (LLE) to create ”normality” distributions in 12 locations of the brain where deviations from the manifolds are estimated by calculating geodesic distances along locally linear planes in the embedding. A smooth mapping function approximating the relationship between ambient and manifold spaces as a joint distribution maps unseen test images in the intrinsic space. We create a set of low-dimensional embeddings from 876 patches of healthy tissue in 73 subjects and test it on 396 patches imaging both WML and healthy areas in 33 subjects with diabetes. Experiments highlight the need of nonlinear techniques to learn the studied data with detection rates over 85% in true-positives, and the relevance of the computed distance for comparing individuals to a specific pathological pattern."
1104208,15226,23735,Force-sensing surgical grasper enabled by pop-up book MEMS,2013,"The small scale of minimally-invasive surgery (MIS) presents significant challenges to developing robust, smart, and dexterous tools for manipulating millimeter and sub-millimeter anatomical structures (vessels, nerves) and surgical equipment (sutures, staples). Robotic MIS systems offer the potential to transform this medical field by enabling precise repair of these miniature tissue structures through the use of teleoperation and haptic feedback. However, this effort is currently limited by the inability to make robust and accurate MIS end effectors with integrated force and contact sensing. In this paper, we demonstrate the use of the novel Pop-Up Book MEMS manufacturing method to fabricate the mechanical and sensing elements of an instrumented MIS grasper. A custom thin-foil strain gage was manufactured in parallel with the mechanical components of the grasper to realize a fully-integrated electromechanical system in a single manufacturing step, removing the need for manual assembly, bonding and alignment. In preliminary experiments, the integrated grasper is capable of resolving forces as low as 30 mN, with a sensitivity of approximately 408 mV/N. This level of performance will enable robotic surgical systems that can handle delicate tissue structures and perform dexterous procedures through the use of haptic feedback guidance."
1934402,15226,9616,A New Approach of Arc Skeletonization for Tree-like Objects Using Minimum Cost Path,2014,"Traditional arc skeletonization algorithms using the principle of Blum's transform, often, produce unwanted spurious branches due to boundary irregularities and digital effects on objects and other artifacts. This paper presents a new robust approach of extracting arc skeletons for three-dimensional (3-D) elongated fuzzy objects, which avoids spurious branches without requiring post-pruning. Starting from a root voxel, the method iteratively expands the skeleton by adding a new branch in each iteration that connects the farthest voxel to the current skeleton using a minimum-cost geodesic path. The path-cost function is formulated using a novel measure of local significance factor defined by fuzzy distance transform field, which forces the path to stick to the centerline of the object. The algorithm terminates when dilated skeletal branches fill the entire object volume or the current farthest voxel fails to generate a meaningful branch. Accuracy of the algorithm has been evaluated using computer-generated blurred and noisy phantoms with known skeletons. Performance of the method in terms of false and missing skeletal branches, as defined by human expert, has been examined using in vivo CT imaging of human intrathoracic airways. Experimental results from both experiments have established the superiority of the new method as compared to a widely used conventional method in terms of accuracy of medialness as well as robustness of true and false skeletal branches."
1089907,15226,9078,Using distributed source coding and depth image based rendering to improve interactive multiview video access,2011,"Multiple-views video is commonly believed to be the next significant achievement in video communications, since it enables new exciting interactive services such as free viewpoint television and immersive teleconferencing. However the interactivity requirement (i.e. allowing the user to change the viewpoint during video streaming) involves a trade-off between storage and bandwidth costs. Several solutions have been proposed in the literature, using redundant predictive frames, Wyner-Ziv frames, or a combination of them. In this paper, we adopt distributed video coding for interactive multiview video plus depth (MVD), taking advantage of depth image based rendering (DIBR) and depth-aided inpainting to fill the occlusion areas. To the authors' best knowledge, very few works in interactive MVD consider the problem of continuity of the playback during the switching among streams. Therefore we survey the existing solutions, we propose a set of techniques for MVD coding and we compare them. As main results, we observe that DIBR can help in rate reduction (up to 13.36% for the texture video and up to 8.67% for the depth map, wrt the case where DIBR is not used), and we also note that the optimal strategy to combine DIBR and distributed video coding depends on the position of the switching time into the group of pictures. Choosing the best technique on a frame-to-frame basis can further reduce the rate from 1% to 6%."
2521370,15226,11052,Video Registration to SfM Models,2014,"Registering image data to Structure from Motion (SfM) point clouds is widely used to find precise camera location and orien- tation with respect to a world model. In case of videos one constraint has previously been unexploited: temporal smoothness. Without tem- poral smoothness the magnitude of the pose error in each frame of a video will often dominate the magnitude of frame-to-frame pose change. This hinders application of methods requiring stable poses estimates (e.g. tracking, augmented reality). We incorporate temporal constraints into the image-based registration setting and solve the problem by pose reg- ularization with model fitting and smoothing methods. This leads to accurate, gap-free and smooth poses for all frames. We evaluate differ- ent methods on challenging synthetic and real street-view SfM data for varying scenarios of motion speed, outlier contamination, pose estima- tion failures and 2D-3D correspondence noise. For all test cases a 2 to 60-fold reduction in root mean squared (RMS) positional error is ob- served, depending on pose estimation difficulty. For varying scenarios, different methods perform best. We give guidance which methods should be preferred depending on circumstances and requirements."
2692354,15226,10994,Unsupervised Footwear Impression Analysis and Retrieval from Crime Scene Data,2014,"Footwear impressions are one of the most frequently securedtypes of evidence at crime scenes. For the investigation of crime seriesthey are among the major investigative notes. In this paper, we introducean unsupervised footwear retrieval algorithm that is able to cope withunconstrained noise conditions and is invariant to rigid transformations.A main challenge for the automated impression analysis is the separationof the actual shoe sole information from the structured backgroundnoise. We approach this issue by the analysis of periodic patterns. Givenunconstrained noise conditions, the redundancy within periodic patternsmakes them the most reliable information source in the image. In thiswork, we present four main contributions: First, we robustly measurelocal periodicity by fitting a periodic pattern model to the image. Second,based on the model, we normalize the orientation of the image andcompute the window size for a local Fourier transformation. In this way,we avoid distortions of the frequency spectrum through other structuresor boundary artefacts. Third, we segment the pattern through robustpoint-wise classification, making use of the property that the amplitudesof the frequency spectrum are constant for each position in a periodicpattern. Finally, the similarity between footwear impressions is measuredby comparing the Fourier representations of the periodic patterns. Wedemonstrate robustness against severe noise distortions as well as rigidtransformations on a database with real crime scene impressions. Moreover,we make our database available to the public, thus enabling standardizedbenchmarking for the first time."
2666719,15226,9004,Anatomy-Guided Discovery of Large-Scale Consistent Connectivity-Based Cortical Landmarks,2013,"Establishment of structural and functional correspondences across different brains is one of the most fundamental issues in the human brain map- ping field. Recently, several multimodal DTI/fMRI studies have demonstrated that consistent white matter fiber connection patterns can predict brain function and represent common brain architectures across individuals and populations, and along this direction, several approaches have been proposed to discover large-scale cortical landmarks with common structural connection profiles. However, an important limitation of previous approaches is that the rich ana- tomical information such as gyral/sulcal folding patterns has not been incorpo- rated into the landmark discovery procedure yet. In this paper, we present a novel anatomy-guided discovery framework that defines and optimizes a dense map of cortical landmarks that possess group-wise consistent anatomical and fiber connectional profiles. This framework effectively integrates reliable and rich anatomical, morphological, and fiber connectional information for land- mark initialization, optimization and prediction, which are formulated and solved as an energy minimization problem. Validation results based on fMRI data demonstrate that the identified 555 cortical landmarks are producible, predictable and exhibit accurate structural and functional correspondences across individuals and populations, offering a universal and individualized brain reference system for neuroimaging research."
1024410,15226,8502,Rotation estimation from cloud tracking,2014,"We address the problem of online relative orientation estimation from streaming video captured by a sky-facing camera on a mobile device. Namely, we rely on the detection and tracking of visual features attained from cloud structures. Our proposed method achieves robust and efficient operation by combining realtime visual odometry modules, learning based feature classification, and Kalman filtering within a robustness-driven data management framework, while achieving framerate processing on a mobile device. The relatively large 3D distance between the camera and the observed cloud features is leveraged to simplify our processing pipeline. First, as an efficiency driven optimization, we adopt a homography based motion model and focus on estimating relative rotations across adjacent keyframes. To this end, we rely on efficient feature extraction, KLT tracking, and RANSAC based model fitting. Second, to ensure the validity of our simplified motion model, we segregate detected cloud features from scene features through SVM classification. Finally, to make tracking more robust, we employ predictive Kalman filtering to enable feature persistence through temporary occlusions and manage feature spatial distribution to foster tracking robustness. Results exemplify the accuracy and robustness of the proposed approach and highlight its potential as a passive orientation sensor."
1316163,15226,9078,A statistical framework for the classification of infant DT images,2014,"This paper introduces a new adaptive atlas-based framework for the automated segmentation of different brain structures from infant diffusion tensor images (DTI). To model the brain images and their desired region maps, we used a joint Markov-Gibbs random field (MGRF) model that accounts for three image descriptors: (i) a 1 st -order visual appearance to describe the empirical distribution of DTI extracted features, (ii) an adaptive shape model, and (iii) a 3D spatially invariant 2 nd -order MGRF homogeneity descriptor. The 1 st -order visual appearance descriptor is accurately modeled using a linear combination of discrete Gaussians (LCDG) model having positive and negative components. The proposed adaptive shape model is constructed from a prior atlas database built using a subset of co-aligned training data sets that is adapted during the segmentation process guided by the visual appearance characteristics of several DTI features. To accurately account for the large inhomogeneity of infant brains, the homogeneity descriptor is modeled by a 2 nd -order translation and rotation invariant MGRF of region labels with analytically estimated potentials. The high accuracy of our segmentation approach was confirmed by testing it on 10 in-vivo infant DTI brain data sets using three metrics: the Dice similarity coefficient, the 95-percentile modified Hausdorff distance, and the absolute brain volume difference."
2226564,15226,21106,Joint Noise Level Estimation from Personal Photo Collections,2013,"Personal photo albums are heavily biased towards faces of people, but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality, others may be corrupted by noise. Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically, we compare geometrically and photo metrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels, which allows for a pair wise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally, we employ the estimated noise levels for automatic denoising using BM3D, and evaluate the quality of denoising on real-world photos through a user study."
1672066,15226,9589,Shedding light on the structure of internet video quality problems in the wild,2013,"The key role that video quality plays in impacting user engagement, and consequently providers' revenues, has motivated recent efforts in improving the quality of Internet video. This includes work on adaptive bitrate selection, multi-CDN optimization, and global control plane architectures. Before we embark on deploying these designs, we need to first understand the nature of video of quality problems to see if this complexity is necessary, and if simpler approaches can yield comparable benefits.   To this end, this paper is a first attempt to shed some light on the structure of video quality problems. Using measurements from 300 million video sessions over a two-week period, we identify recurrent problems using a hierarchical clustering approach over the space of client/session attributes (e.g., CDN, AS, connectivity). Our key findings are that: (1) a small number (2%) of critical clusters account for 83% of join failure problems (44--84% for other metrics); (2) many problem events (50%) persist for at least 2 hours; (3) a majority of these problems (e.g., 60% of join failures, 30--60% for other metrics) are related to content provider, CDN, or client ISP issues. Building on these insights, we evaluate the potential improvement by focusing on addressing these recurrent problems and find that fixing just 1% of these clusters can reduce the number of problematic sessions by 55% for join failures (15%--40% for other metrics)."
2369827,15226,390,Group sparsity based classification for cervigram segmentation,2011,"This paper presents an algorithm to classify pixels in uterine cervix images into two classes, namely normal and abnormal tissues, and simultaneously select relevant features, using group sparsity. Because of the large variations in image appearance due to changes of illumination, specular reflections and other visual noise, the two classes have a strong overlap in feature space, whether features are obtained from color or texture information. Using more features makes the classes more separable and increases the segmentation's quality, but also its complexity. However, the properties of these features have not been well investigated. In most cases, a group of features is selected prior to the segmentation process; features with minor contributions to the results are kept and add to the computational cost. We propose feature selection as a significant improvement in this problem. It provides a robust trade-off between segmentation quality and computational complexity. In this work we formulate the cervigram segmentation problem as a feature-selection-based classification method, and we introduce a regularization-based feature-selection algorithm to leverage both the sparsity and clustering properties of the features used. We implemented our method to automatically segment the biomarker AcetoWhite (AW) regions in a dataset of 200 images of the uterine cervix, for which manual segmentation is available. We compare the performance of several regularization-based feature-selection methods. The experimental results demonstrate that on this dataset, our proposed group-sparsity-based method gives overall better results in terms of sensitivity, specificity and sparsity."
599043,15226,21106,An evaluation framework for stereo-based driver assistance,2011,"The accuracy of stereo algorithms or optical flow methods is commonly assessed by comparing the results against the Middlebury database. However, equivalent data for automotive or robotics applications rarely exist as they are difficult to obtain. As our main contribution, we introduce an evaluation framework tailored for stereo-based driver assistance able to deliver excellent performance measures while circumventing manual label effort. Within this framework one can combine several ways of ground-truthing, different comparison metrics, and use large image databases.#R##N##R##N#Using our framework we show examples on several types of ground-truthing techniques: implicit ground truthing (e.g. sequence recorded without a crash occurred), robotic vehicles with high precision sensors, and to a small extent, manual labeling. To show the effectiveness of our evaluation framework we compare three different stereo algorithms on pixel and object level. In more detail we evaluate an intermediate representation called the Stixel World. Besides evaluating the accuracy of the Stixels, we investigate the completeness (equivalent to the detection rate) of the Stixel World vs. the number of phantom Stixels. Among many findings, using this framework enables us to reduce the number of phantom Stixels by a factor of three compared to the base parametrization. This base parametrization has already been optimized by test driving vehicles for distances exceeding 10000 km."
2282891,15226,390,Boosted Spectral Embedding (BoSE): Applications to content-based image retrieval of histopathology,2011,"In machine learning, non-linear dimensionality reduction (NLDR) is commonly used to embed high-dimensional data into a low-dimensional space while preserving local object adjacencies. However, the majority of NLDR methods define object adjacencies using distance metrics that do not account for the quality of the features in the high-dimensional space. In this paper we present Boosted Spectral Embedding (BoSE), a variant of the traditional Spectral Embedding (SE) that utilizes a Boosted Distance Metric (BDM) to improve the low-dimensional representation of the data. Under the naive assumption that all features are equally important, SE uses the Euclidean distance metric to define object-distance relationships. However, the BDM selectively weights features via the AdaBoost algorithm such that the low-dimensional representation contains only the most discriminating features. In this work BoSE is evaluated against SE in the context of digitized histopathology images using (a) content-based image retrieval and (b) classification via Random Forest of the low-dimensional representation. Using images from a cohort of 58 prostate cancer patient studies, BoSE and SE separated benign and malignant samples with areas under the precision-recall curve (AUPRCs) of 0.95 and 0.67 and classification accuracies using a Random Forest (RF) classifer were 0.93 and 0.79, respectively. For a cohort of 55 breast cancer studies, BoSE and SE performed comparably in terms of both RF accuracy and AUPRC. In addition, a qualitative visualization of the low-dimensional data representations suggests that BoSE exhibits improved class separability over SE."
2131339,15226,9004,Improving accuracy and power with transfer learning using a meta-analytic database,2012,"Typical cohorts in brain imaging studies are not large enough for systematic testing of all the information contained in the images. To build testable working hypotheses, investigators thus rely on analysis of previous work, sometimes formalized in a so-called meta-analysis. In brain imaging, this approach underlies the specification of regions of interest (ROIs) that are usually selected on the basis of the coordinates of previously detected effects. In this paper, we propose to use a database of images, rather than coordinates, and frame the problem as transfer learning: learning a discriminant model on a reference task to apply it to a different but related new task. To facilitate statistical analysis of small cohorts, we use a sparse discriminant model that selects predictive voxels on the reference task and thus provides a principled procedure to define ROIs. The benefits of our approach are twofold. First it uses the reference database for prediction, i.e. to provide potential biomarkers in a clinical setting. Second it increases statistical power on the new task. We demonstrate on a set of 18 pairs of functional MRI experimental conditions that our approach gives good prediction. In addition, on a specific transfer situation involving different scanners at different locations, we show that voxel selection based on transfer learning leads to higher detection power on small cohorts."
1041080,15226,21102,Human behavioural analysis with self-organizing map for ambient assisted living,2014,"This paper presents a system for automatically classifying the resting location of a moving object in an indoor environment. The system uses an unsupervised neural network (Self Organising Feature Map) fully implemented on a low-cost, low-power automated home-based surveillance system, capable of monitoring activity level of elders living alone independently. The proposed system runs on an embedded platform with a specialised ceiling-mounted video sensor for intelligent activity monitoring. The system has the ability to learn resting locations, to measure overall activity levels and to detect specific events such as potential falls. First order motion information, including first order moving average smoothing, is generated from the 2D image coordinates (trajectories). A novel edge-based object detection algorithm capable of running at a reasonable speed on the embedded platform has been developed. The classification is dynamic and achieved in real-time. The dynamic classifier is achieved using a SOFM and a probabilistic model. Experimental results show less than 20% classification error, showing the robustness of our approach over others in literature with minimal power consumption. The head location of the subject is also estimated by a novel approach capable of running on any resource limited platform with power constraints. © 2014 IEEE."
1275169,15226,9616,Dynamic Task Decomposition for Probabilistic Tracking in Complex Scenes,2014,"The employment of visual sensor networks in surveillance systems has brought in as many challenges as advantages. While the integration of multiple cameras into a network has the potential advantage of fusing complementary observations from sensors and enlarging visual coverage, it also increases the complexity of tracking tasks and poses challenges to system scalability. A key approach to tackling these challenges is the mapping of the demanding global task onto a distributed sensing and processing infrastructure. In this paper, we present an efficient and scalable multi-camera multi-people tracking system with a three-layer architecture, in which we formulate the overall task (i.e. tracking all people using all available cameras) as a vision based state estimation problem and aim to maximize utility and sharing of available sensing and processing resources. By exploiting the geometric relations between sensing geometry and people's positions, our method is able to dynamically and adaptively partition the overall task into a number of nearly independent subtasks, each of which tracks a subset of people with a subset of cameras. The method hereby reduces task complexity dramatically and helps to boost parallelization and maximize the real-time throughput and available resources of the system while accounting for intrinsic uncertainty induced, e.g., by visual clutter, occlusion, and illumination changes. We demonstrate the efficiency of our method by testing it with a challenging video sequence."
2418488,15226,22130,Object matching using boundary descriptors,2012,"The problem of object recognition is of immense practical importance and potential, and the last decade has witnessed a number of breakthroughs in the state of the art. Most of the past object recognition work focuses on textured objects and local appearance descriptors extracted around salient points in an image. These methods fail in the matching of smooth, untextured objects for which salient point detection does not produce robust results. The recently proposed bag of boundaries (BoB) method is the first to directly address this problem. Since the texture of smooth objects is largely uninformative, BoB focuses on describing and matching objects based on their post-segmentation boundaries. Herein we address three major weaknesses of this work. The first of these is the uniform treatment of all boundary segments. Instead, we describe a method for detecting the locations and scales of salient boundary segments. Secondly, while the BoB method uses an image based elementary descriptor (HoGs + occupancy matrix), we propose a more compact descriptor based on the local profile of boundary normals’ directions. Lastly, we conduct a far more systematic evaluation, both of the bag of boundaries method and the method proposed here. Using a large public database, we demonstrate that our method exhibits greater robustness while at the same time achieving a major computational saving – object representation is extracted from an image in only 6% of the time needed to extract a bag of boundaries, and the storage requirement is similarly reduced to less than 8%."
1372714,15226,20338,Watching videos from everywhere: a study of the PPTV mobile VoD system,2012,"In this paper, we examine mobile users' behavior and their corresponding video viewing patterns from logs extracted from the servers of a large scale VoD system. We focus on the analysis of the main discrepancies that might exist when users access the VoD system catalog from WiFi or 3G connections. We also study factors that might impact mobile users' interests and video popularity. The users' behavior exhibits strong daily and weekly patterns, with mobile users' interests being surprisingly spread across almost all categories and video lengths, independently of the connection type. However, by examining the activity of users individually, we observed a concentration of interests and peculiar access patterns, which allows to classify the users and thus better predict their behavior. We also find a skewed video popularity distribution and then demonstrate that the popularity of a video can be predicted using its very early popularity level. We further analyzed the sources of video viewing and found that even if search engines are the dominant sources for a majority of videos, they represent less than 10% (resp. 20%) of the sources for the highly popular videos in 3G (resp. WiFi) network. We report that both the type of connections and mobile devices in use have an impact on the viewing time and the source of viewing. Based on our findings, we provide insights and recommendations that can be used to design intelligent mobile VoD systems and help improving personalized services on these platforms."
769380,15226,23735,TansuBot: A drawer-type storage system for supporting object search with contents' photos and usage histories,2013,"In spite of IT innovation, people cannot get rid of non-creative tasks of searching daily-use objects at home. This paper presents a drawer-type storage system for supporting object search, “TansuBot”. By using this system and a smart device (e.g. smart phone), a user can review the photos of contents stored in the system. In addition, the system can present candidate drawers where the searching target object may be stored based on preliminary information such as usage histories. Concretely, LED blinking and pop-up actions (pushing drawers forward) are used for display. To realize these supports, a stacker crane type wall-moving robot is equipped at the backside of storage. The robot has a movable camera and mechanisms to push a drawer forward. For easy installation to a home, storage efficiency and cost reduction should be considered in the design of the instrument. Especially for cost reduction, this paper presents an approach to use wooden parts for main mechanisms. This approach also contributes to user-friendly presence and appearance of the instrument. This paper reports about the development of a prototype and an experiment to evaluate the functions for supporting object search. The results of the experiment prove the importance of the functions realized by the system; displaying contents' photos on a smart device and showing candidate drawers to investigate. The outcomes indicate that those functions have positive effects on reduction of searching time and mental burden."
2093872,15226,21056,Consumer video dataset with marked head trajectories,2013,"Content-based test video corpora usually builds on top of professional material or controlled settings. However, recent years have shown strong increase in user-generated content on the web. The increase in content volume creates challenges in accessibility and utility of the video content. In order to improve the utility of the user-generated videos, better automation for content-based descriptions are needed. Proper test sets are required to develop robust methods for content analysis. Detecting people from video is a common feature that is often seen in both in science and in commercial services. Unfortunately there is a lack of test data for person tracking from consumer videos. In this paper, we introduce a novel video dataset to accommodate this shortage. The dataset is done with two consumer-priced devices: a handheld camcorder and a mobile phone. Both devices were used to store material in indoor and outdoor settings with different attention levels from the people being filmed. The dataset comes with ground truth data that includes person head trajectories and other people marked in the background in MPEG-7-based metadata model. We give description of this metadata model and publish an annotation tool we used for creating the ground truth data. Also, we provide experimental results as a benchmark for all those who would like to use our dataset."
1803099,15226,11470,A low-complexity hardware-oriented mode decision scheme based on rate-distoration estimation,2014,"Video compression plays an important role in mobile applications, because more and more people use video to communicate with each other (like video call etc). However, the resources (energy, memory etc.) on mobile devices are limited, thus how to achieve a high coding performance in these devices becomes a big challenge. The recent standards such as H.264, HEVC and audio video coding standard (AVS) employ Rate distortion optimization (RDO) to select the best coding modes, however it results in extremely high computational complexity. This work presents a hardware friendly mode decision (MD) scheme. First, hardware-oriented RDcost estimation method is proposed by using least squares technique to reduce computational burden of RDO-based MD. Second, reconstructed-original (REC-ORG) united intra prediction scheme is presented to break the data dependency, while maintaining high coding performance. Third, highly efficient MD pipeline architecture is put forward to enhance MD processing capacity. The coding efficiency of our adopted MD scheme far outperforms (0.402 dB PSNR gain in average) the traditional SAD methods and the throughput of our designed pipeline is increased by 29%, 23% and 23% for I, P and B frames, respectively, compared with the existed RDO-based architecture."
1984068,15226,21106,Informative feature selection for object recognition via Sparse PCA,2011,"Bag-of-words (BoW) methods are a popular class of object recognition methods that use image features (e.g., SIFT) to form visual dictionaries and subsequent histogram vectors to represent object images in the recognition process. The accuracy of the BoW classifiers, however, is often limited by the presence of uninformative features extracted from the background or irrelevant image segments. Most existing solutions to prune out uninformative features rely on enforcing pairwise epipolar geometry via an expensive structure-from-motion (SfM) procedure. Such solutions are known to break down easily when the camera transformation is large or when the features are extracted from low-resolution, low-quality images. In this paper, we propose a novel method to select informative object features using a more efficient algorithm called Sparse PCA. First, we show that using a large-scale multiple-view object database, informative features can be reliably identified from a highdimensional visual dictionary by applying Sparse PCA on the histograms of each object category. Our experiment shows that the new algorithm improves recognition accuracy compared to the traditional BoW methods and SfM methods. Second, we present a new solution to Sparse PCA as a semidefinite programming problem using the Augmented Lagrangian Method. The new solver outperforms the state of the art for estimating sparse principal vectors as a basis for a low-dimensional subspace model."
1611840,15226,11470,Equity crowdfunding -A finnish case study,2014,"Crowdfunding grows at rapid pace internationally, and global platforms like Kickstarter and Indegogo are main leaders of this trend. Within the scope of this paper, we investigate the case of crowdfunding in Finland and identify particular national needs and potential what this new phenomenon has. The phenomenon of the Crowdfunding has been growing at a rapid pace in Finland, with it being seen as fundraising and effective marketing tool. Especially start-ups have started using the power of the crowd to promote their ideas and products as well as to raise funding. They are very opened to use Crowdfunding as an additional source of finance during their first round of financing. The effect of global platforms like Kickstarter and Indegogo, and in addition successful national ventures like Timo Vuorensola's campaigns for the new movie Iron Sky popularised Crowdfunding in Finland demonstrates the power of this instrument. In particular the younger audience is opened for this new tool. Within the scope of this paper we analyse the power of Crowdfunding in the Finnish context based on a literature study, and analysis of data gathered from Crowdfunding platforms."
2638043,15226,20332,Efficient Crowdsourcing With Stochastic Production,2012,"A principal seeks production of a good within a limited time-frame with a hard deadline, after which any good procured has no value.  There is inherent uncertainty in the production process, which in light of the deadline may warrant simultaneous production of multiple goods by multiple producers despite there being no marginal value for extra goods beyond the maximum quality good produced. This motivates a crowdsourcing model of procurement. We address efficient execution of such procurement from a social planner's perspective, taking account of and optimally balancing the value to the principal with the costs to producers (modeled as effort expenditure) while, crucially, contending with self-interest on the part of all players. A solution to this problem involves both an algorithmic aspect that determines an optimal effort level for each producer given the principal's value, and also an incentive mechanism that achieves equilibrium implementation of the socially optimal policy despite the principal privately observing his value, producers privately observing their skill levels and effort expenditure, and all acting only to maximize their own individual welfare. In contrast to popular winner take all contests, the efficient mechanism we propose involves a payment to every producer that expends non-zero effort in the efficient policy."
2335645,15226,30,Rapid Assessment of Cardiac Contractility on a Home Bathroom Scale,2011,"Analyzing systolic time intervals-specifically the preejection-period (PEP)-is widely accepted as one of the few methods for the noninvasive assessment of cardiac contractility. In this paper, we investigated the ballistocardiogram (BCG) as a way to noninvasively measure myocardial contractility when combined with the ECG. Specifically, we derived a parameter from the BCG and ECG that we hypothesized would be highly correlated to PEP. This is the time delay between the J-wave peak of the BCG and the R-wave of the ECG, which we refer to as the RJ interval. The RJ interval was correlated to PEP (r 2  = 0.86) for 2126 heart beats across ten subjects, with a y-intercept of 138 ms and slope of 1.05. This suggests that the RJ interval can be reliably used as a noninvasive assessment of cardiac contractility."
2203481,15226,22279,AVSS 2011 demo session: Level of service classification for smart cameras,2011,"Summary form only given. Automated code analysis is technology aimed at locating, describing and repairing areas of weakness in code. Code weaknesses range from security vulnerabilities, logic errors, concurrency violations, to improper resource usage, violations of architectures or coding guidelines. Common to all code analysis techniques is that they build abstractions of code and then check those abstractions for properties of interest. For instance a type checker computes how types are used, abstract interpreters and symbolic evaluators check how values flow, model checkers analyze how state evolves. Building modern program analysis tools thus requires a multi-pronged approach to find a variety of weaknesses. In this talk I will discuss and compare several program analysis tools, which MSR build during the last ten years. They include theorem provers, program verifiers, bug finders, malware scanners, and test case generators. I will describe the need for their development, their innovation, and application. Many of these tools had considerable impact on Microsoft's development practices, as well as on the research community. Some of them are being shipped in products such as the Static Driver Verifier or as part of Visual Studio. Performing program analysis as part of quality assurance is meanwhile standard practice in many software development companies. However several challenges have not yet been resolved. Thus, I will conclude with a set of open challenges in program analysis which hopefully triggers new aspiring directions in our joint quest of delivering predictable software that is free from defect and vulnerabilities."
975656,15226,11470,Research Design for Evaluating How to Engage Students with Urban Public Screens in Students' Neighbourhoods,2012,"Public screens are spreading throughout urban residential environments -- in busses, trains, shopping centers, or at bus stops. Currently they are mostly used for advertising purposes, however, within the scope of this publication we focus on a new non apparent application area: the application of public screens in student villages. However, with new emerging innovative technologies and the increasing demand of students to use the latest technologies, there is a need and desire to bridge residents and businesses in the local vicinity. In addition, social networks shall help to foster a deeper integration of the community and its services. We present a study of the usage of public screen environments in the student vicinity of Kelvin Grove, Brisbane, Australia and Hervanta, Tampere, Finland. The study had three goals: (1) interviews with business owners to evaluate their needs for content and services, (2) student questioner to gain insights into consumer desires and expectations, (3) development of a roadmap and service concepts for public screens in student vicinities."
769316,15226,30,Integrated e-Health Approach Based on Vascular Ultrasound and Pulse Wave Analysis for Asymptomatic Atherosclerosis Detection and Cardiovascular Risk Stratification in the Community,2012,"New strategies are urgently needed to identify subjects at increased risk of atherosclerotic cardiovascular disease (ACVD) development or complications. A National Public University Center (CUiiDARTE) was created in Uruguay, based on six main pillars: 1) integration of experts in different disciplines and creation of multidisciplinary teams, 2) incidence in public and professional education programs to give training in the use of new technologies and to shift the focus from ACVD treatment to disease prevention, 3) implementation of free vascular studies in the community (distributed rather than centralized healthcare), 4) innovation and application of e-Health and noninvasive technology and approaches, 5) design and development of a biomedical approach to determine the target population and patient workflow, and 6) improvement in individual risk estimation and differentiation between aging and ACVD-related arterial changes using population-based epidemiological and statistical patient-specific models. This work describes main features of CUiiDARTE project implementation, the scientific and technological steps and innovations done for individual risk stratification, and sub-clinical ACVD diagnosis."
1519815,15226,30,Bio-Patch Design and Implementation Based on a Low-Power System-on-Chip and Paper-Based Inkjet Printing Technology,2012,"This paper presents the prototype implementation of a Bio-Patch using fully integrated low-power system-on-chip (SoC) sensor and paper-based inkjet printing technology. The SoC sensor is featured with programmable gain and bandwidth to accommodate a variety of biosignals. It is fabricated in a 0.18-μm standard CMOS technology, with a total power consumption of 20 μW from a 1.2 V supply. Both the electrodes and interconnections are implemented by printing conductive nanoparticle inks on a flexible photo paper substrate using inkjet printing technology. A Bio-Patch prototype is developed by integrating the SoC sensor, a soft battery, printed electrodes, and interconnections on a photo paper substrate. The Bio-Patch can work alone or operate along with other patches to establish a wired network for synchronous multiple-channel biosignals recording. The measurement results show that electrocardiogram and electromyogram are successfully measured in in vivo tests using the implemented Bio-Patch prototype."
814582,15226,30,"Health Informatics Design for Assisted Diagnosis of Subclinical Atherosclerosis, Structural, and Functional Arterial Age Calculus and Patient-Specific Cardiovascular Risk Evaluation",2012,"Traditional methods used to assess cardiovascular risk (i.e., the Framingham Risk Score) exhibit clear limitations. To aid in patient-specific risk stratification and diagnosis it has been proposed to evaluate noninvasively structural and functional arterial parameters. A National Public University Center (CUiiDARTE) was created in Uruguay with the aim of developing and applying strategies to improve cardiovascular risk stratification and subclinical vascular disease detection. To this end a health informatics approach and tool was designed, developed, and implemented in CUiiDARTE. Its goals were to: 1) promote screening for subclinical atherosclerosis, 2) develop a centralized database to store information obtained noninvasively from anywhere in our country, 3) develop a biomathematical model integrating values for arterial structure and function into traditional cardiovascular risk assessment, 4) generate a detailed and comprehensive report for the specialist comparing patient data with reference data from the healthy population, 5) generate a similar report (using a structural and functional arterial age calculator) for the patients assessing the state of their arteries. In this paper, we present the main characteristics of the CUiiDARTE health informatics development."
2067630,15226,9099,The new dunites,2012,"The New Dunites is an interdisciplinary media arts research project that investigates the archeological site where the set for Cecile B. DeMille's The Ten Commandments was buried in 1923 [1]. In particular, this multi-phase endeavor involved the gathering of geophysical and archeological data, the historical study of the dawn of cinema in California, and a series of novel interactive multimedia installations that explored new avenues in the representation of scientific and cultural data."
1146778,15226,20515,Trusted BWI: Privacy and trust enhanced biometric web identities,2013,"Trusted web identities, which strongly associate a person with a digital identifier or certificate, are an area where biometrics should play a critical role. Balancing usability, security, and privacy is an important issue for any system that captures/stores users' information, especially for any biometric-based technology. To support biometric web services, the Biometric Identity Assurance Services (BIAS) standard was developed and recently approved. BIAS aims to establish standard biometric web services in order to improve interoperability and platform independence. Because they involve biometric data, the deployment of BIAS (and biometric web services in general) faces many challenges in terms of privacy, trust and security. They also face compatibility issues with widely-deployed systems that combine biometric sensors and Trusted Platform Modules (TPM). In order to address these obstacles, we propose an enhanced design of the recently introduced Biocryptographic Key Infrastructure (BKI). The original BKI enhanced the privacy and trust of remote biometric transactions, but, like most existing biometric systems, ignores the trust issues associated with remote enrollment. Our enhanced BKI design addresses this problem of trusted remote biometric enrollment. In addition, the enhanced design also extends the BKI to support biometric sensors with cryptographically secured on-chip biometric matching. Leveraging the new enhanced version of BKI, we propose the Trusted Biometric Web Identities (Trusted-BWI), as privacy and trust-enhanced biometric web services."
2038396,15226,30,Data Mining to Generate Adverse Drug Events Detection Rules,2011,"Adverse drug events (ADEs) are a public health is sue. Their detection usually relies on voluntary reporting or medical chart reviews. The objective of this paper is to automatically detect cases of ADEs by data mining. 115 447 complete past hospital stays are extracted from six French, Danish, and Bulgarian hospitals using a common data model including diagnoses, drug administrations, laboratory results, and free-text records. Different kinds of outcomes are traced, and supervised rule induction methods (decision trees and association rules) are used to discover ADE detection rules, with respect to time constraints. The rules are then filtered, validated, and reorganized by a committee of experts. The rules are described in a rule repository, and several statistics are automatically computed in every medical department, such as the confidence, relative risk, and median delay of outcome appearance. 236 validated ADE-detection rules are discovered; they enable to detect 27 different kinds of outcomes. The rules use a various number of conditions related to laboratory results, diseases, drug administration, and demographics. Some rules involve innovative conditions, such as drug discontinuations."
2115561,15226,30,Complementary Gene Signature Integration in Multiplatform Microarray Experiments,2011,"The concept of gene signature overlap has been addressed previously in a number of research papers. A common conclusion is the absence of significant overlap. In this paper, we verify the aforementioned fact, but we also assess the issue of similarities not on the gene level, but on the biology level hidden underneath a given signature. We proceed by taking into account the biological knowledge that exists among different signatures, and use it as a means of integrating them and refining their statistical significance on the datasets. In this form, by integrating biological knowledge with information stemming from data distributions, we derive a unified signature that is significantly improved over its predecessors in terms of performance and robustness. Our motive behind this approach is to assess the problem of evaluating different signatures not in a competitive but rather in a complementary manner, where one is treated as a pool of knowledge contributing to a global and unified solution."
2123869,15226,30,Agent-Based Modeling of the Spread of Influenza-Like Illness in an Emergency Department: A Simulation Study,2011,"The objective of this paper was to develop an agent based modeling framework in order to simulate the spread of influenza virus infection on a layout based on a representative hospital emergency department in Winnipeg, Canada. In doing so, the study complements mathematical modeling techniques for disease spread, as well as modeling applications focused on the spread of antibiotic-resistant nosocomial infections in hospitals. Twenty different emergency department scenarios were simulated, with further simulation of four infection control strategies. The agent based modeling approach represents systems modeling, in which the emergency department was modeled as a collection of agents (patients and healthcare workers) and their individual characteristics, behaviors, and interactions. The framework was coded in C + + using Qt4 libraries running under the Linux operating system. A simple ordinary least squares (OLS) regression was used to analyze the data, in which the percentage of patients that be came infected in one day within the simulation was the dependent variable. The results suggest that within the given instance con text, patient-oriented infection control policies (alternate treatment streams, masking symptomatic patients) tend to have a larger effect than policies that target healthcare workers. The agent-based modeling framework is a flexible tool that can be made to reflect any given environment; it is also a decision support tool for practitioners and policymakers to assess the relative impact of infection control strategies. The framework illuminates scenarios worthy of further investigation, as well as counterintuitive findings."
806905,15226,10228,Bringing IoT to Hospital Logistics Systems Demonstrating the concept,2012,"This paper is focused on demonstrating a real testbed for managing logistics systems in hospital environment exploiting the benefits of M2M and IoT paradigms. The work presents an architecture based on low-cost devices interacting with an enterprise application server placed anywhere in the Internet following the architecture proposed by ETSI for M2M systems. Combining multiple communication technologies (NFC, UMTS, 802.11, ZigBee and 802.3) and adapting their capabilities to different functional requirements, we are capable of creating autonomous and self-managed systems. The deployed architecture integrates at end device level, a NFC reader for tracking the level of remaining stocks and multiple output interfaces for exchanging messages with the application server. The system copes with cost, availability, scalability and easy deployment requirements, being the functionality also adaptable to the requirements of potential customers."
1118734,15226,22279,VTrack: Video analytics for automatic video-surveillance,2011,"Summary form only given. TENCON delegates are invited to attend the following tutorial sessions, free of charge, which will be held at the USC Engineering Audio Visual Room, University of San Carlos - Talamban Campus on November 19, 2012. Tutorial 1: Recent Advances in Robotics and Emerging Opportunities. This tutorial reviews the recent exciting developments in robotics centering on humans and operating in unstructured environments. It covers emerging trends in manufacturing, autonomous mobile robots, social robotics, and security and service applications. Current state of the art techniques to realize fundamental capabilities of robotic systems are concisely explained and challenges for research discussed. Tutorial 2: The Next Generation MIMO-OFDM Systems. This tutorial describes the VLSI implementation of our proposed 4x4 MIMO-OFDM (2.6G bps with 160MHz BW) and SxS MIMO-OFDM (3.0G bps with SOMHz BW) systems. A low-latency and a full-pipelined architecture are employed for all processing blocks to provide the real-time operations on OFDM modulation and MIMO detection. The designed transceiver has been evaluated in the circuit size and power dissipation by using a 90-nm CMOS process. In an FPGA board, the proposed total system has been implemented. For the designed system, the circuit behavior on gate size and power consumption is verified. The communication performance is also evaluated."
777861,15226,23735,MRI-powered Closed-loop Control for Multiple Magnetic Capsules,2014,"Control of multiple magnetic particles inside the human body may have many potential applications, such as drug delivery in places where conventional procedures cannot reach. A natural candidate for powering as well as tracking these magnetic particles is the MRI scanner. Although it offers the means to control the particles, it also poses difficulties: the magnetic field is applied uniformly to the group, thus making the independent control of each particle a challenging issue, while the actuation and the tracking are interleaved which can result to delays in actuation and measurement. The closed-loop control of a group of millimeter-scale particles, immersed in fluid, driven by the MRI scanner is studied in this paper. More specifically, this problem is presented in a unified manner, handling such issues as delays, constraints, as well as disturbances, and results in a robustly stable controller. We also propose a condition that effectively answers when the system should be in tracking mode versus actuation mode. In addition to theoretical results, the capability of the proposed controller is illustrated through simulation results."
1786672,15226,30,A Lesion-Specific Coronary Artery Calcium Quantification Framework for the Prediction of Cardiac Events,2011,"CT-based coronary artery calcium (CAC) scanning has been introduced as a noninvasive, low-radiation imaging technique for the assessment of the overall coronary arterial atherosclerotic burden. A 3-D CAC volume contains significant clinically relevant information, which is unused by conventional whole-heart CAC quantification methods. In this paper, we have developed a novel distance-weighted lesion-specific CAC quantification framework that predicts cardiac events better than the conventional whole-heart CAC measures. This framework consists of 1) a novel lesion-specific CAC quantification tool that measures each calcific lesion's attenuation, morphologic and geometric statistics; 2) a distance-weighted event risk model to estimate the risk probability caused by each lesion; and 3) a Naive Bayesian-based technique for risk integration. We have tested our lesion-specific event predictor on 60 CAC positive scans (20 with events and 40 without events), and compared it with conventional whole-heart CAC scores. Experimental results showed that our novel approach significantly improves the predictive accuracy, indicated by an improved area under the curve of receiver operating characteristic analysis from 62% to 68%, an improved specificity by 23-55% at the 80% sensitivity level, and a net reclassification improvement of 30%."
779363,15226,30,Implantable Ultralow Pulmonary Pressure Monitoring System for Fetal Surgery,2012,"Congenital pulmonary hypoplasia is a devastating condition affecting fetal and newborn pulmonary physiology, resulting in great morbidity and mortality. The fetal lung develops in a fluid-filled environment. In this paper, we describe a novel, implantable pressure sensing and recording device which we use to study the pressures present in the fetal pulmonary tree throughout gestation. The system achieves 0.18 cm H  2 O resolution and can record for 21 days continuously at 256 Hz. Sample tracings of in vivo fetal lamb recordings are shown."
2045959,15226,30,Home-Based Monitoring and Assessment of Parkinson's Disease,2011,"As a clinically complex neurodegenerative disease, Parkinson's disease (PD) requires regular assessment and close monitoring. In our current study, we have developed a home-based tool designed to monitor and assess peripheral motor symptoms. An evaluation of the tool was carried out over a period of ten weeks on ten people with idiopathic PD. Participants were asked to use the tool twice daily over four days, once when their medication was working at its best (“on” state) and once when it had worn off (“off” state). Results showed the ability of the data collected to distinguish the “on” and “off” state and also demonstrated statistically significant differences in timed assessments. It is anticipated that this tool could be used in the home environment as an early alert to a change in clinical condition or to monitor the effects of changes in prescribed medications used to manage PD."
1291149,15226,30,An Infobutton For Web 2.0 Clinical Discussions: The Knowledge Linkage Framework,2012,"This paper aims to develop an infobutton to automatically retrieve published papers corresponding to a topic-specific online clinical discussion. The knowledge linkages infobutton is designed to supplement online clinical conversations with pertinent medical literature from Pubmed. The project involves three distinct steps: 1) Clinical messages around a specific problem are grouped together into a thread. 2) These threads are processed using Metamap to link the conversations to keywords from the MeSH lexicon. 3) These keywords are used in a novel search strategy to retrieve a set of papers from Pubmed, which are then returned to the user. A pilot study using the messages from 2007 and 2008, was conducted to compare the knowledge linkage search strategy to a vector space model and extended Boolean model. The knowledge linkage model proved to be significantly better in terms of precision (p = 0.013 and 0.003, respectively) and recall (p = 0.351 and 0.013). Pertinent papers were returned to over 55% of the threads. This approach has demonstrated how clinicians can supplement their peer communications with evidence based research. Future work should focus on how to improve the threading and keyword-mapping strategies."
2245011,15226,30,Dynamic Composition of Semantic Pathways for Medical Computational Problem Solving by Means of Semantic Rules,2011,"This paper presents a semantic rule-based system for the composition of successful algorithmic pathways capable of solving medical computational problems (MCPs). A subset of medical algorithms referring to MCP solving concerns well-known medical problems and their computational algorithmic solutions. These solutions result from computations within mathematical models aiming to enhance healthcare quality via support for diagnosis and treatment automation, especially useful for educational purposes. Currently, there is a plethora of computational algorithms on the web, which pertain to MCPs and provide all computational facilities required to solve a medical problem. An inherent requirement for the successful construction of algorithmic pathways for managing real medical cases is the composition of a sequence of computational algorithms. The aim of this paper is to approach the composition of such pathways via the design of appropriate finite-state machines (FSMs), the use of ontologies, and SWRL semantic rules. The goal of semantic rules is to automatically associate different algorithms that are represented as different states of the FSM in order to result in a successful pathway. The rule-based approach is herein implemented on top of Knowledge-Based System for Intelligent Computational Search in Medicine (KnowBaSICS-M), an ontology-based system for MCP semantic management. Preliminary results have shown that the proposed system adequately produces algorithmic pathways in agreement with current international medical guidelines."
1176062,15226,30,A Framework of Whole Heart Extracellular Volume Fraction Estimation for Low-Dose Cardiac CT Images,2012,"Cardiac CT (CCT) is widely available and has been validated for the detection of focal myocardial scar using a delayed enhancement technique in this paper. CCT, however, has not been previously evaluated for quantification of diffuse myocardial fibrosis. In our investigation, we sought to evaluate the potential of low-dose CCT for the measurement of myocardial whole heart extracellular volume (ECV) fraction. ECV is altered under conditions of increased myocardial fibrosis. A framework consisting of three main steps was proposed for CCT whole heart ECV estimation. First, a shape-constrained graph cut (GC) method was proposed for myocardium and blood pool segmentation on postcontrast image. Second, the symmetric demons deformable registration method was applied to register precontrast to postcontrast images. So the correspondences between the voxels from precontrast to postcontrast images were established. Finally, the whole heart ECV value was computed. The proposed method was tested on 20 clinical low-dose CCT datasets with precontrast and postcontrast images. The preliminary results demonstrated the feasibility and efficiency of the proposed method."
2363216,15226,30,Vascular System Modeling in Parallel Environment - Distributed and Shared Memory Approaches,2011,"This paper presents two approaches in parallel modeling of vascular system development in internal organs. In the first approach, new parts of tissue are distributed among processors and each processor is responsible for perfusing its assigned parts of tissue to all vascular trees. Communication between processors is accomplished by passing messages, and therefore, this algorithm is perfectly suited for distributed memory architectures. The second approach is designed for shared memory machines. It parallelizes the perfusion process during which individual processing units perform calculations concerning different vascular trees. The experimental results, performed on a computing cluster and multicore machines, show that both algorithms provide a significant speedup."
1966368,15226,30,IEEE 802.15.4 MAC With GTS Transmission for Heterogeneous Devices With Application to Wheelchair Body-Area Sensor Networks,2011,"In wireless personal area networks, such as wireless body-area sensor networks, stations or devices have different bandwidth requirements and, thus, create heterogeneous traffics. For such networks, the IEEE 802.15.4 medium access control (MAC) can be used in the beacon-enabled mode, which supports guaranteed time slot (GTS) allocation for time-critical data transmissions. This paper presents a general discrete-time Markov chain model for the IEEE 802.15.4-based networks taking into account the slotted carrier sense multiple access with collision avoidance and GTS transmission phenomena together in the heterogeneous traffic scenario and under nonsaturated condition. For this purpose, the standard GTS allocation scheme is modified. For each non-identical device, the Markov model is solved and the average service time and the service utilization factor are analyzed in the non-saturated mode. The analysis is validated by simulations using network simulator version 2.33. Also, the model is enhanced with a wireless propagation model and the performance of the MAC is evaluated in a wheelchair body-area sensor network scenario."
1976617,15226,30,HBS: A Novel Biometric Feature Based on Heartbeat Morphology,2012,"In this paper, a new feature named heartbeat shape (HBS) is proposed for ECG-based biometrics. HBS is computed from the morphology of segmented heartbeats. Computation of the feature involves three basic steps: 1) resampling and normalization of a heartbeat; 2) reduction of matching error; and 3) shift invariant transformation. In order to construct both gallery and probe templates, a few consecutive heartbeats which could be captured in a reasonably short period of time are required. Thus, the identification and verification methods become efficient. We have tested the proposed feature independently on two publicly available databases with 76 and 26 subjects, respectively, for identification and verification. The second database contains several subjects having clinically proven cardiac irregularities (atrial premature contraction arrhythmia). Experiments on these two databases yielded high identification accuracy (98% and 99.85%, respectively) and low verification equal error rate (1.88% and 0.38%, respectively). These results were obtained by using templates constructed from five consecutive heartbeats only. This feature compresses the original ECG signal significantly to be useful for efficient communication and access of information in telecardiology scenarios."
1992638,15226,30,Content-Based Microscopic Image Retrieval System for Multi-Image Queries,2012,"In this paper, we describe the design and development of a multitiered content-based image retrieval (CBIR) system for microscopic images utilizing a reference database that contains images of more than one disease. The proposed CBIR system uses a multitiered approach to classify and retrieve microscopic images involving their specific subtypes, which are mostly difficult to discriminate and classify. This system enables both multi-image query and slide-level image retrieval in order to protect the semantic consistency among the retrieved images. New weighting terms, inspired from information retrieval theory, are defined for multiple-image query and retrieval. The performance of the system was tested on a dataset including 1666 imaged high power fields extracted from 57 follicular lymphoma (FL) tissue slides with three subtypes and 44 neuroblastoma (NB) tissue slides with four subtypes. Each slide is semantically annotated according to their subtypes by expert pathologists. By using leave-one-slide out testing scheme, the multi-image query algorithm with the proposed weighting strategy achieves about 93% and 86% of average classification accuracy at the first rank retrieval, outperforming the image-level retrieval accuracy by about 38 and 26 percentage points, for FL and NB diseases, respectively."
1789141,15226,30,Cross-Database Evaluation of a Multilead Heartbeat Classifier,2012,"In this paper, we studied the improvement in heartbeat classification achieved by including information from multilead ECG recordings in a previously developed and validated classification model. This model includes features from the RR interval series and morphology descriptors for each lead calculated from the wavelet transform. The experiments were carried out in the INCART database, available in Physionet, and the generalization was corroborated in private and public databases. In all databases, the AAMI recommendations for class labeling and results presentation were followed. Different strategies to integrate the additional information available in the 12-leads were studied. The best performing strategy consisted in performing principal component analysis to the wavelet transform of the available ECG leads. The performance indices obtained for normal beats were sensitivity (S) 98%, positive predictive value (P + ) 93%; for supraventricular beats, (S) 86%, (P + ) 91%; and for ventricular beats (S) 90%, (P + ) 90%. The generalization capability of the chosen strategy was confirmed by applying the classifier to other databases with different number of leads with comparable results. In conclusion, the performance of the reference two-lead classifier was improved by taking into account additional information from the 12-leads."
45443,15226,235,Creating an annotated corpus for extracting canonical citations from classics-related texts by using active annotation,2013,"This paper describes the creation of an annotated corpus supporting the task of extracting information---particularly canonical citations, that are references to the ancient sources---from Classics-related texts. The corpus is multilingual and contains approximately 30,000 tokens of POS-tagged, cleanly transcribed text drawn from the L'Annee Philologique. In the corpus the named entities that are needed to capture such citations were annotated by using an annotation scheme devised specifically for this task.#R##N##R##N#The contribution of the paper is two-fold: firstly, it describes how the corpus was created using Active Annotation, an approach which combines automatic and manual annotation to optimize the human resources required to create any corpus. Secondly, the performances of an NER classifier, based on Conditional Random Fields, are evaluated using the created corpus as training and test set: the results obtained by using three different feature sets are compared and discussed."
1778574,15226,30,Cardiovascular Modeling of Congenital Heart Disease Based on Neonatal Echocardiographic Images,2012,"This paper proposes a 3-D cardiovascular modeling system based on neonatal echocardiographic images. With the system, medical doctors can interactively construct patient-specific cardiovascular models, and share the complex topology and the shape information. For the construction of cardiovascular models with a variety of congenital heart diseases, we propose a set of algorithms and interface that enable editing of the topology and shape of the 3-D models. In order to facilitate interactivity, the centerline and radius of the vessels are used to edit the surface of the heart vessels. This forms a skeleton where the centerlines of blood vessel serve as the nodes and edges, while the radius of the blood vessel is given as an attribute value to each node. Moreover, parent-child relationships are given to each skeleton. They are expressed as the directed acyclic graph, where the skeletons are viewed as graph nodes and the connecting points are graph edges. The cardiovascular models generated from some patient data confirmed that the developed technique is capable of constructing cardiovascular disease models in a tolerable timeframe. It is successful in representing the important structures of the patient-specific heart vessels for better understanding in preoperative planning and electric medical recording of the congenital heart disease."
2078983,15226,30,Depicting Educational Content Repurposing Context and Inheritance,2011,"Educational content is often shared among different educators and is enriched, adapted, and, in general, repurposed so that it can be reused in different contexts. This paper discusses educational content and content repurposing in medical education, presenting different repurposing contexts. Finally, it proposes a novel approach to content repurposing via Web 2.0 social networking of learning resources. The proposed social network is augmented by a graphical representation module in order to capture and depict the relationships among different repurposed medical educational resources, based on educational resource “families” and inheritance. The ultimate goal is to provide a conceptually different approach to educational resource organization and retrieval via “social” associations among learning resources."
1340054,15226,30,Probabilistic modeling of selective stimulation of the human sciatic nerve with a flat Interface Nerve Electrode,2011,Proper ankle control is critical to both standing balance and efficient walking. This study hypothesized that a Flat Interface Nerve Electrode (FINE) placed around the sciatic nerve with a fixed number of contacts at predetermined locations and without a priori knowledge of the nerve's underlying neuroanatomy can selectively control each ankle motion. Models of the human sciatic nerve surrounded by a FINE of varying size were created and used to calculate the probability of selective activation of axons within any arbitrarily designated group of fascicles. Simulations suggest that currently available implantable technology cannot selectively recruit each target plantar flexor individually but can restore plantar flexion or dorsiflexion from a site on the sciatic nerve without spillover to antagonists. Successful activation of individual ankle muscles in 90% of the population can be achieved by utilizing bipolar stimulation and/or by increasing the number of contacts within the cuff.
2200138,15226,30,A Potential Causal Association Mining Algorithm for Screening Adverse Drug Reactions in Postmarketing Surveillance,2011,"Early detection of unknown adverse drug reactions (ADRs) in postmarketing surveillance saves lives and prevents harmful consequences. We propose a novel data mining approach to signaling potential ADRs from electronic health databases. More specifically, we introduce potential causal association rules (PCARs) to represent the potential causal relationship between a drug and ICD-9 (CDC. (2010). International Classification of Diseases, Ninth Revision (ICD-9). [Online]. Available: http://www.cdc.gov/nchs/icd/icd9.html) coded signs or symptoms representing potential ADRs. Due to the infrequent nature of ADRs, the existing frequency-based data mining methods cannot effectively discover PCARs. We introduce a new interestingness measure, potential causal leverage, to quantify the degree of association of a PCAR. This measure is based on the computational, experience-based fuzzy recognition-primed decision (RPD) model that we developed previously (Y. Ji, R. M. Massanari, J. Ager, J. Yen, R. E. Miller, and H. Ying, “A fuzzy logic-based computational recognition-primed decision model,” Inf. Sci., vol. 177, pp. 4338-4353, 2007) on the basis of the well-known, psychology-originated qualitative RPD model (G. A. Klein, “A recognition-primed decision making model of rapid decision making,” in Decision Making in Action: Models and Methods, 1993, pp. 138-147). The potential causal leverage assesses the strength of the association of a drug-symptom pair given a collection of patient cases. To test our data mining approach, we retrieved electronic medical data for 16 206 patients treated by one or more than eight drugs of our interest at the Veterans Affairs Medical Center in Detroit between 2007 and 2009. We selected enalapril as the target drug for this ADR signal generation study. We used our algorithm to preliminarily evaluate the associations between enalapril and all the ICD-9 codes associated with it. The experimental results indicate that our approach has a potential to better signal potential ADRs than risk ratio and leverage, two traditional frequency-based measures. Among the top 50 signal pairs (i.e., enalapril versus symptoms) ranked by the potential causal-leverage measure, the physicians on the project determined that eight of them probably represent true causal associations."
2359092,15226,11470,Media Multiplicity at Your Fingertips: Direct Manipulation Based on Webbles,2012,"Webble Technology is the most recent form of Intelligent Pad. Webbles are objects in a browser window that allow for direct manipulation by drag and drop. One may pick up any Webble and move it over any other one for operational combination. The new Webble is plugged into the previous one and data between them flow through predefined slots. Human users may reconfigure the slot connections of Webbles as necessary. The technology is ready to deal with a great multiplicity of media ranging from text, audio, pictures and video through building blocks of interactive laboratories to highly abstract objects such as decision trees and partial recursive functions. This makes Webbles particularly appropriate for e-learning. The paper aims at an intense workshop discussion of the reach of the technology putting emphasis on the didactic potential of directly manipulating a manifold of digital media types."
1898231,15226,30,Implementation Methodology for Interoperable Personal Health Devices With Low-Voltage Low-Power Constraints,2011,"Traditionally, e-Health solutions were located at the point of care (PoC), while the new ubiquitous user-centered paradigm draws on standard-based personal health devices (PHDs). Such devices place strict constraints on computation and battery efficiency that encouraged the International Organization for Standardization/IEEE11073 (X73) standard for medical devices to evolve from X73PoC to X73PHD. In this context, low-voltage low-power (LV-LP) technologies meet the restrictions of X73PHD-compliant devices. Since X73PHD does not approach the software architecture, the accomplishment of an efficient design falls directly on the software developer. Therefore, computational and battery performance of such LV-LP-constrained devices can even be outperformed through an efficient X73PHD implementation design. In this context, this paper proposes a new methodology to implement X73PHD into microcontroller-based platforms with LV-LP constraints. Such implementation methodology has been developed through a patterns-based approach and applied to a number of X73PHD-compliant agents (including weighing scale, blood pressure monitor, and thermometer specializations) and microprocessor architectures (8, 16, and 32 bits) as a proof of concept. As a reference, the results obtained in the weighing scale guarantee all features of X73PHD running over a microcontroller architecture based on ARM7TDMI requiring only 168 B of RAM and 2546 B of flash memory."
1068656,15226,11470,"Memetic Communication Media - Concepts, Technologies, Applications",2012,"Memetics is an approach to interpret, understand and possibly manage communication and knowledge evolution in a Darwinistic way. Meme Media are implementing Memetics. Intelligent Pad has been the earliest Meme Media middleware. Webble Technology is the most recent form of Intelligent Pad. Webbles are objects in a browser window that allow for direct manipulation by drag and drop. One may pick up any Webble and move it over any other one for operational combination. The new Webble is plugged into the previous one and data between them flow through predefined slots. Human users may reconfigure the slot connections of Webbles as desired. The concepts and the technology are setting the stage for new ways of human communication including playful learning. Webbles at the human users' fingertips allow for exploration and explanation by direct manipulation, for collaboration as well as for competition, and for trial and error investigation."
1694031,15226,11470,Advanced Webble Application Development Directly in the Browser by Utilizing the Full Power of Meme Media Customization and Event Management Capabilities,2012,"A meme media object, also known as a Webble, always come with a set of familiar generic behaviors together with another set of specialized ones for that particular Webble. But what if there is a need for a custom behavior or interface that was not originally intended when first created. With Webble technology, that does not need to be a problem. In this paper we will attempt to show how simple it is, due to the design and construction of Webbles, to insert new customizable behaviors in any Webble available, or control application level events and actions, all through an intuitive, user-friendly interface. We claim that within a few hours of combining generic Webble building blocks and the setting up of configurable event handlers directly in the web browser, without traditional programming, we can create any arbitrary Silver light-based web application, ready to be shared to the cloud and the world."
370932,15226,20515,Social acceptance of epassports,2014,"Using large-scale web survey in six countries we study the societal readiness and acceptance of specific technology options in relation to the potential next generation of ePassports. We find that the public has only limited knowledge of the electronic data and functions ePassports include, and often have no clear opinion on various potential uses for ePassports and related personal data. Still, the public expects from ePassports improvements in protection from document forgery, accuracy and reliability of the identification of persons, and protection from identity theft. The main risks the public associates with ePassports includes the possible use of personal information for purposes other than those initially stated, and covert surveillance. Compared to earlier studies, our research shows that issues of possible privacy invasion and abuse of information are much more perceived by the public. There is a weak correlation between a persons' level of knowledge about ePassports and their willingness to accept the use of advanced biometrics, such as fingerprints or eye iris images, in different identity management and identity checking scenarios. Furthermore, the public becomes more undecided about ePassport applications as we move from the basic state of the art towards more advanced biometric technologies in various scenarios. The successful pathway to greater acceptability of the use of advanced biometrics in ePassports should start from the introduction of perceivably high-benefit and low-risk applications. As the public awareness is low, citizens' belief in government benevolence, i.e. the belief that the government acts in citizens' best interest, comes out as an important factor in the overall context."
2591014,15226,21106,Lessons from the primate visual system,2012,"The primate visual system can perform an astonishing array of tasks as reflected by the correspondingly large portion of the cerebral cortex devoted to analyzing retinal signals. Although a potential source of inspiration for computer vision, with a few exceptions, progress has been slow in this field. Principal obstacles are the lack of any exhaustive list of what vision achieves in humans and the restricting of areas of investigation to a few topics such as motion, object categories and the control of a few actions such as reaching or saccades. Here I will review how we integrated several experimental techniques to address a question that arose from interactions with computer vision scientists more than fifteen years ago: the extraction of 3D surfaces. This goal is achieved by a new type of higher-order visual neuron: the gradient-selective neurons. Neurons selective for speed gradients were initially discovered in motion processing areas, such as MT/V5, MSTd and FST, located in the monkey superior temporal sulcus (STS). Subsequently, neurons selective for disparity gradients were discovered in shape processing areas, such as TEs and AIP. By combining these single-cell studies with fMRI in human and awake monkey, we were able to localize similar neurons to human cortical areas. In the second part I address my present interest in understanding the visual signals related to the actions of conspecifics, which is perhaps the ultimate challenge of motion processing, but which receives surprisingly little attention in vision. The understanding of observed actions exemplifies my statement that to be useful visual signals have to leave the visual system, as signals related to biological motion in the STS are indeed relayed to parietal regions involved in the control of diverse actions to be understood as actions."
1113494,15226,20561,Deaf Culture and Sign Language Writing System -- A Database for a New Approach to Writing System Recognition Technology,2014,"The Deaf have been denied their natural language for over a hundred years, with dire consequences for their health, citizenship and culture. Sign Language is the natural language of the Deaf, used for intellectual development and other human traits that are language related. Writing Systems (sequence of characters to represent a language) store and retrieve information for literature, science, knowledge creation, information dissemination, communication over time and space etc. Sign Writing is a writing system deemed adequate to the spatial-visual nature of Sign Languages. However, current computational technologies fail to provide the Deaf with effective tools for their writing needs (they lack usability, and/or are one-to-one translation from the oral language etc.). This article proposes a new, more natural approach: that of using screen and stylus for online handwritten recognition of Sign Writing. This research makes available a database to be used by computer vision/character recognition to inform design of Sign Writing editors."
2021479,15226,30,Building an Index of Activity of Inhabitants From Their Activity on the Residential Electrical Power Line,2011,"In the framework of context awareness within the home, our team is currently assessing the unobtrusive detection of inhabitants' activity through the monitoring of their use and consumption of electricity. The objective is to develop a system for the remote monitoring of large populations of elderly people living independently at home. To be readily deployable on the field, such a system must be minimally intrusive both for the home environment and for the field professionals (paramedics and social workers) visiting the patients at home. We carried out two successive field experiments to evaluate and to improve our system designed to deliver a single index of daily activity. The first experiment involved 13 elderly persons over a nine-month period (84 240 h data recorded) and the second one 12 elderly over six months (51 840 h). We evaluated both the relevance of the index and the acceptability of the system as a whole. We discovered that electrical activity is a kind of unique “signature” of each person's activity. Moreover, this profile provides unexpected information on the health status of the subject. We confirmed that the system was unobtrusive and well accepted both by the subjects and by the professionals involved. Our unique index of activity, and its trend over time, can provide timely information to the professionals on the patient."
2199914,15226,30,ISWLS: Novel Algorithm for Image Reconstruction in PET,2011,"The purpose of this study is to introduce a novel empirical iterative algorithm for medical image reconstruction, under the short name ISWLS (image space weighted least squares), which is expected to have image space reconstruction algorithm (ISRA) properties in noise manipulation and weighted least-squares (WLS) acceleration of the reconstruction process. We used phantom data from a prototype small-animal positron emission tomography system and the methods presented here are applied to 2-D sinograms. Further, we assess the performance of the new algorithm by comparing it to the simultaneous version of algebraic reconstruction technique (ART), simultaneous algebraic reconstruction technique (SART), to expectation maximization maximum likelihood (EM-ML), ISRA, and WLS. All algorithms are compared in terms of cross-correlation coefficient, reconstruction time, and contrast-to-noise ratios (CNRs). As it turns out, ISWLS presents higher CNRs than EM-ML, ISRA, and SART for objects of different sizes. Also, ISWLS shows similar performance to WLS during the first iterations but it has better noise manipulation. Finally, ordered subsets ISWLS (OS-ISWLS), the OS version of ISWLS, shows its best performance between the first six-nine iterations. Its behavior seems to be a compromise between OS-ISRA and OS-WLS."
2368266,15226,30,Toward Synergy-Based Brain-Machine Interfaces,2011,"This paper demonstrates a synergy-based brain-machine interface that uses low-dimensional command signals to control a high dimensional virtual hand. First, temporal postural synergies were extracted from the angular velocities of finger joints of five healthy subjects when they performed hand movements that were similar to activities of daily living. Two synergies inspired from the extracted synergies, namely, two-finger pinch and whole-hand grasp, were used in real-time brain control, where a virtual hand with 10 degrees of freedom was controlled to grasp or pinch virtual objects. These two synergies were controlled by electrocorticographic (ECoG) signals recorded from two electrodes of an electrode array that spanned motor and speech areas of an individual with intractable epilepsy, thus demonstrating closed loop control of a synergy-based brain-machine interface."
2623577,15226,235,YouCat: Weakly Supervised Youtube Video Categorization System from Meta Data & User Comments using WordNet & Wikipedia,2012,"In this paper, we propose a weakly supervised system, YouCat , for categorizing Youtube videos into different genres like Comedy, Horror, Romance, Sports and Technology The system takes a Youtube video url as input and gives it a belongingness score for ea ch genre. The key aspects of this work can be summarized as: (1) Unlike other ge nre identification works, which are mostly supervised, this system is mostly unsupervised, requiring no labeled data for training. (2) The system can easily incorporate new genres without re quiring labeled data for the genres. (3) YouCat extracts information from the video title , meta description and user comments (which together form the video descriptor ). (4) It uses Wikipedia and WordNet for concept expansion. (5) The proposed algorithm with a time complexity of O(|W|) (where (|W|) is the number of words in the video descriptor) is efficient to be deployed i n web for real-time video categorization. Experimentations have been performed on real world Youtube videos where YouCat achieves an F-score of 80.9% , without using any labeled training set, compared to the supervised, multiclass SVM F-score of 84.36% for single genre prediction . YouCat performs better for multi-genre prediction with an F-Score of 90.48% . Weak supervision in the system arises out of the usage of manually constructed WordNet and genre description by a few root words."
2196439,15226,30,A Hybrid Clustering Method for ROI Delineation in Small-Animal Dynamic PET Images: Application to the Automatic Estimation of FDG Input Functions,2011,"Tracer kinetic modeling with dynamic positron emission tomography (PET) requires a plasma time-activity curve (PTAC) as an input function. Several image-derived input function (IDIF) methods that rely on drawing the region of interest (ROI) in large vascular structures have been proposed to overcome the problems caused by the invasive approach for obtaining the PTAC, especially for small-animal studies. However, the manual placement of ROIs for estimating IDIF is subjective and labor-intensive, making it an undesirable and unreliable process. In this paper, we propose a novel hybrid clustering method (HCM) that objectively delineates ROIs in dynamic PET images for the estimation of IDIFs, and demonstrate its application to the mouse PET studies acquired with [  18 F]Fluoro-2-deoxy-2-D-glucose (FDG). We begin our HCM using k-means clustering for background removal. We then model the time-activity curves using polynomial regression mixture models in curve clustering for heart structure detection. The hierarchical clustering is finally applied for ROI refinements. The HCM achieved accurate ROI delineation in both computer simulations and experimental mouse studies. In the mouse studies, the predicted IDIF had a high correlation with the gold standard, the PTAC derived from the invasive blood samples. The results indicate that the proposed HCM has a great potential in ROI delineation for automatic estimation of IDIF in dynamic FDG-PET studies."
1516166,15226,390,K-T ISD: Compressed sensing with iterative support detection for dynamic MRI,2011,"In this paper, we propose a new k−t Iterative Support Detection (k−t ISD) method to improve the CS reconstruction for dynamic cardiac MRI by incorporating additional information on the support of the dynamic image in x−f space. The proposed method uses an iterative procedure for alternating image reconstruction and support detection in x−f space. Experimental results demonstrate that the proposed k−t ISD method improves the reconstruction quality of dynamic cardiac MRI over the basic CS method in which support information is not exploited."
1643589,15226,9078,Content authentication of halftone video via flickering as sparse signal,2012,"We investigate the issue of content authentication for halftone videos transmitted over mobile devices. With an eye to the flickering that is the unique characteristic of halftone video and possesses the property of sparsity, a compressed sensing (CS)-based halftone video authentication method is presented. We show that the restricted isometry property (RIP) in CS can explain the principle of hash matching between two CS-based hashes. Promising results obtained from simulations demonstrate the feasibility of our method."
1189545,15226,8494,Compact descriptors for mobile visual search and MPEG CDVS standardization,2013,"In this paper, we present the state-of-the-art compact descriptors for mobile visual search. In particular, we introduce our MPEG contributions in global descriptor aggregation and local descriptor compression, which have been adopted by the ongoing MPEG standardization of compact descriptor for visual search (CDVS). Standardization progress will be introduced. Other issues including visual object databases and MPEG CDVS impact on visual search industry will be discussed as well."
1810668,15226,11470,H.264/AVC based near lossless intra codec using line-based prediction and modified CABAC,2011,"In this paper, we propose a new H.264/AVC based intra codec for near lossless coding. The proposed algorithm is composed of two parts: line-based intra prediction and modified context-based adaptive binary arithmetic coding (CABAC). Experimental results show that the proposed method provides about 8.95% bit savings, compared to the current H.264/AVC FRExt high profile."
1205254,15226,30,Editorial: From Information Technology in Biomedicine to Biomedical and Health Informatics,2012,"The IEEE Transactions on Information Technology in Biomedicine (T-ITB) will be retitled as the IEEE Journal of Biomedical and Health Informatics (J-BHI) starting in January, 2013. The IEEE Transactions on Information Technology in Biomedicine was launched with four issues per year in 1997. After more than a decade of steady growth, the journal ventured into new challenges: open access of publication and rapid expansion in all health informatics-related fields. In this concluding issue of T-ITB the authors would like to take this opportunity to report briefly some historical retrospectives of journal with known statistics of it and to extend a special appreciation from the current editorial office to readers, authors, reviewers, associate editors, Engineering in Medicine and Biology Society (EMBS) relevant committees, IEEE publication staff, and especially two previous Editors-in-Chief for their great contributions to make the Transactions successful. The launching of the J-BHI as the new title of T-ITB is an outcome of the vision of the EMBS's 2011 Publications Committee. The title change and scope revision were accomplished through more than two years of team efforts by T-ITB Editorial Board, EMBS Executive Office, Publication Committee, and AdCom. In the next editorial to be published on January issue of J-BHI, we will discuss the rationale behind the title change and scope revision as well as the grand challenges in health informatics with future perspectives."
2379604,15226,23735,Automated surgical planning and evaluation algorithm for spinal fusion surgery with three-dimensional pedicle model,2011,"In this paper, an advanced preoperative planning framework for spinal fusion is presented. The framework is based on spinal pedicle data obtained from computed tomography (CT) images, and provides optimal insertion trajectories and pedicle screw sizes. The proposed approach begins with a safety margin estimation for each potential insertion trajectory that passes through the pedicle volume, followed by procedures to collect a set of insertion trajectories that satisfy operation safety objectives. Among the trajectory candidates, the insertion trajectory, which maximizes the insertable depth of a pedicle screw into the vertebral body, is then chosen as optimal, because the insertable depth enhances the strength of the transpedicular screw-vertebra interface after spinal fusion surgery. The radius of a pedicle screw was chosen as 70% of the pedicle radius. This framework has been tested on 68 spinal pedicles of 8 patients requiring spinal fusion. It was successfully applied, resulting in an average success rate of 100% and a final safety margin of 2.11±0.17mm."
1042096,15226,30,QT Variability Index Changes With Severity of Cardiovascular Autonomic Neuropathy,2012,"Cardiovascular autonomic neuropathy (CAN) has been frequently postulated to increase susceptibility to ventricular arrhythmias and sudden cardiac death in diabetic patients. The relation between the progression of CAN in diabetes and ventricular repolarization remains to be fully described. Therefore, this study examined QT interval variability and heart rate interbeat variability to identify any alterations of cardiac repolarization in diabetic patients in relation to severity of CAN. Seventy control participants without (CAN-) and 74 patients with CAN (CAN+) were enrolled in this study. Among 74 CAN+ patients, 62 are early CAN+ (eCAN+) , and 12 are definite CAN+ (dCAN+) according to autonomic nervous system function tests as described by Ewing. The results showed that the QT variability index (QTVI) was significantly ( p  <; 0.05) higher and positive in the dCAN+ (0.51 ±1.32) group than in the eCAN+ (-0.39 ±0.91) and CAN - (-0.54 ±0.72) groups. The QT variability to heart-rate variability ratio provides a measure of the balance between QT and heart interbeat variability. QTVI was more sensitive in identifying disease progression at all stages. Our study supports the hypothesis that QTVI could be used as a clinical test to identify early CAN and as a marker of CAN progression in diabetic patients and may help physicians in determining the best therapeutic strategy for these patients."
2279099,15226,30,Prognosis of Right Ventricular Failure in Patients With Left Ventricular Assist Device Based on Decision Tree With SMOTE,2012,"Right ventricular failure is a significant complication following implantation of a left ventricular assist device (LVAD), which increases morbidity and mortality. Consequently, researchers have sought predictors that may identify patients at risk. However, they have lacked sensitivity and/or specificity. This study investigated the use of a decision tree technology to explore the preoperative data space for combinatorial relationships that may be more accurate and precise. We retrospectively analyzed the records of 183 patients with initial LVAD implantation at the Artificial Heart Program, University of Pittsburgh Medical Center, between May 1996 and October 2009. Among those patients, 27 later required a right ventricular assist device (RVAD+) and 156 remained on LVAD (RVAD-) until the time of transplantation or death. A synthetic minority oversampling technique (SMOTE) was applied to the RVAD+ group to compensate for the disparity of sample size. Twenty-one resampling levels were evaluated, with decision tree model built for each. Among these models, the top six predictors of the need for an RVAD were transpulmonary gradient (TPG), age, international normalized ratio (INR), heart rate (HR), aspartate aminotransferase (AST), prothrombin time, and right ventricular systolic pressure. TPG was identified to be the most predictive variable in 15 out of 21 models, and constituted the first splitting node with 7 mmHg as the breakpoint. Oversampling was shown to improve the senstivity of the models monotonically, although asymptotically, while the specificity was diminished to a lesser degree. The model built upon 5X synthetic RVAD+ oversampling was found to provide the best compromise between sensitivity and specificity, included TPG (layer 1), age (layer 2), right atrial pressure (layer 3), HR (layer 4,7), INR (layer 4, 9), alanine aminotransferase (layer 5), white blood cell count (layer 5,6, &7), the number of inotrope agents (layer 6), creatinine (layer 8), AST (layer 9, 10), and cardiac output (layer 9). It exhibited 85% sensitivity, 83% specificity, and 0.87 area under the receiver operating characteristic curve (RoC), which was found to be greatly improved compared to previously published studies."
1871340,15226,30,Factors Affecting Acceptance of a Web-Based Self-Referral System,2011,"With the growing availability of health information on the web, people are becoming more knowledgeable on their health conditions and treatment options, and more patients seek specialists by themselves. To aid patients in requesting self-referrals, we have developed and evaluated a web-based self-referral system in three specialty clinics at the University of Washington. Two clinics adopted the system for routine clinical use, while the third clinic decided not to. A major difference between these two groups was in how fast online requests from patients were handled, which significantly influenced patients' satisfaction. Clinic's preparedness for handling the temporarily increased workload due to the introduction of a new health information system played a role as well. Also, we noticed that the physician leadership/championship made a difference in the acceptance of our system."
1877510,15226,30,An Ontology-Based System for Context-Aware and Configurable Services to Support Home-Based Continuous Care,2011,"Continuous care models for chronic diseases pose several technology-oriented challenges for home-based care, where assistance services rely on a close collaboration among different stakeholders, such as health operators, patient relatives, and social community members. This paper describes an ontology-based context model and a related context management system providing a configurable and extensible service-oriented framework to ease the development of applications for monitoring and handling patient chronic conditions. The system has been developed in a prototypal version, and integrated with a service platform for supporting operators of home-based care networks in cooperating and sharing patient-related information and coordinating mutual interventions for handling critical and alarm situations. Finally, we discuss experimentation results and possible further research directions."
1606019,15226,23735,Force-velocity modulation strategies for soft tissue examination,2013,"Advanced tactile tools in minimally invasive surgery have become a pressing need in order to reduce time and improve accuracy in localizing potential tissue abnormalities. In this regard, one of the main challenges is to be able to estimate tissue parameters in real time. In palpation, tactile information felt at a given location is identified by the viscoelastic dynamics of the neighboring tissue. Due to this reason the tissue examination behavior and the distribution of viscoelastic parameters in tissue should be considered in conjunction. This paper investigates the salient features of palpation behavior on soft tissue determining the effectiveness of localizing hard nodules. Experimental studies involving human participants, and validation tests using finite element simulations and a tele-manipulator, were carried out. Two distinctive tissue examination strategies in force-velocity modulation for the given properties of target tissue were found. Experimental results suggest that force-velocity modulations during continuous path measurements are playing an important role in the process of mechanical soft tissue examination. These behavioral insights, validated by detailed numerical models and robotic experimentations shed light on future designs of optimal robotic palpation."
2306500,15226,30,MyCare Card Development: Portable GUI Framework for the Personal Electronic Health Record Device,2011,"In most emergency situations, health professionals rely on patients to provide information about their medical history. However, in some cases patients might not be able to communicate this information, and in most countries an online integrated patient record system has not been adopted yet. Therefore, in order to address this issue the ongoing project MyCare Card (MyC 2 , www.myc2.org) has been established. The aim of this project is to design, implement, and evaluate a prototype patient held electronic health record device. Due to the wide range of user requirements, the device, its communication interface, and its software have to be compatible with many common platforms and operating systems. Thus, this paper is addressing one of the software compatibility matters-the cross-platform GUI implementation. It introduces a portable object-oriented GUI framework, suitable for a declarative layout definition, components customization, and fine model-view code separation. It also rationalizes the hardware and software solutions selected for this project implementation."
1613474,15226,30,SparkMed: A Framework for Dynamic Integration of Multimedia Medical Data Into Distributed m-Health Systems,2012,"With the advent of 4G and other long-term evolution (LTE) wireless networks, the traditional boundaries of patient record propagation are diminishing as networking technologies extend the reach of hospital infrastructure and provide on-demand mobile access to medical multimedia data. However, due to legacy and proprietary software, storage and decommissioning costs, and the price of centralization and redevelopment, it remains complex, expensive, and often unfeasible for hospitals to deploy their infrastructure for online and mobile use. This paper proposes the SparkMed data integration framework for mobile healthcare (m-Health), which significantly benefits from the enhanced network capabilities of LTE wireless technologies, by enabling a wide range of heterogeneous medical software and database systems (such as the picture archiving and communication systems, hospital information system, and reporting systems) to be dynamically integrated into a cloud-like peer-to-peer multimedia data store. Our framework allows medical data applications to share data with mobile hosts over a wireless network (such as WiFi and 3G), by binding to existing software systems and deploying them as m-Health applications. SparkMed integrates techniques from multimedia streaming, rich Internet applications (RIA), and remote procedure call (RPC) frameworks to construct a Self-managing, Pervasive Automated netwoRK for Medical Enterprise Data (SparkMed). Further, it is resilient to failure, and able to use mobile and handheld devices to maintain its network, even in the absence of dedicated server devices. We have developed a prototype of the SparkMed framework for evaluation on a radiological workflow simulation, which uses SparkMed to deploy a radiological image viewer as an m-Health application for telemedical use by radiologists and stakeholders. We have evaluated our prototype using ten devices over WiFi and 3G, verifying that our framework meets its two main objectives: 1) interactive delivery of medical multimedia data to mobile devices; and 2) attaching to non-networked medical software processes without significantly impacting their performance. Consistent response times of under 500 ms and graphical frame rates of over 5 frames per second were observed under intended usage conditions. Further, overhead measurements displayed linear scalability and low resource requirements."
1152364,15226,30,A Methodology for Validating Artifact Removal Techniques for Physiological Signals,2012,"Artifact removal from physiological signals is an essential component of the biosignal processing pipeline. The need for powerful and robust methods for this process has become particularly acute as healthcare technology deployment undergoes transition from the current hospital-centric setting toward a wearable and ubiquitous monitoring environment. Currently, determining the relative efficacy and performance of the multiple artifact removal techniques available on real world data can be problematic, due to incomplete information on the uncorrupted desired signal. The majority of techniques are presently evaluated using simulated data, and therefore, the quality of the conclusions is contingent on the fidelity of the model used. Consequently, in the biomedical signal processing community, there is considerable focus on the generation and validation of appropriate signal models for use in artifact suppression. Most approaches rely on mathematical models which capture suitable approximations to the signal dynamics or underlying physiology and, therefore, introduce some uncertainty to subsequent predictions of algorithm performance. This paper describes a more empirical approach to the modeling of the desired signal that we demonstrate for functional brain monitoring tasks which allows for the procurement of a “ground truth” signal which is highly correlated to a true desired signal that has been contaminated with artifacts. The availability of this “ground truth,” together with the corrupted signal, can then aid in determining the efficacy of selected artifact removal techniques. A number of commonly implemented artifact removal techniques were evaluated using the described methodology to validate the proposed novel test platform."
1973923,15226,30,Ultrasound Beamforming Using Compressed Data,2012,"The rapid advancements in electronics technologies have made software-based beamformers for ultrasound array imaging feasible, thus facilitating the rapid development of high-performance and potentially low-cost systems. However, one challenge to realizing a fully software-based system is transferring data from the analog front end to the software back end at rates of up to a few gigabits per second. This study investigated the use of data compression to reduce the data transfer requirements and optimize the associated trade-off with beamforming quality. JPEG and JPEG2000 compression techniques were adopted. The acoustic data of a line phantom were acquired with a 128-channel array transducer at a center frequency of 3.5 MHz, and the acoustic data of a cyst phantom were acquired with a 64-channel array transducer at a center frequency of 3.33 MHz. The receive-channel data associated with each transmit event are separated into 8 × 8 blocks and several tiles before JPEG and JPEG2000 data compression is applied, respectively. In one scheme, the compression was applied to raw RF data, while in another only the amplitude of baseband data was compressed. The maximum compression ratio of RF data compression to produce an average error of lower than 5 dB was 15 with JPEG compression and 20 with JPEG2000 compression. The image quality is higher with baseband amplitude data compression than with RF data compression; although the maximum overall compression ratio (compared with the original RF data size), which was limited by the data size of uncompressed phase data, was lower than 12, the average error in this case was lower than 1 dB when the compression ratio was lower than 8."
2028954,15226,30,Developing a Wireless Implantable Body Sensor Network in MICS Band,2011,"Through an integration of wireless communication and sensing technologies, the concept of a body sensor network (BSN) was initially proposed in the early decade with the aim to provide an essential technology for wearable, ambulatory, and pervasive health monitoring for elderly people and chronic patients. It has become a hot research area due to big opportunities as well as great challenges it presents. Though the idea of an implantable BSN was proposed in parallel with the on-body sensor network, the development in this area is relatively slow due to the complexity of human body, safety concerns, and some technological bottlenecks such as the design of ultralow-power implantable RF transceiver. This paper describes a new wireless implantable BSN that operates in medical implant communication service (MICS) frequency band. This system innovatively incorporates both sensing and actuation nodes to form a closed-control loop for physiological monitoring and drug delivery for critically ill patients. The sensing node, which is designed using system-on-chip technologies, takes advantage of the newly available ultralow-power Zarlink MICS transceiver for wireless data transmission. Finally, the specific absorption rate distribution of the proposed system was simulated to determine the in vivo electromagnetic field absorption and the power safety limits."
2409102,15226,30,Guest EditorialIntegrated Healthcare Information Systems,2012,"The use of integrated information systems for healthcare has been started more than a decade ago. In recent years, rapid advances in information integration methods have spurred tremendous growth in the use of integrated information systems in healthcare delivery. Various techniques have been used for probing such integrated systems. These techniques include service-oriented architecture (SOA), EAI, workflow management, grid computing, and others. Many applications require a combination of these techniques, which gives rise to the emergence of enterprise systems in healthcare. Development of the techniques originated from different disciplines has the potential to significantly improve the performance of enterprise systems in healthcare. This editorial paper briefly introduces the enterprise systems in the perspective of healthcare informatics."
702482,15226,30,Real-Time Mandibular Angle Reduction Surgical Simulation With Haptic Rendering,2012,"Mandibular angle reduction is a popular and efficient procedure widely used to alter the facial contour. The primary surgical instruments, the reciprocating saw, and the round burr, employed in the surgery have a common feature: operating at a high speed. Generally, inexperienced surgeons need a long-time practice to learn how to minimize the risks caused by the uncontrolled contacts and cutting motions in manipulation of instruments with high-speed reciprocation or rotation. A virtual reality-based surgical simulator for the mandibular angle reduction was designed and implemented on a compute unified device architecture (CUDA)-based platform in this paper. High-fidelity visual and haptic feedbacks are provided to enhance the perception in a realistic virtual surgical environment. The impulse-based haptic models were employed to simulate the contact forces and torques on the instruments. It provides convincing haptic sensation for surgeons to control the instruments under different reciprocation or rotation velocities. The real-time methods for bone removal and reconstruction during surgical procedures have been proposed to support realistic visual feedbacks. The simulated contact forces were verified by comparing against the actual force data measured through the constructed mechanical platform. An empirical study based on the patient-specific data was conducted to evaluate the ability of the proposed system in training surgeons with various experiences. The results confirm the validity of our simulator."
2452261,15226,390,GPU-based real-time implementation of 3D+T image reconstruction with application to cerebral angiography,2011,"Time sequences of 3D images of cerebral and other vasculature blood flow during surgery and other medical procedures allow enhanced visual feedback. The visual feedback constitutes an enhancement over the existing 2D time series of Xray projections as it facilitates the detection and observation of pathological abnormalities such as stenoses, aneurysms, and blood clots. An algorithm that outputs 3D+T sequences by fusing a single static 3D model of the vasculature with two time sequences of 2D projections was presented in [1]. Practical clinical use demands that the reconstruction be completed within a 5 minute time frame. When compared to CPU implementations, past GPU-based CT (computational tomography) implementations typically achieved one order-of-magnitude speed improvement, still insufficient speed for this application. To obtain further needed GPU speedup, we exploit the sparse structure of blood vasculature in order to achieve a total of two orders-of-magnitude performance increase. Our GPU implementation generates a 3D+T time series reconstruction in 2 minutes, enabling real time clinical use and safer, shorter procedures. Included in our approach is an architecture-aware partitioning method that accelerates the solution to a wide class of variational problems."
2455998,15226,30,Assessment of Tremor Activity in the Parkinson’s Disease Using a Set of Wearable Sensors,2012,"Tremor is the most common motor disorder of Parkinson's disease (PD) and consequently its detection plays a crucial role in the management and treatment of PD patients. The current diagnosis procedure is based on subject-dependent clinical assessment, which has a difficulty in capturing subtle tremor features. In this paper, an automated method for both resting and action/postural tremor assessment is proposed using a set of accelerometers mounted on different patient's body segments. The estimation of tremor type (resting/action postural) and severity is based on features extracted from the acquired signals and hidden Markov models. The method is evaluated using data collected from 23 subjects (18 PD patients and 5 control subjects). The obtained results verified that the proposed method successfully: 1) quantifies tremor severity with 87 % accuracy, 2) discriminates resting from postural tremor, and 3) discriminates tremor from other Parkinsonian motor symptoms during daily activities."
1077379,15226,65,Audio-based human activity recognition using Non-Markovian Ensemble Voting,2012,"Human activity recognition is a key component for socially enabled robots to effectively and naturally interact with humans. In this paper we exploit the fact that many human activities produce characteristic sounds from which a robot can infer the corresponding actions. We propose a novel recognition approach called Non-Markovian Ensemble Voting (NEV) able to classify multiple human activities in an online fashion without the need for silence detection or audio stream segmentation. Moreover, the method can deal with activities that are extended over undefined periods in time. In a series of experiments in real reverberant environments, we are able to robustly recognize 22 different sounds that correspond to a number of human activities in a bathroom and kitchen context. Our method outperforms several established classification techniques."
1138648,15226,65,Unsupervised object exploration using context,2014,"In order for robots to function in unstructured environments in interaction with humans, they must be able to reason about the world in a semantic meaningful way. An essential capability is to segment the world into semantic plausible object hypotheses. In this paper we propose a general framework which can be used for reasoning about objects and their functionality in manipulation activities. Our system employs a hierarchical segmentation framework that extracts object hypotheses from RGB-D video. Motivated by cognitive studies on humans, our work leverages on contextual information, e.g., that objects obey the laws of physics, to formulate object hypotheses from regions in a mathematically principled manner."
1907786,15226,30,Apnea MedAssist: Real-time Sleep Apnea Monitor Using Single-Lead ECG,2011,"We have developed a low-cost, real-time sleep apnea monitoring system ``Apnea MedAssist” for recognizing obstructive sleep apnea episodes with a high degree of accuracy for both home and clinical care applications. The fully automated system uses patient's single channel nocturnal ECG to extract feature sets, and uses the support vector classifier (SVC) to detect apnea episodes. “Apnea MedAssist” is implemented on Android operating system (OS) based smartphones, uses either the general adult subject-independent SVC model or subject-dependent SVC model, and achieves a classification F-measure of 90% and a sensitivity of 96% for the subject-independent SVC. The real-time capability comes from the use of 1-min segments of ECG epochs for feature extraction and classification. The reduced complexity of “Apnea MedAssist” comes from efficient optimization of the ECG processing, and use of techniques to reduce SVC model complexity by reducing the dimension of feature set from ECG and ECG-derived respiration signals and by reducing the number of support vectors."
1445657,15226,23593,An FPGA based parallel architecture for music melody matching,2013,"We propose an FPGA-based high performance parallel architecture for music retrieval through singing. The database consists of monophonic MIDI files which are modeled into strings, and the user sung query is modeled as a set of regular expressions (regexp), with consideration of possible key transpositions and tempo variations to tolerate imperfectly sung queries. An approximate regexp matching algorithm is developed to calculate the similarity between a regexp and a string, using edit distance as the metrics. The algorithm supports user sung queries starting anywhere in the database song, not necessarily from the beginning. Using the proposed formal models and algorithms, the similarity between the user sung query and each song in the database can be evaluated and the top-10 most similar results will be reported. We designed the approximate regexp matching algorithm in such way that all terms of the regexp can execute concurrently, which perfectly fits the massive parallelism provided by FPGA. The FPGA implemented melody matching engine (MME) is a parameterized modular architecture that can be reconfigured to implement different regexps by simply updating their parameter registers, and can therefore avoid the time-consuming code re-synthesis. MME also includes an on-board DDR2 memory to store the database, so that they can be read in to calculate edit distances locally on the board. This way, each MME forms a self-contained system and multiple MMEs can be clustered to increase parallel processing power, with virtually no overhead. MME is evaluated using the query corpus of ThinkIT with 355 sung files and database of 5563 MIDI files. It achieves a top-10 hit rate of 90.7% and a runtime of 19.4 seconds, averaging 54.6 milliseconds for a single query. MME achieves significant speedup over software-based systems while providing the same level of flexibility."
1877016,15226,11470,Automatic synchronization of electronic and audio books via TTS alignment and silence filtering,2011,"The e-book industry is starting to flourish due, in part, to the availability of affordable and user-friendly e-book readers. As users are increasingly moving from traditional paper books to e-books, there is an opportunity to reinvent and enhance their reading experience, for example, by leveraging the multimedia capabilities of these devices in order to turn the act of reading into a real multimedia experience. In this paper, we focus on the augmentation of the written text with its associated audiobook, so that users can listen to the book they are (currently) reading. We propose an audiobook-to-ebook alignment system by applying a Text-to-Speech (TTS)-based text to audio alignment algorithm, and enhance it with a silence filtering algorithm to cope with the difference on reading style between the TTS output and the speakers in the ebook environment. Experiments done using 12 five-minute excerpts of 6 different audio-books (read by men and women) yield usable word alignment errors below 120ms for 90% of the words. Finally, we also show a user interface implementation in the Ipad for synchronized e-book reading while listening to the associated audiobook."
55868,15226,22113,Sensorimotor models of space and object geometry,2011,"A baby experiencing the world for the first time faces a considerable challenging sorting through what William James called the blooming, buzzing confusion of the senses. With the increasing capacity of modern sensors and the complexity of modern robot bodies, a robot in an unknown or unfamiliar body faces a similar and equally daunting challenge.#R##N##R##N#Addressing this challenge directly by designing robot agents capable of resolving the confusion of sensory experience in an autonomous manner would substantially reduce the engineering required to program robots and the improve the robustness of resulting robot capabilities. Working towards a general solution to this problem, this work uses distinctive state abstractions and sensorimotor embedding to generate basic knowledge of sensor structure, local geometry, and object geometry starting with uninterpreted sensors and effectors."
1614865,15226,9099,Recipe sub-goals and graphs: an evaluation by cooks,2012,"Following recipes can be difficult for cooks. Many recipes use technical culinary language and condense their instructions into brief sentences, cooks may also get lost in long paragraphs as they jump around the recipe to find tasks to perform in parallel. Multimedia content has been shown to increase the confidence of cooks but few comparative evaluations have been reported. In this study we evaluated the effect of adding pictures of interim goal states to a plain text recipe and the effect of presenting recipe steps in a dependency graph representation. Initial results indicate that cooks value pictures of interim goals states to compare their ingredients against, and prefer a graph representation of a recipe because it supports the cook's non-linear path through recipe instructions."
1960897,15226,30,"Addressing Mental Health Epidemic Among University Students via Web-based, Self-Screening, and Referral System: A Preliminary Study",2011,"The prevalence and severity of mental health problems in college and university communities are alarming. However, the majority of students with mental disorders do not seek help from professionals. To help students assess their mental conditions and encourage them to take an active role in seeking care, we developed a web-based self-screening, referral, and secure communication system and evaluated it at the University of Washington for 17 months. The system handled more than 1000 screenings during the study period. Of the subjects who used the system, 75% noted that the system helped them to make a decision to receive help from professionals. The system was able to provide outreach to students with mental health concerns effectively, allow them to self-screen their conditions, and encourage them to receive professional assistance. The system provided students with 24/7 web-based access to the clinic, and more than 50% of the system use was made during off-hours. The system was well received by patients, referral managers, and care providers, and it was transferred to the clinic for daily clinical use. We believe that a web-based system like ours could be used as one way to tackle the growing epidemic of mental health problems among college and university students."
2122825,15226,30,Monitoring and Evaluation of Blood Pressure Changes With a Home Healthcare System,2011,"We investigated changes in blood pressure with exercise, including walking and ergometer training, sleep, and body weight. Blood pressure was monitored over a period of about 1 year in 61 subjects in Osaka, Japan. The morning systolic blood pressures were analyzed using multivariate regression analysis, and the correlations between systolic blood pressure and the above parameters were determined. The systolic blood pressure distribution was classified into improved, stable, and ingravescence groups. In the improved group, exercise intensity and total calories were important factors controlling the systolic blood pressure. More than 300 kcal per day was needed to improve the systolic blood pressure. In the stable and ingravescence groups, body weight control was also an important factor in maintaining blood pressure. An increase of 1 kg in body weight was associated with systolic blood pressure increases of 3 and 6 mmHg in the stable and ingravescence groups, respectively. The long-term repeated use of home blood pressure testing may be a good self-care strategy for monitoring daily health."
1019643,15226,30,A Smart Health Monitoring Chair for Nonintrusive Measurement of Biological Signals,2012,"We developed nonintrusive methods for simultaneous electrocardiogram, photoplethysmogram, and ballistocardiogram measurements that do not require direct contact between instruments and bare skin. These methods were applied to the design of a diagnostic chair for unconstrained heart rate and blood pressure monitoring purposes. Our methods were operationalized through capacitively coupled electrodes installed in the chair back that include high-input impedance amplifiers, and conductive textiles installed in the seat for capacitive driven-right-leg circuit configuration that is capable of recording electrocardiogram information through clothing. Photoplethysmograms were measured through clothing using seat mounted sensors with specially designed amplifier circuits that vary in light intensity according to clothing type. Ballistocardiograms were recorded using a film type transducer material, polyvinylidenefluoride (PVDF), which was installed beneath the seat cover. By simultaneously measuring signals, beat-to-beat heart rates could be monitored even when electrocardiograms were not recorded due to movement artifacts. Beat-to-beat blood pressure was also monitored using unconstrained measurements of pulse arrival time and other physiological parameters, and our experimental results indicated that the estimated blood pressure tended to coincide with actual blood pressure measurements. This study demonstrates the feasibility of our method and device for biological signal monitoring through clothing for unconstrained long-term daily health monitoring that does not require user awareness and is not limited by physical activity."
2233228,15226,30,Esophagus Silhouette Extraction and Reconstruction From Fluoroscopic Views for Cardiac Ablation Procedure Guidance,2011,"Cardiac ablation involves the risk of serious complications when thermal injury to the esophagus occurs. This paper proposes to reduce the risk of such injuries by a proactive visualization technique, improving physician awareness of the esophagus location in the absence of or in addition to a reactive monitoring device such as a thermal probe. This is achieved by combining a graphical representation of the esophagus with live fluoroscopy. Toward this goal, we present an automated method to reconstruct and visualize a 3-D esophagus model from fluoroscopy image sequences acquired using different C-arm viewing directions. In order to visualize the esophagus under fluoroscopy, it is first biomarked by swallowing a contrast agent such as barium. Images obtained in this procedure are then used to automatically extract the 2-D esophagus silhouette and reconstruct a 3-D surface of the esophagus internal wall. Once the 3-D representation has been computed, it can be visualized using fluoroscopy overlay techniques. Compared to 3-D esophagus imaging using CT or C-arm CT, our proposed fluoroscopy method requires low radiation dose and enables a simpler workflow on geometry-calibrated standard C-arm systems."
2096651,15226,30,A Novel Emotion Elicitation Index Using Frontal Brain Asymmetry for Enhanced EEG-Based Emotion Recognition,2011,"This paper aims at providing a novel method for evaluating the emotion elicitation procedures in an electroencephalogram (EEG)-based emotion recognition setup. By employing the frontal brain asymmetry theory, an index, namely asymmetry Index (AsI), is introduced, in order to evaluate this asymmetry. This is accomplished by a multidimensional directed information analysis between different EEG sites from the two opposite brain hemispheres. The proposed approach was applied to three-channel (Fp1, Fp2, and F3/F4 10/20 sites) EEG recordings drawn from 16 healthy right-handed subjects. For the evaluation of the efficiency of the AsI, an extensive classification process was conducted using two feature-vector extraction techniques and a SVM classifier for six different classification scenarios in the valence/arousal space. This resulted in classification results up to 62.58% for the user independent case and 94.40% for the user-dependent one, confirming the efficacy of AsI as an index for the emotion elicitation evaluation."
1167628,15226,30,3-D Streaming Supplying Partner Protocols for Mobile Collaborative Exergaming for Health,2012,"Childhood obesity is nowadays considered one of the major health problems that many societies suffer from today. The obesity epidemic leads to several life threatening conditions such as diabetes, heart disease, high blood pressure, and mental health problems like depression, anxiety, and loneliness just to mention a few. Several approaches, including physical exercises, strict diet, and exergames among others, have been adopted to address the obesity epidemic. Exergames are considered the innovative approach for fighting several health problems such as obesity, where a combination of “exercise” and 3-D “gaming” are proposed to incite kids to exercise as a team. Collaborative exergaming became even more popular given that it addresses the social side of the obesity epidemic, and it motivates kids to socialize with other kids. Traditional exergames are based on the client-server approach where the server is responsible for streaming the 3-D environment. However, this can lead to latency and server bottleneck if many clients participate in the exergame, which leads to the kids stopping exercising. Having an exergame application that does not suffer from networking problem such as delay, is very important given that it increases the exercise hours. In this paper, we propose a new trend of mobile collaborative exergaming applications that is based on the peer-to-peer architecture, as well as two supplying partner selection protocols that aim at selecting the suitable source responsible for streaming the relevant 3-D data. Our system, that we refer to as MOSAIC, is intended for mobile collaborative exergames that incite kids to move inside a large area, using thin mobile devices such as head-mounted devices, have physical exercises, and collaborate with other kids which in consequence address several health problems such as the obesity epidemic on the physical and social plans. Our proposed mobile collaborative exergame aims at inciting the kids to exercise as a team for a longer time by improving the quality of the streaming and reducing the delay. This is accomplished by our proposed supplying partner selection protocols that provide a quick discovery of multiple supplying partners, by minimizing the time required to acquire the data. The performance evaluation that we have obtained to evaluate our suite of protocols using a realistic set of exergame scenarios for obese kids is then presented and discussed."
1747621,15226,30,A System for Seismocardiography-Based Identification of Quiescent Heart Phases: Implications for Cardiac Imaging,2012,"Seismocardiography (SCG), a representation of mechanical heart motion, may more accurately determine periods of cardiac quiescence within a cardiac cycle than the electrically derived electrocardiogram (EKG) and, thus, may have implications for gating in cardiac computed tomography. We designed and implemented a system to synchronously acquire echocardiography, EKG, and SCG data. The device was used to study the variability between EKG and SCG and characterize the relationship between the mechanical and electrical activity of the heart. For each cardiac cycle, the feature of the SCG indicating Aortic Valve Closure was identified and its time position with respect to the EKG was observed. This position was found to vary for different heart rates and between two human subjects. A color map showing the magnitude of the SCG acceleration and computed velocity was derived, allowing for direct visualization of quiescent phases of the cardiac cycle with respect to heart rate."
2445423,15226,20515,Addressing biometrics security and privacy related challenges in China,2012,"There has been significant advancement improvement in the capabilities of biometrics and data protection technologies in last decade. The significant reduction in cost, improvements in speed and accuracy has resulted in increased deployment of such technologies in day-to-day business and public utilities. The increasing use of biometrics and data protection technologies has also raised concern on the unethical use of personal information. There are increasing number of incidents and concerns in the public over the infringement of personal privacy in China. This paper has investigated such emerging privacy related concerns in the deployment of biometrics and data protection technologies in China. This paper also includes a study on public attitudes toward such technologies and attempts to make comparison with the same in the difference with such emerging concerns in other developed countries. This paper has developed an online survey to ascertain people's understanding on the various aspects of privacy and thus willingness to tradeoff with the benefits of increased security. The online survey was conducted in February - March 2012 and revealed great deal of information from the 305 Hong Kong people. We have attempted to analyze the survey results which illustrate interesting findings on the use of CCTV, biometrics technologies, social networking, disclosure of personal information and recent (2012) privacy policy adjustments in popular websites."
1789969,15226,30,Subject-Specific Estimation of Central Aortic Blood Pressure Using an Individualized Transfer Function: A Preliminary Feasibility Study,2012,"This paper presents a new approach to the estimation of unknown central aortic blood pressure waveform from a directly measured peripheral blood pressure waveform, in which a physics-based model is employed to solve for a subject- and state-specific individualized transfer function (ITF). The ITF provides the means to estimate the unknown central aortic blood pressure from the peripheral blood pressure. Initial proof-of-principle for the ITF is demonstrated experimentally through an in vivo protocol. In swine subjects taken through wide range of physiologic conditions, the ITF was on average able to provide central aortic blood pressure waveforms more accurately than a nonindividualized transfer function. Its usefulness was most evident when the subject's pulse transit time deviated from normative values. In these circumstances, the ITF yielded statistically significant reductions over a nonindividualized transfer function in the following three parameters: 1) 30% reduction in the root-mean-squared error between estimated versus actual central aortic blood pressure waveform (p  -4 ), 2) >;50% reduction in the error between estimated versus actual systolic and pulse pressures ( p  ;3 mmHg, p  -4 ). In conclusion, the ITF may offer an attractive alternative to existing methods that estimates the central aortic blood pressure waveform, and may be particularly useful in nonnormative physiologic conditions."
1036710,15226,30,Highly comparative fetal heart rate analysis,2012,"A database of fetal heart rate (FHR) time series measured from 7 221 patients during labor is analyzed with the aim of learning the types of features of these recordings that are informative of low cord pH. Our ‘highly comparative’ analysis involves extracting over 9 000 time-series analysis features from each FHR time series, including measures of autocorrelation, entropy, distribution, and various model fits. This diverse collection of features was developed in previous work [1]. We describe five features that most accurately classify a balanced training set of 59 ‘low pH’ and 59 ‘normal pH’ FHR recordings. We then describe five of the features with the strongest linear correlation to cord pH across the full dataset of FHR time series. The features identified in this work may be used as part of a system for guiding intervention during labor in future. This work successfully demonstrates the utility of comparing across a large, interdisciplinary literature on time-series analysis to automatically contribute new scientific results for specific biomedical signal processing challenges."
1504142,15226,30,Multi-platform Data Integration in Microarray Analysis,2011,"An increasing number of studies have profiled gene expressions in tumor specimens using distinct microarray plat forms and analysis techniques. One challenging task is to develop robust statistical models in order to integrate multi-platform findings. We compare some methodologies on the field with respect to estrogen receptor (ER) status, and focus on a unified-among platforms scale implemented by Shen et at. in 2004, which is based on a Bayesian mixture model. Under this scale, we study the ER intensity similarities between four breast cancer datasets derived from various platforms. We evaluate our results with an independent dataset in terms of ER sample classification, given the derived gene ER signatures of the integrated data. We found that integrated multi-platform gene signatures and fold-change variability similarities between different platform measurements can assist the statistical analysis of independent microarray datasets in terms of ER classification."
2444487,15226,30,Irregular Breathing Classification From Multiple Patient Datasets Using Neural Networks,2012,"Complicated breathing behaviors including uncertain and irregular patterns can affect the accuracy of predicting respiratory motion for precise radiation dose delivery. So far investigations on irregular breathing patterns have been limited to respiratory monitoring of only extreme inspiration and expiration. Using breathing traces acquired on a Cyberknife treatment facility, we retrospectively categorized breathing data into several classes based on the extracted feature metrics derived from breathing data of multiple patients. The novelty of this paper is that the classifier using neural networks can provide clinical merit for the statistical quantitative modeling of irregular breathing motion based on a regular ratio representing how many regular/irregular patterns exist within an observation period. We propose a new approach to detect irregular breathing patterns using neural networks, where the reconstruction error can be used to build the distribution model for each breathing class. The proposed irregular breathing classification used a regular ratio to decide whether or not the current breathing patterns were regular. The sensitivity, specificity, and receiver operating characteristiccurve of the proposed irregular breathing pattern detector was analyzed. The experimental results of 448 patients' breathing patterns validated the proposed irregular breathing classifier."
1954142,15226,30,Automated Delineation of Lung Tumors in PET Images Based on Monotonicity and a Tumor-Customized Criterion,2011,"Reliable automated or semiautomated lung tumor delineation methods in positron emission tomography should provide accurate tumor boundary definition and separation of the lung tumor from surrounding tissue or “hot spots” that have similar intensities to the lung tumor. We propose a tumor-customized downhill (TCD) method to achieve these objectives. Our approach includes: 1) automatic formulation of a tumor-customized criterion to improve tumor boundary definition, 2) a monotonic property of the standardized uptake value (SUV) of tumors to separate the tumor from adjacent regions of increased metabolism (“hot spot”), and 3) accounts for tumor heterogeneity. Three simulated lesions and 30 PET-CT studies, grouped into “simple” and “complex” groups, were used for evaluation. Our main findings are that TCD, when compared to the threshold based on 40% and 50% maximum SUV, adaptive threshold, Fuzzy c-means, and watershed techniques achieved the highest Dice's similarity coefficient average for simulation data (0.73) and “complex” group (0.71); the least volumetric error in the “simple” (1.76 mL) and the “complex” group (14.59 mL); and TCD solves the problem of leakage into adjacent tissues when many other techniques fail."
1443526,15226,30,Single-trial EEG discrimination between wrist and finger movement imagery and execution in a sensorimotor BCI,2011,"Brain-computer interface (BCI) may be used to control a prosthetic or orthotic hand using neural activity from the brain. The core of this sensorimotor BCI lies in the interpretation of the neural information extracted from electroencephalogram (EEG). It is desired to improve on the interpretation of EEG to allow people with neuromuscular disorders to perform daily activities. This paper investigates the possibility of discriminating between the EEG associated with wrist and finger movements. The EEG was recorded from test subjects as they executed and imagined five essential hand movements using both hands. Independent component analysis (ICA) and time-frequency techniques were used to extract spectral features based on event-related (de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used for feature reduction. Mahalanobis distance (MD) clustering and artificial neural networks (ANN) were used as classifiers and obtained average accuracies of 65 % and 71 % respectively. This shows that EEG discrimination between wrist and finger movements is possible. The research introduces a new combination of motor tasks to BCI research."
2499079,15226,30,Investigating the Minimum Required Number of Genes for the Classification of Neuromuscular Disease Microarray Data,2011,"The discovery of potential microarray markers, which will expedite molecular diagnosis/prognosis and provide reliable results to clinical decision-making and treatment selection for patients, is of paramount importance. Feature selection techniques, which aim at minimizing the dimensionality of the microarray data by keeping the most statistically significant genes, are a powerful approach toward this goal. In this paper, we investigate the minimum required subsets of genes, which best classify neuromuscular disease data. For this purpose, we implemented a methodology pipeline that facilitated the use of multiple feature selection methods and subsequent performance of data classification. Five feature selection methods on datasets from ten different neuromuscular diseases were utilized. Our findings reveal subsets of very small number of genes, which can successfully classify normal/disease samples. Interestingly, we observe that similar classification results may be obtained from different subsets of genes. The proposed methodology can expedite the identification of small gene subsets with high-classification accuracy that could ultimately be used in the genetics clinics for diagnostic, prognostic, and pharmacogenomic purposes."
2205287,15226,9099,Intelligent menu planning: recommending set of recipes by ingredients,2012,"With the growth of recipe sharing services, online cooking recipes associated with ingredients and cooking procedures are available. Many recipe sharing sites have devoted to the development of recipe recommendation mechanism. However, there is a need for users to plan menu of meals by ingredients. While most research on food related research has been on recipe recommendation and retrieval, little research has been done on menu planning. In this paper, we investigate an intelligent menu planning mechanism which recommending sets of recipes by user-specified ingredients. Those recipes which are well-accompanied and contain the query ingredients are returned. We propose a graph-based algorithm for menu planning. The proposed approach constructs a recipe graph to capture the co-occurrence relationships between recipes from collection of menus. A menu is generated by approximate Steiner Tree Algorithm on the constructed recipe graph. Evaluation of menu collections from Food.com shows that the proposed approach achieves encouraging results."
2419096,15226,390,Hierarchical and graphical analysis of fMRI network connectivity in healthy and schizophrenic groups,2011,"Understanding the changes or disruption in the connectivity among brain networks is important for identifying potential biological markers for neuropsychiatric diseases. Multivariate and data-driven methods, especially independent component analysis (ICA), have proven to be a powerful tool in this field. Here, we introduce a novel analysis scheme that incorporates hierarchical and graphical techniques to study the connectivity differences between healthy controls and schizophrenia patients, using the spatial dependence among ICA components as an index of network connectivity. We find that compared to healthy controls, the schizophrenic group presents an altered hierarchy with a number of unusual connections and a significantly decreased small-world index, suggesting disease-related changes in the organization of brain connectivity."
2490196,15226,30,Optimized Weighted Performance Index for Objective Evaluation of Border-Detection Methods in Dermoscopy Images,2011,"Quantitative evaluation of the existing border-detection methods is commonly performed by using different metrics. This is inherently problematic due to the different characteristics of each metric. This paper presents a novel approach for objective evaluation of border-detection methods in dermoscopy images by introducing a comprehensive evaluation metric: optimized weighted performance index. The index is formed as a non linear weighted function of the six commonly used metrics of sensitivity, specificity, accuracy, precision, border error, and similarity. Constrained nonlinear multivariable optimization techniques are applied to determine the optimal set of weights that result in the maximum value of the index. This index is used as an effective measure of the value of a given border-detection method and, thus, provides a basis for comparison with other methods. To demonstrate the effectiveness of the proposed index, it is used to evaluate five recent border-detection methods applied on a set of 55 high resolution dermoscopy images."
1579884,15226,23735,Movement of artificial bacterial flagella in heterogeneous viscous environments at the microscale,2012,"Swimming microrobots have the potential to be used in medical applications such as targeted drug delivery. The challenges for navigating microrobots in the human body lie not only in the viscosity of body fluids but also in the existence of different types of fibers and cells such as blood cells or protein strands. This paper investigates artificial bacterial flagella (ABFs), which are helical microrobots actuated by an external magnetic field, in methyl cellulose solutions of different concentrations. It can be shown that the microrobots can be propelled in these gel-like heterogeneous solutions and successful swimming was demonstrated in solutions with a viscosity of more than 20 times that of water. Furthermore, results indicate that the existence of fibers can help ABFs swim more effectively, which agrees with previous experimental results reported for natural bacteria."
1514820,15226,9099,Tele echo tube: beyond cultural and imaginable boundaries,2013,"Currently, human-computer interaction (HCI) is primarily focused on human-centric interactions; however, people experience many nonhuman-centric interactions during the course of a day. Interactions with nature, such as experiencing the sounds of birds or trickling water, can imprint the beauty of nature in our memories. In this context, this paper presents an interface of such nonhuman interactions to observe people's reaction to the interactions through an imaginable interaction with a mythological creature. Tele Echo Tube (TET) is a speaking tube interface that acoustically interacts with a deep mountain echo through the slightly vibrating lampshade-like interface. TET allows users to interact with the mountain echo in real time through an augmented echo-sounding experience with the vibration over a satellite data network. This novel interactive system can create an imaginable presence of the mythological creature in the undeveloped natural locations beyond our cultural and imaginable boundaries. The results indicate that users take the reflection of the sound as a cue that triggers the nonlinguistic believability in the form of the mythological metaphor of the mountain echo. This echo-like experience of believable interaction in an augmented reality between a human and nature gave the users an imaginable presence of the mountain echo with a high degree of excitement. This paper describes the development and integration of nonhuman-centric design protocols, requirements, methods, and context evaluation."
2390673,15226,21106,Can modern technologies defeat nazi censorship,2012,"Censorship of parts of written text was and is a common practice in totalitarian regimes. It is used to destroy information not approved by the political power. Recovering the censored text is of interest for historical studies of the text. This paper raises the question, whether a censored postcard from 1942 can be made legible by applying multispectral imaging in combination with laser cleaning. In the fields of art conservation (e.g. color measurements), investigation (e.g. analysis of underdrawings in paintings), and historical document analysis, multispectral imaging techniques have been applied successfully to give visibility to information hidden to the human eye.#R##N##R##N#The basic principle of laser cleaning is to transfer laser pulse energy to a contamination layer by an absorption process that leads to heating and evaporation of the layer. Partial laser cleaning of postcards is possible; dirt on the surface can be removed and the obscured pictures and writings made visible again. We applied both techniques to the postcard. The text could not be restored since the original ink seems to have suffered severe chemical damage."
907739,15226,11470,Computer-assisted self-training system for sports exercise using kinects,2013,"Self-training plays an important role in sports exercise. However, if not under the instruction of a coach, improper training postures can cause serious harm to muscles and ligaments of the body. Hence, the development of computer-assisted self-training systems for sports exercise is a recently emerging research topic. In this paper, we propose a Yoga self-training system, entitled YogaST, which aims at instructing the user/practitioner to perform the asana (Yoga posture) correctly and preventing injury caused by improper postures. Involving professional Yoga training knowledge, YogaST analyzes the practitioner's posture from both front and side views using two Kinects with perpendicular viewing directions and assists him/her in rectifying bad postures. The contour, skeleton, and feature axes of the human body are extracted as posture representation. Then, YogaST analyzes the practitioner's posture and presents visualized instruction for posture rectification so that the practitioner can easily understand how to adjust his/her posture."
1365547,15226,11317,Deconstructing and restyling D3 visualizations,2014,"The D3 JavaScript library has become a ubiquitous tool for developing visualizations on the Web. Yet, once a D3 visualization is published online its visual style is difficult to change. We present a pair of tools for deconstructing and restyling existing D3 visualizations. Our deconstruction tool analyzes a D3 visualization to extract the data, the marks and the mappings between them. Our restyling tool lets users modify the visual attributes of the marks as well as the mappings from the data to these attributes. Together our tools allow users to easily modify D3 visualizations without examining the underlying code and we show how they can be used to deconstruct and restyle a variety of D3 visualizations."
551714,15226,20515,Speedup for European epassport authentication,2014,"The overall ePassport authentication procedure should be fast to have a sufficient throughput of people at border crossings such as airports. At the same time, the ePassport and its holder should be checked as thoroughly as possible. By speeding up the ePassport authentication procedure, more time can be spend on verification of biometrics. We demonstrate that our proposed solution allows to replace the current combination of PACE and EAC with a more efficient authentication procedure that provides even better security and privacy guarantees. When abstracting away from the time needed for the ePassport to verify the terminal's certificate, a speed-up of at least 40% in comparison with the current ePassport authentication procedure is to be expected."
2448804,15226,11470,A study of developing the procedural logic learning system using the concept of Therbligs,2011,"According to analysis and design of logic flow, the Therbligs of computer assembling are divided into position seeking, object selection, object rotation, object assembly, pressing, object loosening, fastening and inspection etc. Respective multimedia program language is used to build corresponding system function, so as to train the operators on production line to master standard operation procedures. Learners can correctly operate the procedures of hardware assembling and pay attention to installation key points, avoiding the irreversible damage as learners go directly into real machine operation before knowing well or without paying attention to the key procedures, in which case even mainboard have to be replaced by a new one. This study changes the traditional evaluation method of learning performance. Apart from multiple-choice question and true-false question, it develops the learning and operation testing system of simulated training and puts forward a logic learning script method. The concept of Therbligs is adopted to analyze the procedures of computer hardware assembling."
1837599,15226,30,GRISSOM Platform: Enabling Distributed Processing and Management of Biological Data Through Fusion of Grid and Web Technologies,2011,"Transcriptomic technologies have a critical impact in the revolutionary changes that reshape biological research. Through the recruitment of novel high-throughput instrumentation and advanced computational methodologies, an unprecedented wealth of quantitative data is produced. Microarray experiments are considered high-throughput, both in terms of data volumes (data intensive) and processing complexity (computationally intensive). In this paper, we present grids for in silico systems biology and medicine (GRISSOM), a web-based application that exploits GRID infrastructures for distributed data processing and management, of DNA microarrays (cDNA, Affymetrix, Illumina) through a generic, consistent, computational analysis framework. GRISSOM performs versatile annotation and integrative analysis tasks, through the use of third-party application programming interfaces, delivered as web services. In parallel, by conforming to service-oriented architectures, it can be encapsulated in other biomedical processing workflows, with the help of workflow enacting software, like Taverna Workbench, thus rendering access to its algorithms, transparent and generic. GRISSOM aims to set a generic paradigm of efficient metamining that promotes translational research in biomedicine, through the fusion of grid and semantic web computing technologies."
1385562,15226,30,Patient-Specific Prediction of Coronary Plaque Growth From CTA Angiography: A Multiscale Model for Plaque Formation and Progression,2012,"Computational fluid dynamics methods based on in vivo 3-D vessel reconstructions have recently been identified the influence of wall shear stress on endothelial cells as well as on vascular smooth muscle cells, resulting in different events such as flow mediated vasodilatation, atherosclerosis, and vascular remodeling. Development of image-based modeling technologies for simulating patient-specific local blood flows is introducing a novel approach to risk prediction for coronary plaque growth and progression. In this study, we developed 3-D model of plaque formation and progression that was tested in a set of patients who underwent coronary computed tomography angiography (CTA) for anginal symptoms. The 3-D blood flow is described by the Navier-Stokes equations, together with the continuity equation. Mass transfer within the blood lumen and through the arterial wall is coupled with the blood flow and is modeled by a convection-diffusion equation. The low density lipoprotein (LDL) transports in lumen of the vessel and through the vessel tissue (which has a mass consumption term) are coupled by Kedem-Katchalsky equations. The inflammatory process is modeled using three additional reaction-diffusion partial differential equations. A full 3-D model was created. It includes blood flow and LDL concentration, as well as plaque formation and progression. Furthermore, features potentially affecting plaque growth, such as patient risk score, circulating biomarkers, localization and composition of the initial plaque, and coronary vasodilating capability were also investigated. The proof of concept of the model effectiveness was assessed by repetition of CTA, six months after the baseline evaluation. Besides the low values of local shear stress, plaque characteristics, risk profile, pattern of circulating adhesion molecules, and reduced coronary flow reserve at baseline appeared to affect plaque progression toward flow-limiting lesions at follow-up evaluation. Although preliminary, our multidisciplinary approach to a “personalized” prediction of coronary plaque progression suggests that incorporation in atherosclerotic models of systemic and local hemodynamic features may better predict evolution of plaques in coronary artery disease stable patients."
2167209,15226,30,Rhythmogram-Based Analysis for Continuous Electrographic Data of the Human Brain,2012,"Ecologically relevant stimuli are rarely used in scientific studies because they are difficult to control. Instead, researchers employ simple stimuli with sharp boundaries (in space and time). Here, we explore how the rhythmogram can be used to provide much needed rigorous control of natural continuous stimuli like music and speech. The analysis correlates important features in the time course of stimuli with corresponding features in brain activations elicited by the same stimuli. Correlating the identified regularities of the stimulus time course with the features extracted from the activations of each voxel of a tomographic analysis of brain activity provides a powerful view of how different brain regions are influenced by the stimulus at different times and over different (user-selected) timescales. The application of the analysis to tomographic solutions extracted from magnetoencephalographic data recorded while subjects listen to music reveals a surprising and aesthetically pleasing aspect of brain function: an area believed to be specialized for visual processing is recruited to analyze the music after the acoustic signal is transformed to a feature map. The methodology is ideal for exploring processing of complex stimuli, e.g., linguistic structure and meaning and how it fails, for example, in developmental dyslexia."
2380035,15226,30,Mobility Support for Health Monitoring at Home Using Wearable Sensors,2011,"We present a simple but effective handoff protocol that enables continuous monitoring of ambulatory patients at home by means of resource-limited sensors. Our proposed system implements a 2-tier network: one created by wearable sensors used for vital signs collection, and another by a point-to-point link established between the body sensor network coordinator device and a fixed access point (AP). Upon experiencing poor signal reception in the latter network tier when the patient moves, the AP may instruct the sensor network coordinator to forward vital signs data through one of the wearable sensor nodes acting as a temporary relay if the sensor-AP link has a stronger signal. Our practical implementation of the proposed scheme reveals that this relayed data operation decreases packet loss rate down to 20% of the value otherwise obtained when solely using the point-to-point, coordinator-AP link. In particular, the wrist location yields the best results over alternative body sensor positions when patients walk at a 0.5 m/s."
1816719,15226,30,Determining Level of Postural Control in Young Adults Using Force-Sensing Resistors,2011,"A force-sensing platform (FSP), sensitive to changes of the postural control system was designed. The platform measured effects of postural perturbations in static and dynamic conditions. This paper describes the implementation of an FSP using force-sensing resistors as sensing elements. Real-time qualitative assessment utilized a rainbow color scale to identify areas with high force concentration. Postprocessing of the logged data provided end-users with quantitative measures of postural control. The objective of this research was to establish the feasibility of using an FSP to test and gauge human postural control. Tests were conducted in eye open and eye close states. Readings obtained were tested for repeatability using a one-way analysis of variance test. The platform gauged postural sway by measuring the area of distribution for the weighted center of applied pressure at the foot. A fuzzy clustering algorithm was applied to identify regions of the foot with repetitive pressure concentration. Potential application of the platform in a clinical setting includes monitoring rehabilitation progress of stability dysfunction. The platform functions as a qualitative tool for initial, on-the-spot assessment, and quantitative measure for postacquisition assessment on balance abilities."
2083599,15226,30,Strengths and Weaknesses of 1.5T and 3T MRS Data in Brain Glioma Classification,2011,"Although magnetic resonance spectroscopy (MRS) methods of 1.5Tesla (T) and 3T have been widely applied during the last decade for noninvasive diagnostic purposes, only a few studies have been reported on the value of the information extracted in brain cancer discrimination. The purpose of this study is threefold. First, to show that the diagnostic value of the information extracted from two different MRS scanners of 1.5T and 3T is significantly influenced in terms of brain gliomas discrimination. Second, to statistically evaluate the discriminative potential of publicly known metabolic ratio markers, obtained from these two types of scanners in classifying low-, intermediate-, and high-grade gliomas. Finally, to examine the diagnostic value of new metabolic ratios in the discrimination of complex glioma cases where the diagnosis is both challenging and critical. Our analysis has shown that although the information extracted from 3T MRS scanner is expected to provide better brain gliomas discrimination; some factors like the features selected, the pulse-sequence parameters, and the spectroscopic data acquisition methods can influence the discrimination efficiency. Finally, it is shown that apart from the bibliographical known, new metabolic ratio features such as N-acetyl aspartate/ S , Choline/ S , Creatine/ S  , and myo-Inositol/ S  play significant role in gliomas grade discrimination."
2490679,15226,30,Hemodynamic Flow Modeling Through an Abdominal Aorta Aneurysm Using Data Mining Tools,2011,"Geometrical changes of blood vessels, called aneurysm, occur often in humans with possible catastrophic outcome. Then, the blood flow is enormously affected, as well as the blood hemodynamic interaction forces acting on the arterial wall. These forces are the cause of the wall rupture. A mechanical quantity characteristic for the blood-wall interaction is the wall shear stress, which also has direct physiological effects on the endothelial cell behavior. Therefore, it is very important to have an insight into the blood flow and shear stress distribution when an aneurysm is developed in order to help correlating the mechanical conditions with the pathogenesis of pathological changes on the blood vessels. This insight can further help in improving the prevention of cardiovascular diseases evolution. Computational fluid dynamics (CFD) has been used in general as a tool to generate results for the mechanical conditions within blood vessels with and without aneurysms. However, aneurysms are very patient specific and reliable results from CFD analyses can be obtained by a cumbersome and time-consuming process of the computational model generation followed by huge computations. In order to make the CFD analyses efficient and suitable for future everyday clinical practice, we have here employed data mining (DM) techniques. The focus was to combine the CFD and DM methods for the estimation of the wall shear stresses in an abdominal aorta aneurysm (AAA) underprescribed geometrical changes. Additionally, computing on the grid infrastructure was performed to improve efficiency, since thousands of CFD runs were needed for creating machine learning data. We used several DM techniques and found that our DM models provide good prediction of the shear stress at the AAA in comparison with full CFD model results on real patient data."
1161846,15226,65,Robust fingertip tracking for constructing an intelligent room,2012,"In a watch-over system, the machine has to detect intention of human user according to circumstance, and such detection has to be able to be achieved naturally without special interface gadget like a hand controller. Hence, in this paper, we propose a hand detection and fingertip tracking method for use in an intelligent room. We are constructing an intelligent room in which any user can control home appliances with intuitive gestures. The presented intelligent room is a room in which home appliances and an interaction monitoring robot for elderly people are operated using intuitive gestures, without any additional equipment and any restriction for positions. The proposed method detects the hand region from a range data, which is robust for environmental changing in complex background. Using weak classifiers, the shape of extracted hand and direction of the fingertip is estimated without needing to know the location of the face. These processes are computationally fast enough to track an object in real time. In addition, characters written on a virtual 2D plane by the user's finger movement were estimated using incremental simple PCA for operator recognition."
965185,15226,30,Comparison of Block Matching and Differential Methods for Motion Analysis of the Carotid Artery Wall From Ultrasound Images,2012,"Motion of the carotid artery wall is important for the quantification of arterial elasticity and contractility and can be estimated with a number of techniques. In this paper, a framework for quantitative evaluation of motion analysis techniques from B-mode ultrasound images is introduced. Six synthetic sequences were produced using 1) a real image corrupted by Gaussian and speckle noise of 25 and 15 dB, and 2) the ultrasound simulation package Field II. In both cases, a mathematical model was used, which simulated the motion of the arterial wall layers and the surrounding tissue, in the radial and longitudinal directions. The performance of four techniques, namely optical flow (OF HS ), weighted least-squares optical flow (OF LK(WLS) ), block matching (BM), and affine block motion model (ABMM), was investigated in the context of this framework. The average warping indices were lowest for OF LK(WLS)  (1.75 pixels), slightly higher for ABMM (2.01 pixels), and highest for BM (6.57 pixels) and OF HS  (11.57 pixels). Due to its superior performance, OF LK(WLS)  was used to quantify motion of selected regions of the arterial wall in real ultrasound image sequences of the carotid artery. Preliminary results indicate that OF LK(WLS)  is promising, because it efficiently quantified radial, longitudinal, and shear strains in healthy adults and diseased subjects."
2392219,15226,30,A Reliable Transmission Protocol for ZigBee-Based Wireless Patient Monitoring,2012,"Patient monitoring systems are gaining their importance as the fast-growing global elderly population increases demands for caretaking. These systems use wireless technologies to transmit vital signs for medical evaluation. In a multihop ZigBee network, the existing systems usually use broadcast or multicast schemes to increase the reliability of signals transmission; however, both the schemes lead to significantly higher network traffic and end-to-end transmission delay. In this paper, we present a reliable transmission protocol based on anycast routing for wireless patient monitoring. Our scheme automatically selects the closest data receiver in an anycast group as a destination to reduce the transmission latency as well as the control overhead. The new protocol also shortens the latency of path recovery by initiating route recovery from the intermediate routers of the original path. On the basis of a reliable transmission scheme, we implement a ZigBee device for fall monitoring, which integrates fall detection, indoor positioning, and ECG monitoring. When the triaxial accelerometer of the device detects a fall, the current position of the patient is transmitted to an emergency center through a ZigBee network. In order to clarify the situation of the fallen patient, 4-s ECG signals are also transmitted. Our transmission scheme ensures the successful transmission of these critical messages. The experimental results show that our scheme is fast and reliable. We also demonstrate that our devices can seamlessly integrate with the next generation technology of wireless wide area network, worldwide interoperability for microwave access, to achieve real-time patient monitoring."
2439583,15226,30,Vessel Tree Segmentation in Presence of Interstitial Lung Disease in MDCT,2011,"The automated segmentation of vessel tree structures is a crucial preprocessing stage in computer aided diagnosis (CAD) schemes of interstitial lung disease (ILD) patterns in multidetector computed tomography (MDCT). The accuracy of such preprocessing stages is expected to influence the accuracy of lung CAD schemes. Although algorithms aimed at improving the accuracy of lung fields segmentation in presence of ILD have been reported, the corresponding vessel tree segmentation stage is under-researched. Furthermore, previously reported vessel tree segmentation methods have only dealt with normal lung parenchyma. In this paper, an automated vessel tree segmentation scheme is proposed, adapted to the presence of pathologies affecting lung parenchyma. The first stage of the method accounts for a recently proposed method utilizing a 3-D multiscale vessel enhancement filter based on eigenvalue analysis of the Hessian matrix and on unsupervised segmentation. The second stage of the method is a texture-based voxel classification refinement to correct possible over-segmentation. The performance of the proposed scheme, and of the previously reported technique, in vessel tree segmentation was evaluated by means of area overlap (previously reported: 0.715 ± 0.082, proposed: 0.931 ± 0.027), true positive fraction (previously reported: 0.968 ± 0.019, proposed: 0.935 ± 0.036) and false positive fraction (previously reported: 0.400 ± 0.181, proposed: 0.074 ± 0.031) on a dataset of 210 axial slices originating from seven ILD affected patient scans (used for performance evaluation out of 15). The pro posed method demonstrated a statistically significantly (p <; 0.05) higher performance as compared to the previously reported vessel tree segmentation technique. The impact of suboptimal vessel tree segmentation in a reticular pattern quantification system is also demonstrated."
1238519,15226,30,Automatic Skin Lesion Segmentation via Iterative Stochastic Region Merging,2011,"An automatic method for segmenting skin lesions in conventional macroscopic images is presented. The images are acquired with conventional cameras, without the use of a dermoscope. Automatic segmentation of skin lesions from macroscopic images is a very challenging problem due to factors such as illumination variations, irregular structural and color variations, the presence of hair, as well as the occurrence of multiple unhealthy skin regions. To address these factors, a novel iterative stochastic region-merging approach is employed to segment the regions corresponding to skin lesions from the macroscopic images, where stochastic region merging is initialized first on a pixel level, and subsequently on a region level until convergence. A region merging likelihood function based on the regional statistics is introduced to determine the merger of regions in a stochastic manner. Experimental results show that the proposed system achieves overall segmentation error of under 10% for skin lesions in macroscopic images, which is lower than that achieved by existing methods."
2468701,15226,30,Fisher–Tippett Region-Merging Approach to Transrectal Ultrasound Prostate Lesion Segmentation,2011,"In this paper, a computerized approach to segmenting prostate lesions in transrectal ultrasound (TRUS) images is presented. The segmentation of prostate lesions from TRUS images is very challenging due to issues, such as poor contrast, low SNRs, and irregular shape variations. To address these issues, a novel approach is employed to segment the lesions from the surrounding prostate, where region merging is performed via a region-merging likelihood function based on regional statistics, as well as Fisher-Tippett statistics. Experimental results using TRUS prostate images demonstrate that the proposed Fisher-Tippett region-merging approach achieves more accurate segmentation of prostate lesions when compared to other segmentation methods."
2592608,15226,11052,Seeing is Worse than Believing: Reading People’s Minds Better than Computer-Vision Methods Recognize Actions,2014,"This work was supported, in part, by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF - 1231216. AB, DPB, NS, and JMS were supported, in part, by Army Research Laboratory (ARL) Cooperative Agreement W911NF-10-2-0060, AB, in part, by the Center forBrains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, WC, CX, and JJC, in part, by ARL Cooperative Agreement W911NF-10-2-0062 and NSF CAREER grant IIS-0845282, CDF, in part, by NSF grant CNS-0855157, CH and SJH, in part, by the McDonnell Foundation, and BAP, in part, by Science Foundation Ireland grant 09/IN.1/I2637."
822414,15226,23735,Bio-inspired TauPilot for automated aerial 4D docking and landing of Unmanned Aircraft Systems,2012,"This paper presents the development and experimental validation of a bio-inspired autopilot, called TauPilot, that is based on the ecological Tau Theory proposed by the psychologist David Lee. Tau theory postulates that animals and humans use the tau (τ) variable (or Time-To-Contact) and simple guidance strategies to prospectively control most of their purposeful movements. This research investigates the feasibility and effectiveness of applying tau theory principles for guiding some crucial maneuvers of Unmanned Aircraft Systems (UAS) such as braking, automated aerial docking, automatic landing and moving target interception. The developed TauPilot includes a tau-Guidance system, a tau-Navigation system and a tau-Controller, resulting in 4D (time as the fourth dimension) GN&C system that has the capability to accurately fit maneuvers or actions into 4D slots using only a universal temporal variable tau. TauPilot has been integrated into two rotorcraft UAS and demonstrated in more than one thousand (about 1114) successful tau-controlled flights."
764820,15226,11470,Contrast enhancement based single image dehazing VIA TV-l 1 minimization,2014,"ABSTRACTIn this paper, we propose a general algorithm to removinghaze from single images using total variation minimization.Our approach stems from two simple yet fundamental ob-servations about haze-free images and the haze itself. First,clear-day images usually have stronger contrast than imagesplagued by bad weather; and second, the variations in natu-ral atmospheric veil, which highly depends on the depth ofobjects, always tend to be smooth. Integrating these two cri-teria together leads to a new effective dehazing model, whichencourages the gradient l 1 sparsity of atmospheric veil andimplicitly maximizes the global contrast of haze-free imagein the meanwhile. We also show that the proposed dehazingmodel can be efﬁciently solved using the TV-l 1 minimiza-tion. Compared to alternative state-of-the-art methods, ourapproach is physically plausible and works well for all typesof hazy situations. Comparative study and quantitative evalu-ation on both synthetic and natural images validate the supe-rior performance and the generality of our approach.Index Terms— Image dehazing, visibility restoration,contrast enhancement, total variation minimization1. INTRODUCTIONThe photographs we get in our daily life are very easy to beplaguedby various aerosols suspendedin the air, such as dust,mist, and/or fumes. Due to the existing aerosols, the raysreﬂected by the surfaces of objects are not only attenuated b utalso blendedwith the airlight, beforetheyreach the observers.This inevitably results in poor visibility and contrast lost inthe image. As a result, the quality of photographs taken inhazy scenes are seriously degraded.The goal of dehazing is to recover color, visibility anddetails of the scene from hazy images. There are many real-world circumstances, where accurate dehazing results are"
1876302,15226,30,Electromagnetic Interference of Cardiac Rhythmic Monitoring Devices to Radio Frequency Identification: Analytical Analysis and Mitigation Methodology,2011,"Increasing density of wireless communication and development of radio frequency identification (RFID) technology in particular have increased the susceptibility of patients equipped with cardiac rhythmic monitoring devices (CRMD) to environmental electro magnetic interference (EMI). Several organizations reported observing CRMD EMI from different sources. This paper focuses on mathematically analyzing the energy as perceived by the implanted device, i.e., voltage. Radio frequency (RF) energy transmitted by RFID interrogators is considered as an example. A simplified front-end equivalent circuit of a CRMD sensing circuitry is proposed for the analysis following extensive black-box testing of several commercial pacemakers and implantable defibrillators. After careful understanding of the mechanics of the CRMD signal processing in identifying the QRS complex of the heart-beat, a mitigation technique is proposed. The mitigation methodology introduced in this paper is logical in approach, simple to implement and is therefore applicable to all wireless communication protocols."
22703,15226,23735,Preliminary evaluation of a micro-force sensing handheld robot for vitreoretinal surgery,2012,"Highly accurate positioning is fundamental to the performance of vitreoretinal microsurgery. Of vitreoretinal procedures, membrane peeling is among the most prone to complications since extremely delicate manipulation of retinal tissue is required. Associated tool-to-tissue interaction forces are usually below the threshold of human perception, and the surgical tools are moved very slowly, within the 0.1–0.5 mm/s range. During the procedure, unintentional tool motion and excessive forces can easily give rise to vision loss or irreversible damage to the retina. A successful surgery includes two key features: controlled tremor-free tool motion and control of applied force. In this study, we present the potential benefits of a micro-force sensing robot in vitreoretinal surgery. Our main contribution is implementing fiber Bragg grating based force sensing in an active tremor canceling handheld micromanipulator, known as Micron, to measure tool-to-tissue interaction forces in real time. Implemented auditory sensory substitution assists in reducing and limiting forces. In order to test the functionality and performance, the force sensing Micron was evaluated in peeling experiments with adhesive bandages and with the inner shell membrane from chicken eggs. Our findings show that the combination of active tremor canceling together with auditory sensory substitution is the most promising aid that keeps peeling forces below 7 mN with a significant reduction in 2–20 Hz oscillations."
2052709,15226,30,A Wireless Implantable Sensor Network System for In Vivo Monitoring of Physiological Signals,2011,"A wireless implantable sensor network system (WISNS) is designed for in vivo monitoring physiological signals of a population of animals. WISNS can simultaneously monitor more than 15 animals, communicating three kinds of analog information among sensor nodes. Analog signals are transmitted to relay node at 800-KHz carrier by AM. Relay nodes digitalize and package them into messages, and then forward to the Wireless sensor network by Nordic RF technology (NWSN). Smaller overall dimensions ( 3 ), lower power regulation, and dedicated packaging make the system suitable and compatible for implantable devices. The implantable sensor node, protocol stack of NWSN, and performance of the system are evaluated and optimized with ECG monitoring test of rats. Compared with those commercially available sensor nodes, our implantable one is leading in the weight and volume miniaturization, and our WISNS solution shows huge potential in achieving the compatibility of different animals."
753386,15226,30,Using CamiTK for rapid prototyping of interactive Computer Assisted Medical Intervention applications,2013,"Computer Assisted Medical Intervention (CAMI hereafter) is a complex multi-disciplinary field. CAMI research requires the collaboration of experts in several fields as diverse as medicine, computer science, mathematics, instrumentation, signal processing, mechanics, modeling, automatics, optics, etc. CamiTK 1  is a modular framework that helps researchers and clinicians to collaborate together in order to prototype CAMI applications by regrouping the knowledge and expertise from each discipline. It is an open-source, cross-platform generic and modular tool written in C++ which can handle medical images, surgical navigation, biomedicals simulations and robot control. This paper presents the Computer Assisted Medical Intervention ToolKit (CamiTK) and how it is used in various applications in our research team."
2141684,15226,30,Real-Time Sleep Apnea Detection by Classifier Combination,2012,"To find an efficient and valid alternative of polysomnography (PSG), this paper investigates real-time sleep apnea and hypopnea syndrome (SAHS) detection based on electrocardiograph (ECG) and saturation of peripheral oxygen (SpO 2 ) signals, individually and in combination. We include ten machine-learning algorithms in our classification experiment. It is shown that our proposed SpO 2  features outperform the ECG features in terms of diagnostic ability. More importantly, we propose classifier combination to further enhance the classification performance by harnessing the complementary information provided by individual classifiers. With our selected SpO 2  and ECG features, the classifier combination using AdaBoost with Decision Stump, Bagging with REPTree, and either kNN or Decision Table achieves sensitivity, specificity, and accuracy all around 82% for a minute-based real-time SAHS detection over 25 sleep-disordered-breathing suspects' full overnight recordings."
926181,15226,30,Evaluation of Thermal and Nonthermal Effects of UHF RFID Exposure on Biological Drugs,2012,"The radio frequency identification (RFID) technology promises to improve several processes in the healthcare scenario, especially those related to the traceability of people and things. Unfortunately, there are still some barriers limiting the large-scale deployment of these innovative technologies in the healthcare field. Among these, the evaluation of potential thermal and nonthermal effects due to the exposure of biopharmaceutical products to electromagnetic fields is very challenging, but still slightly investigated. This paper aims to setup a controlled RF exposure environment, in order to reproduce a worst case exposure of pharmaceutical products to the electromagnetic fields generated by the UHF RFID devices placed along the supply chain. Radiated powers several times higher than recommended by current normative limits were applied (10 and 20 W). The electric field strength at the exposed sample location, used in tests, was as high as 100 V/m. Nonthermal effects were evaluated by chromatography techniques and in vitro assays. The results obtained for a particular case study, the ActrapidTM human insulin preparation, showed temperature increases lower than 0.5 °C and no significant changes in the structure and performance of the considered drug."
909155,15226,30,Night-Time EKG and HRV Monitoring With Bed Sheet Integrated Textile Electrodes,2012,"A system for unobtrusive night-time electrocardiogram (EKG) and heart rate variability (HRV) monitoring as well as data analysis methods are presented, comparing bed sheet HR and HRV values with corresponding parameters obtained by a reference measurement. Our system uses eight embroidered textile electrodes attached laterally to a bed sheet for measuring bipolar contact EKG from multiple channels. The electrodes are arranged in a line so that at least two adjacent electrodes make sufficient skin contact. The focus of the signal processing development has been on selecting the best measurement channel for further analysis and minimizing the amount of incorrectly detected R-peaks. The test measurements were performed with four healthy men without previously known cardiac disorders and one who frequently had premature ventricular contractions (ectopic beats). For healthy test subjects, an average of 94.9% heartbeat detection coverage was achieved with the system during 29 measurement nights (in total 213.8 h of data). In most cases, the quality of the signal obtained from bed sheet electrodes is good enough for the computer-assisted cardiac arrhythmia detection. Applications for EKG derived RR-interval data include the calculation of HRV parameters that can be utilized in sleep quality analysis and other wellness-related topics as well as sleep apnoea detection."
2218482,15226,30,Mobile Social Network Services for Families With Children With Developmental Disabilities,2011,"As Internet technologies evolve, their applications have changed various aspects of human life. Here, we attempt to examine their potential impact on services for families with developmentally delayed children. Our research is thus designed to utilize wireless mobile communication technologies, location services, and search technology in an effort to match families of specific needs with potential care providers. Based on the investigation conducted by our counselors, this paper describes a platform for smooth communication between professional communities and families with children with developmental disabilities (CDD). This research also looks into the impact of management of mobile social network services and training on the operation of these services. Interaction opportunities, care, and support to families with CDD are introduced."
1159104,15226,30,Link Technologies and BlackBerry Mobile Health (mHealth) Solutions: A Review,2012,"The number of wearable wireless sensors is expected to grow to 400 million by the year 2014, while the number of operational mobile subscribers has already passed the 5.2 billion mark in 2011. This growth results in an increasing number of mobile applications including: Machine-to-Machine (M2M) communications, Electronic-Health (eHealth), and Mobile-Health (mHealth). A number of emerging mobile applications that require 3G and 4G mobile networks for data transport relate to telemedicine, including establishing, maintaining, and transmitting health-related information, research, education, and training. This review paper takes a closer look at these applications, specifically with regard to the healthcare industry and their underlying link technologies. The authors believe that the BlackBerry platform and the associated infrastructure (i.e., BlackBerry Enterprise Server) is a logical and practical solution for eHealth, mHealth, sensor and M2M deployments, which are considered in this paper."
2463432,15226,30,"Emergency Fall Incidents Detection in Assisted Living Environments Utilizing Motion, Sound, and Visual Perceptual Components",2011,"This paper presents the implementation details of a patient status awareness enabling human activity interpretation and emergency detection in cases, where the personal health is threatened like elder falls or patient collapses. The proposed system utilizes video, audio, and motion data captured from the patient's body using appropriate body sensors and the surrounding environment, using overhead cameras and microphone arrays. Appropriate tracking techniques are applied to the visual perceptual component enabling the trajectory tracking of persons, while proper audio data processing and sound directionality analysis in conjunction to motion information and subject's visual location can verify fall and indicate an emergency event. The postfall visual and motion behavior of the subject, which indicates the severity of the fall (e.g., if the person remains unconscious or patient recovers) is performed through a semantic representation of the patient's status, context and rules-based evaluation, and advanced classification. A number of advanced classification techniques have been examined in the framework of this study and their corresponding performance in terms of accuracy and efficiency in detecting an emergency situation has been thoroughly assessed."
1387500,15226,30,A Feasibility Study of Enhancing Independent Task Performance for People with Cognitive Impairments Through the Use of a Handheld Location-Based Prompting System,2012,"An autonomous task-prompting system is presented to increase workplace and life independence for people with cognitive impairments such as traumatic brain injury, intellectual disability, schizophrenia, and down syndrome. This paper describes an approach to providing distributed cognition support of work engagement for persons with cognitive disabilities. In the pilot study, a prototype was built and tested in a community-based rehabilitation program involving preservice food preparation training of eight participants with cognitive impairments. The results show improvement in helping with task engagement is statistically significant compared to the oral-instruction method. A follow-up comparative study with two participants evaluated the shadow-team approach against the proposed system. Although the number of participants was few, the participants were studied in depth and the findings were very promising. The results in the autonomous task prompting without staff intervention indicate that the performance is statistically as good as the shadow-team approach. Our findings suggest that acquisition of job skills may be facilitated by the proposed system in conjunction with operant conditioning strategies."
2040112,15226,30,Discrimination Power of Short-Term Heart Rate Variability Measures for CHF Assessment,2011,"In this study, we investigated the discrimination power of short-term heart rate variability (HRV) for discriminating normal subjects versus chronic heart failure (CHF) patients. We analyzed 1914.40 h of ECG of 83 patients of which 54 are normal and 29 are suffering from CHF with New York Heart Association (NYHA) classification I, II, and III, extracted by public databases. Following guidelines, we performed time and frequency analysis in order to measure HRV features. To assess the discrimination power of HRV features, we designed a classifier based on the classification and regression tree (CART) method, which is a nonparametric statistical technique, strongly effective on nonnormal medical data mining. The best subset of features for subject classification includes square root of the mean of the sum of the squares of differences between adjacent NN intervals (RMSSD), total power, high-frequencies power, and the ratio between low- and high-frequencies power (LF/HF). The classifier we developed achieved sensitivity and specificity values of 79.3% and 100 %, respectively. Moreover, we demonstrated that it is possible to achieve sensitivity and specificity of 89.7% and 100 %, respectively, by introducing two nonstandard features ΔAVNN and ΔLF/HF, which account, respectively, for variation over the 24 h of the average of consecutive normal intervals (AVNN) and LF/HF. Our results are comparable with other similar studies, but the method we used is particularly valuable because it allows a fully human-understandable description of classification procedures, in terms of intelligible “if ... then ...” rules."
2145695,15226,30,Distributed Intelligent Sensor Network for the Rehabilitation of Parkinson's Patients,2011,"The coordination between locomotion and respiration of Parkinson's disease (PD) patients is reduced or even absent. The degree of this disturbance is assumed to be associated with the disease severity [S. Schiermeier, D. Schäfer, T. Schäfer, W. Greulich, and M. E. Schläfke, “Breathing and locomotion in patients with Parkinson's disease,” Eur. J. Physiol., vol. 443, No. 1, pp. 67-71, Jul. 2001]. To enable a long-term and online analysis of the locomotion-respiration coordination for scientific purpose, we have developed a distributed wireless communicating network. We aim to integrate biofeedback protocols with the real-time analysis of the locomotion-respiration coordination in the system to aid rehabilitation of PD patients. The network of sensor nodes is composed of intelligent network operating devices (iNODEs). The miniaturized iNODE contains a continuous data acquisition system based on microcontroller, local data storage, capability of on-sensor digital signal processing in real time, and wireless communication based on IEEE 802.15.4. Force sensing resistors and respiratory inductive plethysmography are applied for motion and respiration sensing, respectively. A number of experiments have been undertaken in clinic and laboratory to test the system. It shall facilitate identification of therapeutic effects on PD, allowing to measure the patients' health status, and to aid in the rehabilitation of PD patients."
2461255,15226,390,Surface membrane based bladder registration for evaluation of accumulated dose during brachytherapy in cervical cancer,2011,"A new optimisation strategy for surface mesh registration is proposed based on energy minimisation of an elastic surface membrane. The energy term is evaluated in three-dimensional space, optimisation however is performed while constraining the moving surface to a parametrised spherical space of the fixed surface. Optional landmark based matches can be included in the suggested iterative solver. The technique is demonstrated for bladder registration in brachytherapy treatment evaluation of cervical cancer. It holds promise to better estimate the accumulated but unintentional dose delivered to organs at risk."
907688,15226,30,Investigation on Cardiovascular Risk Prediction Using Genetic Information,2012,"Cardiovascular disease (CVD) has become the primary killer worldwide and is expected to cause more deaths in the future. Prediction and prevention of CVD have therefore become important social problems. Many groups have developed prediction models for asymptomatic CVD by classifying its risk based on established risk factors (e.g., age, sex, etc.). More recently, studies have uncovered that many genetic variants are associated with CVD outcomes/traits. If treated as single or multiple risk factors, the genetic information could improve the performance of prediction models as well as promote the development of individually tailored risk models. In this paper, eligible genome-wide association studies for CVD outcomes/traits will be overviewed. Clinical trials on CVD prediction using genetic information will be summarized from overall aspects. As yet, most of the single or multiple genetic markers, which have been evaluated in the follow-up clinical studies, did not significantly improve discrimination of CVD. However, the potential clinical utility of genetic information has been uncovered initially and is expected for further development."
2463411,15226,390,Automated posteriorwall thickness measurement from B-mode ultrasound,2013,"In this paper, we present a robust algorithm to segment the posterior wall region and estimate wall thickness from parasternal long axis(PLAX) view cardiac US B-mode images. Posterior wall thickness (PWd), Septal wall thickness (SWTd) and Left ventricular Internal diameter(LVId) are used to detect and measure the extent of Left Ventricular Hypertrophy (LVH). Manual measurements of PWd suffers from large inter and intra observer variability due to weak endocardial boundary intertangled with speckle and poor contrast, movement of the fibrous structures like the chordae,papillary muscles and posterior mitral leaflet. The proposed algorithm seeks to address some of these issues by automating the measurement algorithm. The algorithm initially detects epicardial boundary by pericardium detection and later segments the endocardial boundary by a 1D active contour evolution. We have designed the algorithm on a pilot data set of 42 images and validated on 88 patient data sets.The measurement values are in excellent agreement with expert measurements with error = 2.06mm ± 1.5mm."
900504,15226,30,Experimental Study of a Hybrid Microwave Radiometry—Hyperthermia Apparatus With the Use of an Anatomical Head Phantom,2012,"This paper presents the latest progress made concerning a hybrid diagnostic and therapeutic system able to provide focused microwave radiometric temperature and/or conductivity variation measurements and hyperthermia treatment. Previous experimental studies of our group have demonstrated the system performance and focusing properties in phantom as well as human experiments. The system is able to detect temperature and conductivity variations with frequency-dependent detection depth and spatial sensitivity. Numerous studies have also demonstrated the improvement of the system focusing properties attributed to the use of dielectric and left handed matching layers. In this study, similar experimental procedures are performed but this time using an anatomical head model as phantom aiming to achieve a more accurate modeling of the system's future real function. This way, another step is made toward the deeper understanding of the system's capabilities, with the view to further use it in experimental procedures with laboratory animals and human volunteers."
1129895,15226,30,Image Analysis and Length Estimation of Biomolecules Using AFM,2012,"There are many examples of problems in pattern analysis for which it is often possible to obtain systematic characterizations, if in addition a small number of useful features or parameters of the image are known a priori or can be estimated reasonably well. Often, the relevant features of a particular pattern analysis problem are easy to enumerate, as when statistical structures of the patterns are well understood from the knowledge of the domain. We study a problem from molecular image analysis, where such a domain-dependent understanding may be lacking to some degree and the features must be inferred via machine-learning techniques. In this paper, we propose a rigorous, fully automated technique for this problem. We are motivated by an application of atomic force microscopy (AFM) image processing needed to solve a central problem in molecular biology, aimed at obtaining the complete transcription profile of a single cell, a snapshot that shows which genes are being expressed and to what degree. Reed et al. (“Single molecule transcription profiling with AFM,” Nanotechnology, vol. 18, no. 4, 2007) showed that the transcription profiling problem reduces to making high-precision measurements of biomolecule backbone lengths, correct to within 20-25 bp (6-7.5 nm). Here, we present an image processing and length estimation pipeline using AFM that comes close to achieving these measurement tolerances. In particular, we develop a biased length estimator on trained coefficients of a simple linear regression model, biweighted by a Beaton-Tukey function, whose feature universe is constrained by James-Stein shrinkage to avoid overfitting. In terms of extensibility and addressing the model selection problem, this formulation subsumes the models we studied."
1461270,15226,30,Automated Recognition of Obstructive Sleep Apnea Syndrome Using Support Vector Machine Classifier,2012,"Obstructive sleep apnea (OSA) is a common sleep disorder that causes pauses of breathing due to repetitive obstruction of the upper airways of the respiratory system. The effect of this phenomenon can be observed in other physiological signals like the heart rate variability, oxygen saturation, and the respiratory effort signals. In this study, features from these signals were extracted from 50 control and 50 OSA patients from the Sleep Heart Health Study database and implemented for minute and subject classifications. A support vector machine (SVM) classifier was used with linear and second-order polynomial kernels. For the minute classification, the respiratory features had the highest sensitivity while the oxygen saturation gave the highest specificity. The polynomial kernel always had better performance and the highest accuracy of 82.4% (Sen: 69.9%, Spec: 91.4%) was achieved using the combined-feature classifier. For subject classification, the polynomial kernel had a clear improvement in the oxygen saturation accuracy as the highest accuracy of 95% was achieved by both the oxygen saturation (Sen: 100%, Spec: 90.2%) and the combined-feature (Sen: 91.8%, Spec: 98.0%). Further analysis of the SVM with other kernel types might be useful for optimizing the classifier with the appropriate features for an OSA automated detection algorithm."
2457992,15226,339,Understanding OSN-based facial disclosure against face authentication systems,2014,"Face authentication is one of promising biometrics-based user authentication mechanisms that have been widely available in this era of mobile computing. With built-in camera capability on smart phones, tablets, and laptops, face authentication provides an attractive alternative of legacy passwords for its memory-less authentication process. Although it has inherent vulnerability against spoofing attacks, it is generally considered sufficiently secure as an authentication factor for common access protection. However, this belief becomes questionable since image sharing has been popular in online social networks (OSNs). A huge number of personal images are shared every day and accessible to potential adversaries. This OSN-based facial disclosure (OSNFD) creates a significant threat against face authentication. In this paper, we make the first attempt to quantitatively measure the threat of OSNFD. We examine real-world face-authentication systems designed for both smartphones, tablets, and laptops. Interestingly, our results find that the percentage of vulnerable images that can used for spoofing attacks is moderate, but the percentage of vulnerable users that are subject to spoofing attacks is high. The difference between systems designed for smartphones/tablets and laptops is also significant. In our user study, the average percentage of vulnerable users is 64% for laptop-based systems, and 93% for smartphone/tablet-based systems. This evidence suggests that face authentication may not be suitable to use as an authentication factor, as its confidentiality has been significantly compromised due to OSNFD. In order to understand more detailed characteristics of OSNFD, we further develop a risk estimation tool based on logistic regression to extract key attributes affecting the success rate of spoofing attacks. The OSN users can use this tool to calculate risk scores for their shared images so as to increase their awareness of OSNFD."
1248891,15226,30,Prediction of High-Risk Asymptomatic Carotid Plaques Based on Ultrasonic Image Features,2012,"Carotid plaques have been associated with ipsilateral neurological symptoms. High-resolution ultrasound can provide information not only on the degree of carotid artery stenosis but also on the characteristics of the arterial wall including the size and consistency of atherosclerotic plaques. The aim of this study is to determine whether the addition of ultrasonic plaque texture features to clinical features in patients with asymptomatic internal carotid artery stenosis (ACS) improves the ability to identify plaques that will produce stroke. 1121 patients with ACS have been scanned with ultrasound and followed for a mean of 4 years. It is shown that the combination of texture features based on second-order statistics spatial gray level dependence matrices (SGLDM) and clinical factors improves stroke prediction (by correctly predicting 89 out of the 108 cases that were symptomatic). Here, the best classification results of 77 ±1.8% were obtained from the use of the SGLDM texture features with support vector machine classifiers. The combination of morphological features with clinical features gave slightly worse classification results of 76 ±2.6%. These findings need to be further validated in additional prospective studies."
2312909,15226,30,Diagnosis of Cardiovascular Abnormalities From Compressed ECG: A Data Mining-Based Approach,2011,"Usage of compressed ECG for fast and efficient telecardiology application is crucial, as ECG signals are enormously large in size. However, conventional ECG diagnosis algorithms require the compressed ECG packets to be decompressed before diagnosis can be performed. This added step of decompression before performing diagnosis for every ECG packet introduces unnecessary delay, which is undesirable for cardiovascular diseased (CVD) patients. In this paper, we are demonstrating an innovative technique that performs real-time classification of CVD. With the help of this real-time classification of CVD, the emergency personnel or the hospital can automatically be notified via SMS/MMS/e-mail when a life-threatening cardiac abnormality of the CVD affected patient is detected. Our proposed system initially uses data mining techniques, such as attribute selection (i.e., selects only a few features from the compressed ECG) and expectation maximization (EM)-based clustering. These data mining techniques running on a hospital server generate a set of constraints for representing each of the abnormalities. Then, the patient's mobile phone receives these set of constraints and employs a rule-based system that can identify each of abnormal beats in real time. Our experimentation results on 50 MIT-BIH ECG entries reveal that the proposed approach can successfully detect cardiac abnormalities (e.g., ventricular flutter/fibrillation, premature ventricular contraction, atrial fibrillation, etc.) with 97% accuracy on average. This innovative data mining technique on compressed ECG packets enables faster identification of cardiac abnormality directly from the compressed ECG, helping to build an efficient telecardiology diagnosis system."
2117853,15226,30,Numerical Characterization and Modeling of Subject-Specific Ultrawideband Body-Centric Radio Channels and Systems for Healthcare Applications,2012,"The paper presents a subject-specific radio propagation study and system modeling in wireless body area networks using a simulation tool based on the parallel finite-difference time-domain technique. This technique is well suited to model the radio propagation around complex, inhomogeneous objects such as the human body. The impact of different digital phantoms in on-body radio channel and system performance was studied. Simulations were performed at the frequency of 3-10 GHz considering a typical hospital environment, and were validated by on-site measurements with reasonably good agreement. The analysis demonstrated that the characteristics of the on-body radio channel and system performance are subject-specific and are associated with human genders, height, and body mass index. Maximum variations of almost 18.51% are observed in path loss exponent due to change of subject, which gives variations of above 50% in system bit error rate performance. Therefore, careful consideration of subject-specific parameters are necessary for achieving energy efficient and reliable radio links and system performance for body-centric wireless network."
1124783,15226,30,Noncontact Millimeter-Wave Real-Time Detection and Tracking of Heart Rate on an Ambulatory Subject,2012,"This paper presents a solution to an aiming problem in the remote sensing of vital signs using an integration of two systems. The problem is that to collect meaningful data with a millimeter-wave sensor, the antenna must be pointed very precisely at the subject's chest. Even small movements could make the data unreliable. To solve this problem, we attached a camera to the millimeter-wave antenna, and mounted this combined system on a pan/tilt base. Our algorithm initially finds a subject's face and then tracks him/her through subsequent frames, while calculating the position of the subject's chest. For each frame, the camera sends the location of the chest to the pan/tilt base, which rotates accordingly to make the antenna point at the subject's chest. This paper presents a system for concurrent tracking and data acquisition with results from some sample scenarios."
1343982,15226,30,Embedded Ubiquitous Services on Hospital Information Systems,2012,"Hospital information systems (HIS) have turned a hospital into a gigantic computer with huge computational power, huge storage, and wired/wireless local area network. On the other hand, a modern medical device, such as echograph, is a computer system with several functional units connected by an internal network named a bus. Therefore, we can embed such a medical device into the HIS by simply replacing the bus with the local area network. This paper designed and developed two embedded systems, a ubiquitous echograph system, and a networked digital camera. Evaluations of the developed systems clearly show that the proposed approach, embedding existing clinical systems into HIS, drastically changes productivity in the clinical field. Once a clinical system becomes a pluggable unit for a gigantic computer system, HIS, the combination of multiple embedded systems with application software designed under deep consideration about clinical processes may lead to the emergence of disruptive innovation in the clinical field."
1418377,15226,30,An Adaptive Window-Setting Scheme for Segmentation of Bladder Tumor Surface via MR Cystography,2012,"This paper proposes an adaptive window-setting scheme for noninvasive detection and segmentation of bladder tumor surface in T_1 -weighted magnetic resonance (MR) images. The inner border of the bladder wall is first covered by a group of ball-shaped detecting windows with different radii. By extracting the candidate tumor windows and excluding the false positive (FP) candidates, the entire bladder tumor surface is detected and segmented by the remaining windows. Different from previous bladder tumor detection methods that are mostly focusing on the existence of a tumor, this paper emphasizes segmenting the entire tumor surface in addition to detecting the presence of the tumor. The presented scheme was validated by ten clinical T 1 -weighted MR image datasets (five volunteers and five patients). The bladder tumor surfaces and the normal bladder wall inner borders in the ten datasets were covered by 223 and 10 491 windows, respectively. Such a large number of the detecting windows makes the validation statistically meaningful. In the FP reduction step, the best feature combination was obtained by using receiver operating characteristics or ROC analysis. The validation results demonstrated the potential of this presented scheme in segmenting the entire tumor surface with high sensitivity and low FP rate. This study inherits our previous results of automatic segmentation of the bladder wall and will be an important element in our MR-based virtual cystoscopy or MR cystography system."
937761,15226,30,Cardiovascular Genomics: A Biomarker Identification Pipeline,2012,"Genomic biomarkers are essential for understanding the underlying molecular basis of human diseases such as cardiovascular disease. In this review, we describe a biomarker identification pipeline for cardiovascular disease, which includes 1) high-throughput genomic data acquisition, 2) preprocessing and normalization of data, 3) exploratory analysis, 4) feature selection, 5) classification, and 6) interpretation and validation of candidate biomarkers. We review each step in the pipeline, presenting current and widely used bioinformatics methods. Furthermore, we analyze several publicly available cardiovascular genomics datasets to illustrate the pipeline. Finally, we summarize the current challenges and opportunities for further research."
1102771,15226,30,A Serious Game for Learning Ultrasound-Guided Needle Placement Skills,2012,"Ultrasound-guided needle placement is a key step in a lot of radiological intervention procedures such as biopsy, local anesthesia, and fluid drainage. To help training future intervention radiologists, we develop a serious game to teach the skills involved. We introduce novel techniques for realistic simulation and integrate game elements for active and effective learning. This game is designed in the context of needle placement training based on the some essential characteristics of serious games. Training scenarios are interactively generated via a block-based construction scheme. A novel example-based texture synthesis technique is proposed to simulate corresponding ultrasound images. Game levels are defined based on the difficulties of the generated scenarios. Interactive recommendation of desirable insertion paths is provided during the training as an adaptation mechanism. We also develop a fast physics-based approach to reproduce the shadowing effect of needles in ultrasound images. Game elements such as time-attack tasks, hints, and performance evaluation tools are also integrated in our system. Extensive experiments are performed to validate its feasibility for training."
2454483,15226,390,Spherical Bessel Filter for 3D object detection,2011,"The detection of 3D objects and landmarks in arbitrary orientations is one of the most challenging tasks in biomedical 3D image analysis. In this paper we introduce the spherical Bessel Filter (BF) for rotation invariant 3D object detection tasks. The BF is based on the Harmonic Filter (HF) and thus inherits all the gentle properties of the HF, in particular the data driven adaptability and the processing speed. In contrast to the HF the BF benefits from a better object representation based on local spherical Fourier basis functions leading to noticeably better object detections and localizations."
1986557,15226,30,Measuring and Reflecting Depth of Anesthesia Using Wavelet and Power Spectral Density,2011,This paper evaluates depth of anesthesia (DoA) monitoring using a new index. The proposed method preconditions raw EEG data using an adaptive threshold technique to remove spikes and low-frequency noise. We also propose an adaptive window length technique to adjust the length of the sliding window. The information pertinent to DoA is then extracted to develop a feature function using discrete wavelet transform and power spectral density. The evaluation demonstrates that the new index reflects the patient's transition from consciousness to unconsciousness with the induction of anesthesia in real time.
1194215,15226,30,Cross-Layer Design for Optimized Region of Interest of Ultrasound Video Data Over Mobile WiMAX,2012,"The application of advanced error concealment techniques applied as a postprocess to conceal lost video information in error-prone channels, such as the wireless channel, demands additional processing at the receiver. This increases the delivery delay and needs more computational power. However, in general, only a small region within medical video is of interest to the physician and thus if only this area is considered, the number of computations can be curtailed. In this paper, we present a technique whereby the region of interest specified by the physician is used to delimit the area where the more complex concealment techniques are applied. A cross-layer design approach in mobile worldwide interoperability for microwave access wireless communication environment is adopted in this paper to provide an optimized quality of experience in the region that matters most to the mobile physician while relaxing the requirements in the background, ensuring real-time delivery. Results show that a diagnostically acceptable peak signal-to-noise-ratio of about 36 dB can still be achieved within reasonable decoding time."
26098,15226,235,Extraction of relevant figures and tables for multi-document summarization,2012,"We propose a system that extracts the most relevant figures and tables from a set of topically related source documents. These are then integrated into the extractive text summary produced using the same set. The proposed method is domain independent. It predominantly focuses on the generation of a ranked list of relevant candidate units (figures/tables), in order of their computed relevancy. The relevancy measure is based on local and global scores that include direct and indirect references. In order to test the system performance, we have created a test collection of document sets which do not adhere to any specific domain. Evaluation experiments show that the system generated ranked list is in statistically significant correlation with the human evaluators' ranking judgments. Feasibility of the proposed system to summarize a document set which contains figures/tables as their salient units is made clear in our concluding remark."
769903,15226,30,A Novel Semiautomated Atherosclerotic Plaque Characterization Method Using Grayscale Intravascular Ultrasound Images: Comparison With Virtual Histology,2012,"Intravascular ultrasound (IVUS) virtual histology (VH-IVUS) is a new technique, which provides automated plaque characterization in IVUS frames, using the ultrasound backscattered RF-signals. However, its computation can only be performed once per cardiac cycle (ECG-gated technique), which significantly decreases the number of characterized IVUS frames. Also atherosclerotic plaques in images that have been acquired by machines, which are not equipped with the VH software, cannot be characterized. To address these limitations, we have developed a plaque characterization technique that can be applied in grayscale IVUS images. Our semiautomated method is based on a three-step approach. In the first step, the plaque area [region of interest (ROI)] is detected semiautomatically. In the second step, a set of features is extracted for each pixel of the ROI and in the third step, a random forest classifier is used to classify these pixels into four classes: dense calcium, necrotic core, fibrotic tissue, and fibro-fatty tissue. In order to train and validate our method, we used 300 IVUS frames acquired from virtual histology examinations from ten patients. The overall accuracy of the proposed method was 85.65% suggesting that our approach is reliable and may be further investigated in the clinical and research arena."
1320972,15226,369,Neural Network Based Situation Detection and Service Provision in the Environment of IoT,2013,"The safety production in coal mine has attracted considerable research attentions due to the frequently occurred mining accidents. In order to ensure the safety production in coal mine, technology of the Internet of Things (IoT) is widely used to detect the situation in coal mine. Here the situation is composed of several elements. Existing solutions for such situation detection are mainly based on the directed graph or automatic machine. These methods are only effective when few situation element change simultaneously or the change(s) can be determined clearly. However, when the situation comprises a lot of elements or the element's change is ambiguous, these methods cannot effectively determine the situation. In this paper, we propose a situation detection method based on neural network. Trained neural network can detect the situation well, especially when multiple situation elements change at the same time or changes are ambiguous."
1496075,15226,30,"A Two-Class Approach to the Detection of Physiological Deterioration in Patient Vital Signs, With Clinical Label Refinement",2012,"Hospital patient outcomes can be improved by the early identification of physiological deterioration. Automatic methods of detecting patient deterioration in vital-sign data typically attempt to identify deviations from assumed “normal” physiological conditions, which is a one-class approach to classification. This paper investigates the use of a two-class approach, in which “abnormal” physiology is modeled explicitly. The success of such a method relies on the accuracy of data labels provided by clinical experts, which may be incomplete (due to large dataset size) or imprecise (due to clinical labels covering intervals, rather than each data point within those intervals). We propose a novel method of refining clinical labels such that the two-class classification approach may be adopted for identifying patient deterioration. We demonstrate the effectiveness of the proposed methods using a large dataset acquired in a 24-bed hospital step-down unit."
1557,15226,235,Detecting players personality behavior with any effort of concealment,2012,"We introduce a novel natural language processing component using machine learning techniques for prediction of personality behaviors of players in a serious game, Land Science, where players act as interns in an urban planning firm and discuss in groups their ideas about urban planning and environmental science in written natural language. Our model learns vector space representations for various features extraction. In order to apply this framework, input excerpts must be classified into one of six possible personality classes. We applied this personality classification task using several machine learning algorithms, such as: Naive Bayes, Support Vector Machines, and Decision Tree. Training is performed on a relatively dataset of manually annotated excerpts. By combining these features spaces from psychology and computational linguistics, we perform and evaluate our approaches to detecting personality, and eventually develop a classifier that is nearly 83% accurate on our dataset. Based on the feature analysis of our models, we add several theoretical contributions, including revealing a relationship between different personality behaviors in players' writing."
2489547,15226,30,Adaptive Beat-to-Beat Heart Rate Estimation in Ballistocardiograms,2011,"A ballistocardiograph records the mechanical activity of the heart. We present a novel algorithm for the detection of individual heart beats and beat-to-beat interval lengths in ballistocardiograms (BCGs) from healthy subjects. An automatic training step based on unsupervised learning techniques is used to extract the shape of a single heart beat from the BCG. Using the learned parameters, the occurrence of individual heart beats in the signal is detected. A final refinement step improves the accuracy of the estimated beat-to-beat interval lengths. Compared to many existing algorithms, the new approach offers heart rate estimates on a beat-to-beat basis. The agreement of the proposed algorithm with an ECG reference has been evaluated. A relative beat-to-beat interval error of 1.79% with a coverage of 95.94% was achieved on recordings from 16 subjects."
1240553,15226,30,Spatiotemporal Sparse Bayesian Learning With Applications to Compressed Sensing of Multichannel Physiological Signals,2014,"Energy consumption is an important issue in continuous wireless telemonitoring of physiological signals. Compressed sensing (CS) is a promising framework to address it, due to its energy-efficient data compression procedure. However, most CS algorithms have difficulty in data recovery due to nonsparsity characteristic of many physiological signals. Block sparse Bayesian learning (BSBL) is an effective approach to recover such signals with satisfactory recovery quality. However, it is time-consuming in recovering multichannel signals, since its computational load almost linearly increases with the number of channels. This work proposes a spatiotemporal sparse Bayesian learning algorithm to recover multichannel signals simultaneously. It not only exploits temporal correlation within each channel signal, but also exploits inter-channel correlation among different channel signals. Furthermore, its computational load is not significantly affected by the number of channels. The proposed algorithm was applied to brain computer interface (BCI) and EEG-based driver's drowsiness estimation. Results showed that the algorithm had both better recovery performance and much higher speed than BSBL. Particularly, the proposed algorithm ensured that the BCI classification and the drowsiness estimation had little degradation even when data were compressed by 80%, making it very suitable for continuous wireless telemonitoring of multichannel signals."
831032,15226,30,Blind Integrity Verification of Medical Images,2012,"This paper presents the first method of digital blind forensics within the medical imaging field with the objective to detect whether an image has been modified by some processing (e.g., filtering, lossy compression, and so on). It compares two image features: the histogram statistics of reorganized block-based discrete cosine transform coefficients, originally proposed for steganalysis purposes, and the histogram statistics of reorganized block-based Tchebichef moments. Both features serve as input of a set of support vector machine classifiers built in order to discriminate tampered images from original ones as well as to identify the nature of the global modification one image may have undergone. Performance evaluation, conducted in application to different medical image modalities, shows that these image features can help, independently or jointly, to blindly distinguish image processing or modifications with a detection rate greater than 70%. They also underline the complementarity of these features."
1517869,15226,30,Wavelet-Based Energy Features for Glaucomatous Image Classification,2012,"Texture features within images are actively pursued for accurate and efficient glaucoma classification. Energy distribution over wavelet subbands is applied to find these important texture features. In this paper, we investigate the discriminatory potential of wavelet features obtained from the daubechies (db3), symlets (sym3), and biorthogonal (bio3.3, bio3.5, and bio3.7) wavelet filters. We propose a novel technique to extract energy signatures obtained using 2-D discrete wavelet transform, and subject these signatures to different feature ranking and feature selection strategies. We have gauged the effectiveness of the resultant ranked and selected subsets of features using a support vector machine, sequential minimal optimization, random forest, and naïve Bayes classification strategies. We observed an accuracy of around 93% using tenfold cross validations to demonstrate the effectiveness of these methods."
2135294,15226,30,An Intelligent Scoring System and Its Application to Cardiac Arrest Prediction,2012,"Traditional risk score prediction is based on vital signs and clinical assessment. In this paper, we present an intelligent scoring system for the prediction of cardiac arrest within 72 h. The patient population is represented by a set of feature vectors, from which risk scores are derived based on geometric distance calculation and support vector machine. Each feature vector is a combination of heart rate variability (HRV) parameters and vital signs. Performance evaluation is conducted on the leave-one-out cross-validation framework, and receiver operating characteristic, sensitivity, specificity, positive predictive value, and negative predictive value are reported. Experimental results reveal that the proposed scoring system not only achieves satisfactory performance on determining the risk of cardiac arrest within 72 h but also has the ability to generate continuous risk scores rather than a simple binary decision by a traditional classifier. Furthermore, the proposed scoring system works well for both balanced and imbalanced datasets, and the combination of HRV parameters and vital signs shows superiority in prediction to using HRV parameters only or vital signs only."
2457501,15226,390,Improved generation of probabilistic atlases for the expectation maximization classification,2011,"Probabilistic atlases present prior knowledge about the spatial distribution of various structures or tissues in a population, used commonly in segmentation. We propose three methods for generating probabilistic atlases: 1) the atlases are constructed in a template space using dense non-rigid transformations and transformed to the space of unseen data, 2) as the method 1 but atlas selection is performed in addition, and 3) the atlases are constructed directly in the space of the unseen data. The methods were evaluated in the segmentation of the hippocampus in 340 images from the Alzheimer's Disease Neuroimaging Initiaitve (ADNI). Dice overlaps (similarity index, SI) were 0.84, 0.85 and 0.87 with reference segmentations and the correlation coefficients for the volumes were 0.84, 0.92 and 0.96 for the three methods tested. Our results show clearly the importance of probabilistic atlases in segmentation."
2447836,15226,390,L 0 -compressed sensing for parallel dynamic MRI using sparse Bayesian learning,2011,"Since the advent of compressed sensing in dynamic MR imaging area, a number of l 1 -compressed sensing algorithms have been proposed to improve the resolution. Recently, it was shown that by solving an l p  minimization problem with 0 ≤ p &#60; 1, the number of required measurements for an exact sparse reconstruction is more reduced than in solving an l 1  minimization. However, when 0 ≤ p &#60; 1, the problem is not convex and there exist many local minima. To deal with this problem, we adopted an empirical Bayesian approach called sparse Bayesian learning (SBL). The main contribution of this paper is to extend the idea for parallel dynamic MR imaging problems. By exploiting the simultaneous sparsity, the algorithm outperforms other methods, especially when the coil sensitivity map is not accurate. Numerical results confirms the theory."
