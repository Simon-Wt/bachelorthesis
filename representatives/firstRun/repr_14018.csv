ID_Article,communityId,ID_RelatedVenue,title,year,abstract
2192449,14018,8806,OSBS: online storm-water billing system,2014,"Storm-water runoff costs the City of Bridgeport millions of dollars per year to treat. To reduce the storm-water runoff and help the city move toward a greener society, a capstone project team at Fairfield University developed the Online Storm-water Billing Systems (OSBS) which mathematically quantify runoff entering the sewer system from each parcel of impervious surface. Using the geo data provided by the city, the OSBS was implemented using ArcGIS Online with the capability of sharing and finding geographic information published by Esri, ArcGIS users, and other authoritative data providers. We used this online service to create a map for a Bridgeport city, upload the data layers containing information about each parcel, and configure a pop up to show specific data including tax billing information. The OSBS was developed using ArcGIS and HTML/CSS/JQuery. This paper provides information of GIS, Bridgeport GIS system, ArcGIS, and describes the details of the implementation of OSBS. Future enhancement of OSBS is also discussed."
1724920,14018,8806,Dreams: a framework for distributed synchronous coordination,2012,"Synchronous coordination systems, such as  Reo , exchange data via indivisible actions, while distributed systems are typically asynchronous and assume that messages can be delayed or get lost. To combine these seemingly contradictory notions, we introduce the Dreams framework. Coordination patterns in Dreams are described using a synchronous model based on the  Reo  language, whereas global system behaviour is given by the runtime composition of autonomous actors communicating asynchronously. Dreams also exploits the use of actors in the composition of synchronous coordination patterns to allow communication whenever possible, increasing the scalability of the implementation."
1630475,14018,8806,A systematic mapping on gamification applied to education,2014,"Gamification is a term that refers to the use of game elements in  non-game contexts  with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning."
1652652,14018,8806,Implementing distributable real-time threads in the Linux kernel: programming interface and scheduling support,2012,"We present an implementation of Real-Time CORBA's distributable threads (DTs) as a first-class, end-to-end realtime programming and scheduling abstraction in the Linux kernel. We use Ingo Molnar's PREEMPT_RT kernel patch, which enables nearly complete kernel pre-emption, and add local (real-time) scheduling support to the Linux kernel, atop which we build DT scheduling support. We implement DTs using Linux's threading capabilities. Our implementation of a suite of independent and collaborative DT schedulers confirm the effectiveness of our implementation."
1447142,14018,8806,Deceit detection via online behavioral learning,2011,"Behavioral Psychologists generally believe that deception causes physiological reactions such as high blood pressure, increased heart rate, and an increased respiration rate. The physiological reaction is the consequence of arousal that is associated with high-stake deception [1]. A high-stake lie is one told when the person lying stands to get a notable gain, or faces a notable loss by telling the truth."
1511695,14018,8806,An evolutionary spline fitting algorithm for identifying filamentous cyanobacteria,2013,"Bright field cellular microscopy is a simple and non-invasive method for capturing cytological images. However, the resulting micrographs prove challenging for image segmentation, especially with samples that have tightly clustered or overlapping cells. Filamentous cyanobacteria grow as linearly arranged cells forming chain-like filaments that often touch and overlap. Existing bright field cell segmentation methods perform poorly with these bacteria, and are incapable of identifying the filaments. Existing filament tracking methods are rudimentary, and cannot reliably account for overlapping or parallel touching filaments. We propose a new approach for identifying filaments in bright field micrographs by combining information about both filaments and cells. This information is used by an evolutionary strategy to iteratively construct a continuous spline representation that tracks the medial line of the filaments. We demonstrate that overlapping and parallel touching filaments are segmented correctly in many difficult cases."
2019018,14018,8806,An empirical investigation of perceived reliability of open source Java programs,2012,"Background:  Open Source Software (OSS) is used by a continuously growing number of people, both end-users and developers. The quality of OSS is thus an issue of increasing interest. Specifically, OSS stakeholders need to trust OSS with respect to a number of qualities.    Objective:  This paper focuses on the level of trust that OSS stakeholders have in OSS reliability, one of the most important software qualities. The goal of the work reported here is to investigate to what extent the perception of reliability by users depends on objectively measurable characteristics of software.    Method:  We collected subjective user evaluations of the reliability of 22 Java OSS products, and then we measured their code characteristics that are generally believed to affect the quality of software. Finally, we carried out a correlational study to predict the perceived level of reliability of OSS based on the measured characteristics of the software code.    Result:  We obtained a set of statistically significant quantitative models, collectively called MOSST\REL, which account for the dependence of the perceived reliability of OSS on objectively observable qualities of Java code.    Conclusions:  The models we obtained can be used by: 1) endusers and developers that would like to reuse existing OSS products and components, to evaluate the perceived level of reliability of these OSS products that can be expected based on the characteristics of code; 2) the developers of OSS products, who can set code quality targets based on the level of perceived reliability they want to achieve."
2020215,14018,8806,"PRAM wear-leveling algorithm for hybrid main memory based on data buffering, swapping, and shifting",2012,"We propose a novel wear-leveling algorithm for the hybrid main memory architecture which exploits both fast read and write speed of DRAM and low power consumption and high density of PRAM. The wear-leveling algorithm consists of three techniques: DRAM buffering for reducing the write count, multiple data swapping for evening out the write count among all pages, and data shifting evening out the write count among all pages and lines. In order to evaluate performance, we implement a PIN-based wear-leveling simulator. In SPEC CPU2006, our proposed schemes can reduce the write count and maintain the write count equally among all pages and lines with little additional overhead."
1600959,14018,8806,Core BPEL: syntactic simplification of WS-BPEL 2.0,2012,"We present a core subset of the Web Services Business Process Execution Language 2.0 (WS-BPEL), called Core BPEL, and a complete, idempotent transformation from WS-BPEL to the Core BPEL subset. This has two benefits: (1) it clarifies the WS-BPEL semantics by showing how complex constructs map to a composition of simpler constructs, and (2) it reduces the effort required for working formally with WS-BPEL, as one, without loss of generality, needs only consider the much simpler Core BPEL.   We provide an XML Schema for Core BPEL, XSLT implementations of the presented transformations, and a web application where one may apply the transformations to WS-BPEL processes."
1225877,14018,8806,Effective tumor feature extraction for smart phone based microwave tomography breast cancer screening,2014,"Mobile Microwave Tomography (MMT) is a new alternative technique to detect breast cancer using smart phone based electronic healthcare system. In this paper, we propose a new solution to extract tumor information from MMT raw data for early breast cancer screening. MMT reflects water contents of breast tissue by measuring their electrical properties and sends permittivity and conductivity raw data to processing servers in hospital via WiFi or 3G/4G networks. In this approach we investigate three different sets of MMT tumor features and perform a comparative study to investigate their set of accuracy measurements for each classification. Through extensive empirical study of the classification results, we have identified the following six parameters as useful to extract tumor information: average permittivity of healthy tissue (APHT), average permittivity of probable tumor area (APPTA), maximum and minimum values of permittivity of probable tumor area (MaxPPTA, and MinPPTA), and energy values of healthy tissue (EVHT) and probable tumor area (EVPA)."
1608444,14018,8806,Area diversity in computer science collaborations,2012,"The contribution of this paper is to look at the diversity of areas of computer science fields in paper collaborations. To study collaborations, we have collected publication information from the Association for Computing and Machinery (ACM) Digital Library and modelled the collaborations as a social network. We have studied the community structure of these collaborations and discovered that multidisciplinary characteristics of communities only weakly correlate to a higher number of citations per paper or publications per authors in those communities."
1279954,14018,8806,Spatial network modeling for databases,2011,"Spatial networks like transportation, power, and pipeline networks are a ubiquitous spatial concept in everyday life and play an important role for navigational and routing purposes. Database support for large spatial networks in order to represent, store, query, and manipulate them is rare. Our paper aims to provide the beginning of a conceptual, abstract, and formal model of spatial networks, called Spatial Network Algebra ( SNA ), that includes types and operations and is supposed to serve as a specification for their later implementation in spatial database systems and GIS. Finally, we show how our spatial network concepts can be embedded into an SQL-like query language."
1208004,14018,8806,A model checker for Bigraphs,2012,"We present a model checking tool for Bigraphical Reactive Systems that may be instantiated as a model checker for any formalism or domain-specific modelling language encoded as a Bigraphical Reactive System. We describe the implementation of the tool, and how it can be used to verify correctness properties of some infinite-state models by applying a static analysis to reaction rules that permits the exclusion of some infinite branches of execution shown to always be free of violations. We give a proof of correctness for this method, and illustrate the usage of the tool with two examples --- a textbook implementation of the Dining Philosophers problem, and an example motivated by a ubiquitous computing application."
2174922,14018,8806,Programming and deployment of active objects with application-level scheduling,2012,"We extend and implement a modeling language based on concurrent active objects with application-level scheduling policies. The language allows a programmer to assign priorities at the application level, for example, to method definitions and method invocations, and assign corresponding policies to the individual active objects for scheduling the messages. Thus, we leverage scheduling and performance related issues, which are becoming increasingly important in multi-core and cloud applications, from the underlying operating system to the application level. We describe a tool-set to transform models of active objects extended with application-level scheduling policies into Java. This tool-set allows a direct use of Java class libraries; thus, we obtain a full-fledged programming language based on active objects which allows for high-level control of deployment related issues."
1390570,14018,8228,Classifying service flows in the encrypted skype traffic,2012,"In this paper, we consider the problem of detecting Skype traffic and classifying Skype service flows such as voice calls, skypeOut, video conferencing, chat, file upload and download. We propose a classification method for Skype encrypted traffic based on the Statistical Protocol IDentification (SPID) that analyzes statistical values of some traffic attributes. We have evaluated our method on a representative dataset to show excellent performance in terms of Precision and Recall."
1141309,14018,8806,An offer evaluation system based on buyers' interests,2011,"Matchmaking systems failed to provide the best matched offer to each individual. In this paper, we develop an interest-based offer evaluation system for semantic matchmakers. Our system returns the best offer by sorting the request-matched offers according to the buyer's favors and interests. The best offer represents the maximum satisfaction of the buyer. Our system captures and analyzes individual interests to bring better results for each buyer as demonstrated in our case study."
1893473,14018,8494,Simplified logic design methodology for fuzzy membership function based robust detection of maternal modulus maxima location: A low complexity Fetal ECG extraction architecture for mobile health monitoring systems,2011,"This paper proposes a simplified logic design methodology for the fuzzy membership function used for robust and reliable detection of modulus-maxima locations in wavelet domain for fetal ECG extraction from the abdominal composite ECG signal. This simplification is achieved by exploiting the inherent time-position information of the wavelet coefficients decomposed at different resolution levels. Subsequently, a low complexity VLSI architecture for Fetal ECG extraction is presented which is designed using the recently proposed memory-efficient, multiplierless Discrete Wavelet Transform method. The generic memory model within this architecture will provide the flexibility to configure the on-chip memory with any type of orthonormal wavelets suitable for different applications. Total synthesized cell area of the proposed architecture is 14.2mm 2  and power consumption is 101.5µW at 1.2 V @ 1 MHz frequency using 0.13µm standard cell technology. The proposed architecture is targeted for the personalized health monitoring applications within a mobile home-care medical device in the resource constrained environment."
1926261,14018,8806,Linked education: interlinking educational resources and the Web of data,2012,"Research on interoperability of technology-enhanced learning (TEL) repositories throughout the last decade has led to a fragmented landscape of competing approaches, such as metadata schemas and interface mechanisms. However, so far Web-scale integration of resources is not facilitated, mainly due to the lack of take-up of shared principles, datasets and schemas. On the other hand, the Linked Data approach has emerged as the de-facto standard for sharing data on the Web and offers a large potential to solve interoperability issues in the field of TEL. In this paper, we describe a general approach to exploit the wealth of already existing TEL data on the Web by allowing its exposure as Linked Data and by taking into account automated enrichment and interlinking techniques to provide rich and well-interlinked data for the educational domain. This approach has been implemented in the context of the mEducator project where data from a number of open TEL data repositories has been integrated, exposed and enriched by following Linked Data principles."
1449649,14018,8806,A coordination-based access control model for space-based computing,2012,"Shared tuple spaces serve as coordination medium for independent processes to exchange data and messages in a flexible way. If communication takes place over the Internet, security becomes an important issue. It must be forbidden that unauthorized processes get access to protected data. In this paper, we present an authorization model for a space-based middleware that uses its own coordination mechanisms for defining fine-grained access control policies. By integrating coordination and security mechanisms into one single concept, the model allows for flexible and secure distributed collaboration."
2173632,14018,8806,Similar MRI object retrieval based on modified contour to centroid triangulation with arc difference rate,2014,"In this paper, we propose a new image retrieval method based on Sectored Contour to Centroid Triangulation (SCTCT) using distinctive shape feature, named Arc Difference Rate (ADR). We utilized Support Vector Machine (SVM) method as an extraction tool to extract suspicious tumor area as binary object image from the breast MRI. Therefore extracted 100 binary object images are used as test cases in the experimental study. The results from proposed method show the improvement in finding correct matches compare to the traditional SCTCT."
2178717,14018,8806,Start time and duration distribution estimation in semi-structured processes,2013,"Semi-structured processes are business workflows, where the execution of the workflow is not completely controlled by a workflow engine, i.e., an implementation of a formal workflow model. Examples are workflows where actors potentially have interaction with customers reporting the result of the interaction in a process aware information system. Building a performance model for resource management in these processes is difficult since the required information is only partially recorded. In this paper we propose a systematic approach for the creation of an event log that is suitable for available process mining tools. This event log is created by an incrementally cleansing of data. The proposed approach is evaluated in an experiment."
1583286,14018,8806,Face recognition based on global and local features,2014,"This paper presents an evaluation of different methods considering the usually problems in face recognition. We consider variations in illumination, facial expression and facial details to propose a new method combining global and local face image features. This approach combines PCA, 2D-DCT and Gabor Wavelet Transform to obtain the global and local features representation. The Nearest Neighbor using the Euclidean distance performs the classification. The experiments were performed in the classical ORL and Yale face recognition databases. The proposed approach presented interesting results in comparison with the literature methods."
2202443,14018,9080,A discrete artificial bee colony algorithm for the multi-objective redistricting problem,2012,"In this paper, the performance of two classical algorithms (simulated annealing and a discrete artificial bee colony) are compared on the redistricting problem, using a real example in Mexico and highlighting the superiority of the latter."
793398,14018,9080,The gaussian polytree EDA for global optimization,2011,This paper explains how to construct Gaussian polytrees and their application to estimation of distribution algorithms in continuous variables.
1329937,14018,9080,Evolutionary software repair,2012,The tutorial summarizes recent work applying evolutionary computation methods to the problem of automatically repairing software defects.
1801302,14018,8806,Parallel constraint-based local search on the HA8000 supercomputer (abstract),2011,We present a parallel implementation of a constraint-based local search algorithm and investigate its performance results on hardware with several hundreds of processors.
637240,14018,9080,Simple tools for multimodal optimization,2011,"We compare various approaches for multimodal optimization; we focus on comparing restart and more sophisticated approaches, and on the use of quasi-random numbers."
1299646,14018,8806,Rethinking agribusiness models in Africa,2014,This paper couples the agriculture scenario in Africa with requirement engineering method to develop a new business model that is configurable and scalable.
1130242,14018,8806,Improved named entity recognition: patterns in columns model (PCM),2014,The goal of this paper is to improve the Named Entity Recognition for automatic information extraction related to record based data in text documents.
2137121,14018,9080,(1+2)-evolution strategy for fitting a straight shuffle of min to a dataset,2011,This paper proposes an Evolution Strategy to find a shuffle of M  that suits to a batch of datasets that serve as benchmark. Results are compared with wavelet estimation.
1315874,14018,9080,Generalisation in genetic programming,2011,Genetic programming can evolve large general solutions using a tiny fraction of possible fitness test sets. Just one test may be enough.
2338145,14018,8806,Query execution on a mobile database system,2012,This paper proposes strategies for query execution in mobile databases distributed over a mobile network. The underlying network architecture of the database is based on 3G telephone networks.
2147968,14018,9704,Quantum bacterial foraging optimization algorithm,2014,"This paper proposes a novel swarm intelligence optimization method which integrates bacterial foraging optimization (BFO) with quantum computing, called quantum bacterial foraging optimization (QBF ..."
1724012,14018,9080,Numerical optimization by multi-gene genetic programming,2013,"This paper presents a new method for numerical optimization problems based on Multi-Gene Genetic Programming. We discuss theoretical aspects, operators, representation, and experimental results."
1179486,14018,9080,Emergence of altruism in open-ended evolution in a population of autonomous agents,2011,This paper summarizes recent works on the evolution of altruism to solve the tragedy of commons in the context of open-ended evolution with a fixed number of robotic agents.
1632794,14018,21102,Closed form fuzzy interpolation with interval type-2 fuzzy sets,2014,"Conference Name:2014 IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2014. Conference Address: Beijing, China. Time:July 6, 2014 - July 11, 2014."
1568253,14018,8806,Probabilistic embedding: experiments with tuple-based probabilistic languages,2013,"We introduce  probabilistic (modular) embedding  as an extension to the well-known notion of  modular embedding  [5] conceived to capture the expressiveness of stochastic systems, focussing here on tuple-based probabilistic languages."
967580,14018,21102,Strongly prime fuzzy ideals over noncommutative rings,2013,"In this paper it is defined the concept of strongly prime fuzzy ideal for noncommutative rings. Also, it is proved that the Zadeh's extension preserves strongly fuzzy primeness and that every strongly prime fuzzy ideal is a prime fuzzy ideal as well as every fuzzy maximal is a strongly prime fuzzy ideal."
1680653,14018,9080,Estimation of distribution algorithms based on copula functions,2011,The main objective of this doctoral research is to study Estimation of Distribution Algorithms (EDAs) based on copula functions. This new class of EDAs has shown that it is possible to incorporate successfully copula functions in EDAs.
1514932,14018,21102,Domination in bipolar fuzzy graphs,2013,"The concepts of cardinality, dominating set, independent set, total dominating number and independent dominating number of bipolar fuzzy graphs are introduced and investigated. The notion of an irredundance number of a bipolar fuzzy graph is also discussed."
1287028,14018,8806,Row manipulation in the heterogeneous tabular forms with a hexadecimal grid graph model,2012,"We consider hexadeci-grids as a model of multiply layered heterogeneous tabular forms, which are generalization of the octgrid model. We introduce column deletion and multiple row deletion algorithms on the hexadeci-grid model."
1184851,14018,9080,Improving recruitment effectiveness using genetic programming techniques,2013,"A real-world problem, namely to improve the recruitment effectiveness of a certain company, is tackled here by evolving accurate and human-readable classifiers by means of grammar-based genetic programming techniques."
221053,14018,20332,Convergence properties of (μ + λ) evolutionary algorithms,2011,We present a number of convergence properties of population-based Evolutionary Algorithms (EAs) on a set of test functions. Focus is on EA using k-Bit-Swap (kBS) operator. We compare our findings to past research.
1177449,14018,8806,A layered coordination framework for optimizing resource allocation in adapting cloud-based applications,2012,In this paper we propose a framework that adapts a cloud-based software application by providing an enhanced assembly of resources using the Pareto-optimal solution to optimize the resource allocation with tight cooperation between the cloud layers.
2433121,14018,21102,Fuzzy delta separation axioms,2011,"We introduce a new type of separation axioms, which is called fuzzy δ-separation axioms by using the concept of fuzzy δ-open sets. Also we investigate the relation between the separation property and the subspaces. We show that fuzzy δ-separation axioms are hereditary in fuzzy regular open subspaces."
1717707,14018,21102,Fuzzy quaternion numbers,2013,"In this paper we build the concept of fuzzy quaternion numbers as a natural extension of fuzzy real numbers. We discuss some important concepts such as their arithmetic properties, distance, supremum, infimum and limit of sequences."
1269641,14018,8806,A model transformation approach for verifying multi-agent systems using SPIN,2011,"In our previous work, we developed a nested Petri net framework for modeling multi-agent systems. In this paper, we present a method to analyze the nested Petri net model using model checking. Our method systematically translates a nested Petri net model into a PROMELA program in SPIN."
1337466,14018,9080,LAMM-MMA: multiobjective memetic algorithm with local aggregate meta-model,2011,In this paper we describe a multiobjective memetic algorithm utilizing local distance based meta-models. This algorithm is evaluated and compared to standard multiobjective evolutionary algorithms (MOEA) as well as to a similar algorithm with a global meta-model.
1338682,14018,9080,Evolution of neural networks topologies and learning parameters to produce hyper-heuristics for constraint satisfaction problems,2011,This paper describes a model which constructs hyper-heuristics for variable ordering within Constraint Satisfaction Problems (CSPs) by running a genetic algorithm that evolves the topology of neural networks and some learning parameters.
1218382,14018,21102,On fuzzy ideals of fuzzy lattice,2012,"We characterize a fuzzy lattice through a fuzzy partial order relation, define a fuzzy ideal and fuzzy filter of fuzzy lattice, characterize a fuzzy ideal of fuzzy lattice using its level set and its support and show that a subset of a fuzzy lattice is a fuzzy ideal if and only if its support is a crisp ideal. Similarly, we show the same for its level set."
788445,14018,9080,Genetic invention of fast and optimal broad-band stokes/mueller polarimeter designs,2011,"We have applied a genetic algorithm to generate optimal polarimeter designs for a selected wavelength interval, assuming known dispersion relations of the components. Our results are improvements on previous patented designs based on ferroelectric liquid crystals."
2064842,14018,9080,An evaluation of cellular population model for improving quantum-inspired evolutionary algorithm,2012,"This work empirically evaluates the impact of Von Neumann Cellular population model on Quantum inspired Evolutionary algorithms (QEA), specifically for solving Massively Multimodal Deceptive Problem (MMDP) and Knapsack problems."
2159000,14018,21102,Relationship between intuitionistic fuzzy similarity measures,2011,"Numerous similarity measures between intuitionistic fuzzy sets are proposed in literature using different approaches. In this paper, relationship between some existing intuitionistic fuzzy similarity and distance measures are investigated. These relations are paramount for the choice of a similarity or a distance measure and its application for any research topic."
941467,14018,9080,Evolving optimal agendas and strategies for negotiation in dynamic environments: a surrogate based approach,2012,"Two key problems in a negotiation are: i) for the players to de- cide what issues to include in a negotiation, and ii) what strategy to use for negotiating over the chosen issues. In general, there will be many (say m) issues available for negotiation and the players must choose a subset of g"
2208468,14018,9080,Performing with CUDA,2011,Recently a GPGPU application had to be redesigned to overcome performance problems. A number of software engineering lessons were learnt from this and other projects. We describe those about obtaining high performance from nVidia GPUs and practical aspects of CUDA C software development.
830789,14018,9080,"Ant colony optimisation and the traveling salesperson problem: hardness, features and parameter settings",2013,"Our study on ant colony optimization (ACO) and the Travelling Salesperson Problem (TSP) attempts to understand the effect of parameters and instance features on performance using statistical analysis of the hard, easy and average problem instances for an algorithm instance."
760727,14018,8806,Towards a ranking framework for software components,2013,This paper presents a framework for ranking components by utilizing types of failures and pre-defined ranking features. The ultimate goal is that such a framework can help system developers in prioritizing components systematically for achieving a more reliable software system.
1964237,14018,21102,On another approach to the definition of an L-fuzzy valued integral,2011,We continue to develop a construction of an L-fuzzy valued measure extending a crisp measure defined on a σ-algebra of crisp sets to an L-fuzzy valued measure defined on a T M -tribe. We describe two equivalent approaches to define an L-fuzzy valued integral of non-negative measurable functions.
1027241,14018,9080,Designing artificial tetris players with evolution strategies and racing,2011,This article describes how racing procedures in evolution strategies can help reduce the number of evaluations. This idea is illustrated on learning Tetris players which can be addressed as a stochastic optimization problem. Different experiments show the benefits of the racing procedures in evolution strategies which can significantly reduce the number of evaluations.
1777444,14018,8806,Towards a private vector space model for confidential documents,2013,"We introduce in this paper a method to anonymize document vector spaces. These vector spaces can be used to analyze confidential documents without disclosing private information. The method is inspired in microaggregation, a popular technique used in statistical disclosure control."
2511636,14018,21102,Elimination search for puzzle games: An application for Hashi solver,2011,"This paper proposes an efficient method to solve Hashi, a logical-type puzzle game with N by M grid. By using two methods, intersection method and elimination search, we can solve Hashi quickly and efficiency. The solver is authenticated by solving problems taken from Internet."
1722820,14018,9080,Evolving artificial neural networks with FINCH,2013,We present work with the FINCH automatic evolutionary programming tool to evolve code that generates Artificial Neural Networks (ANNs) that perform desired tasks. We show how FINCH can be used to evolve code that generates an ANN that performs a simple classifying task with proficiency.
1101025,14018,21102,Noise reduction in time series using F-transform,2013,"In this paper, we will focus on the application of fuzzy transform (F-transform) in the analysis of time series. We assume that the time series is decomposed into two constituents: the trend-cycle and random noise. We will demonstrate that using the F-transform we can reduce the variability of random noise which consequence is an extraction of the trend-cycle."
1092838,14018,9704,Testing a Particle Swarm Optimization and Artificial Bee Colony Hybrid algorithm on the CEC13 benchmarks,2013,"In this paper we test a hybrid Particle Swarm Optimization (PSO) and Artificial Bee Colony (ABC) algorithm on the CEC13 testbed. The hybridization technique is a component-based one, where the PSO algorithm is augmented with an ABC component to improve the personal bests of the particles."
1364811,14018,21102,On the relationship between clustering and coding theory,2012,"In this paper we discuss the relations between clustering and error correcting codes. We show that clustering can be used for constructing error correcting codes. We review the previous works found in the literature about this issue, and propose a modification of a previous work that can be used for code construction from a set of proposed codewords."
810937,14018,9080,Search-based construction of finite-state machines with real-valued actions: new representation model,2013,In this paper a search-based method for constructing finite-state machines (FSMs) with continuous (real-valued) output actions is improved. A more flexible FSM representation model is presented and compared with the previous one on the problem of unmanned aircraft control.
1221421,14018,21102,A method for estimating criteria weights from interval-valued intuitionistic fuzzy preference relation,2014,"Interval-valued intuitionistic fuzzy preference relation is a useful tool to express decision maker's interval- valued intuitionistic fuzzy preference information over criteria in the process of multi-criteria decision making. How to derive the priority weights from an interval-valued intuitionistic fuzzy preference relation is an interesting and important issue in decision making with interval-valued intuitionistic fuzzy preference relation(s). In this paper, some new concepts such as interval-valued interval fuzzy sets, interval-valued interval fuzzy preference relation and consistent interval-valued intuitionistic fuzzy preference relation, are defined, and the equivalent interval-valued interval fuzzy preference relation of interval- valued intuitionistic fuzzy preference relation is given. Then a method for estimating criteria weights from interval- valued intuitionistic fuzzy preference relations is developed, two numerical examples are provided to illustrate the developed method."
1594830,14018,9080,Differential evolution strategies with random forest regression in the bat algorithm,2013,"In this paper, we present a novel solution for the hybridization of the bat algorithm with differential evolution strategies and a random forests machine learning method. Extensive experiments and tests on standard benchmark functions have shown that these hybridized algorithms improved the original bat algorithm significantly."
2145311,14018,21102,Sum of squares solutions assuring non-quadratic discrete stability,2011,"A new stability condition in terms of SOS is studied in this paper for discrete-time fuzzy systems. Based on a parameter-dependent Lyapunov function, we study asymptotically SOS relaxation families, reducing the conservatism that commonly exists in the quadratic stability approaches."
1671059,14018,21102,XFSML: An XML-based modeling language for fuzzy systems,2012,"This paper presents a new modeling language for fuzzy systems called XFSML. It is an XML-based language and it is proposed as a starting point for the definition of a standard modeling language in the fuzzy community. The main features of the language are its high expressiveness and its independence from specific platforms, tools or programming languages."
2233007,14018,21102,On a certain type of unary operators,2012,"In our study we give a general form for modifiers that includes negation, different types of hedge and the sharpness operators. We will show that the four operators have a common form in the Pliant system and they will be called modifier operators. By changing the parameter value of a modifier we get the modalities, negation and the sharpness operators."
1171727,14018,21102,Copula-based universal integrals,2013,"A hierarchical family of copula-based integrals is introduced and discussed. When considering the product copula, a family of decomposition integrals independently introduced by Even and Lehrer, and by Mesiar and Stupnanova, is recovered. Boundary members are distinguished universal integrals introduced by Klement at al. in 2010."
1239660,14018,21102,The use of t-norms in mathematical models of epidemics,2013,"We study different t-norms in epidemiological mathematical models. We explore the SI model, which encounter between individuals is originally modelled by the product operation (also a t-norm). We discuss the differences and advantages of using one or another t-norm. Finally, we compare the solutions provided by each approach."
810984,14018,9080,Finding a diverse set of decision variables in evolutionary many-objective optimization,2013,"In this paper, we modify an evolutionary many-objective optimization algorithm so that it can find a diverse set of solutions in the decision variable space. The modification is based on considering the Euclidean distance in the decision variable space. The effect of our modification is examined by using benchmark test problems. From computational experiments, we can say that a diverse set of solutions in the decision variable space is searched by the modification."
845240,14018,23827,A middleware and algorithms for trust calculation from multiple evidence sources,2012,"Trust is a concept that has been used to support better decision-making when there is incomplete information. Trust requires evidence. There are multiple evidence sources. One or more evidence sources may be used in trust calculation. This paper presents a middleware that takes this into account, the algorithms used and experimental results."
1645326,14018,9080,Improving clonal colony optimization to evolve robust solutions,2012,"In this article we work with a recently introduced metaheuristic for robust optimization, inspired by the structure and behavior of biologic clonal colonies. We propose some improvements to increase their exploration and the exploitation capabilities. Our approach is compared to other robust optimization techniques, focusing in how the population is managed during the search."
944611,14018,8806,Filtering XFD toward interoperability,2013,"This paper describes an algorithm for computing the maximal set of XML functional dependencies (XFD) that is guaranteed to hold across a set of local systems, each with its own defined set of XFD. Our method is based on an axiom system that we have proved to be sound and complete. An implementation demonstrates its feasibility."
1006805,14018,8806,Run-time checking of data- and protocol-oriented properties of Java programs: an industrial case study,2013,"We introduce SAGA, a general framework that  combines  monitoring and run-time assertion checking. SAGA integrates both data-flow and control flow properties of Java classes  and interfaces  in a single formalism. We evaluate the framework by conducting an industrial case study."
1327004,14018,8806,Tree-based search for stochastic simulation algorithm,2012,"In this paper, we present an efficient tree-based formulation for exact stochastic simulation algorithm (SSA) to improve the search for the next reaction firing. There are two implementations considered: one based on a complete binary tree and one based on the Huffman tree, an optimal tree for data compression."
1371791,14018,21102,A decomposition theorem for T-indistinguishability operators. The continuous strict Archimedean case,2012,"In this paper we study the relationship between the cuts of a T-indistinguishability operator and the t-norm T chosen to define fuzzy transitivity. The key result is, strict Archimedean t-norms provide equivalence relations as their zero cuts, and that property characterizes such t-norms. As a consequence, a decomposition theorem for such T-indistinguishabilities is proved."
1423459,14018,9704,A new gravitational search algorithm using fuzzy logic to parameter adaptation,2013,"In this paper we propose a new Gravitational Search Algorithm (GSA) using fuzzy logic to change alpha parameter and give a different gravitation and acceleration to each agent in order to improve its performance, we use this new approach for mathematical functions and present a comparison with original approach."
557686,14018,11052,Modelling Primate Control of Grasping for Robotics Applications,2014,"European Conference on Computer Vision (ECCV) Workshops, Zurich, Switzerland, 7 September 2014.  Due to copyright restrictions, the attached PDF file only contains the abstract of the full text item. For access to the full text item, please consult the publisher's website."
1694030,14018,9080,A surrogate multiobjective evolutionary strategy with local search and pre-selection,2012,"In this paper we present an evolutionary strategy for multiobjective optimization. This evolution strategy is based on a surrogate memetic operator and a surrogate preselection model which provides several individuals in each generation. Thus, the optimization may be easily parallelized. The proposed algorithm is compared to some of existing evolutionary algorithms from the literature."
1554028,14018,8806,Smell classification using weakly responding data,2014,"This paper considers an array sensing system of odors and adopts a layered neural network for classification. In order to classify odors, we use data from all fourteen sensors even if some of them are not sensitive so much. We will propose three methods to use the data by insensitive sensors to find the features of odors."
1564061,14018,21102,Heart rate modeling and robust control during cycling exercise,2012,A Hammerstein ARX model is proposed to represent the cycling power/heart rate system. The equivalent fuzzy Takagi-Sugeno model is derived and then a robust fuzzy control strategy is applied to effectively control the system. Simulation and real time results are presented.
1901343,14018,21102,The dynamic measurement system design for Stewart platform by using digital image processing method,2011,"In this study, the digital image processing method in the dynamic measurement system for Stewart platform is investigated. The measured signals of the platform based on the digital image processing method represent as the feedback signal of control system. In the study, the accuracy of control system is reasonable by capturing the image information of platform by using the digital image processing method and getting the detail of platform action directly."
994097,14018,8806,BOwL: exploiting Boolean operators and lesk algorithm for linking ontologies,2012,"BOwL  applies word sense disambiguation techniques for tagging ontology entities with WordNet words. Boolean operators that appear in names of ontology entities are interpreted based on their semantics and are used during the ontology matching stage accordingly. Experimental results are shown, demonstrating the feasibility of the approach."
924415,14018,8806,A case study in the use of Groovy and Grails,2012,"The work described in this paper aims, through the use of four case studies, to investigate the benefits and problems associated with the use of Rapid Application Development (RAD) with Groovy and Grails for the development of web applications within a short time period in the financial services sector."
730020,14018,8806,Service-oriented paradigm for analyzing hydrological processes,2012,This paper considers using the service-oriented paradigm to analyze the hydrological processes. The practical part is implemented in the hydrological process called evapotranspiration (ET). The estimation of the ET is of great importance for the management of water resources and its implementation by means of using service-oriented paradigm and Web services. This can be useful for the decision makers.
1401830,14018,9080,Development and investigation of biologically inspired algorithms cooperation metaheuristic,2013,Cooperation of biologically inspired algorithms as an optimization meta-heuristic is considered. Its performance evaluation and comparison with component algorithms performance on benchmark optimization problems is fulfilled. Workability of the meta-heuristic is demonstrated with artificial neural networks based classifiers tuned to two real world problems.
2111756,14018,21102,On extension of consequent parts of T-S inference model,2011,"This paper discusses the extension of consequent parts of T-S inference model. First, it proposes an extended T-S inference model in which the coefficients of consequent parts of the fuzzy rules are extended from constants to functions. It also shows the property of the proposed model from the point of view of the equivalence and the monotonicity, by using simplified model of the proposed model."
1354780,14018,9080,A memory scheme for genetic network programming with adaptive mutation,2011,"Recently, a new approach named Genetic Network Programming (GNP) has been proposed for especially solving complex problems in dynamic environments. In this paper, we propose a memory scheme for GNP to enhance the performance of GNP and use SARSA learning based adaptive mutation mechanism to guide the GNP evolution process."
786713,14018,9080,"Push-forth: a light-weight, strongly-typed, stack-based genetic programming language",2013,"This paper defines the push-forth language, a recombination of Push [3] and Joy [7], borrowing type-safety considerations from Alp [2]. Push-forth is stack-based, strongly typed and easy to extend. The concept of an Evolutionary Development Environment is presented, and some informal experiments are described to illustrate the utility of such an environment."
2455026,14018,21102,On some clustering approaches for graphs,2011,In this paper we discuss some tools for graph perturbation with applications to data privacy. We present and analyse two different approaches. One is based on matrix decomposition and the other on graph partitioning. We discuss these methods and show that they belong to two traditions in data protection: noise addition/microaggregation and k-anonymity.
2979385,14018,9704,Proposal of Business Process Visualization Tool,2012,"Since around 2000, many companies have worked to create interorganizational cooperation, which can solve the problem of a lack of resources when creating new business. In the real world, many companies cannot find the holder of necessary resources, and they cannot conduct proper interorganizational cooperation. To solve these problems, we propose a business process visualization tool."
2258136,14018,23735,The distributed co-evolution of an embodied simulator and controller for swarm robot behaviours,2011,"Embodied fitness assessment of robotic controllers is slow but grounded, while assessment in a simulated environment is fast but can run foul of the ‘reality gap’. We present a distributed co-evolutionary method to adapt the environmental model of an on-board simulator within the context of swarm robotics."
1903256,14018,21102,"Knowledge management, education and firm's performance",2011,"In this paper, we examine how new CIO's education affects the association between knowledge management and firm's operating performance. Using OLS regression analysis, we find that the positive relationship between the effectiveness of knowledge management and firm's operating performance is more prominent when the firm's new CIO has a PHD degree. Our results therefore provide evidence that CIO's education facilitates the application of knowledge management."
2350946,14018,21102,On spline methods of approximation under L-fuzzy information,2011,"This work is closely related to our previous papers on algorithms of approximation under L-fuzzy information. In the classical theory of approximation central algorithms were worked out on the basis of usual, that is crisp splines. We describe central methods for solution of linear problems with balanced L-fuzzy information and develop the concept of L-fuzzy splines."
2571668,14018,21102,Efficient centroid computation of general type-2 fuzzy sets with linear secondary membership function,2011,"This paper provides root finding methods for the centroid computation of interval type-2 fuzzy sets and general type-2 fuzzy sets with linear secondary membership function. When the FOUs of the type-2 fuzzy sets are provided, the centroid computation methods in this paper is accurate and computational efficient, which provide new methods for the type-2 fuzzy systems computation."
748117,14018,8806,Clustering approach algorithm for image interpolation,2012,Several interpolation methods for preserving edges were proposed in literature. This work presents a new interpolation technique simple and easy to implement based on clustering which aims to increase the resolution of a gray scale image preserving edges. Results showed that the technique is quite competitive and meets the stated goals.
1182330,14018,9080,A new packing method for two dimensional rectilinear polygons using genetic algorithm,2011,This paper proposes a new placement method for two dimensional rectilinear polygons. The proposed method is based on the idea of corner junction method and uses only genetic algorithm approach with a new hierarchical chromosome structure to pack rectilinear polygons on container. Experimental results show the proposed method is succeeded in placement of some complicated two dimensional rectilinear polygons in feasible time.
2266007,14018,9080,Dynamic l-systems in GE,2011,"In this paper, I describe how to use Grammatical Evolution to implement a parameterized Lindenmayer System (L-System), where the number of production rules of the L-System is determined by the genome of the individual, rather than being determined by the user before hand. This leaves the number of production rules as a free parameter and allows the underlying topology of the system to be optimized by the evolutionary algorithm."
2202610,14018,8806,Towards a definition of sustainability in and for software engineering,2013,Sustainability is not supported by traditional software engineering methods. This lack of support leads to inefficient efforts to address sustainability or complete omission of this important concept. Defining and developing adequate support requires a commonly accepted definition of what sustainability means  in  and  for  software engineering.   We contribute a description of the aspects of sustainability in software engineering.
1115722,14018,8806,Random forgery attacks against DTW-based online signature verification algorithm,2011,"Skilled forged signatures are one of the most difficult to distinguish from genuine signatures and randomly forged signatures are considered to be less difficult than skilled forged signatures. However, since the FARs are generally calculated by averaging over all considered signatures (all considered signers), it might be possible that attacks using some selected signer's random forged signatures are stronger than those using skilled forged signatures. Therefore, we investigated the FARs against randomly forged signatures written by each signer. We evaluated a DTW-based online signature verification algorithm by using the SVC2004 database task 1 and task 2. The experimental results show that attacks using some signers' signatures are stronger than those using skilled forged signatures."
1488991,14018,9080,On the effect of using multiple GPUs in solving QAPs with CUDA,2012,"In this paper, we implement ACO algorithms on a PC which has 4 GTX 480 GPUs. We implement two types of ACO models; the island model, and the other is the master/slave model. When we compare the island model and the master/slave model, the island model shows promising speedup values on class (iv) QAP instances. On the other hand, the master/slave model showed promising speedup values both on classes (i) and (iv) with large-size QAP instances."
1646892,14018,9080,Improving uniformity of solution spacing in biobjective evolution,2013,"We introduce a new synergistic combination of features, some of which have previously been used individually but not together, to improve uniformity of spacing in evolved non-dominated sets, especially in biobjective problems. On five standard biobjective benchmark tests, these features are shown to enhance performance in distinct and complementary ways."
747028,14018,9704,Self-configuring genetic programming algorithm with modified uniform crossover,2012,For genetic programming algorithms new variants of uniform crossover operators that introduce selective pressure on the recombination stage are proposed. Operators probabilistic rates based approach to GP self-configuration is suggested. Proposed modifications usefulness is demonstrated on benchmark test and real world problems.
1070287,14018,21102,Fast and direct Karnik-Mendel algorithm computation for the centroid of an interval type-2 fuzzy set,2012,"Karnik-Mendel (KM) algorithms type reduction are commonly used in the centroid computation of interval type-2 fuzzy logic systems (IT2 FLSs). Some properties and improvements of the KM algorithms have be proposed. This paper proposed a new iteration formula for the centroid computation of interval type-2 fuzzy sets, which is faster than the current computation methods and can be used as a direct method for the centroid computation of interval type-2 fuzzy sets."
1918571,14018,21102,"On monotonicity of type 〈1,1〉 fuzzy quantifiers determined by fuzzy measures",2011,"In this contribution, we study a very important semantic property of generalized quantifiers called the monotonicity for fuzzy quantifiers of type 〈1,1〉 defined using fuzzy measures and Sugeno type of fuzzy integrals. We show that fuzzy integrals can ensure under some natural conditions the monotonicity of fuzzy quantifiers. Finally, we propose the concept of concave fuzzy quantifiers and prove that each concave fuzzy quantifier is expressible as the conjunction of a non-increasing and a non-decreasing fuzzy quantifier."
2408428,14018,9704,Frankenstein PSO applied to neural network weights and architectures,2011,"In this paper we present the FPSO A -FPSO P , a modified version of Frankstein Particle Swarm Optimization (FPSO), which is used to adjust the weights and architectures of a feedforward neural network. To evaluate the algorithm we used benchmark classification problems in medical care area. The results were compared with other algorithms which use the same methodology to find out the weights and architectures."
916356,14018,9080,"Noise, fitness distribution, and selection intensity in genetic algorithms",2011,"Many Genetic Algorithm (GA) problems have noisy fitness functions. In this paper, we describe a mathematical model of the noise distribution after selection and then show how this model of the noise distribution can be used to model the real, underlying selection intensity of the GA population, which promises to give us a better way to model GA convergence in the presence of noise."
1609955,14018,9704,An asynchronous parallel genetic algorithm for the maximum likelihood phylogenetic tree search,2012,"A phylogenetic tree represents the evolutionary relationships among biological species. Although parallel computation is essential for the phylogenetic tree searches, it is not easy to maintain the diversity of population in a parallel genetic algorithm. In this paper, we design a new asynchronous parallel genetic algorithm for tree optimization which maintain the diversity of population without any communication or synchronization."
867090,14018,8806,Inferring situation-based interests for mobile users: a case based reasoning approach,2011,"In this poster, we propose to personalize the search results for mobile users by modeling the user on three semantic dimensions: time, location and interests. A case based reasoning approach (CBR) is adopted to select the appropriate user profile for re-ranking the search results. Our experiments show that our retrieval approach is effective."
1294490,14018,8806,Building a scalable spatial OLAP system,2013,"We present a modular design of a spatial OLAP system. Contrary to most previous work on spatial OLAP systems, which combine existing OLAP systems and geographic information systems using a middleware layer, our system directly integrates OLAP and the processing of spatial data and, thus, provides the full power of spatial OLAP without performance penalty and allows a unified treatment of spatial and non-spatial data."
1143999,14018,9080,A new approach for generating numerical constants in grammatical evolution,2011,"A new approach for numerical-constant generation in Grammatical Evolution is presented. Experiments comparing our method with the three most popular methods for constant creation are performed. By varying the number of bits to represent a constant, we can increase our method's precision to the desired level of accuracy, overcoming by a large margin the other approaches."
2497724,14018,21102,Statistical scheme via AIC for evaluating the optimal cut off level in fuzzy clustering,2011,In this paper we show a new statistical scheme to find the optimal cut off level in fuzzy clustering which is an improvement of Uesu and Shinkai et. al [4]∼[7]. Deterministic algorithms which seek a certain equilibrium cluster level have essential disadvantage in principle. We focus in it and propose a statistical scheme via AIC.
1719805,14018,8806,Towards automated malware creation: code generation and code integration,2014,This short paper proposes two different ways for exploiting an evolutionary algorithm to devise malware: the former targeting heuristic-based anti-virus scanner; the latter optimizing a Trojan attack. An extended internal on the same the subject can be downloaded from http://www.cad.polito.it/downloads/
971204,14018,9704,An algorithm to find quantum templates,2012,"Identity circuits can be used as templates to minimize reversible circuits. This paper describes an algorithm to find all quantum templates with 3 qubits. The algorithm is used to find all templates with up to 8 gates. The method first finds a quantum identities of 3 qubits, template matching is used to verify if the identity is a template. Experiments show the significance of templates the new to reduce the quantum costs of benchmark circuits."
1373181,14018,9080,Noisy optimization convergence rates,2013,"We consider noisy optimization problems, without the assumption of variance vanishing in the neighborhood of the optimum. We show mathematically that evolutionary algorithms with simple rules with exponential number of resamplings lead to a log-log convergence rate (log of the distance to the optimum linear in the log of the number of resamplings), as well as with number of resamplings polynomial in the inverse step-size."
2134411,14018,21102,A formula for fuzzy linear regression analysis,2011,"The purpose of this paper is to deal with the problem of least-squares multiple regression with fuzzy data. The constant regression coefficient is assumed to be symmetric triangular, and other coefficients are real (crisp). By applying symmetric triangular approximations of fuzzy numbers, a new method for computing the regression coefficients is proposed. The new method is efficient and easy to determine the coefficients."
1170965,14018,8806,SCAling: SLA-driven cloud auto-scaling,2013,"This paper proposes SCAling, a platform and an approach driven by Service Level Agreement (SLA) for Cloud auto-scaling. Simulation experiments indicate that our model successfully keeps the best trade-off between SaaS provider profit and customer satisfaction without requiring predefined scaling rules such as in Amazon Auto-Scaling."
816800,14018,9773,Using Readers' Highlighting on Monochromatic Documents for Automatic Text Transcription and Summarization,2011,"Very often interested readers highlight documents with felt pens. Such marking may be seen as personal view of the most important aspects of the document, which are used to instantly draw the readers' attention at. This article addresses ways of using the highlighting made by the reader of a book or a paper documents to automatically generate its text summary."
1640093,14018,21102,Generalized quadratic programming problem with interval uncertainty,2013,"In this paper, a quadratic programming model is considered, wherein all parameters and decision variables take values in intervals. Existence of optimal solution for this model with certain acceptable level is justified and a methodology is proposed to derive such a solution. Finally, the theoretical development is illustrated by means of an example of portfolio selection."
969187,14018,21102,A novel adaptive fuzzy c-means algorithm for interval data type,2012,A novel extension of the fuzzy c-means clustering algorithm for interval data type based on an adaptive Euclidean distance is presented. The proposed method furnishes a fuzzy partition and a prototype for each cluster by optimizing a criterion based on an adaptive Euclidean distance that changes at each algorithm iteration. Experiments with real and synthetic data sets show the usefulness of this method.
1090718,14018,9704,Investigation on the performance of a new multiple choice strategy for PSO Algorithm in the task of large scale optimization problems,2013,"In this paper, a novel strategy for particle swarm optimization is presented and investigated over its ability to improve the performance of PSO algorithm in the task of large scale optimization problems. This proposed strategy alters the way the velocity of each particle is determined. Promising results of this innovative strategy are presented in the results section and briefly analyzed."
1371818,14018,21102,Construction of weak homogeneity from interval homogeneity. Application to image segmentation,2013,"In this paper we axiomatically define weak homogeneity of a fuzzy subset, which means that its membership function fulfills at least the minimum properties required to represent the homogeneity of a region. We also provide several construction methods based on the homogeneity of an interval. Besides, we show an illustrative example of these functions applied to the problem of image segmentation."
2558080,14018,9704,Equality constrained long-short portfolio replication by using probabilistic model-building GA,2012,"Portfolio replication problem is to optimize the portfolio such that its proportion-weighted combination is the same as the given benchmark portfolio. However, the benchmark portfolio generally opens only the return to the public but other information such as the assets included in the portfolio, the proportion-weighted combination, the rebalancing date and the investment strategies is closed to the public. In order to optimize such portfolios, we propose an optimization method based on the probabilistic model-building GA in this paper."
2537570,14018,21102,The sensing system for the autonomous mobile robot Emmy III,2011,This paper shows the results of the sensing system which was designed for the autonomous mobile robot Emmy III. The proposed Sensing System has as its mean part the Paraconsistent Neural Network. This artificial neural network is based on the Paraconsistent Evidential Logics — Et. The objective of the Sensing System is to inform the other robot components about the obstacle position. The reached results have been satisfactory.
806630,14018,21102,Monotonicity preserving SIRMs-connected fuzzy inference systems with a new monotonicity index: Learning and tuning,2013,"Recent research on Single Input Rule Modules (SIRMs)-connected fuzzy inference system (FIS) focuses on its monotonicity property fulfillment. The aim of this paper is to propose an alternative approach for modeling of monotonicity-preserving SIRMs-connected FIS. A new monotonicity index (MI) for approximating the monotonicity property fulfillment of an SIRMs-connected FIS is proposed. A hybrid of Harmony Search (HS), SIRMs-connected FIS, and the new MI is investigated. A proposed data-driven monotonicity-preserving SIRMs-connected FIS model with HS is then presented. The use of MI for tuning of an SIRMs-connected FIS is demonstrated too."
1028914,14018,21102,A robust clustering algorithm for interval data,2012,In this paper we propose a robust clustering algorithm for interval data. The proposed method is based on similarity measure that is not necessary to specify a cluster number and initials. Several numerical examples demonstrate the effectiveness of the proposed robust clustering algorithm. We then apply this algorithm to the real data set with cities temperature interval data. The proposed clustering algorithm actually presents its robustness.
1560826,14018,9080,A novel meta-heuristic based on soccer concepts to solve routing problems,2013,"In this paper, we describe a new meta-heuristic to solve routing problems. This meta-heuristic is called Golden Ball (GB), and it is based on soccer concepts. To prove its quality we apply it to the Vehicle Routing Problem with Backhauls (VRPB) and we compare its results with the results obtained by a basic Genetic Algorithm (GA) and an Evolutionary Algorithm (EA)."
858312,14018,9080,Group-based ant colony optimization,2013,We introduce Group-Based Ant Colony Optimization which uses a parallel construction principle on group-structured solution encodings. We compare the parallel construction method with the classical sequential one. In this context we also perform simulation experiments for the Vehicle Routing Problem with Time Windows using the Solomon [8] and the Homberger & Gehring [5] instances.
1205324,14018,9704,Running Up Those Hills: Multi-modal search with the niching migratory multi-swarm optimiser,2014,"Copyright © 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works."
1181315,14018,8806,Respiratory motion estimation using visual coded markers for radiotherapy,2014,Respiratory motion is an important issue in radiotherapy as it can cause many problems such as damages to healthy tissues. Different methods have been introduced to estimate the respiratory motion but most of them need some electronic devices or expensive materials. In this research we proposed a respiratory motion estimation method by tracking inexpensive and easy to use visual coded markers using stereo vision.
1093779,14018,9704,A replacement strategy for balancing convergence and diversity in MOEA/D,2014,"This paper studies the replacement schemes in MOEA/D and proposes a new replacement named global replacement. It can improve the performance of MOEA/D. Moreover, trade-offs between convergence and diversity can be easily controlled in this replacement strategy. It also shows that different problems need different trade-offs between convergence and diversity. We test the MOEA/D with this global replacement on three sets of benchmark problems to demonstrate its effectiveness."
1250442,14018,21102,A fuzzy clustering algorithm based on adaptive city-block distances,2012,This paper gives an adaptive version of the fuzzy clustering algorithm based on city-block distances. The proposed method gives a fuzzy partition and a prototype for each cluster by optimizing an adequacy criterion based on an adaptive city-block distance that changes at each algorithm's iteration and is different from one cluster to another. Experiments with real data sets show the usefulness of this algorithm.
1037876,14018,21102,On a functional equation related to distributivity of fuzzy implications,2013,"Recently in some considerations connected with the distributivity laws of fuzzy implications over triangular norms and conorms, the following functional equation appeared f(min(x + y, a)) = min(f(x) + f(y)m b). In the current paper we consider a generalized version of this equation, namely the equation f(m 1 (x + y)) = m 2 (f(x) + f(y)), where m 1 , m 2  are functions defined on some intervals of ℝ satisfying additional assumptions. We analyze the cases when m 2  is injective and when m 2  is not injective."
1398969,14018,9080,Estimating swarm parameters by evolutionary learning.,2011,"If we assume that the collective dynamics of wild animals can be modelled, it would be desirable to recover the dynamics of the model via interaction with them. In this paper, we demonstrate that it is possible to recover the parameters of a shoaling model used by a swarm. This can be achieved by evolving the parameters of a single agent that interacts with the swarm. We present an evaluation of this approach, using a genetic algorithm to learn the parameters of a shoaling model."
1535915,14018,8806,ABOI: a novel strategy to mitigate the blocking due to outdated information in OCS/OBS network,2013,"The blocking probability in all-optical networks caused by out of date information from the network has been studied basically in the OCS paradigm. This paper presents a performance evaluation study that considers the problem of outdated information in hybrid optical networks (OCS/OBS). The ABOI is proposed to mitigate this problem. In our simulations, the ABOI strategy decreased the blocking probability due to outdated information from the network."
1050501,14018,21102,On stabilization of discrete-time periodic TS systems,2013,"In this paper we consider controller design for periodic Takagi-Sugeno fuzzy models. For this, we use a periodic nonquadratic Lyapunov function defined at the time instants when the subsystems switch. Using the proposed conditions we are able to handle periodic Takagi-Sugeno systems where the local models or even the subsystems are unstable or cannot be stabilized. The application of the conditions is illustrated on numerical examples."
1277843,14018,8806,Abstract program slicing of database query languages,2013,"In this paper, the notions of semantic relevancy of statements, semantic data dependences and conditional dependences are extended to the case of programs embedding SQL statements in both concrete and abstract domains. This allows us to refine traditional syntax-based Database-Oriented Program Dependence Graphs, yielding to a more accurate semantics-based abstract program slicing algorithm."
1206388,14018,21102,A case study on the application of instance selection techniques for Genetic Fuzzy Rule-Based Classifiers,2012,"When considering data sets characterized by a large number of instances, the computational time required to apply Genetic Algorithms for generating Fuzzy Rule-Based Classifiers increases considerably, mainly due to the fitness evaluation. Another important problem associated to these kinds of data sets is an undesired increase of the obtained model complexity."
1035940,14018,8806,Enhancing scientific information systems with semantic annotations,2013,"Scientific Information Systems aim to produce or improve knowledge on a subject through activities of research and development. The management of scientific data requires some essential properties. We propose SemLab an architecture that supports interoperability, data quality and extensibility through a unique paradigm: semantic annotation. We present two applications that validate our architecture."
1367842,14018,9080,On the idea of evolving decision matrix hyper-heuristics for solving constraint satisfaction problems,2011,"When solving a Constraint Satisfaction Problem (CSP), the order in which the variables are selected to be instantiated has implications in the complexity of the search. This work presents the first ideas for evolving hyper-heuristics as decision matrices where the elements in the matrix represent the variable ordering heuristic to apply according to the constraint density and tightness of the current instance."
1023880,14018,21102,Fuzzy Pexider equations and applications to fuzzy control,2012,Pexider equations are widely investigated functional equations in algebraic structures. We consider a special type of such equations involving fuzzy sets and fuzzy correspondences. We give necessary and sufficient conditions for their solutions in the framework of fuzzy structures with the co-domain being a complete lattice. We also give some solutions using the cut-worthy approach
1123318,14018,9704,Chopin or not? A memetic approach to music composition,2013,"This paper describes preliminary studies on automatic music composition. The aim is to investigate if it possible to generate music that would be difficult to distinguish from the music created by human composers. We describe a memetic algorithm used to achieve the goal of the studies, the influence of its parameters on the quality of composed pieces and conclusions stemmed from several tests."
991607,14018,9704,Codynamic fitness landscapes of coevolutionary minimal substrates,2014,Coevolutionary minimal substrates are simple and abstract models that allow studying the relationships and codynamics between objective and subjective tness. Using these models an approach is presented for dening and analyzing tness landscapes of coevolutionary problems. We devise similarity measures of codynamic tness landscapes and experimentally study minimal substrates of test{based and compositional problems for both cooperative and competitive interaction.
1351887,14018,9080,Validating design choices in a pool-based distributed evolutionary algorithms architecture,2012,"This paper introduces SofEA, a pool-based architecture built over CouchDB for distributing evolutionary algorithms (EAs) across computer network in an asynchronous and decentralized way. Clients perform different functions (evaluation, reproduction, selection) which leads to a complex behavior that will be examined in this paper, looking for the values that yield the best performance."
742619,14018,9704,DE-TDQL: An adaptive memetic algorithm,2012,Memetic algorithms are population-based meta-heuristic search algorithms that combine the composite benefits of natural and cultural evolution. In this paper a synergism of the classical Differential Evolution algorithm and Q-learning is used to construct the memetic algorithm. Computer simulation with standard benchmark functions reveals that the proposed memetic algorithm outperforms three distinct Differential Evolution algorithms.
1125151,14018,8806,Experiences with client/server interactions in a reservation-based system,2012,"This paper presents some experiences with client/server communication in a reservation-based system (a modified Linux kernel, implementing a reservation-based scheduler named SCHED_DEADLINE is used). The experiments show that when tasks communicate, the predictability and temporal isolation provided by resource reservations can be compromised, and that this problem can be solved by using the BandWidth Inheritance (BWI) algorithm."
1641071,14018,9080,Towards a repulsive and adaptive particle swarm optimization algorithm,2013,"This paper proposes a Repulsive Adaptive PSO (RAPSO) variant that adaptively optimizes the velocity weights of every particle at every iteration. RAPSO optimizes the velocity weights during every outer PSO iteration, and optimizes the solution of the problem in an inner PSO iteration. We compare RAPSO to Global Best PSO (GBPSO) on nine benchmark problems, and the results show that RAPSO out-performs GBPSO on difficult optimization problems."
1360371,14018,9080,Quantitative genetics in multi-objective optimization algorithms: from useful insights to effective methods,2011,"This paper shows that statistical algorithms proposed for the quantitative trait loci (QTL) mapping problem, and the equation of the multivariate response to selection can be of application in multi-objective optimization. We introduce the conditional dominance relationships between the objectives and propose the use of results from QTL analysis and G-matrix theory to the analysis of multi-objective evolutionary algorithms (MOEAs)."
2169166,14018,9080,Dynamic ant: introducing a new benchmark for genetic programming in dynamic environments,2011,"In this paper we present a new variant of the Ant Problem in the Dynamic Problem Domain. This approach presents a functional dynamism to the problem landscape, where by the behaviour of the ant is driven by its ability to explore the search space being constrained. This restriction is designed in such a way as to ensure that no generalised solution to the problem is possible, thus providing a functional change in behaviour."
1903877,14018,21102,Generalized intuitionistic fuzzy soft set and its application in practical medical diagnosis problem,2011,"In this paper, a generalized intuitionistic fuzzy soft set (GIFSS) is introduced and its various properties are presented. The generalized intuitionistic soft fuzzy relations on GIFSS have been defined and their properties are discussed. We have also devised a new scoring function to compare two intuitionistic fuzzy numbers. An application of GIFSS, generalized intuitionistic soft fuzzy relations on GIFSS and the novel score function are demonstrated through a practical example of a multi-criteria medical diagnosis problem."
798349,14018,8806,Experimental and theoretical analyses of memory allocation algorithms,2014,"In this paper, we present an experimental study to compare six user-level memory allocators. In addition, we compare the experimental results with the asymptotic analyses of the evaluated algorithms. The experimental results show that parallelism affects negatively the investigated allocators. The theoretical analysis of the execution time demonstrated that all evaluated allocators show linear complexity with respect to the number of allocations."
1451200,14018,9080,Evolutionary algorithms for overlapping correlation clustering,2014,"In Overlapping Correlation Clustering (OCC), a number of objects are assigned to clusters. Two objects in the same cluster have correlated characteristics. As opposed to traditional clustering where objects are assigned to a single cluster, in OCC objects may be assigned to one or more clusters. In this paper, we present Biased Random-Key Genetic Algorithms for OCC. We present computational experiments such results outperformed the state of art methods for OCC."
1735208,14018,21102,jFuzzyLogic: a robust and flexible Fuzzy-Logic inference system language implementation,2012,"This work introduces jFuzzyLogic, an open source library for fuzzy systems which allow us to design Fuzzy Logic Controllers supporting the standard for Fuzzy Control Programming published by the International Electrotechnical Commission. This library is written in Java and is available as open source from jfuzzylogic.sourceforge.net. The use of jFuzzyLogic is illustrated through the analysis of one case study."
1735791,14018,9080,Artificial development of connections in SHRUTI networks using a multi objective genetic algorithm,2013,SHRUTI is a model of how first-order logic can be represented and reasoned upon using a network of spiking neurons in an attempt to model the brain's ability to perform reasoning. This paper extends the biological plausibility of the SHRUTI model by presenting a genotype representation of connections in a SHRUTI network using indirect encoding and showing that networks represented in this way can be generated by an evolutionary process.
864947,14018,9704,Evolutionary design of polymorphic circuits with the improved evolutionary repair,2013,"In our previous work [1], the evolutionary repair technique has been introduced into the evolutionary design of the combinational logic circuits. In this paper, the evolutionary repair technique is improved, in which the number of the input vectors of the repair circuit is usually smaller than that of the corresponding incomplete circuit. The evolutionary algorithm with the improved evolutionary repair technique (i.e. erEDAII) is used to generate the polymorphic circuits. Experimental results demonstrate that some polymorphic circuits are evolved by erEDAII effectively. Especially, the polymorphic circuit with 8 inputs and 8 outputs could be evolved by the erEDAII."
1039281,14018,9704,Chaos PSO algorithm driven alternately by two different chaotic maps - An initial study,2013,"In this paper, a new approach for chaos driven PSO algorithm is proposed. Two different chaotic maps are alternately used as pseudorandom number generators and switched over during the run of chaos driven PSO algorithm. The motivation for this research came from the previous successful experiments with PSO algorithm driven by different chaotic maps. Promising results of this innovative approach are presented in the results section and briefly analyzed."
1702843,14018,21102,Reducing conservativeness in stability conditions of affine fuzzy systems using fuzzy Lyapunov function,2013,"This paper presents the relaxed stability condition for the affine fuzzy system using the fuzzy Lyapunov function which is the Lyapunov function with the membership function. Based on Lyapunov-stability theory, the relaxed stability condition for the affine fuzzy system is derived and represented to linear matrix inequalities(LMIs) with added null product term. Slack matrix variables are considered to analyze further relaxed stability condition. Finally, a simulation example is given to illustrate the merits of the proposed methods."
2473697,14018,21102,Generalized fuzzy imaginary ideals of rings,2011,"Our aim in this paper is to introduce and study the new type of fuzzy ideals of a ring called generalized fuzzy imaginary right (resp. left) ideals and the direct products of them. The equivalence relation of them is given, besides, the properties of their intersection, union and level sets are discussed. Finally, we also give the properties of their homomorphic preimage."
870538,14018,9080,Evolving automatic frame splitters,2011,"This paper extends the application of Genetic Programming into a new area, automatically splitting video frames based on the content. Compared with human written video splitting programs, GP generated splitters are more accurate. Moreover these video splitting programs have high tolerance to noises. They can still achieve reasonable performance even when the noisy videos are not easily recognizable by human eyes."
781232,14018,9080,Multiobjectivization for classifier parameter tuning,2013,We present a multiobjectivization approach to the parameter tuning of RBF networks and multilayer perceptrons. The approach works by adding two new objectives -- maximization of kappa statistic and minimization of root mean square error -- to the originally single-objective problem of minimizing the classification error of the model. We show the performance of the multiobjectivization approach on five datasets.
2228698,14018,21102,H ∞ disturbance attenuation of fuzzy large-scale systems,2011,"This paper is concerned with the disturbance attenuation problem of fuzzy large-scale systems which consist of N interconnected subsystems which are represented by Takagi-Sugeno fuzzy models. Using Lyapunov function and linear matrix inequalities (LMIs), a criterion is proposed to have a prescribed level of disturbance attenuation. A numerical example is given to illustrate the control design procedure and its effectiveness."
2075302,14018,21102,Dualities and isomorphisms between indistinguishabilities and related concepts,2011,"Given a T-indistinguishability operator, the corresponding set He of extensional sets and the operators ⊘E and ΨE that provide upper and lower approximations of a fuzzy subset by extensional ones can be defined. Reciprocally, given any of these, the initial indistinguishability operator can be retrieved. In order to understand in depth how this relation works, it will be shown that there is an isomorphism between the corresponding lattices and how this isomorphism acts over the operations and orderings of the lattices will be studied."
1011971,14018,9080,A service oriented evolutionary architecture: applications and results,2013,"This paper shows the stage of development of a Service Oriented Architecture for Evolutionary Algorithms and the first results obtained in two different areas. The abstract architecture is presented, with its assocciated implementation using a widely used technology. Results attained in experiments with parameter adaptation in distributed heterogeneous machines are presented and the usage of the architecture in Evolutionary Art is also applied."
1190108,14018,8806,Style Avatar: a visualization system for teaching C coding style,2011,"We present an automated visualization system, called Style Avatar, which checks the coding style of source codes and visualizes the coding style score using facial expression. Our system shows the facial expressions of an artificial character, say an avatar, according to the coding style score of the source code. According to the experimental results, the style-violation ratio of student programs when the students are taught using the Style Avatar was reduced above 30% than that without it."
728361,14018,9704,An external archive guided multiobjective evolutionary approach based on decomposition for continuous optimization,2014,"In this paper, we propose a decomposition based multiobjective evolutionary algorithm that extracts information from an external archive to guide the evolutionary search for continuous optimization problem. The proposed algorithm used a mechanism to identify the promising regions(subproblems) through learning information from the external archive to guide evolutionary search process. In order to demonstrate the performance of the algorithm, we conduct experiments to compare it with other decomposition based approaches. The results validate that our proposed algorithm is very competitive."
1599110,14018,9080,Analysis of the effects of enhanced selection concepts for genetic programming based structure identification using fine-grained population diversity estimation,2011,"In this paper we use a formalism for estimating the structural similarity of formulas for measuring the genetic diversity among GP populations. As we show in the results section of this paper, population diversity differs a lot in the test runs depending on the selection schemata used; especially the use of strict offspring selection has a significant effect on the progress of the population's diversity."
1283688,14018,9080,Interactive differential evolution for prostate ultrasound image thresholding,2012,Image thresholding is a method of image segmentation which applies to grayscale images. Thresholding is a challenging task and many techniques have been introduced to offer a global technique that can be applied to all kind of images. In this paper an interactive method is introduced and its result is compared to a well known method (Otsu). The experimental verifications are conducted on prostate ultrasound images and also non-medical images.
1383656,14018,9704,A fast wrapper feature subset selection method based on binary particle swarm optimization,2013,"Although many particle swarm optimization (PSO) based feature subset selection methods have been proposed, most of them seem to ignore the difference of feature subset selection problems and other optimization problems. We analyze the search process of a PSO based wrapper feature subset selection algorithm and find that characteristics of feature subset selection can be used to optimize this process. We compare wrapper and filter ways of evaluating features and define the domain knowledge of feature subset selection problems and we propose a fast wrapper feature subset selection algorithm based on PSO employed the domain knowledge of feature subset selection problems. Experimental results show that our method can work well, and the new algorithm can improve both the running time and the classification accuracy."
1708381,14018,21102,Universal fuzzy models and universal fuzzy controllers based on generalized T-S fuzzy models,2012,"This paper investigates the universal fuzzy models problem and universal fuzzy controllers problem for discrete-time general nonlinear systems based on a class of generalized T-S fuzzy models. The generalized T-S fuzzy models, which are shown to be universal function approximators, are also proved to be universal fuzzy models for non-affine nonlinear systems under some sufficient conditions. The results of static and dynamic universal fuzzy controllers for two classes of nonlinear systems are then given, respectively, and constructive procedures to obtain the universal fuzzy controllers are also provided."
1240989,14018,9080,Incremental genetic programming via genetic transposition,2011,"Genetic transposition is a process of moving sequences of DNA to different positions within the genome of a single cell. Inspired by the role of genetic transposons in biology, we introduce a genetic transposition inspired mechanism in genetic programming (GP). This mechanism, a simple variation from seeding in incremental evolution, provides a more effective approach to the evolution of systems with multiple features."
2255241,14018,21102,Fuzzy rough positive region based nearest neighbour classification,2012,This paper proposes a classifier that uses fuzzy rough set theory to improve the Fuzzy Nearest Neighbour (FNN) classifier. We show that previous attempts to use fuzzy rough set theory to improve the FNN algorithm have some shortcomings and we overcome them by using the fuzzy positive region to measure the quality of the nearest neighbours in the FNN classifier. A preliminary experimental evaluation shows that the new approach generally improves upon existing methods.
1222182,14018,8806,A comparative study and analysis on K-view based algorithms for image texture classification,2011,"Several K-View based algorithms have been made for developing image texture classification. These are K-View-Template algorithm (K-View-T), K-View-Datagram algorithm (K-View-D), Fast Weighted K-View-Voting algorithm (K-View-V), and K-View Using Rotation-Invariant Feature algorithm (K-View-R). In this paper, we review those K-View based algorithms and perform an empirical study for comparison by using classification accuracy, efficiency and stability. In addition, we propose a new K-View algorithm Using Gray Level Co-Occurrence Matrix algorithm (K-View-G) which also is compared with other K-View algorithms to demonstrate its effectiveness in image texture classification."
1560910,14018,21102,A novel cluster validity criterion for the bilinear models and its application to the T-S fuzzy bilinear model identification,2012,The objective of this paper is to use the T-S fuzzy bilinear model to describe nonlinear system with high accuracy and with as fewer IF-THEN rules as possible via input/output data which collect from original nonlinear system and fuzzy c-regression models (FCRM) clustering algorithm applied to bilinear models. A novel cluster validity criterion suitable for the bilinear models will be presented. The simulation example is provided to demonstrate the accuracy of the fuzzy bilinear model.
1179172,14018,21102,A critical survey on the use of Fuzzy Sets in Speech and Natural Language Processing,2012,"This paper shows how the use and applications of Fuzzy Sets (FS) in Speech and Natural Language Processing (SNLP) have seen a steady decline to a point where FS are virtually unknown or unappealing for most of the researchers currently working in the SNLP field, tries to find the reasons behind this decline, and proposes some guidelines on what could be done to reverse it and make FS assume a relevant role in SNLP."
1594833,14018,9080,On the impact of a small initial population size in the IPOP active CMA-ES with mirrored mutations on the noiseless BBOB testbed,2012,"Active Covariance Matrix Adaptation and Mirrored Mutations have been independently proposed as improved variants of the well-known optimization algorithm Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for numerical optimization. This paper investigates the impact of the algorithm's population size when both active covariance matrix adaptation and mirrored mutation are used in the CMA-ES. To this end, we compare the CMA-ES with standard population size λ, i.e., λ = 4 + ⌊ 3 log(D) ⌋ with a version with half this population size where D is the problem dimension."
1640887,14018,9080,Coastal current prediction using CMA evolution strategies,2011,We propose a data-driven evolutionary approach to the modeling of marine currents in the Bay of Monaco. The CMA (Covariance Matrix Adaptation) evolution strategy is used to optimize the parameters of a predictive model that may be used as a surrogate of expensive and time-consuming finite-element simulations. The models obtained are reasonably accurate and easy to interpret.
1520546,14018,9080,A comparison of GE and TAGE in dynamic environments,2011,"The lack of study of genetic programming in dynamic environments is recognised as a known issue in the field of genetic programming. This study compares the performance of two forms of genetic programming, grammatical evolution and a variation of grammatical evolution which uses tree-adjunct grammars, on a series of dynamic problems. Mean best fitness plots for the two representations are analysed and compared."
1591887,14018,9080,Maximizing the number of polychronous groups in spiking networks,2012,In this paper we investigate the effect of biasing the axonal connection delay values in the number of polychronous groups produced for a spiking neuron network model. We use an estimation of distribution algorithm (EDA) that learns tree models to search for optimal delay configurations. Our results indicate that the introduced approach can be used to considerably increase the number of such groups.
2426898,14018,21102,Medical care system evaluation based on DEA of prefectures in Japan,2011,"In order to reveal disparity and its cause in medical care among Japanese prefectures, this paper describes how DEA works as follows: (1) DEA/BCC model estimates the disparity between prefectures, (2) “RM-DEA using Cone-Ratio approach” estimates the cause of the disparity, and (3) Improved MF-DEA (Multi-Frontier based DEA) model estimates the disparity between eastern and western Japan. This paper also illustrates three numerical experiments."
122303,14018,23827,A Framework for Optimizing Malware Classification by Using Genetic Algorithm,2011,"Malware classification is a vital in combating the malware. Malware classification system is important and work together with malware identifica- tion to prepare the right and effective antidote for malware. Current techniques in malware classification do not give a good classification result when it deals with the new and unique types of malware. For this reason, we proposed the us- age of Genetic Algorithm to optimize the malware classification system as well as help in malware prediction. The new malware classification system is based on malware target and its operation behavior. The result from this study will create a new framework that designed to optimize the classification of malware. This new malware classification system also has an ability to train and learn by itself, so that it can predict the current and upcoming trend of malware attack."
1067038,14018,8806,Towards an enterprise ontology pattern language,2014,"Enterprise ontologies are useful for many purposes. Over the years, there have been a number of efforts aiming at building them. However, due to the complexity of the enterprise domain, enterprise ontologies tend to be complex and difficult to reuse. In this paper, we advocate in favor of organizing Core Enterprise Ontologies as Ontology Pattern Languages, since ontology patterns are more and more recognized as an approach that favors ontology reuse. Moreover, we present an initial version of the Enterprise Ontology Pattern Language (E-OPL), and show how it was used for building an enterprise ontology for a specific domain."
872328,14018,8806,A two-level procedure based on genetic algorithms to optimize an aeronautical composite structure,2012,"Optimal design of complex engineering systems, such as aircraft composite structures, can often be accomplished only by applying decomposition techniques. In this paper, the optimal design of a composite wing-box is addressed by using a two-level scheme based on the Genetic Algorithm meta-heuristic. The two-level optimization strategy was evaluated for variations both in the angular step of the composite layers and in the strength of material."
2395119,14018,9704,Hybrid DE algorithm with adaptive crossover operator for solving real-world numerical optimization problems,2011,"In this paper, the results for the CEC 2011 Competition on testing evolutionary algorithms on real world optimization problems using a hybrid differential evolution algorithm are presented. The proposal uses a local search routine to improve convergence and an adaptive crossover operator. According to the obtained results, this algorithm shows to be able to find competitive solutions with reported results."
1633699,14018,8806,Towards cosimulating network and electrical systems for performance evaluation in smart grid,2013,"In this paper, we construct a novel simulation framework OOCoSim which can simulate the distribution domain of Smart Grid. OOCoSim consists of OPNET, OpenDSS and CoSim module and it can perform dynamic and organic cosimulation. In order to present the effectiveness of OOCoSim, we firstly define a simulation model and then conduct a simulation study with the OOCoSim. The simulation results shows that OOCoSim can successfully simulate the integrated scenario of the power and network systems."
1461125,14018,20561,Trajectory Mining on Capability Space: Its Concept and Potential Application,2013,"To find the growth path for companies, this paper proposes the concept of trajectory mining. The central idea is to find the constraints and normal growth paths from the volumes of the trajectories on the capability space where capability space is spanned by multi-dimensional axes. The identified trajectory shows the reasonable direction for future growth. An application for software outsourcing clients demonstrates the proposed concept."
2420232,14018,21102,Signal transduction ability measurement of signaling pathways in intracellular communication via fuzzy method,2011,"Signal transduction plays an important role in biological organisms. Here, we proposed a method to access the information of the signal transduction ability of signal transduction pathway in the cells. Due to the nonlinear signal transduction pathway model, we have to solve the Hamilton-Jacobi inequalities (HJIs) to get the measurement. Instead of directly solving HJIs, fuzzy interpolation method was employed such that we can systematically measure signal transduction ability by solving the linear matrix inequalities (LMIs). The measurement will provide an insight to the characteristics of the signal transduction networks."
1430907,14018,9080,A test function with full controllability over overlapping: estimation of distribution algorithms,2011,"This work proposes a test function to study overlapping. The test function provides full controllability over overlapping. To achieve full controllability, the building block assigning problem is reduced to a bipartite matching problem which allow us to directly assign extent of overlapping to each gene. At the end, an experiment on overlapping shows that to four chosen crossover methods, the problem difficulty increases exponentially with the extent of overlapping."
741581,14018,9080,"Comparison of cooperative, multiobjective cooperative and classical evolutionary algorithms for global supply chain optimisation",2011,"This paper discusses global optimisation from a business perspective in the context of the supply chain operations. A two-silo supply chain was built for experimentation and three approaches were used for global optimisation: a classical evolutionary approach, a cooperative coevolutionary approach and a cooperative coevolutionary approach with non-dominated partner selection. The second approach produced higher quality solutions due to its use of communication between silos."
1980328,14018,9704,Optimizing qubit Hamiltonian parameter estimation algorithm using PSO,2012,"In this paper we develop qubit Hamiltonian single parameter estimation techniques using Bayesian approach. The algorithms considered are restricted to projective measurements in a fixed basis, and are derived under the assumption that the qubit measurement is much slower than the characteristic qubit evolution. The non-adaptive algorithm is optimized using particle swarm optimization and compared to previously developed locally-optimal scheme."
829178,14018,21102,A DEA model for two-stage systems with Fuzzy data,2013,Data Envelopment Analysis (DEA) aims at assessing the relative efficiency of a number of comparable operating units. Network DEA is used when the units consist of interrelated processes. The simplest network DEA structure is a two-stage serial system. In this paper a well-known Fuzzy DEA approach is extended to this network DEA context. The proposed approach is simpler than the existing approach with which it is compared using a dataset from the literature.
1555721,14018,21102,Computing with nouns and verbs,2012,"The paper makes a case for expanding the range of words that Computing With Words typically considers to, eventually, all the words in a natural language, thus accounting accurately for the inherent vagueness of natural language meaning and creating an overlap with computational semantics. The claim is illustrated with examples of a few English nouns and verbs rather than the usual adjectives and their close derivatives."
1471703,14018,9704,Royal road functions and the (1 + λ) evolutionary algorithm: Almost no speed-up from larger offspring populations,2013,"We analyze the runtime of the (1 + λ) evolutionary algorithm (EA) on the classic royal road test function class. For a royal road function defined on bit-strings of length n having block sized ≥ log n + (c + 1 + e) log d, we prove that the (1 + λ) EA with λ = Θ(n c ) finds the optimum in an expected number of O(2 d /d c  · n/d log n/d) generations. Together with our lower bound of Ω(2 d /d c ), this shows that for royal road functions even very large offspring populations do not reduce the runtime significantly."
1061089,14018,9704,Elitism Levels Traverse Mechanism for the derivation of upper bounds on unimodal functions,2012,In this article we present an Elitism Levels Traverse Mechanism that we designed to find bounds on population-based Evolutionary Algorithms solving unimodal functions. We prove its efficiency theoretically and test it on OneMax function deriving bounds cμn log n-O(μn). This analysis can be generalized to any similar algorithm using variants of elitist selection and genetic operators that flip or swap only 1 bit in each string.
1883950,14018,21102,Natural topology via fuzzy metric,2011,"In this paper, we propose a natural way to define topologies by using fuzzy metrics; here the value of the distance between two points is a non-negative fuzzy number. With such topology, we also define notions like continuity, convergent sequence, Cauchy sequences and complete spaces. As an application, we provide a fixed point theorem quite similar to the classical Banach's theorem."
1742678,14018,9080,A new data pre-processing approach for the dendritic cellalgorithm based on fuzzy rough set theory,2013,"The aim of this paper is to develop a new data pre-processing method for the dendritic cell algorithm (DCA) based on Fuzzy Rough Set Theory (FRST). In this new fuzzy-rough model, the data pre-processing phase is based on the fuzzy positive region and the fuzzy dependency degree concepts. Results show that applying FRST is more convenient for the DCA data pre-processing phase yielding much better performance in terms of accuracy."
2110400,14018,9080,Partially connected topologies for particle swarm,2013,"The effects of dynamic and partially connected 2-dimensional topologies on the particle swarm are studied. The particles are positioned on 2-dimensional grids of nodes, where they move according to a simple rule. The von Neumann neighborhood is used to decide which particles influence each individual. Structures with growing size are tested on a classical benchmark. The partially connected grids with von Neumann neighborhood structure perform more consistently than other strategies."
1562249,14018,9704,Free Lunch for Optimisation under the Universal Distribution,2014,"Function optimisation is a major challenge in computer science. The No Free Lunch theorems state that if all functions with the same histogram are assumed to be equally probable then no algorithm outperforms any other in expectation. We argue against the uniform assumption and suggest a universal prior exists for which there is a free lunch, but where no particular class of functions is favoured over another. We also prove upper and lower bounds on the size of the free lunch."
1923639,14018,8806,The bacterial strains characterization problem,2011,"The accurate characterization of collections of bacterial strains is a major scientific challenge, since bacteria are indeed responsible of significant plant diseases and thus subjected to official control procedures (e.g., in Europe, Directive 2000/29/EC). The development of diagnostic tests is therefore an important issue in order to routinely identify strains of these species."
948656,14018,8806,A delivery method considering communication loads for sensor data stream with different collection cycles,2013,"Due to the prevalence of sensors such as security cameras or environmental sensors, sensor data stream delivery which means delivering the sensor data every cyclic collection attracts great attention. For sensor data stream delivery, various methods to distribute communication loads in the case of delivering the same sensor data streams to multiple clients have been studied. Although these methods assume that the sensor data streams have the same data collection cycles, data collection cycles sometimes differ. Hence, we propose a method to distribute communication loads in the case of delivering the sensor data stream that have different data collection cycles. The proposed method distributes communication loads by re-delivering the sensor data that have the same collection time but are included in different sensor data streams."
1862921,14018,9704,Investigating the impact of alternative evolutionary selection strategies on multi-method global optimization,2011,Algorithm selection is an important consideration in multi-method global optimization. This paper investigates the use of various algorithm selection strategies derived from well known evolutionary selection mechanisms. Selection strategy performance is evaluated on a diverse set of floating point benchmark problems and meaningful conclusions are drawn with regard to the impact of selective pressure on algorithm selection in a multi-method environment.
2442521,14018,21102,An interval-based approach to fuzzy regression for fuzzy input-output data,2011,"A novel approach is introduced to construct a fuzzy regression model when the data available of independent and dependent variables are fuzzy numbers. The approach, consisting on the least-squares method, uses the α-level sets of fuzzy observations to estimate the crisp parameters of the model. A competitive study shows the performance and efficiency of the proposed approach with respect to some well-known methods."
2498081,14018,21089,langid.py: An Off-the-shelf Language Identification Tool,2012,"We present langid.py, an off-the-shelf language identification tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 long-document datasets, and 2 datasets from the microblog domain. We find that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data."
1509215,14018,9704,Optimization of a radio frequency energy harvesting device,2012,"This paper proposes the optimized design of a wireless energy harvesting device (EHD), by means of a recently developed evolutionary optimization method. This procedure is developed in order to increase the transfer efficiency and the robustness of the coupling, with the aim of minimizing the average power loss, and to increase the lifetime of a wireless sensor network by scavenging RF energy available in the environment."
2422355,14018,21102,An interactive satisficing method for multiobjective random fuzzy programming problems through the possibility-based probability model,2011,This paper considers multiobjective linear programming problems where each coefficient of the objective functions is expressed by a random fuzzy variable. A new decision making model is proposed in order to maximize both of possibility and probability with respect to the objective function values. An interactive algorithm is constructed to obtain a satisficing solution for a decision maker from among a set of Pareto optimal solutions.
1557926,14018,9080,Acceleration experiment of genetic computations for sudoku solution on multi-core processors,2011,"We focus on parallel-processing effect for Sudoku-solving and we show that diversifying initial values can reduce the Sudoku solution time. In an experiment using the commercially available Intel Corei7 multi-core processor, we show that a correct solution rate of 100% can be achieved with an average execution time of several tens of seconds even for super-difficult problems."
1676920,14018,9475,Data projection method for sensor Fault Detection and Isolation on nonlinear systems based on Takagi Sugeno model,2011,"Robust sensor Fault Detection and Isolation for nonlinear system using partial knowledge of the system is proposed in the paper. First, a suitable Takagi-Sugeno model for uncertain nonlinear system is determined using sector nonlinear method. Then the matrix input-output relation is given and the robust FDI residual is computed by projecting the relation onto the Kernel of the predetermined matrix."
1485584,14018,9080,Dependence trees with copula selection for continuous estimation of distribution algorithms,2011,"In this paper, a new Estimation of Distribution Algorithm (EDA) is presented. The proposed algorithm employs a dependency tree as a graphical model and bivariate copula functions for modeling relationships between pairwise variables. By selecting copula functions it is possible to build a very flexible joint distribution as a probabilistic model. The experimental results show that the proposed algorithm has a better performance than EDAs based on Gaussian assumptions."
1379264,14018,21102,Learning convergence analysis for Takagi-Sugeno Fuzzy Neural Networks,2012,"In this paper, we provide a mathematical formulation of the Takagi-Sugeno Fuzzy Neural Network (TS-FNN) to study convergence properties. Note that we describe both information retrieval and learning rules by algebraic equations in matrix form. We then investigate the convergence characteristics and learning behaviors for the TS-FNN by use of these algebraic equations and the eigenvalues of derived matrices. Numerical examples are carried out to further verify the analysis."
942760,14018,9080,Evolving players that use selective game-tree search with genetic programming,2012,"We present the application of genetic programming (GP) to evolving game-tree search in board games. Our work expands previous results in evolving board-state evaluation functions for multiple board games, now evolving a search-guiding evaluation function alongside it. Our system implements strongly typed GP trees, explicitly defined introns, and a selective directional crossover method."
804944,14018,8806,Solution neighbourhoods for constraint-directed local search,2012,"We propose  solution neighbourhoods , which contain  only solutions  to a chosen constraint, as the solutions to a constraint capture the structure of the constraint. We save the time needed for neighbourhood evaluation of that constraint by using a solution neighbourhood. This may be useful especially for constraints for which there exists no known constant-time algorithm for neighbour evaluation. We design a solution neighbourhood for the very useful  automaton  constraint, and demonstrate the practicality of our approach on a library of nurse scheduling instances. We show the feasibility of designing solution neighbourhoods for other constraints."
1546364,14018,9080,Mutation strength control by meta-ES on the sharp ridge,2012,"This paper investigates mutation strength control using Meta-ES on the sharp ridge. The asymptotical analysis presented allows for the prediction of the dynamics in ridge as well as in radial direction. Being based on this analysis the problem of the choice of population size  λ  and isolation parameter  γ  will be tackled. Remarkably, the qualitative convergence behavior is not determined by  γ  alone, but rather by the number of function evaluations  λ γ  devoted to the inner ES."
2591538,14018,9677,Cross-domain Feature Selection for Language Identification,2011,"We show that transductive (cross-domain) learning is an important consideration in building a general-purpose language identification system, and develop a feature selection method that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identification. We provide an implementation of the method and show that our system is faster than popular standalone language identification systems, while maintaining competitive accuracy."
79353,14018,22113,Finite-valued Lukasiewicz modal logic Is PSPACE-complete,2011,"It is well-known that satisfiability (and hence validity) in the minimal classical modal logic is a PSPACE-complete problem. In this paper we consider the satisfiability and validity problems (here they are not dual, although mutually reducible) for the minimal modal logic over a finite Lukasiewicz chain, and show that they also are PSPACE-complete. This result is also true when adding either the Delta operator or truth constants in the language, i.e. in all these cases it is PSPACE-complete."
852729,14018,9080,Analysis of epistasis correlation on NK landscapes with nearest-neighbor interactions,2011,Epistasis correlation is a measure that estimates the strength of interactions between problem variables. This paper presents an empirical study of epistasis correlation on a large number of random problem instances of NK landscapes with nearest neighbor interactions. The results are analyzed with respect to the performance of hybrid variants of two evolutionary algorithms: (1) the genetic algorithm with uniform crossover and (2) the hierarchical Bayesian optimization algorithm.
1641885,14018,9080,Particles prefer walking along the axes: experimental insights into the behavior of a particle swarm,2013,"We study the frequently observed phenomenon of stagnation in the context of particle swarm optimization (PSO). We show that in certain situations the particle swarm is likely to move almost parallel to one of the axes, which may cause stagnation. We provide an experimentally supported explanation in terms of a potential of the swarm and are therefore able to adapt the PSO algorithm slightly such that this weakness can be avoided."
1925352,14018,21102,A heuristic search and its roughness,2011,"This paper introduces an abstract model of heuristic search for handling uncertainty. In the search, heuristic information and evaluation are precisely defined mathematically. As special cases, the model includes a previous probabilistic game-tree search and a pattern search. This paper also considers the relationship between the model and the rough set approach. Each rough set formulation can be reformulated as a special case of the abstract model."
998245,14018,9080,EpochX: genetic programming in java with statistics and event monitoring,2012,"EpochX is a Genetic Programming (GP) framework written in Java. It allows the creation of tree-based and grammar-based GP systems. It has been created to reflect typical ways in which Java programmers work, for example, borrowing from the Java event model and taking inspiration from the Java collections framework. This paper presents EpochX in general, and gives particular attention to the event model and the statistics analysis framework."
1387635,14018,21102,Direct centroid computation of fuzzy numbers,2012,"In this paper, we give two direct centroid computation methods for fuzzy numbers: One is to use the membership function and the other is to use the alpha-cuts. Compared with the current centroid computation methods, the new methods are simple both in expression and computation. Weighted samples computation method are also proposed to improve the computational accuracy with numerical integration technique. Three examples illustrate the application of proposed methods."
1601986,14018,9080,Developmental scalable hierarchies for multi-agent swarms,2011,"Hierarchical control structures for multi-agent systems represent a promising middle ground between fully-distributed systems and centralized control. In this paper we present a developmental approach for evolving hierarchical control structures for large (100-800 member), multi-agent swarms. The results show that this approach can successfully generate control hierarchies that improve the performance of fully distributed swarms and that scale well."
831918,14018,9475,Application of Takagi-Sugeno observers for state estimation in a quadrotor,2011,"In this paper, the validity of Takagi-Sugeno observers to estimate the angular positions and speeds in the experimental platform of a quadrotor will be assessed. Takagi-Sugeno observers are compared to observers based on the linearized model designed with the same optimization criteria and design parameters. Experimental results confirm that Takagi-Sugeno models and observers behave similarly to linear ones around the linearization point, and have a better performance over a larger operating range."
766488,14018,9080,Clans and cooperation in the iterated prisoner's dilemma,2012,"In evolutionary algorithms that evolve populations of strategies for the Iterated Prisoner's Dilemma, higher levels of cooperation evolve when the strategies engage in longer contests. When IPD-playing organisms are segregated into clans, with different strategies for contests with members of the same and other clans, populations evolve higher levels of cooperation for contests within clans. Cooperation between members of different clans increases with the length of their contests, but decreases when there are more clans. When organisms use the same strategy for contests with friends and with strangers, the difference between cooperation within and between clans is smaller, but persists."
783950,14018,8806,FPGA based parallel transitive closure algorithm,2011,"In this paper, we propose a FPGA-based parallel algorithm to compute the transitive closure of the relation matrix on a fixed-size PE array. Experimental results showed that speedup increases with the problem size. The speedup against a single PE is between 11.3 and 195.9. Compared to a general CPU solution, this algorithm achieves acceleration rate of 3.7 and 376 under the worst and best situations, respectively."
2499495,14018,9080,Testing of the multi-objective alliance algorithm on benchmark functions,2013,A new version of the Multi-objective Alliance Algorithm (MOAA) is described. The MOAA's performance is compared with that of NSGA-II using the epsilon and hypervolume indicators to evaluate the results. The benchmark functions chosen for the comparison are from the ZDT and DTLZ families and the main classical multi-objective (MO) problems. The results show that the new MOAA version is able to outperform NSGA-II on almost all the problems.
1530953,14018,8806,Cross-platform mobile development: a study on apps with animations,2014,"The paper presents a comparison between different frameworks for cross-platform mobile development, MoSync, Titanium, jQuery Mobile and Phonegap, with particular attention to development of applications with animations. We define a set of criteria for the evaluation and we develop the same game as case study app, with the aim to provide an unbias judgement.   Our analysis recommends Titanium as the best framework to develop mobile applications with animations."
817365,14018,8806,"An assessment of data matrix barcode recognition under scaling, rotation and cylindrical warping",2011,"Data Matrix is a 2D barcode with high information density and wide applicability. When printed in documents or used for labeling products, the barcode may be subject to deformations. This paper looks at the recognition of Data Matrix barcodes is under scaling, rotation and cylindrical warping. The results suggest that the readability of Data Matrix barcodes is immune to rotation transformations and responds well to scaling, while it is more seriously impaired by cylindrical warping."
2182549,14018,8806,Trend cluster based interpolation everywhere in a sensor network,2012,"The information acquisition in a pervasive sensor network is often affected by faults due to power outage at nodes, wrong time synchronizations, interference, network transmission failures, sensor hardware issues or excessive energy consumption for communications. These issues impose a trade-off between the precision of the measurements and the costs of communication and processing, which are directly proportional to the number of sensors."
2305796,14018,9080,Universal information distance for genetic programming,2014,"This paper presents a genotype-level distance metric for Genetic Programming (GP) based on the  symmetric difference  concept: first, the  information  contained in individuals is expressed as a set of symbols (the content of each node, its position inside the tree, and recurring parent-child structures); then, the difference between two individuals is computed considering the number of elements belonging to one, but not both, of their symbol sets."
972001,14018,9704,Generalized opposition-based artificial bee colony algorithm,2012,"The Artificial Bee Colony (ABC) algorithm is a relatively new algorithm for function optimization. The algorithm is inspired by the foraging behavior of honey bees. In this work, the performance of ABC is enhanced by introducing the concept of generalized opposition-based learning. This concept is introduced through the initialization step and through generation jumping. The performance of the proposed generalized opposition-based ABC (GOABC) is compared to the performance of ABC and opposition-based ABC (OABC) using the CEC05 benchmarks library."
874791,14018,9078,Intestinal event segmentation for endoluminal video analysis,2014,"In this paper, we tackle the problem of unsupervised segmentation of intestinal events using various motility descriptors extracted from the endoluminal video. The segments of constant intestinal activity are detected with a robust statistical test that is based on Hoeffding's inequality. The qualitative analysis of the results shows that the segments adjust well to the motility information and thus, this segmentation can constitute basic units for higher-level motility description."
1036416,14018,8806,On the completeness of test suites,2014,"Test suite generation for Finite State Machines (FSMs) has been largely investigated. Here, we describe necessary and sufficient conditions for  m -completeness of test suites when the specification and implementation are modeled as FSMs. Many earlier works imposed several conditions upon the specification or on the implementation models. We impose weak  a priori  restrictions on the models. In particular, we do not require reduced models nor complete specifications."
1458191,14018,21102,A hybrid ARIMA-DENFIS method for wind speed forecasting,2013,"This paper proposes a hybrid autoregressive integrated moving average - dynamic evolving neural-fuzzy inference system (ARIMA-DENFIS) model for wind speed forecasting. The theory of ARIMA, DENFIS and the hybrid of the two are discussed. The proposed model is evaluated with NDBC wind speed data and the results show that the proposed hybrid ARIMA-DENFIS model outperforms DENFIS model in most of the cases. It has comparable or better error measures than ARIMA model. In addition, when the forecasting horizon increases, the advantage of the proposed ARIMA-DENFIS model becomes more significant."
1698634,14018,8806,An architecture proposal for the prosumerized enterprise,2011,"The prosumer movement has made a great impact in different areas of the consumer goods industry and provides new market opportunities for both individuals and companies. The following article investigates the implications of the prosumer movement taking the perspective of companies. Examples from different industries are provided which incorporate different aspects of the prosumer movement. Based on this, requirements are derived for an IT-enabled, prosumerized enterprise and a draft of such an enterprise architecture is outlined."
1661482,14018,8806,Modeling fundamentals for smart grid enabled ecodistricts,2013,"Although much work has been done to study the different aspects of the smart grid using models and simulations, but little is done to combine those efforts to study the smart grid and ecodistrict in the same context. In this paper we discuss the role of smart grid in an ecodistrict context and the mathematical tools such as complex networks theory and game theory for modeling smart grids and ecodistricts. We also discuss the agent based modeling approach for smart grid and ecodistricts, with the help of multiagent modeling guideline ASPECS and an example of ecodistrict model."
2504620,14018,9704,Character evolution approach to generative storytelling,2011,"We propose a generative drama approach that integrates human creativity by using an agent-based system where the characters are developed using interactive evolution. The author can then create a scenario from the agents' interaction which provides a foundation for the desired story, which is given its final artistic form through a mapping to a visual representation. This approach can provide the storytelling authors with efficient tools to inspire them in their creation process."
1206835,14018,9080,Equitable solutions in QoS-aware service optimization,2012,"Web services QoS optimization problem using the concept of Lorenz dominance is addressed. Lorenz solutions are equitable and well balanced. Such an approach could simplify the Decision Maker's choice. Evolutionary detection of Lorenz solution seem to be an appealing one. Some state-of-the art MOEAs are slightly modified for addressing the QoS optimization problem. Lorenz solutions drastically reduce the number of solutions in the Pareto set, and thus the decision costs."
2393518,14018,9704,The Greening of IT: How Discourse Informs IT Sustainability Innovation,2011,"We review three sociological theories of technology innovation and diffusion, which highlight the importance of discourses on the social shaping and trajectory of IT innovations. We consider how key concepts of each theory are applicable to understanding Green IT as an area of innovation. We highlight how these theories might inform action in this innovation movement to help guide the collective actions of participants in Green IT and areas of research on Green IT discourses that may prove beneficial."
684517,14018,21102,A measure of contradiction based on the notion of N-weak-contradiction,2013,"In this work we elaborate on the notion of contradiction between fuzzy sets introduced by Trillas et al in a fuzzy logic context. Our approach is parametric in that the operator used to define contradiction is rather a variable than a constant introduced prior to the analysis of contradiction. We give several motivations to consider weaker operators than the usual involutive negations, and obtain some preliminary results which validate this proposal."
821677,14018,9704,Modeling and replicating higher-order dependencies in genetic algorithms,2012,"Problems exhibiting a building-block structure but lacking pairwise dependencies are hard for linkage learning mechanisms and consequently can be very hard to optimize. The current methods capable of building-block wise crossover begin the search for their models by exploiting dependencies between pairs of variables, thus fail to capture higher-order interactions that can not be easily decomposed into lower ones."
1535830,14018,9080,Hypervolume-based local search in multi-objective evolutionary optimization,2014,"This paper describes a surrogate based multi-objective evolutionary algorithm with hyper-volume contribution-based local search. The algorithm switches between an NSGA-II phase and a local search phase. In the local search phase, a model for each of the objectives is trained and CMA-ES is used to optimize the hyper-volume contribution of each individual with respect to its two neighbors on the non-dominated front. The performance of the algorithm is evaluated using the well known ZDT and WFG benchmark suites."
2301357,14018,9080,Exact computation of the expectation curves of the bit-flip mutation using landscapes theory,2011,"Bit-flip mutation is a common operation when a genetic algorithm is applied to solve a problem with binary representation. We use in this paper some results of landscapes theory and Krawtchouk polynomials to exactly compute the expected value of the fitness of a mutated solution. We prove that this expectation is a polynomial in p, the probability of flipping a single bit. We analyze these polynomials and propose some applications of the obtained theoretical results."
832551,14018,21102,Weak T -transitivity and weak closures of interval-valued fuzzy relations,2012,"In this paper a new weak transitive property for interval-valued fuzzy relations (IVFRs) is introduced. Weak T -transitivity is equivalent to T -transitivity when the IVFR is a fuzzy relation and T = (T, T) for a triangular norm T. Otherwise it is a weaker property than T -transitive one relaxing the need that all intervals must be comparable and must satisfy all the transitive inequalities by just the need of not having comparable non T -transitive cycles. This paper also defines a weak concept of closure, and it is proved that it exists is just one T -transitive and weak T -transitive closure, it does not exists a T -transitive weak closure, but there are many weak T -transitive weak closures of an IVFRs."
1974700,14018,21102,Fuzzy control of a bi-directional inverter with nonlinear inductance for DC microgrids,2011,This paper proposes the design of fuzzy control of a bidirectional converter with variable inductance for dc microgrids. The nonlinear inductance model is developed and taken into consideration in the controller design of the bi-directional inverter to promote the performance. A novel fuzzy control strategy is proposed to increase the magnetizing voltage such that the current distortion is reduced. Simulated results demonstrated the effectiveness of the design.
2299165,14018,9704,Chemical reaction optimization for the set covering problem,2014,"The set covering problem (SCP) is one of the representative combinatorial optimization problems, having many practical applications. This paper investigates the development of an algorithm to solve SCP by employing chemical reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a wide range of benchmark instances of SCP. The simulation results indicate that this algorithm gives outstanding performance compared with other heuristics and metaheuristics in solving SCP."
1549150,14018,9704,Biased random-key genetic algorithm for nonlinearly-constrained global optimization,2013,"Global optimization seeks a minimum or maximum of a multimodal function over a discrete or continuous domain. In this paper, we propose a biased random key genetic algorithm for finding approximate solutions for bound-constrained continuous global optimization problems subject to nonlinear constraints. Experimental results illustrate its effectiveness on some functions from CEC2006 benchmark (Liang et al. [2006])."
899039,14018,9080,Simultaneous tuning of metaheuristic parameters for various computing budgets,2011,"Many heuristics require a number of parameters to be tuned. One way to do this is meta-optimization: a higher level heuristic searches for the best parameter settings of a lower level heuristic which solves the optimization problem. However, the optimal parameter settings depend on the computational budget or running time available to the lower level heuristic. In this paper, we present a new meta-optimization approach to identify the best parameter settings simultaneously for various computational budgets."
870003,14018,21102,Andness and orness as a mean of overall importance,2012,"This paper investigates basic semantic aspects of evaluation logic. We propose a verbalized approach to the design and use of the Generalized Conjunction/disjunction (GCD) aggregators. The main goals of verbalized approach are to help in specifying semantic components of GCD, and to facilitate the use of soft computing evaluation logic and corresponding evaluation methods (such as LSP), making those more accessible to general nonprofessional decision makers."
1745397,14018,8806,Student research abstract: demand behavior analysis and coordination scheduling for energy saving in smart building,2014,"A large proportion of energy is consumed in buildings, which could be saved with optimized operation and management without changing building structure and hardware configuration. A new framework is proposed to reduce whole energy consumption of smart buildings based on load behavior analysis and coordination scheduling, consisting of two parts: Load Behavior Analysis System (LBAS) and Distributed Scheduling Systems (DSS)."
1557086,14018,9080,A preliminary study of crowding with biased crossover,2013,"This paper proposes a novel crowding method, which is called Crowding with Biased Crossover (CBX). The Biased crossover operator begins with two parents. Then two offspring individuals are created, each offspring taking more characteristics from one of the two parents. This is an easy method to perform replacement between parents and offspring individuals. Experimental results showed that CBX is very effective in finding both single global solutions and multiple solutions (niching)."
1708241,14018,9704,Towards a more complete classification system for dynamically changing environments,2012,"In spite of substantial research applying evolutionary algorithms and swarm based algorithms to solve dynamic problems, the classification of dynamic environments is missing universal standards. This paper examines the various methods used so far to characterise dynamic optimisation problems and proposes an inclusive classification system. Additionally, a way to generate environments of each type using the moving peak benchmark is described."
907603,14018,9080,Dynamic selection of migration flows in island model differential evolution,2013,"In this paper, a new approach to the topology configuration problem in the Island Model (IM) is proposed. The mechanism proposed works with a pool of candidates for migration and the choice of immigrants is performed using the usual selection techniques of evolutionary algorithms. Computational tests on IM versions of the Differential Evolution show positive effects of the proposed approach in terms of the number of function evaluations required for convergence."
1740224,14018,9080,A robust real-coded genetic algorithm using an ensemble of crossover operators,2013,"Although a lot of crossover operators have been developed for genetic algorithms (GAs), there is not much research on combining different crossover operators to form robust real-coded GAs. In this work, we propose an ensemble of crossover operators which is realized by two different parallel populations. The effectiveness of the proposed method is evaluated for traditional 6 benchmark functions. Results demonstrated that the proposed method has good generalization performance."
1309056,14018,9080,Simultaneous gene selection and cancer classification using a hybrid group search optimizer,2013,"Constructing classifier models for gene expression datasets using informative features enhances prediction performance of concerned models. Here, we propose a hybrid Group Search based feature selection (GSO-FS) algorithm which can select relevant gene subsets that can optimally predict cancerous tissue samples. Our experimental results show that the GSO-FS algorithm in combination with SVM classifier performs quite well."
700828,14018,21102,Relaxed stabilization of T-S fuzzy systems with time-delay,2013,"In this paper, the relaxed stabilization problem for Takagi and Sugeno (T-S) fuzzy systems with time-delay is investigated. Based on the concept of homogeneous polynomials and Polya's theorem, a stabilization condition for T-S fuzzy systems with time-delay is proposed in terms of a linear matrix inequality to guarantee the asymptotic stabilization of T-S fuzzy systems with time-delay. In addition, by using the Kronecker product, a more relaxed stabilization condition is proposed. Lastly, an example is given to demonstrate that the proposed stabilization condition can provide the maximum allowable delay time than some existing results."
1382204,14018,9704,A simulation optimization approach for design space exploration of soft real-time embedded systems,2013,"In this work, the problem of design space exploration of soft real-time embedded systems is formulated as a stochastic simulation optimization problem. A novel multiobjective genetic algorithm is proposed to address this problem. In the proposed algorithm, design metrics such as price and size are optimized while deadline violations are minimized. Experimental results show the advantages of our approach over previous work that consider no deadline violation is tolerable."
2046586,14018,21102,On a strengthening connective for flexible database querying,2011,"In most of query languages, conjunctive and disjunctive combinations of conditions remain the usual way for aggregation. Fuzzy query languages also offer trade-off operators, such as means in order to compensate between elementary conditions. In this paper, we investigate a new type of condition basically founded on the interaction between two predicates, thus enriching the panoply of tools the user is provided with and enhancing the power of query languages."
1601060,14018,9704,Search-based evolutionary operators for extensionally-defined search spaces: Applications to image search,2012,"This paper explores the idea of applying evolutionary algorithms to those search spaces that are defined extensionally, i.e. by listing every item in the space. When these spaces are with a function that returns similar elements given a key element, analogies of mutation and crossover can be defined. This idea is discussed in general, and specific examples are given where the search is for images, in particular where image search is carried out using an interactive genetic algorithm."
1390162,14018,21102,Multi-objective optimization based on fuzzy if-then rules,2013,"In this paper, a fuzzy multi-objective mathematical programming problem is considered where functional relationship between the decision variables and the objective functions is not completely known to us. It is assumed that the information source from where some knowledge may be obtained about objective functions consists of a block of fuzzy if-then rules. The focus in this work is to solve the multi-objective optimization problem for the above situations. A numerical example is also presented to illustrate the method."
1186828,14018,21102,The law x ≤ I(y; x) and the three main classes of fuzzy implications,2012,"Properties valid on classical theory (Boolean laws) have been extended to fuzzy set theory. Such generalizations of Boolean laws (Boolean-like laws) are not always valid in any standard fuzzy set theory and this fact induced a wide investigation. In this paper we show the conditions that the Boolean-like law x ≤ I(y; x) holds in fuzzy logic. We focus the investigation on three main classes of fuzzy implications, namely: (S,N)-, R- and QL-implications. Further, we show that the operator I satisfies this Boolean-like law if, and only if, Φ-conjugate of I also satisfies it."
1685717,14018,8806,Mirrored orthogonal sampling with pairwise selection in evolution strategies,2014,"In this paper, an improvement of the mirrored sampling method, called  mirrored orthogonal sampling , is propsed. The convergence rates on the sphere function are estimated. It is also applied to Covariance Matrix Adaptation Evolution Strategy (CMA-ES). The resulting algorithm, termed (μ/μ w , λ o  m )-CMA-ES, is benchmarked on the Black-box optimization benchmark (BBOB). The newly proposed technique is found to outperform both the standard (μ/μ w , λ)-CMA-ES and its mirrored variant."
1842938,14018,9704,Execution trace caching for Linear Genetic Programming,2011,"In this paper we propose a new caching algorithm for Linear Genetic Programming (LGP) based on exploiting inter-generation program relationships. For each program we cache a partial summary of program execution, and use this summary to expedite the execution of all progeny. We study the theory behind our new caching algorithm and derive equations for optimizing algorithm performance. Through both theoretical and empirical results we demonstrate that our caching algorithm can decrease LGP execution time by up to 50%."
1574128,14018,9704,Combining BPM and EA in Complex IT Projects: (A Business Architecture Discipline),2011,"The paper presents the up-to-date subject of why BPM and Enterprise Architecture should be an integrated part of any complex IT project. While nearly all Enterprise Architecture frameworks and methods include approaches of how to handle or work with process, none of the existing EA approaches actually incorporate Business Process Management in their EA disciplines. The benefits as well as different ways of how to combine these disciplines will be elaborated and illustrated in this paper."
1381730,14018,9704,Preliminary results for GAMMI: Genetic algorithms for motif-module inference,2012,"We have developed GAMMI, an approach to the inference of cis-regulatory modules that employs an evolutionary search to identify modules conserved across a set of DNA sequences from different species. This paper describes the motivation and system design for GAMMI, and presents the results of initial tests of the system using artificial sequences with implanted artificial modules. Based on these preliminary tests, GAMMI appears to be a promising approach to the module inference problem."
1423236,14018,8806,A novel double linear-cubic convolution interpolation for digital image scaling,2014,"As the better high quality image is required for digital image scaling, longer processing time is required so the technology that can make the high quality image quickly is needed. We propose the double linear--cubic convolution interpolation creating the high quality image with low complexity. When compared to peak signal-to-noise ratio(PSNR) and computation time, the proposed interpolation provided better PSNR and low complexity than bicubic convolution interpolation."
1499897,14018,8806,Communication support at the OS level to enhance design space exploration in multiprocessed embedded systems,2013,"Currently, Embedded Systems (ESs) based on Multiprocessed System-on-Chip (MPSoCs) count on resources previously available only on general purpose machines, leading to an increased design complexity, especially when dealing with communication-dependent applications. In this context, we present a communication protocol developed in a Real Time OS (RTOS) to provide a transparent communication interface for both bus- and NoC-based MPSoCs' applications."
1451193,14018,21102,Retrieval of similar time series with similarity degree of linguistic expressions for global trend and local features,2012,We have various kinds of time series such as stock prices. We understand them via their linguistic expressions in a natural language rather than conventional stochastic models. We have proposed a method to extract their linguistic expressions for global trend and local features in a natural language. In this paper we propose a similarity degree of linguistic expressions for retrieving similar time series. And we tune the terms of the temporal axis to have better linguistic expressions. Then we illustrate retrieval of similar time series by our similarity degree.
1048417,14018,9080,A classification of dynamic multi-objective optimization problems,2011,"A classification of dynamic multi-objective optimization problems is proposed in this article. As compared to previous studies, we focus not on the changes or the effects that are induced in the Pareto optimal front or set but on the components that lead to the observed dynamic behaviour. Four main classes are identified, including parameter and function time-dependent evolution as well as state-dependent parameter and function transforms or environment changes."
2244829,14018,9080,An evaluation of sequential model-based optimization for expensive blackbox functions,2013,"We benchmark a sequential model-based optimization procedure, SMAC-BBOB, on the BBOB set of blackbox functions. We demonstrate that with a small budget of 10x D  evaluations of  D -dimensional functions, SMAC-BBOB in most cases outperforms the state-of-the-art blackbox optimizer CMA-ES. However, CMA-ES benefits more from growing the budget to 100x D , and for larger number of function evaluations SMAC-BBOB also requires increasingly large computational resources for building and using its models."
1721687,14018,21102,A structure learning method for concise fuzzy systems,2012,This paper presents a structure learning method for fuzzy systems following our previous work on a Structure Evolving Learning Method for Fuzzy Systems (SELM) and an Evolving Construction Scheme for Fuzzy Systems (ECSFS). Here we extend our previous work to a structure learning method for fuzzy systems which results in more concise systems. We analyse and compare the proposed concise structure learning strategies in terms of three aspects: (1) how to detect the splitting points for the structure learning process; (2) how to set a starting point for the fuzzy system; (3) how the proposed method is applied to Mamdani and TS fuzzy systems.
1687475,14018,8806,Optimization of out of memory killer for embedded Linux environments,2011,"When a swapless Linux system runs out of memory, the system reclaims memory by invoking Out of Memory (OOM) Killer. The OOM killer terminates arbitrary processes. The running time of selecting victim processes is proportional to the number of processes in the system. In this paper, we propose a novel selection scheme which runs in O(1) time. The experimental results showed that the proposed selection scheme outperformed the existing one in terms of the running time."
1432471,14018,8806,A multiple feature vector framework for forest species recognition,2013,"In this work we focus on investigating the use of multiple feature vectors for forest species recognition. As consequence, we propose a framework to deal with the extraction of multiple feature vectors based on two approaches: image segmentation and multiple feature sets. Experiments conducted on a 112 species database containing microscopic images of wood demonstrate that with the proposed framework we can increase the recognition rates of the system from about 55.7% (with a single feature vector) to about 93.2%."
1511639,14018,9080,Caching for parallel linear genetic programming,2011,Parallel Linear Genetic Programming (PLGP) is an exciting new approach to Linear Genetic Programming (LGP) which decreases building block disruption and significantly improves performance by the introduction of a parallel architecture. We introduce a caching algorithm for PLGP which exploits this parallel architecture to avoid the majority of instruction executions. This allows PLGP programs to be executed an order of magnitude faster than LGP programs with an equal number of instructions.
1142099,14018,9080,Evolving EFSMs solving a path-planning problem by genetic programming,2012,"In this paper, we present an approach to evolving of an algorithm encoded as an extended finite-state machine that solves a simple path-planning problem - finding a path in an unknown area filled with obstacles using a constant amount of memory - by means of genetic programming. Experiments show that in 100% of cases a reasonably correct EFSM with behavior similar to one of the BUG algorithms is evolved."
774520,14018,21102,Fuzzy equational classes,2012,"The paper deals with fuzzy equational classes. These are defined as classes of particular fuzzy algebras refereing to a fuzzy equality (which replaces the crisp one), closed with respect to fuzzy identities. In this fuzzy framework we introduce basic notions of universal algebra, (fuzzy) subalgebras, homomorphisms and direct products. Our main result is that every fuzzy equational class is closed under these three constructions, hence forming a fuzzy variety."
1182534,14018,9080,Considerations of the nature of the relationship between generalization and interpretability in evolutionary fuzzy systems,2011,Performance out of sample is a clear determinant of the usefulness of any prediction model regardless of the application. Fuzzy knowledge base systems are also useful due to interpretability; this factor is often cited as an advantage over black box systems which make model verification by expert users more difficult. Here we examine additional advantages of interpretability for promoting general performance out side training data.
2324766,14018,9080,Sensitivity analysis of a crawl gait multi-objective optimization system,2013,"This paper describes the analysis of a stable crawl gait multi-objective optimization system that combines bio-inspired Central Patterns Generators (CPGs) and a multi-objective evolutionary algorithm. In order to optimize the crawl gait, a multi-objective problem, an optimization system based on NSGA-II allows to find a set of non-dominated solutions that correspond to different motor solutions.   The experimental results highlight the effectiveness of this multi-objective approach."
1046520,14018,9080,Growing and evolving soft robots with a face-encoding tetrahedral grammar,2012,"In this work we describe how the morphology of mobile soft robots can be evolved using a grammatical encoding operating on tetrahedral meshes. Actuation of the soft bodies is achieved by uniformly cycling tetrahedral stiffness. Evolved morphologies exhibit functional differentiation across body regions, and the grammatically derived phenotypes exhibit stable fitness across a range of their ontogenic trajectory, suggesting scalability."
1123619,14018,9080,Pricing transmission rights using ant colony optimization,2011,"We propose a novel idea for pricing Transmission Rights (which are similar to financial options) using a nature inspired meta heuristic algorithm, Ant Colony Optimization (ACO). ACO has been used extensively in combinatorial optimization problems and recently in dynamic applications such as mobile ad-hoc networks. Specifically, the proposed ACO algorithm have been applied to totally different application, Transmission Rights, in the current study."
1890399,14018,21102,On some fuzzy relations for color information,2011,"In this paper we introduce several fuzzy resemblance relations between different combinations of crisp and fuzzy colors. As a starting point, we discuss about fuzzy colors and fuzzy color spaces as a way to represent color information, providing examples of conjunctive and disjunctive use of fuzzy colors for this purpose. Then we determine different types of relations measuring the degree of matching between color information of different kinds, using well known results from fuzzy sets and possibility theories. We study the particular case in which the user considers there is no resemblance between the fuzzy colors comprising the fuzzy color space under consideration. The proposal is based on definitions of fuzzy color and fuzzy color space provided by the authors in previous works."
1376481,14018,8806,Towards a total recall: an activity tracking and recall mechanism for mobile devices,2013,"This paper presents an activity tracking and recall mechanism so called  TR  for mobile handheld devices such as smartphones. A goal of the  TR  is to provide both a practical and extensible information tracking, storing, and retrieval way for user-centric mobile devices. We implemented the proposed mechanism on Android and Tizen mobile platforms and tested its performance in terms of the CPU usage, and the amount of energy consumed by the  TR ."
1127887,14018,9080,On the correlations between developmental diversity and genomic composition,2011,"In this work we target to measure genomic properties in EvoDevo systems as to predict phenotypic properties related to the emergence of artificial organisms. We propose a measurement, λ d, based on the composition of the genome, that can give prediction on how the emerging organism will develop. The experimental approach uses a minimalistic developmental model. The result show that the parameter λ d can predict phenotypic properties. The aim of introducing a parameter like λ d is to get more knowledge on the relation between genomic properties and phenotypic properties of developing organisms."
1486656,14018,8806,A three step based approach for Web Service adaptation,2012,"In this paper, we present a three step based approach to resolve both signature and protocol incompatibility problems that may exist between Web Service protocols. Our proposal makes use of a set of incompatibility and adaptation patterns in order to automatise the generation of adapters that can be applied to incoming messages to modify the structure, type and number of messages sent to the destination. A proof-of-concept implementation is also discussed."
1357534,14018,8806,A software development kit to implement integration solutions,2012,"Typical companies rely on their software ecosystems to support and optimise their business processes. There are a few proposals to help software engineers devise enterprise application integration solutions. Some companies need to adapt these proposals to particular contexts. Unfortunately, our analysis reveals that they are not so easy to maintain as expected. This motivated us to work on a new proposal that has been carefully designed in order to reduce maintainability efforts."
1449807,14018,9080,A comparison between geometric semantic GP and cartesian GP for boolean functions learning,2014,"Geometric Semantic Genetic Programming (GSGP) is a recently defined form of Genetic Programming (GP) that has shown promising results on single output Boolean problems when compared with standard tree-based GP. In this paper we compare GSGP with Cartesian GP (CGP) on comprehensive set of Boolean benchmarks, consisting of both single and multiple outputs Boolean problems. The results obtained show that GSGP outperforms also CGP, confirming the efficacy of GSGP in solving Boolean problems."
1325760,14018,8806,On three fuzzy connectives for flexible data retrieval and their axiomatization,2011,"In most of query languages, conjunctive and disjunctive combinations of conditions remain the usual way for aggregation. Fuzzy query languages also offer trade-off operators, such as means in order to compensate between elementary conditions. In this paper, we would like to introduce a new type of condition basically founded on the interaction between two predicates, thus enriching the panoply of tools the user is provided with and the power of query languages."
2455787,14018,21102,Production planning with uncertain demands,2011,"The paper deals with a single-item production planning problem with uncertain demands modeled by fuzzy intervals whose membership functions are possibility distributions for the values of the uncertain demands. Optimization criteria, in the setting of possibility theory, that lead to choose robust production plans under fuzzy demands are given. Algorithms for determining optimal robust production plans with respect to the proposed criteria are provided and some computational experiments are presented."
2479729,14018,9704,Immune memory with associativity: Perspectives on dynamical systems,2012,Immune memory can be considered as an equilibrium state of immune network system with nonlinear dynamical behavior. The rapid response of immune systems to the secondtime antigen is owing to the stable structure of memory state forming by a closed loop of the idiotypic immune network. A dynamical system of cell population is proposed which explains how the memory state is formed and stabilized in the immune network. stability analysis of antibody dynamics also explains the associativity of immune memory.
1143062,14018,8806,A new device discovery scheme in lighting control networks,2014,"In the Remote Device Management (RDM) protocol, lighting devices are discovered by conducting a simple binary search based on the 48-bit Unique ID (UID). However, the existing binary search scheme tends to require large delay for device discovery. In this paper we propose a partition-based discovery scheme in RDM. From numerical simulations, we can see that the proposed partition-based scheme can reduce the device discovery time, compared to the existing binary search scheme."
1182829,14018,9080,Challenges of applying optimization methodology in industry,2013,"This presentation starts with two case studies of applying optimization methodology in industry, one involving numerical optimization based on simulation models, and the other combinatorial optimization with specific constraints and objectives. These case studies serve to identify some of the challenges frequently met by solution providers for industrial optimization problems. Based on our experience in applying optimization methodology in industry, we then provide suggestions for dealing with these challenges in order to bridge the gap between academia and industry in optimization."
1676237,14018,9080,Stateful program representations for evolving technical trading rules,2011,"A family of stateful program representations in grammar-based Genetic Programming are being compared against their stateless counterpart in the problem of binary classification of sequences of daily prices of a financial asset. Empirical results suggest that stateful classifiers learn as fast as stateless ones but generalise better to unseen data, rendering this form of program representation strongly appealing to the automatic programming of technical trading rules."
947618,14018,8806,Minimum multiple characterization of biological data using partially defined boolean formulas,2012,"In this paper, we adress a characterization problem coming from plant biology. We consider different groups of experiments, each corresponding to the indentification of a given bacteria with regards to a given set of characters for diagnosis purposes. We have to compute simultaneously a complete minimal set of characterization formulas for each group. We propose two different approaches, based on Boolean functions, that allow us to study the satisfiability and the underlying complexity of this problem."
1933912,14018,21102,Non-monotone averaging aggregation,2011,"We advance the theory of aggregation operators and introduce non-monotone aggregation methods based on minimization of a penalty for inputs disagreements. The application in mind is processing data sets which may contain noisy values. Our aim is to filter out noise while at the same time preserve signs of unusual values. We review various methods of robust estimators of location, and then introduce a new estimator based on penalty minimisation."
998780,14018,21102,A new method for operations of interval numbers,2012,"The present interval arithmetic often results in unnecessary diameter expansions in interval operations. To solve this problem, this paper defines a new operation of intervals using their algebraic representations. On the basis of algebraic interval representations, the arithmetic operations are investigated and formulated. Then, the capability of the proposed operation methods in solving unnecessary diameter expansions is analyzed. An application of the proposed model to a typical game problem is provided in the end to demonstrate the feasibility of the proposed model."
1281659,14018,8806,Analysis of client/server interactions in a reservation-based system,2013,"This paper presents a theoretical schedulability analysis of client/server communication in a reservation-based system. The inheritance mechanism previously implemented in a reservation-based system (based on the SCHED_DEADLINE Linux patch, which implements the Constant Bandwidth Server (CBS) algorithm in the Linux kernel) is improved to support predictable client/server communications, and the modified SCHED_DEADLINE has been used to run an extensive set of experiments showing the effectiveness of the proposed approach and analysis."
1054472,14018,9080,Density-based evolutionary outlier detection,2012,A novel density-based distance measure and an outlier detection method using evolutionary search are presented in this paper. A fitness function based on nearest neighbor distances is proposed and the genetic recombination operators are designed to achieve a balance of exploration and exploitation in the nearest neighborhood space. The methodology is tested on datasets of varying sizes (small to moderate) and dimensionalities and performance is compared to existing evolutionary methods for outlier detection.
1011075,14018,8806,A conceptual framework for collective adaptive systems,2013,"In this paper we propose a conceptual framework to characterize Collective Adaptive Systems. By following the separation of concerns we represent these systems as a composition of three components: execution, context and adaptation, and we give a formal definition of all their concepts, defining their corresponding semantics and pointing out the interactions among them. Moreover, we formalize also the main properties that these systems should have, abstracting from any precise specification language or model."
1364271,14018,9080,An evolutionary subspace clustering algorithm for high-dimensional data,2012,"We present an algorithm for generating subspace clusterings of large data sets with many attributes. An evolutionary algorithm is used to form groups of relevant attributes. Those groups are replaced by their centroids, making it possible to cluster the objects in a much lower dimensional space. Preliminary experiments with scalable synthetic data sets suggest that the algorithm generates competitive clusterings while scaling quite well."
850495,14018,9080,Evolving board-game players with genetic programming,2011,"We present the application of genetic programming (GP) to zero-sum, deterministic, full-knowledge board games. Our work expands previous results in evolving board-state evaluation functions for Lose Checkers to a 10x10 variant of Checkers, as well as Reversi. Our system implements strongly typed GP trees, explicitly defined introns, and a selective directional crossover method."
1200231,14018,9704,"Genetic algorithm, MIP and improvement heuristic applied to the MLCLP with backlogging",2013,The present paper solves the multi-level capacitated lot sizing problem with backlogging (MLCLSPB) combining a genetic algorithm with the solution of mixed-integer programming models and the improvement heuristic fix and optimize. This approach is evaluated over sets of benchmark instances and compared to methods from literature. Computational results indicate competitive results applying the proposed method when compared with other literature approaches.
2218964,14018,9704,An evolutionary approach for blind deconvolution of barcode images with nonuniform illumination,2011,"This paper deals with a joint nonuniform illumination estimation and blind deconvolution for barcode signals by using evolutionary algorithms. Indeed, such optimization problems are highly non convex and a robust method is needed in case of noisy and/or blurred signals and nonuniform illumination. Here, we present the construction of a genetic algorithm combining discrete and continuous optimization which is successfully applied to decode real images with very strong noise and blur."
905964,14018,21102,Fuzzy singleton-type SIC fuzzy inference model,2013,"This paper firstly proposes a fuzzy singleton-type Single Input Connected fuzzy inference model (fuzzy singleton-type SIC model) which attaches weights to the rules of the conventional SIC model. Second, it shows the property of the proposed model from the point of view of the equivalence. Thirdly, the learning algorithm of the fuzzy singleton-type SIC model is derived by using steepest descent method. Finally, the proposed model is applied to medical diagnosis, and compared with the conventional fuzzy inference model. From the above results, the applicability of the proposed model is clarified."
2106204,14018,9704,Classifying glyphs by combining evolution and learning,2011,"Artificial neural networks are used to classify the writing system of an unseen glyph. The complexity of the problem necessitates a large network, which hampers the training of the weights. Three hybrid algorithms — combining evolution and back-propagation learning — are compared to the standard back-propagation algorithm. The results indicate that pure back-propagation is preferable to any of the hybrid algorithms. Back-propagation had both the best classification results and the fastest runtime, in addition to the least complex implementation."
1962710,14018,9704,A Framework to Manage Knowledge from Defect Resolution Process,2011,"This paper presents a framework for the management, the processing and the reuse, of information relative to defects. This framework is based on the fact that each defect triggers a resolution process in which information about the detected incident (i.e. the problem) and about the applied protocol to resolve it (i.e. the solution) is collected. These different types of information are the cornerstone of the optimization of corrective and preventive processes for new defects. Experimentations show that our prototype provides a very satisfactory quality of results with good performances."
847583,14018,21102,Possibilistic logistic regression for fuzzy categorical response data,2013,"A new possibilistic logistic regression is investigated, which can be used in cases where the explanatory variables are crisp observations but the values of the response variable are non-precise and are measured by linguistic terms. For evaluating the model, a goodness-of-fit criterion which is called the mean of capability index is employed. A numerical example in a real clinical study about child's appetite status is given to explain the method."
2584793,14018,20332,A modular framework for the automatic reconstruction of shredded documents,2013,"The unshredding problem is of interest in various domains such as forensics, investigative sciences and archeology, and has therefore been approached in many different ways. This paper tries to bridge the gap between previous, disparate, efforts by proposing a modular, probabilistic, solution. Novel approaches to two of the independent subproblems are presented and shown to both have good theoretical properties and to empirically outperform previously published methods."
1571830,14018,8806,Design of a cellular automata cell with rule 30 on quantum-dot cellular automata,2014,"This paper designs and simulates a cellular automata (CA) cell with rule 30 on quantum-dot cellular automata (QCA). In order to implement a cell on QCA, we use Shamsabadi et al.'s  D  Flip-flop, a new  XOR  logic gate in  QCADesigner.  The simulation result shows that outputs of the proposed design for corresponding inputs are correct with high polarization, and it dose not noise."
777867,14018,21102,"Quality, frequency and similarity based fuzzy nearest neighbor classification",2013,"This paper proposes an approach based on fuzzy rough set theory to improve nearest neighbor based classification. Six measures are introduced to evaluate the quality of the nearest neighbors. This quality is combined with the frequency at which classes occur among the nearest neighbors and the similarity w.r.t. the nearest neighbor, to decide which class to pick among the neighbor's classes. The importance of each aspect is weighted using optimized weights. An experimental study shows that our method, Quality, Frequency and Similarity based Fuzzy Nearest Neighbor (QFSNN), outperforms state-of-the-art nearest neighbor classifiers."
960417,14018,21102,Construction of extended Lyapunov functions and control laws for discrete-time TS systems,2012,"Non-quadratic Lyapunov functions have now been more and more frequently used for the analysis and design of Takagi-Sugeno fuzzy models. In this paper, we use a delayed non-quadratic Lyapunov function to develop controller design conditions for TS systems. The conditions can easily be formulated as LMIs. We show that in certain cases the developed conditions are more relaxed than current state-of-the-art methods. In order to further reduce the conservativeness, we also extend the conditions to α-sample variation."
1085027,14018,9704,A Feature-Based Analysis on the Impact of Set of Constraints for e-Constrained Differential Evolution,2014,"Different types of evolutionary algorithms have been developed for constrained continuous optimization. We carry out a feature-based analysis of evolved constrained continuous optimization instances to understand the characteristics of constraints that make problems hard for evolutionary algorithm. In our study, we examine how various sets of constraints can influence the behaviour of e-Constrained Differential Evolution. Investigating the evolved instances, we obtain knowledge of what type of constraints and their features make a problem difficult for the examined algorithm."
741887,14018,9704,Multi-method algorithms: Investigating the entity-to-algorithm allocation problem,2013,"This paper investigates the algorithm selection problem, otherwise referred to as the entity-to-algorithm allocation problem, within the context of three recent multi-method algorithm frameworks. A population-based algorithm portfolio, a meta-hyper-heuristic and a bandit based operator selection method are evaluated under similar conditions on a diverse set of floating-point benchmark problems. The meta-hyper heuristic is shown to outperform the other two algorithms."
2257048,14018,21102,Component-based search engine for blogs,2011,"A wrapper is a program that selectively extracts a necessary part (component) from Web pages. Automatic or semi-automatic wrapper construction is crucial to achieve a fine grained search engine for Web pages. However, this is not an easy task to achieve. This paper proposes a component-based search engine in which the content components gain a high score in the search results. Thus, the required segments for a query can be obtained without using a wrapper."
849471,14018,8806,Affine invariant shape matching using radon transform and dynamic time warping distance,2012,"An affine invariant shape matching method using the Radon transform and the dynamic time warping distance is proposed. Our descriptor based on the Radon transform is robust to shapes rotation, uniform scaling, and translation. For non-uniform scaling and shearing, our descriptor has a non-linear sparse and dense distortion relative to the angle coordinates. Therefore, we apply the dynamic time warping to be robust to these transformations. Our performances are better for any affine transformations than the conventional other methods."
947575,14018,21102,Choquet integral vs. TOPSIS: An intuitionistic fuzzy approach,2013,"In this study, two approaches are compared to solve the supplier selection problem under intuitionistic fuzzy environment. The first approach deals with interactive criteria utilizing the Choquet integral. The second approach extends the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method. Two case studies are presented. The results obtained give an insight into the effect of interactive criteria on the final outcome."
914158,14018,21102,Relation between polling and Likert-scale approaches to eliciting membership degrees clarified by quantum computing,2013,"In fuzzy logic, there are two main approaches to eliciting membership degrees: an approach based on polling experts, and a Likert-scale approach, in which we ask experts to indicate their degree of certainty on a scale - e.g., on a scale form 0 to 10. Both approaches are reasonable, but they often lead to different membership degrees. In this paper, we analyze the relation between these two approaches, and we show that this relation can be made much clearer if we use models from quantum computing."
2154279,14018,9080,Ion channel modeling with analog circuit evolution,2011,"We propose that analog electrical circuits are a natural representation for modeling the dynamical systems that arise in neuroscience. Here we use analog circuit evolution, a reverse engineering technique designed to search through analog circuit space, to automatically design circuit models of ion channel behavior. Results comparing several different multiobjective and coevolutionary techniques demonstrate the importance of evaluating the fitness of evolved circuits under multiple behavioral conditions."
2140066,14018,8806,Software testing-resource allocation with operational profile,2012,"In this paper, we formulate three software development planning models with constraints, based on the expected total software cost, cumulative software testing resources (efforts), and expected residual number of software faults. Taking account of the software operational profile, we solve the non-linear optimization problems to allocate the optimal testing resources for a given software release time. An illustrative example is given under realistic assumptions."
844500,14018,21102,Feature extraction using fuzzy complete linear discriminant analysis,2012,"In pattern recognition, feature extraction techniques are widely employed to dimensionality reduction. In this paper, a novel feature extraction method, fuzzy complete linear discriminant analysis (Fuzzy-CLDA), is proposed by combining the complete linear discriminant analysis (CLDA) and the membership degrees of samples. Furthermore, we calculate the sample membership degrees with different distance metrics and compare the effectiveness of the distance metrics. In addition, experiments are provided for analyzing and illustrating our results."
2055010,14018,21102,Fuzzy controller design using group-crossover particle swarm optimization for truck reversing control,2011,"This paper proposes a fuzzy controller design using group-crossover particle swarm optimization (GCPSO) algorithm. The GCPSO uses a group-based framework for defining particle neighborhood topology and incorporating crossover operation into particle swarm optimization. The GCPSO dynamically forms different groups for selecting parents in crossover operations, particle updates and replacements. The objective of GCPSO is to improve fuzzy control accuracy. Comparisons with different population-based optimizations on truck reversing control problem demonstrate the performance of GCPSO algorithm."
1051409,14018,9080,Fitness function evaluation for MA trading strategies based on genetic algorithms,2011,"This paper presents a new approach to optimize an investment strategy based on moving averages (MA). The proposed approach optimizes the entry and exit points, for both long and short positions, using a genetic algorithm (GA) kernel. This approach outperforms B&H strategy and explores alternative functions to the classical absolute return fitness function. The approach is demonstrated for major market indexes, such as, S&P 500, FTSE100, DAX30, NIKKEI225."
980276,14018,21102,On-line design of LUT controllers based on desired closed loop plant: Vertex Placement Principle,2012,"This paper presents a systematic approach to perform an on-line global design of look up table (LUT) controllers. This paper uses Vertex Placement Principle to design a controller for an initially unknown objective plant (P) based on a desired plant (DP) model. Both the desired plant and the objective plant are approximated with piecewise bilinear (PB) models. After the on-line identification of the initially unknown plant (P), the LUT-controller (C) is designed so as to make the closed loop plant (CP) behave like a desired plant."
1309678,14018,9080,Tuning parameters across mixed dimensional instances: a performance scalability study of Sep-G-CMA-ES,2011,"Sep-G-CMA-ES is a variant of G-CMA-ES with lower time complexity. In this paper, we evaluate the impact that various ways of tuning have on the performance of Sep-G-CMA-ES on scalable continuous benchmark functions. We have extracted seven parameters from Sep-G-CMA-ES and tuned them across training functions with different features using an automatic algorithm configuration tool called Iterated F-Race. The best performance of Sep-G-CMA-ES was obtained when it was tuned using functions of different dimensionality (a strategy that we call  mixed dimensional ). Our comparative study on scalable benchmark functions also shows that the default Sep-G-CMA-ES outperforms G-CMA-ES. Moreover, the tuned version of Sep-G-CMA-ES significantly improves over both G-CMA-ES and default Sep-G-CMA-ES."
1091666,14018,8806,Estimating the size of data mart projects,2013,"In order to better manage software projects, we need to estimate adequately the effort of its development, independently of their peculiarities. This paper presents an adaptation of Function Point Analysis (FPA) approach for estimating the size of Data Mart (DM) systems. We validate the proposed approach using real data from finished DM projects and we also show that our approach could offer better results compared with the original FPA."
1291842,14018,9080,How many neurons?: a genetic programming answer,2011,"The goal of this paper is to derive predictive models that take as input a description of a problem and produce as output an estimate of the optimal number of hidden nodes in an Artificial Neural Network (ANN). We call such computational tools Direct Estimators of Neural Network Topology (DENNT), an use Genetic Programming (GP) to evolve them. The evolved DENNTs take as input statistical and complexity descriptors of the problem data, and output an estimate of the optimal number of hidden neurons."
874060,14018,9080,Intrinsic evolvable hardware for combinatorial synthesis based on SoC+FPGA and GPU platforms,2011,"This paper presents a novel a parallel genetic programming (PGP) boolean synthesis implementation on a low cost cluster of an embedded open platform called SIE. Some tasks of the PGP have been accelerated through a hardware coprocessor called FCU, that allows to evaluate individuals onchip as intrinsic evolution. Results have been compared with GPU and HPC implementations, resulting in speedup values up to approximately 2 and 180 respectively."
1343486,14018,8806,A hierarchical model for traceability between requirements and architecture,2014,This paper presents a formal traceability model that describes the linkage between requirements and architecture. This model creates a hierarchical structure of describing how the architecture is decomposed recursively level by level thereby making the trace creation scalable. The proposed traceability model enables the architect to establish and understand the traces over different decomposition levels in a consistent manner. Benefits of the proposed model are demonstrated with a nontrivial case study.
755659,14018,9704,Cultural learning for Multi-Agent System and its application to fault management,2014," Abstract—It is usually agreed that a system capable of learning deserves to be called intelligent; and conversely, a system being considered as intelligent is, among other things, usually expected to be able to learn. Learning always has to do with the self-improvement of future behavior based on past experience. In this paper we present a learning model for Multi-Agent System, which aims to the optimization of coordination schemes through a collective learning process based on Cultural Algorithms."
740994,14018,8806,Constrained global types for dynamic checking of protocol conformance in multi-agent systems,2013,"Global types are behavioral types for specifying and verifying multiparty interactions between distributed components, inspired by the process algebra approach.   In this paper we extend the formalism of global types in multi-agent systems resulted from our previous work with a mechanism for easily expressing constrained shuffle of message sequences; accordingly, we extend the semantics to include the newly introduced feature, and show the expressive power of these constrained global types."
854623,14018,9080,Is there a computational advantage to representing evaporation rate in ant colony optimization as a gaussian random variable,2012,"We propose an ACO (Ant Colony Optimization) variation in which the evaporation rate, instead of being constant as is common in standard ACO algorithms, is a Gaussian random variable with non-negligible variance. In experimental results in the context of MAX-MIN Ant System (MMAS) and the Traveling Salesman Problem (TSP), we find that our variation performs considerably better than MMAS when the number of iterations is small, and that its performance is slightly better than MMAS when the number of iterations is large."
1623583,14018,9080,Idealized dynamic population sizing for uniformly scaled problems,2011,"This paper explores an idealized dynamic population sizing strategy for solving additive decomposable problems of uniform scale. The method is designed on top of the foundations of existing population sizing theory for this class of problems, and is carefully compared with an optimal fixed population sized genetic algorithm. The resulting strategy should be close to a lower bound in terms of what can be achieved, performance-wise, by self-adjusting population sizing algorithms for this class of problems."
1419174,14018,21102,Improvements on non-quadratic stabilization of continuous-time Takagi-Sugeno descriptor models,2013,"This paper presents a relaxed approach for stabilization and H ∞  disturbance rejection of continuous-time Takagi-Sugeno models in descriptor form. Based on Finsler's Lemma, the control law can be conveniently decoupled from a non-quadratic Lyapunov function. These developments include and outperform previous results on the same subject while preserving the advantage of being expressed as linear matrix inequalities. Two examples are presented to illustrate the improvements."
878642,14018,21102,New controllers and new designs for continuous-time Takagi-Sugeno models,2012,This work presents a new non-PDC controller design based on different Lyapunov functions for continuous-time Takagi-Sugeno models. Based on this new control law and on Finsler's lemma a new way to derive LMI constraints problems is presented. Both quadratic and non quadratic Lyapunov functions are under consideration. Some examples show the capability of outperforming existing results without increasing significantly the LMI problems.
1666259,14018,8806,Mapping of the synchronization mechanisms of the Linux kernel to the response-time analysis model,2014,"Response-time analysis is a method for determining the schedulability of real-time systems based on fixed priority. Although the Linux kernel with the PREEMPT_RT patch is a real-time system based on fixed priority, the tools are used only to measure its latency. The kernel is generally considered as a black box. This paper associates the variables used in the response-time analysis with the kernel functions that cause blocking and interference. The objective of the present study is to identify the points of interest within the Linux kernel for the response-time analysis."
2938455,14018,9704,Equilibrium Selection under Limited Control - An Experiment on Network Hawk Dove Games,2012,"Equilibrium selection in networks is difficult. Players have to both choose their contacts and their actions. In this paper, we formally and experimentally analyze three variants of the game varying the strategy set of the players. We formally show, that limiting the strategy set can both increase and decrease the number of sub game perfect equilibria in the game. Human participants playing these games end up in similar equilibria even though they find it more difficult to reach them, the more equilibria exist."
1097497,14018,9080,Quantum-inspired tabu search algorithm for solving 0/1 knapsack problems,2011,"In this paper, we propose a novel quantum-inspired evolutionary algorithm, called quantum-inspired Tabu search (QTS). QTS is based on the classical Tabu search and the characteristic of quantum computation such as superposition. We will present how we implement QTS to solve 0/1 knapsack problem. Furthermore, the results of experiments are also compared with other quantum-inspired evolutionary algorithm and other heuristic algorithms' experimental results. The final outcomes show that QTS performs much better than the other heuristic algorithms on 0/1 knapsack problem, without premature convergence and more efficiency."
2666809,14018,11187,Improved swap heuristic for the multiple knapsack problem,2013,"In this paper, we describe two new improvements of the well known Martello and Toth Heuristic Method (MTHM). Our new improvements are very simple and at the same time they are very efficient since they yield to more than 15% over MTHM with an excellent execution time performance in relatively large problem instances. Further, the new improvements give a very close results to sophisticated meta-heuristics namely Genetic Algorithms with a gap less than 1% within a time slot less than a second."
1494188,14018,9080,Optimal OpAmp sizing based on a fuzzy-genetic kernel,2011,"In this paper an innovative fuzzy-genetic approach is proposed to address the problem of analog circuit sizing. The proposed approach introduces a fuzzy mutation operator which models expert design knowledge and this way not only avoids local minima but also reduces the search dynamically the space. The proposed approach is compared against a state-of-the art genetic approach, for the optimal operational amplifier sizing, and presents a faster convergence rate."
919093,14018,21102,Clustering in tweets using a fuzzy neighborhood model,2012,Clustering of keywords in tweets is studied. A series of tweets is handled as a sequence of words and an inner product space is introduced to a set of keywords on the basis of positive definite kernels using a fuzzy neighborhood defined on that sequence. Methods of agglomerative hierarchical clustering as well as c-means clustering are applied. Pairwise constraints are moreover introduced to improve interpretability of clusters. Real tweets are analyzed with discussion of the resulting clusters.
2139408,14018,9704,A Simple Model for Automated Negotiations over Collaboration Agreements in ebXML,2011,"A serious limitation in successful deployment of ebXML have been conflicting CPP contents, preventing automatic generation of CPA that can satisfy collaborating partners. In the paper a method for resolving CPP conflicts with a simple bilateral bargaining protocol using utility function and discount factor is proposed. It enables negotiating CPAs by taking into account hierarchical structure of CPPs, which has not been considered properly by existing CPA negotiation protocols."
1870294,14018,21102,Fuzzy bipolar conditions of type ”or else”,2011,"Previously studied fuzzy bipolar conditions of type ”and if possible” are made of a mandatory condition c and an optional condition w. They allow expressing complex preferences of a conjunctive nature. We define in this paper, a new kind of fuzzy bipolar conditions of the form ”or else” which express complex preferences of a disjunctive nature. We show that the ”or else” form can be used as a negation operator of the ”and if possible” form and vice versa. We also show that these both forms are compatible and, therefore, fuzzy bipolar conditions of both types can be used together in the same bipolar query."
1513696,14018,9704,Heuristic Space Diversity Management in a Meta-Hyper-Heuristic Framework,2014,This paper introduces the concept of heuristic space diversity and investigates various strategies for the management of heuristic space diversity within the context of a meta-hyper-heuristic algorithm. Evaluation on a diverse set of floating-point benchmark problems show that heuristic space diversity has a significant impact on hyper-heuristic performance. The increasing heuristic space diversity strategies performed the best out of all strategies tested. Good perfor- mance was also demonstrated with respect to another popular multi-method algorithm and the best performing constituent algorithm.
1667745,14018,9080,Co-evolution of the dynamics in population games: the case of traffic flow assignment,2012,"In population games, one of the main interests is the evolution of the dynamics, i.e., how the distribution of individuals change along time. This is an abstract but elegant way to model population of drivers selecting routes. In this paper, a three-population asymmetric game is used to investigate the co-evolution of drivers' strategies. It is shown that the convergence to one of the Nash equilibria is achieved when the three populations co-evolve, under different rates of mutants in these populations."
1201661,14018,21102,Approximation capability of SISO Fuzzy Relational Inference systems based on fuzzy implications,2013,"In this work, we show that fuzzy inference systems based on Fuzzy Relational Inference (FRI) with implicative interpretation of the rule base are universal approximators under suitable choice of operations for the rest of the components of the fuzzy system. The presented proofs make no assumption on the form or representations of the considered fuzzy implications and hence show that a much larger class of fuzzy implications other than what is typically considered in the literature can be employed meaningfully in FRIs based on implicative models."
1252245,14018,8806,Using faults for buffer overflow effects,2012,"Fault attacks have been developed in the cryptographic community to extract secret information on hardware implementations. They have also been used to bypass security checks during authentication processes for example. Here, we show that they can be exploited to make more damage, taking the control of a machine as buffer overflow attacks do for instance. In this short paper, we demonstrate by using one example that countermeasures against buffer overflow must also be used for software running on embedded processors."
1290065,14018,9704,A fast modular simulator for combinational logic circuits generated by genetic algorithm,2013,"This paper presents a structure for a fast generic simulator for combinational logic circuits, intended for use with genetic algorithm (GA) multi-objective optimization. A modular structure is proposed containing a chromosome translator layer, so to isolate the simulator from the idiosyncrasies of the chosen genetic codification. The simulator archieved high performance, considering its software-only implementation, and it was successfully used to generate valid circuits while integrated into a GA."
2232690,14018,9704,A study on Genetic Programming with layered learning and incremental sampling,2011,"In this paper, we investigate the impact of a layered learning approach with incremental sampling on Genetic Programming (GP). The new system, called GPLL, is tested and compared with standard GP on twelve symbolic regression problems. While GPLL does not differ from standard GP on univariate target functions, it has better training efficiency on problems with bivariate targets. This indicates the potential usefulness of layered learning with incremental sampling in improving the efficiency of GP evolutionary learning."
2007260,14018,9080,Optimization of a supersonic airfoil using the multi-objective alliance algorithm,2013,"A baseline NACA0012 two-dimensional (2D) airfoil is optimized for supersonic flight conditions using a recently introduced optimization algorithm: the multi-objective alliance algorithm (MOAA). The efficacy of the algorithm is demonstrated through comparisons with NSGA-II for 300, 600 and 1000 function evaluations. Through epsilon/hypervolume indicators and the Mann-Whitney statistical test, we show that MOAA outperforms NSGA-II on this problem."
892595,14018,21102,Fuzzy linguistic propositional logic based on refined hedge algebra,2013,"We consider a fuzzy linguistic propositional logic having the truth domain as a refined hedge algebra. The syntax and semantic are defined, the resolution is chosen as the inference system. The soundness and completeness of the resolution procedure are proved using semantic tree technique. In order to capture the approximate nature of the resolution inference, we introduce the concept of reliability of resolution inference. The greater the reliability is the more certain the resolution inference is. Finally, we give an optimized resolution procedure which guarantees that each resolution proof has the maximal reliability."
1585369,14018,8806,Large-scale message synchronization in challenged networks,2014,"In this paper we introduce Dispersy, a message synchronization platform capable of running inside a challenged network. Dispersy uses Bloom filters to let peers advertise their local state and receive missing messages. However, in contrast to previous work, the efficiency and effectiveness of our design allows for deployment in networks of unprecedented scale. We show, through extensive experimental evidence that peers have no difficulties in synchronizing over 100,000 messages.   We integrate in Dispersy a NAT traversal technique able to puncture 77% of NAT-firewalls. Not puncturing these firewalls would prevent up to 64% of peers from receiving any synchronization requests. Implementing a NAT traversal technique proved essential when we used Dispersy to extend the functionalities of a BitTorrent client. To date, over 350,000 users used our modified BitTorrent client which included Dispersy, synchronizing in overlays consisting of more than 500,000 messages.   We emulate an overlay consisting of 1000 unmodified Dispersy peers in order to show the propagation and synchronization speed, bandwidth requirements, churn resilience, and overall throughput of Dispersy. Dispersy is able to synchronize a single message to all peers within 19 synchronization steps, withstand an average session-time of 30 seconds, and achieve an average throughput of 10,000 messages/s."
1929148,14018,8806,An efficient security framework for mobile WiMAX,2012,"WiMAX is a technology that provides continuous high data throughput with low delays for various user types and modes of operation. The security protocols proposed for WiMAX impose a heavy performance overhead, especially on mobile subscribers running real time applications, such as VoIP and IPTV. We propose a hybrid security framework for WiMAX combining Hierarchical Identity-Based Cryptography (HIBC) and certificate based approaches to achieve lower bandwidth usage and higher reliability for stationary as well as mobile subscribers. Our security architecture efficiently resolves the performance issues existing with the current WiMAX security standards and can perform fast handovers. Our framework provides upto a 87% improvement in bandwidth compared to WiMAX's current security standard."
1429084,14018,8806,Applying software product line engineering in building web portals for supercomputing services,2013,"Supercomputing centers, typically non-profit, government or university-based organizations with scarce resources, are increasingly being requested to provide customized web portals for user-centered access to their services in order to support a demanding customer base. These portals often have very similar architectures and meet similar requirements, with the variations primarily being in the specialized analysis applications, and in the input and output of these applications. Given these characteristics, Software Production Line Engineering (SPLE) approaches will be valuable in enabling development teams to cost-effectively meet demands. In this paper, we demonstrate a suite of web portals developed at The Ohio Supercomputer Center (OSC) by applying SPLE methodologies. We show how we applied feature modeling on these applications to identify commonalities in their application level features despite differences in their problem domains. We describe a common framework (we term it Per User DrupaL, or PUDL), which serves as the common foundation for these portals. We demonstrate the effectiveness of SPLE in terms of reduced development time and effort, and discuss the technical challenges faced in this process. Finally we propose, as an extension to our work, an automation framework for portal generation, which users could build their own customized portals."
1006070,14018,8806,Machine classification of melanoma and nevi from skin lesions,2011,"We describe a method using a Support Vector Machine (SVM) to classify and diagnose skin biopsies from patients as either melanoma or nevi based on H&E stained histological slides alone. Our method differs from other approaches to digital melanoma diagnoses in using the histology slide, not digital clinical pictures of the patients' skin to make the classification. Using only the histological criterion of irregularities in the nucleus, our best SVM utilizes nucleus perimeter/area ratio and nucleus major/minor axis ratio as features to give a classification accuracy of 90%, sensitivity of 100% and specificity of 75%, (at magnification of 400 times) in our data set. The performance is remarkable given a dermatological pathologist typically examines a plethora of features to make a diagnosis. Our SVM in conjunction with clinical digital diagnoses systems could reduce the number of missed melanoma diagnoses."
1408905,14018,8806,Fault class prioritization in Boolean expressions,2012,"A recent study has classified faults in Boolean expressions into ten classes and has proved that there are five key fault classes, namely  CCF, CDF, ORF, ENF  and  ASF , such that if a test suite can kill all faulty versions of these five core fault classes, if can kill all faulty versions of all fault classes. In order to generate more effective test suites, we should prioritize these five fault classes further, such that test cases with stronger fault detection capability could be generated as early as possible. Such a process is referred to as the fault class prioritization. Based on the observation in the fault class hierarchy, we divide the five fault classes into two groups { CCF, CDF } and { ORF, ENF, ASF }. Two strategies of fault class prioritization are proposed to generate test cases efficiently. We design experiments using TCAS Boolean expressions and some randomly generated Boolean expressions. The experimental results suggest that if we generate test cases for  CCF  and  CDF  firstly, the final test suite always have a higher efficiency of killing faults."
1553031,14018,8806,Concurrent typed intermediate language,2013,"Typed assembly languages have been designed to ensure safety on low-level code, avoiding the dynamic loading of erroneous code, generated by unreliable compilers, thus compromising the correct behavior of virtual machines and processors. Among the most challenging errors to detect are errors related to concurrent behaviors and thread synchronization. We present a typed intermediate language for a stack-based virtual machine, comprising a primitive object model and native support for concurrency. By using primitives to deal with concurrency, we establish a more abstract level where it is possible to represent and enforce a linear discipline on thread values, and pave the way for more sophisticated type based aliasing and concurrency analyses."
1838000,14018,8806,Fusion in fingerprint authentication: two finger types vs. two scanner types,2011,"This paper presents our study on fingerprint fusion in particular in three scenario sets: a) two fingers captured by the same scanner; b) the same finger captured by two different scanners; and c) two fingers both captured by two different scanners. As a test data set we use GUC100 multi-scanner fingerprint database which contains fingerprint images of all ten fingers from 100 subjects using a number of different fingerprint scanners. In total 780 fusion scenarios are studied. Our analysis indicate that score level fusion using average rule provides improvement in all scenarios. The fusion of the same fingers from different scanners appears to provide more performance improvement compared to the fusion of different fingers from the same scanner. Furthermore, we also reveal that fusion of different fingers both collected by different scanners are the best. Interestingly, results suggest that for fusion differences in scanner type are more valuable than differences in finger type."
2035267,14018,8806,Mapping the business model canvas to ArchiMate,2012,"Many IT projects fail to succeed in the market, as they start purely from technology. Much effort is therefore wasted, while the potential benefits are not realized. We argue that the design process should start with creating a business model, which is then translated to an architecture to ensure fitness for market of the future system. Therefore, we propose a mapping from Osterwalder's business modeling canvas and ontology to the enterprise architecture modeling standard ArchiMate, which makes the above translation possible and represents a formal basis for business modeling in ArchiMate. A case study illustrates the mapping between the two languages."
1514404,14018,8806,3D cloud detection and tracking for solar forecast using multiple sky imagers,2014,"Cloud detection and tracking (CDT) is the most challenging problem in integrating solar energy into the smart grid. In this paper, we present a novel 3D cloud detection and tracking using images from three TSI (Total Sky Imager), and propose to incorporate history into a multi-layer cloud detection pipeline. Our pilot study shows that the new CDT significantly improves the short-term solar irradiance forecasting and enable regional radiation prediction, which is impossible with a single TSI."
1818975,14018,507,CloudAlloc: a monitoring and reservation system for compute clusters,2012,"Cloud computing has emerged as a promising environment capable of providing flexibility, scalability, elasticity, fail-over mechanisms, high availability, and other important features to applications. Compute clusters are relatively easy to create and use, but tools to effectively share cluster resources are lacking.  CloudAlloc  addresses this problem and schedules workloads to cluster resources using allocation algorithms that can be easily changed according to the objectives of the enterprise. It also monitors resource utilization and thus, provides accountability for actual usage.  CloudAlloc  is a lightweight, flexible, easy-to-use tool for cluster resource allocation that has also proved useful as a research platform. We demonstrate its features and also discuss its allocation algorithms that minimize power usage.  CloudAlloc  was implemented and is in use at HP Labs."
2436306,14018,8806,A consistency rule for graph isomorphism problem,2012,"This paper describes an algorithm for graph isomorphism problem. A consistency rule is proposed to detect as soon as possible the isomorphism permutation. The algorithm, called CRGI, tries to find an isomorphism between two input graphs through a backtracking exploration that uses a proposed consistency rule to prune the tree-search. This rule is based on changing cases positions of one adjacency matrix to obtain exactly the second adjacency matrix, according to a permutation that must be defined. If such permutation exists, an isomorphism is detected. The proposed rule is able to prune as early as possible unfruitful branches of the tree-search which leads to reduce the practical time complexity. Experimental results comparing CRGI with other popular algorithms show the effectiveness of CRGI especially for random graphs and trees."
1643722,14018,8806,Composite trust-based public key management in mobile ad hoc networks,2013,"Public key management in mobile ad hoc networks (MANETs) has been studied for several decades. Yet no single solution has completely resolved well known design challenges resulting from the unique characteristics of MANETs. These challenges include no centralized trusted entities, resource constraints, and high security vulnerabilities. This work proposes a fully distributed trust-based public key management approach for MANETs using a soft security mechanism based on the concept of trust. Instead of using hard security approaches, as in traditional security techniques, to eliminate security vulnerabilities, our work aims to maximize performance by trading off risk (i.e., security vulnerability) for trust. In this work, we propose a composite trust-based public key management (CTPKM) with no centralized trust entity with the goal of maximizing performance (e.g., service availability or efficiency) while mitigating security vulnerability. Each node employs a trust threshold to determine whether or not to trust another node. Each node's decision making using the given trust threshold affects performance and security of CTPKM. Our simulation experimental results show that there exists an optimal trust threshold that can best balance and meet the conflicting goals between performance and security, exploiting the inherent tradeoff between trust and risk."
2413172,14018,8806,DREAM: a distributed fRamework for customized dEployment of a vAriety of indexing engines over million-node overlays,2012,"In this paper we present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The key aim is to provide the appropriate integrated set of tools that allows numerous applications with large-scale, different requirements to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes - billions of keys). Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup."
1592616,14018,8806,A process for facilitating interaction design through automated GUI generation,2014,"A model representing an interaction design is a prerequisite for model-driven generation of graphical user interfaces (GUIs). Related state-of-the-art methodologies typically assume that a suitable interaction model is already available and do not support the exploration of design alternatives or focus on how a high-quality interaction model can be developed. Our tool-supported process facilitates the exploration and evaluation of interaction design alternatives in an iterative manner, using automated GUI generation to achieve a running application more quickly and with reduced effort in comparison to manual (prototype) development. This allows the designer to quickly find a suitable alternative. In general, this approach facilitates the development of high-quality interaction models through automated GUI generation."
1701906,14018,8806,Well-constrained completion for under-constrained geometric constraint problem based on connectivity analysis of graph,2011,"This paper presents a method based on connectivity analysis of graph to solve structurally under-constrained constraint problems frequently occurred during design process in parametric CAD. We give a partial solution to the optimal well-constrained completion problem, that is, adding automatically new constraints to the graph corresponding to an under-constrained geometric constraint problem  G  in such a way that  G  is well-constrained and the set of equations needed to be solved simultaneously in order to solve  G  has the smallest size. With this method, a connected, bi-connected, or tri-connected structurally under-constrained problem in 2D can be transformed into a structurally well-constrained one by adding new constraints automatically during the process of decomposing it into a decomposition tree."
1712973,14018,8806,Hierarchical topology adaptation for distributed convergecast applications,2014,"Sink based spanning tree topologies are effectively used in convergecast applications for efficient packet delivery with minimum forwarding delay, eliminating packet loss or duplicate packet delivery. For tree based convergecast, this paper theoretically shows that there is a trade-off between the topology architecture and the network traffic load. Depending on this trade-off, a hierarchical topology adaptation scheme is proposed for the tree based convergecast to improve overall application performances. The effectiveness of the proposed scheme is analyzed using simulation results."
1363214,14018,8806,Text clustering using one-mode projection of document-word bipartite graphs,2013,"Many real life networks have an underlying bipartite structure based on which similarity between two nodes or data instances can be defined. For example, in the case of a document corpus, the similarity between a pair of documents can be assumed to arise from the words that co-occur in them, and this document-word co-occurrence relationship can be modeled as a bipartite graph. A document similarity graph can be obtained by taking a one-mode projection of the bipartite graph, which is a popular technique for studying similar networks which arise from bipartite structure. A graph-based clustering algorithm can then be applied to this projection graph to obtain clusters of documents. In this paper we study the use of one-mode projection of the document-word bipartite graph and the subsequent application of a modularity optimization algorithm to cluster the documents. In particular, we propose an alternative and faster algorithm, which works in two-steps: first, finding the documents that are easy to cluster, and then, assigning the remaining documents to the existing or new clusters. We show that the algorithms based on one-mode projections perform significantly better than traditional clustering approaches. In addition, our method has similar or better clustering performance than the most popular algorithm for modularity optimization, while also running four times faster."
1311026,14018,8806,Efficient dynamic scheduling of heterogeneous applications in hybrid architectures,2014,"The emergence of different applications that deal with growing amounts of data at reasonable times, has stimulated the development of new computing architectures consisting of different processing units (PU). Runtime environments have been proposed in order to exploit these resource as much as possible by offering a variety of methods for dynamically scheduling tasks on different PUs. These schedulers determine which PU is better suited for executing each task, based upon a set of task parameters such as the amount of data, computation requirements, etc. Although large number of applications are heterogeneous, composed of tasks with different characteristics, the current techniques focus on these characteristics as isolated features leading to inefficient executions in several situations. In this work we present two new scheduling strategies, combining different existing strategies, that leads to more efficient executions in different scenarios. Our results show that our approach can be up to 20% more efficient than current techniques."
995568,14018,8806,Adaptive hybridization strategies,2011,"During the last decades, significant improvements have been achieved for solving complex combinatorial optimization problems issued from real world applications. To tackle large scale instances and intricate problem structures, sophisticated solving techniques have been developed, combined, and hybridized to provide efficient solvers. Combinatorial problems are often modeled as Constraint Satisfaction Problems or constraint optimization problems, which consist of a set of variables, a set of possible values for these variables and a set of constraints to be satisfied. However, solvers or hybridization of solvers become more and more complex: the user must select various solving and hybridization strategies and tune numerous parameters. Moreover, it is well-known that an a priori decision concerning strategies and parameters is very difficult since strategies and parameters effects are rather unpredictable and may change during solving."
2248428,14018,8806,"Generative adaptation of model transformation assets: experiences, lessons and drawbacks",2014,"Model transformation is a central activity in Model Driven Engineering (MDE) as it specifies how models are consumed to generate other models or code. Complex scenarios typically involve the execution of several transformations that, due to variability of solutions to develop software projects, need to be tailored to attempt different implementation technologies, libraries, patterns, etc. Recent proposals to tailor these assets suggested that current Software Product Line (SPL) techniques can be used to promote reuse of model transformation assets. However, in a recent case study, we have found lacks in techniques, including ours, to apply reuse for this domain in particular. Thus, this paper presents this case study and discusses implications and challenges in tailoring these assets with SPL techniques."
2230364,14018,8806,Using polynomial reductions to test the suitability of metaheuristics for solving NP-complete problems,2013,"We present a method to compare the suitability of evolutionary computation metaheuristics for solving different NP-complete problems. Instead of checking the performance of each metaheuristic for each problem by using a specific benchmark of that problem, polynomial reductions are used to transform instances of a problem into the other. In this way, we avoid assessing each metaheuristic in terms of incomparable benchmarks. Several pairs of NP-complete problems are compared. For each pair, the impact of the difficulty of the polynomial reduction on the capability of an evolutionary method (in particular, genetic algorithms) to achieve a similar performance for both problems is studied."
741595,14018,8806,New hybrid genetic algorithm for solving optimal communication spanning tree problem,2011,"Optimal Communication Spanning Tree (OCST) is a well-known NP-hard problem on the graph that seeks for the spanning tree with the lowest cost. The tree cost depends on the demand and distance between each pair of nodes. This paper presents a Hybrid Genetic Algorithm (HGA) combining the basic GA with the ideas of the Particle Swarm Optimization (PSO) algorithm. In HGA, each individual exploits information of its own experience to search through the solution space with genetic operator. The experiment results show that our HGA outperforms the previous GAs with faster convergence and better solution."
1713118,14018,8806,HW resource componentizing for smooth migration from single-function ECU to multi-function ECU,2012,"The automotive design paradigm is shifting from the one-function on one ECU (Electronic Control Unit) paradigm toward the multi-function on one ECU paradigm to reduce the ever increasing number of ECUs in a vehicle. In order to support such paradigm shift, this paper proposes a  HW (hardware) resource componentizing  technique that provides the illusion of a physically isolated ECU component for each automotive SW (software) component even when a multi-function ECU is actually shared by multiple concurrent SW components. With this technique, each SW component's physical properties such as timing behavior can be invariant with its surrounding SW components and hence the automaker can easily compose and verify a complex future automotive system. In order to effectively realize this HW resource componentizing, we propose anactive window based share provisioning that provides a HW share to a SW component only when it is active. This way, the ECU capacity can be divided not only in the spatial (i.e., share) domain but also in the temporal domain, providing a large number of fine-granular HW components for serving a large number of SW components. The effectiveness of the proposed HW com-ponentizing technique is validated through both simulation and actual implementation with real automotive workload."
1752896,14018,8806,Teaching intelligible speech to the autistic children by interactive computer games,2011,"Autism is considered to be a disorder of neural development which affects about 1 in every 150 kids. Specifically, some children with ASD are not fortunate enough to acquire the ability to communicate in their own language. Since speech is an important media of communication, socialization, and interaction with the world, these children need assistance while delivering speech to communicate to the world. Without proper speaking skills, these autistic children face difficulties in expressing their needs and emotions, too. Doctors, therapists, and special teachers usually help kids with autism to overcome many difficulties. However, the traditional methods of teaching clear speech to the autistic children suffer from being monotonous, laborious and not successful in many cases. Hence, we developed an interactive computer game which will be helpful to increase intelligibility in autistic children. During our five months of intervention with the autistic children of Autism Welfare Foundation (AWF) at Dhaka, we checked the effectiveness of this therapy and got some encouraging results."
1075574,14018,8806,REME-D: a reflective epidemic message-oriented debugger for ambient-oriented applications,2011,"Debuggers are an integral part, albeit often neglected, of the development of distributed applications. Ambient-oriented programming (AmOP) is a distributed paradigm for applications running on mobile ad hoc networks. In AmOP the complexity of programming in a distributed setting is married with the network fragility and open topology of mobile applications. To our knowledge, there is no comprehensive debugging approach that tackles both these issues. In this paper we present REME-D, an online debugger that integrates techniques from distributed debugging (event-based debugging, message breakpoints) and proposes facilities to deal with ad hoc, fragile networks -- epidemic debugging, and support for frequent disconnections. A prototype for REME-D is implemented for the AmbientTalk language using the meta-actor protocol provided by AmbientTalk to implement its features."
1623291,14018,8806,A cognition-based interactive game platform for learning Chinese characters,2011,"Evidence observed in classrooms and findings reported in neurolinguistic research have suggested that awareness of the correspondence between graphemes and phonemes in Chinese is instrumental for effective learning of Chinese characters. We collected and analyzed errors in written Chinese characters, and found that phonologically related factors also participated in a large proportion of the reported errors. To verify and evaluate the influence of the phonetic awareness on learning Chinese, we built interactive games for computer assisted learning of Chinese characters. A software tool was implemented to recommend Chinese characters that are phonetically or visually similar, and we applied this tool to assist the task of selecting useful characters in designing the games. We discuss the related findings of ours and the designs of the games, but results of the final evaluation will be available only in the oral presentation."
1492116,14018,8806,Social health data integration using semantic Web,2012,"This project addresses how to link scattered health-related data from different Web communities, and provide integrated knowledge of health information. Specifically, we integrate data from social media-based patient communities, curated sites with expert content, and the research community. Our approach is based on medical concept extraction using the Unified Medical Language System (UMLS), Resource Description Framework (RDF) semantic modeling to represent diverse social health and medical experiences, and summarization of integrated health data. A prototype implementation annotates medical terms occurring in blogs with summarized health experience data, medical expert data and medical research data that enables users, such as patients, doctors or other health care providers to have integrated and linked view of health-related knowledge. Currently, the system integrates information from PatientsLikeMe, WebMD, and PubMed, and can be used to annotate a wide variety of text based blogs. This system uses ontology-based information extraction and semantic modeling of social health data to integrate informally specified information, which is typical of content written by patients."
1810117,14018,8806,Fine-grained annotations for pointcuts with a finer granularity,2013,"A number of authors have suggested that AspectJ-like pointcut languages are too limited, and that they cannot select every possible join point in a program. Many enhanced pointcut languages have been proposed; they require virtually no change to the original code, but their improved expressive power comes often at the cost of making the pointcut expression too tightly connected with the structure of the programs that are being advised. Other solutions consist in simple extensions to the base language; they require only small changes to the original code, but they frequently serve no other immediate purpose than exposing pieces of code to the weaver. Annotations are a form of metadata that has been introduced in Java 5. Annotations have a number of uses: they may provide hints to the compiler, information to code processing tools and they can be retained at runtime. At the moment of writing, runtime-accessible annotations in the Java programming language can only be applied to classes, fields and methods. The support to annotate expressions and blocks feels like a natural extension to Java's annotation model, that can be also exploited to expose join points at a finer-grained level. In this paper we present an extension to the  AspectJ  language to select block and expression annotations in the  @Java  language extension."
1643529,14018,8806,Student research abstract: android malware detection based on Kullback-Leibler divergence,2014,"A recent study shows that more than 50% of mobile devices running Google's Android mobile operating system (OS) have unpatched vulnerabilities, opening them up to malicious applications and malware attacks. The starting point of becoming a potential victim due to malware is to allow the installation of applications without knowing in advance the operations that an application can perform. In particular, many recent reports suggest that malware applications caused unwanted billing by sending SMS messages to premium numbers without the knowledge of the victim [1, 2]. Given that, there is a need for techniques to identify malicious behaviors of applications before installing them."
1582429,14018,8806,Semantics and proof rules of invariant based programs,2011,"Invariant based programming  is an approach where we start to construct a program by first identifying the basic situations (pre- and postconditions as well as invariants) that could arise during the execution of the algorithm. These situations are identified before any code is written. After that, we identify the transitions between the situations, which will give us the flow of control in the program. The transitions are verified at the time when they are constructed. The correctness of the program is thus established as part of constructing the program. The program structure in invariant based programs is determined by the information content of the situations, using  nested invariant diagrams . The control structure is secondary to the situation structure, and will usually not be well-structured in the classical sense, i.e., it is not necessarily built out of single-entry single-exit program constructs.   We study in this paper the semantics and proof rules for invariant-based programs. The total correctness of an invariant diagram is established by proving that each transition preserves the invariants and decreases a global variant. The proof rules for invariant-based programs are shown to be correct and complete with respect to an operational semantics. The proof of correctness and completeness introduces the weakest precondition semantics for invariant diagrams, and uses a special construction, based on well-ordered sets, of the least fixpoint of a monotonic function on a complete lattice. The results presented in this paper have been mechanically verified in the PVS theorem prover."
1439239,14018,8806,Performance and perceptions of human computation games for image tagging,2011,"Applications that use games to harness human intelligence to perform various computational tasks are known as human computation games (HCGs). Most HCGs are collaborative in nature where players to cooperate to score points. Competitive versions, where players work against each other have been argued to address shortcomings of collaborative HCGs. However, there is yet little work done in understanding how different HCG genres affect players' perceptions and performance. In this paper, we focus on image tagging HCGs, where games are used to generate keywords for images. Three versions were created: collaborative HCG, competitive HCG and a control application for manual tagging. The applications were evaluated to examine the quality of the tags generated and users' perceptions of these genres. Results suggest while participants reported liking the collaborative and competitive HCGs over the control application, those using the latter seemed to generate better quality of tags."
1024513,14018,8806,Multi-fingered reactive grasping with active guided camera systems,2012,This paper presents an innovative approach of how to integrate a simple camera into reactive grasping manipulations using the multi-fingered gripper SDH2. The enrichment of blind and reactive grasping systems is the main goal of this idea. Therefore the dextrous 3 finger hand SDH2 is mounted to a robot arm and grasps the camera in order to detect object positions or to measure object dimensions in a scene and therewith to support and facilitate reactive grasping manipulations. Our system provides a flexible way of combining camera hardware and image processing algorithms to carry out exploration of an unknown scene and to detect and grasp objects with high accuracy.
792476,14018,8806,Faster seam carving with minimum energy windows,2014,Content-aware image retargeting is the problem of adapting images to different display sizes and aspect ratios while minimizing distortions to the most important regions of those images. Seam carving is an operator for content-aware image retargeting that iteratively removes 8-connected pixel paths (seams) from an image until a target resolution is reached. Finding optimal seams for seam carving is computationally expensive. We have proposed the concept of minimum energy windows as an approach to reduce the computational load of seam finding. Our results demonstrate that it is possible to find nearly-optimal seams and obtain high quality results with a significant performance improvement.
1882678,14018,8806,Software reuse in agile development organizations: a conceptual management tool,2011,"The reuse of knowledge is considered a major factor for increasing productivity and quality. In the software industry knowledge is embodied in software assets such as code components, functional designs and test cases. This kind of knowledge reuse is also referred to as software reuse. Although the benefits can be substantial, software reuse has never reached its full potential. Organizations are not aware of the different levels of reuse or do not know how to address reuse issues. This paper proposes a conceptual management tool for supporting software reuse. Furthermore the paper presents the findings of the application of the management tool in an agile development organization."
1197055,14018,8806,Composition of HCI evaluation methods for hybrid virtual environments,2011,"Research about interaction design and evaluation of virtual environment (VE) applications is recent and lacks well-established methods and techniques. The focus of this study is to show the results of applying a composition of methods to evaluate the communicability and usability of the HybridDesk, which provides three environments for the interaction with VEs, supporting 3D scene visualization and navigation, as well as 3D object manipulation and annotation. This study performs a qualitative evaluation of the HybridDesk by applying traditional usability evaluation methods, like heuristic evaluation, usage observation sessions, questionnaires and interviews, as well as the communicability evaluation method (CEM), which is based on semiotic engineering. It then compares the evaluation results of these various methods, demonstrating that they all contribute in distinct ways to the evaluation of a hybrid environment. These results also highlight the importance of compatibility among the various signification systems, produced by distinct designers, which a user needs to interpret and understand during interaction."
777141,14018,8806,Radio resource management in coordinated antenna system deployments,2013,"Distributed Antenna Systems (DAS), wherein multiple antennas are distributed across the cell with the antennas being connected to the base station via a wired connector have been proposed as a means to improve coverage and capacity of traditional cellular systems. In this paper, we introduce the concept of Coordinated Antenna System (CAS), a variant of the DAS system wherein the antennas are context aware. A CAS deployment can be visualised as a controller unit and several remote antenna units (RUs) distributed in a physical premises such as an office, a shopping mall etc. with the controller unit handling the resource management of the different RUs connected to it. Even though such an architecture looks promising from the perspective of improving coverage and capacity (through better frequency reuse), interference issues need to be carefully addressed. This paper proposes a resource management mechanism for CAS (called CRRM) and highlights findings from its performance evaluation. CRRM is simple from an implementation perspective, adaptive to changes in the underlying radio environment and helps to avoid/minimise interference as indicated by the findings from the performance evaluation."
1531876,14018,8806,Pervasive ecosystems: a coordination model based on semantic chemistry,2012,"Future and emerging pervasive computing systems call for new service models and coordination approaches enforcing self-organisation as an inherent property of component interaction. We introduce the concept of a pervasive ecosystem, and present the coordination approach grounded upon it, which revolves around ( i ) the notion of a distributed and dynamic space of live semantic annotations (wrapping data, knowledge, and activities of humans, devices, and services) and ( ii ) a set of chemical-resembling coordination rules that are applied to such annotations semantically. As an application example we present a simulated scenario of crowd steering in an exhibition centre."
1042839,14018,8806,RetriBlog: a framework for creating blog crawlers,2012,"Blogs are becoming an important social tool. By means of blogs, bloggers share their likes and dislikes, express their opinions, report news and form groups related to some subjects. Thus, the available information on the Blogsphere can certainly helps in the creation of interesting applications in various domains, such as e-learning, e-commerce, and e-government. However, due to the increasing number of blogs posted every day on the Web, and the dynamic nature of the Blogsphere, the tasks of collecting and extracting relevant information from blogs have become hard and time consuming. In this paper, we use techniques both from information retrieval and information extraction fields to deal with this problem. Since the blogs have many points of variability it is necessary to provide applications that can be easily adapted. We present the RetriBlog system, a framework for the development of blog crawlers dealing the variations in blogs. This paper presents the RetriBlog details and an evaluation of the proposed algorithms."
1546986,14018,8806,Performance of MIMO 802.11n in heterogeneous deployments,2012,"802.11n-based Wireless Local Area Networks (WLANs) are likely to be used in a network that also accommodates other legacy devices. This heterogeneous deployment of different technologies in the same area raises a legitimate question about the effect of legacy nodes on the 802.11n high throughput ones and consequently on the overall network performance. This paper presents results of empirical experiments for studying and providing insight on the mutual impact of legacy 802.11bg devices and 802.11n-based devices when they share the same network. The results show that high throughput devices only provide the specified objective rates when they operate independently from legacy devices. Hence, coexistence mechanisms are crucial for the network to achieve the desired performance."
1766742,14018,8806,Evolutionary optimization of wetlands design,2013,"Wetlands are artificial ponds, designed to filter and purify running water through the contact with plant stems and roots. Wetland layouts are traditionally designed by experts through a laborious and time-consuming procedure: in principle, small patches of vegetation with purifying properties are tentatively placed, then the resulting water flow is verified by fluid dynamics simulators and when a satisfying outcome is reached, the wetland final layout is decided. This paper proposes to automate wetland design exploiting an evolutionary algorithm: a population of candidate solutions is cultivated by the evolutionary core, and their efficiency is evaluated using a state-of-the-art fluid-dynamics simulation framework. Experimental results show that the results obtained by the proposed approach are qualitatively comparable with those provided by experts, despite the complete absence of human intervention during the optimization process."
2132778,14018,8806,"Combining self-organisation, context-awareness and semantic reasoning: the case of resource discovery in opportunistic networks",2013,"The increasing prevalence of networked devices brings ever more opportunities for delivering content and services to users that result from situated interactions between computational devices in their surrounding environment. Resource discovery, a vital component in this process, becomes challenging in such an open, dynamic and distributed setting. Building on earlier work that outlined a novel semantics-based approach to resource discovery in such environments, this paper provides a general solution to incorporating application-specific contextual factors into the resource discovery process, and proposes a mechanism to support the runtime evolution of resource discovery tasks in a mobile setting."
1561965,14018,8806,DADS: dynamic and automatic disk scheduling,2012,"The selection of the right I/O scheduler for a given workload can greatly improve the performance of a system. But, this is not an easy task because several factors should be considered, and even the scheduler deemed the best can change at any moment. So, we present a Dynamic and Automatic Disk Scheduling framework ( DADS ) that compares different Linux I/O schedulers and automatically and dynamically selects that which achieves the best performance for any workload. The implementation described here compares two schedulers by running two instances of a disk simulator inside the Linux kernel, each one having a different scheduler. Our proposal compares the schedulers' service times, and changes the scheduler in the real disk if the performance is expected to improve.  DADS  has been analyzed by using different workloads, hard disks, and schedulers. Results show that it selects the best scheduler of the two compared at each moment, improving the performance and exempting system administrators from selecting a suboptimal scheduler."
1474185,14018,8806,SageRobotics: open source framework for symbolic computation of robot models,2012,"This paper presents the  SageRobotics  framework to symbolically compute robot dynamics. Such framework is open source and is built upon the freely available and open source  Sage  mathematical software. Given a set of Denavit-Hartenberg parameters describing a serial robot the framework is capable of constructing geometric, kinematic and dynamic models. It offers both Euler-Lagrange and Recursive Newton-Euler formulations for dynamic equation generation, including inverse dynamics, isolated dynamic equation terms and linear to parameters form. The calculation of a minimal base parameter set is also a key feature of this framework. Tools to generate code from symbolic equations are included, enabling the construction of fast numerical functions."
1456104,14018,8806,Student research abstract: mechanisms to ensure quality of service for the internet of things,2014,"This Student Research Abstract proposes the development of QoS mechanisms to the Internet of Things -- IoT paradigm. Due to the integration of various technologies in the IoT and the expected high number of nodes, new requirements emerge to existing standards such as RFID, Wireless Sensor Networks, Near Field Communication, among others. Specially the problem of collisions can cause significant delays in identifying and increase the power consumption of the reader. This fact creates new challenges to be investigated to ensure the requirements of Quality of Service in this new paradigm."
1672915,14018,8806,LOCCAM - loosely coupled context acquisition middleware,2013,"Sensors of all kinds are being integrated with mobile and portable devices (tablets, smartphones). This opens up the possibility of context-aware applications to effectively be able to adapt their behavior, user interfaces and content according to the current user's situation. Frequently, context-aware applications require an infrastructure for acquisition, aggregation and reasoning of contextual information. However, existing context management infrastructures are not always appropriated to the heterogeneous and particular environment of mobile devices. In this paper, we present a context management middleware called LoCCAM (Loosely Coupled Context Acquisition Middeware) to provide self-adaptive acquisition of contextual information. It can execute both locally, on a single device, or distributed among nearby devices. The middleware proposes a model for publication, and notification of contextual information based on tuple spaces. As consequence, it offers a lower coupling among applications and the context acquisition layer. In this paper, we also present a performance evaluation of the adaptation mechanism."
1199285,14018,8806,Toward a visual pedometer,2012,"Traditional inertial-sensor-based pedometers are not user-friendly because of the limited availability of inertial sensors in existing mobile computing devices. To remedy the drawback, we propose a visual pedometer scheme. It employs a body-mounted camera to capture images and measures the number of steps based on a time-series analysis of image features. We address several key issues raised in the measurement and demonstrate the applicability of the proposed visual pedometer through a four-second experiment. Compared with traditional inertial-sensor-based pedometers, the proposed visual pedometer could be more user-friendly because of the extensive use of camera phones and digital cameras."
1382614,14018,8806,Self-managing and self-organising mobile computing applications: a separation of concerns approach,2014,"Although the research area of self-organising systems is well established, their construction is often  ad hoc.  Consequently, such software is difficult reuse across applications that require similar functionality of have similar goals. The development of self-organising applications and,  a fortiori , self-organising mobile applications is therefore limited to developers who are experts in specific self-organising mechanisms. As a first step towards addressing this, this paper discusses the notion of self-organising mechanisms provided as services for building higher level functionality in a modular way. This eases reuse and thus provides separation of concerns. Additionally, because of the dynamic and heterogeneous nature of mobile networks, services need to adapt themselves in order to ensure both functional and non-functional requirements. This paper discusses whether the self-management of self-organising mobile applications can be achieved in a modular fashion, via the self-management of low level self-organising services it employs, rather than considering the management of the complex system as a whole. We empirically investigate two non-functional aspects: resource optimisation and accuracy."
1653577,14018,8806,Evaluating a process for developing a capability maturity model,2013,"Maturity Models have been proven to be powerful tools to assess to current state of an organization regarding a certain aspect and drive improvement. However, maturity models are often developed ad hoc, without following a well-documented design and development method, and often do not provide a pathway to further extend and update the model to foster systematic enhancements and extensions. This paper discusses a systematic approach to maturity model development and applies it to the concrete domain of long-term information management. We trace the steps from problem definition to maturity model evaluation and apply the maturity model to a specific organizational scenario."
1769382,14018,8806,Kernel-level time composability for avionics applications,2013,"State-of-the-art approaches to the development and analysis of real-time embedded systems assume seamless composition of the functional and timing behaviour of the distinct applications that compose the system. Unfortunately, the sharing of complex and stateful hardware resources is a serious threat to time composability: the dependences produced by their history of use cannot always be accurately accounted for by state-of-the-art timing analysis techniques. More recently, the attention paid to the contribution that the Real-Time Operating System (RTOS) can make to achieving -- or breaking -- time composability has grown. In this paper we present and experimentally evaluate a proof-of-concept implementation of a time-composable RTOS design concept in the specific incarnation as an ARINC-compliant partitioned system for avionics applications."
1283758,14018,8806,Student research abstract: an optimized handover scheme based on media independent handover standard IEEE 802.21,2014,"In this article, we proposed an optimized handover management scheme for heterogeneous wireless networks. This scheme obtains information of available access networks from Media Independent Information Server (MIIS) and optimized the data rate, cost, and link quality for a Mobile Node (MN) for selecting best network among heterogeneous networks. Furthermore, this scheme efficiently minimized false handover indication in the regions where number of handover is frequent. Our proposed scheme efficiently enables an MN to select an access network with required resources. Based on simulation results, our proposed scheme perform efficiently then the available schemes used for similar purpose."
1440678,14018,8806,Mechanism design for decentralized vehicle routing problem,2012,"In this paper, we present a  strategyproof  mechanism design to tackle the multi-depot vehicle routing problem in multi-agent setting. In a  small-world  network environment (any node is connected to every other node through a short path) wherein different self-interested agencies are controlling their own vehicle fleets, we use an innovative game theoretic approach to distribute products to customers without any central authority. The game is using a reverse Vickrey auction that takes place in several rounds until all customers are assigned. Payments are given to depots as incentive for fairness of serving cost offering. The procedure leads to overall near-optimal routing for serving all customers. We briefly describe the use of a heuristic approach to solve the game-based procedure in bounded time and memory. We also provide benchmarks for several known problem instances."
1093459,14018,8806,Restoration of motion blurred document images,2012,"Motion blur often decreases the quality of document image and makes the text information within the document images unreachable by optical character recognition (OCR) or by a person. This paper presents a blur correction technique that aims to correct motion blur within document images. Given a blurred document image, an alpha channel map is first constructed based on specific image characteristics that are associated with text documents. Several blur parameters including blur direction and blur extent are then estimated from the constructed alpha channel map. Finally the blurred document image is restored by using Richardson-Lucy deconvolution technique based on the estimated blur parameters. Experiments on a number of document images with motion blur show that the proposed technique improves the document visual quality as well as the OCR performance significantly."
2069098,14018,8806,A Kalman filter based approach to probabilistic gas distribution mapping,2013,"Building a model of gas concentrations has important industrial and environmental applications, and mobile robots on their own or in cooperation with stationary sensors play an important role in this task. Since an exact analytical description of turbulent flow remains an intractable problem, we propose an approximate approach which not only estimates the concentrations but also their variances for each location. Our point of view is that of sequential Bayesian estimation given a lattice of 2D cells treated as hidden variables. We first discuss how a simple Kalman filter provides a solution to the estimation problem. To overcome the quadratic computational complexity with the mapped area exhibited by a straighforward application of Kalman filtering, we introduce a sparse implementation which runs in constant time. Experimental results for a real robot validate the proposed method."
1034826,14018,8806,Design of an automatic demand-side management system based on evolutionary algorithms,2014,"Demand-Side Management (DSM) refers to programs that aim to control the energy consumption at the customer side of the meter. Different techniques have been proposed to achieve this. Perhaps the most popular techniques are those based on smart pricing (e.g., critical-peak pricing, real-time pricing). The idea, in a nutshell, is to encourage end users to shift their load consumption based on the price at a particular time (e.g., the higher the price, the less number of electric appliances are expected to be turned on). Motivated by these techniques (e.g., a strong positive correlation between the number of appliances being used and the electricity cost), we propose the use of an stochastic evolutionary-based optimisation technique, Evolutionary Algorithms, to  automatically  generate optimal, or nearly optimal, solutions that represent schedules to charge a number of electric vehicles (EVs) with two goals: (a) that each EV is as fully charged as possible at time of departure, and (b) to avoid charging them at the same time, whenever possible (e.g., load reduction at the transformer level). Instead of using a price signal to shift load consumption, we achieve this by considering what all the EVs might do at a particular time, rather than considering an interaction between an utility company and its user, as normally adopted in DSM programs. We argue that exploiting the interaction of these EVs is crucial at achieving excellent results because it carries the notion of smart pricing (e.g., balance energy usage), which is highly popular in DSM systems. Thus, the main contribution of this work is the notion of load shifting, borrowed from smart pricing methods, implemented in an evolutionary-based algorithm to automatically generate optimal solutions. To test our proposed approach, we used a dynamic scenario, where the state of charge of each EV is different for every day of our 28 days testing period. The results obtained by our proposed approach are highly encouraging in both: EVs being almost fully charged at time of the departure and the transformer load being reduced as a result of avoiding turning on the EVs at the same time."
1353318,14018,8806,A personalized model for monitoring vital signs using camera of the smart phone,2014,"Smart phones with optical sensors have created new opportunities for low cost and remote monitoring of vital signs. In this paper, we present a novel approach to find heart rate, perfusion index and oxygen saturation using the video images captured by the camera of the smart phones with mathematical models. We use a technique called principal component analysis (PCA) to find the band that contain most plethysmographic information. Also, we showed a personalized regression model works best for accurately detecting perfusion index and oxygen saturation. Our model has high accuracy of the physiological parameters compared to the traditional pulse oxymeter. Also, an important relationship between frame rate for image capture, minimum peak to peak distance in the pulse wave form and accuracy has been established. We showed that there is an optimal value for minimum peak to peak distance for detecting heart rate accurately. Moreover, we present the evaluation of our personalized models."
1338128,14018,8806,Intention based semantic approach for service sourcing,2012,"The evolution of Service Oriented Architectures (SOA) has complicated the choice of the best service to the client who is facing a big number of offers. Given this multiplicity, the client becomes unable to distinguish between service offerings in order to select the best service among them especially when providers and customers don't share the same knowledge degree. In our research activities, we aim to help the client to select the most appropriate service provider according to his intentions by giving him the ability to freely express his requirements using his own knowledge. In this paper, we propose a novel semantic approach for service sourcing based on ontologies. We used a semantic matching step and we proposed a method to evaluate each provider's offer according to its appropriateness to the client's requirements in order to choose the best offer."
1025709,14018,8806,A low-latency service composition approach in mobile ad hoc networks,2014,"In order to offer complex services to the users, separate services located at different devices in MANET should be composed in a mobile ad hoc network. A distributed approach to search for the  Service Composition Path (SCP)  with a low latency is proposed, which is based on two methods,  Path Filtering  and  Path Combination.  These two methods avoid unnecessary message transmissions, and greatly improve the searching efficiency. The experiment results show the superiorities of the approach to its counterpart."
1066963,14018,8806,Investigation of hungarian mating schemes for genetic algorithms,2014,"Mating scheme is the way of selecting two parents to make offspring. It takes effect on the performance of genetic algorithms. In this paper, we investigate mating schemes using the Hungarian method. The schemes include i) minimizing the sum of matching distances, ii) maximizing the sum, and iii) random matching for comparison. We apply the schemes to well-known combinatorial optimization problems, the traveling salesman problem and the graph bisection problem, and analyze how the quality of the best individual changes over generations. Based on the analysis, we finally suggest a new hybrid mating scheme. The suggested scheme showed better performance than the non-hybrid schemes."
1466219,14018,8806,Power management schemes for heterogeneous clusters under quality of service requirements,2011,"For modern computer systems, both performance and power consumption must be considered to reduce the maintenance cost for quality of service guarantees. This paper proposes efficient and effective power management schemes for heterogeneous clusters. Distinct from existing heuristic approaches, we propose power management schemes with approximation factor guarantees, compared to the optimal power management. Our greedy power management schemes have 1.5-approximation or 2-approximation guarantees depending on the complexity. We also propose dynamic-programming approach which can trade the quality of the resulting solutions with different time/space complexity. Simulation results wrt different power consumption models show that the proposed schemes are effective for the minimization of the power consumption for large scale clusters."
899246,14018,8806,Using maude rewriting system to modularize and extend SQL,2013,"One of the deficiencies of SQL is that it allows little practical support for factoring, parametrization and modularization of complex SQL statements. Using dynamic SQL or macroprocessors for this purpose is error prone and inconvenient. This paper shows how the rewriting systems such as Maude can be used to build better SQL preprocessors which understand the SQL syntax and thus reduce the possibility of error. Rewriting systems also allow creating of language extensions supporting modularity and data model abstractions. We use as an example the Maude based system currently developed at our department."
1273673,14018,8806,Traffic 411: a traffic congestion routing and awareness platform for Nairobi,2014,"According to UN-HABITAT, the city of Nairobi loses half a million USD daily due to congestion on roads designed for a city 10 times smaller. To address the traffic congestion problem in Nairobi, we develop a platform called Traffic 411 that provides drivers with real-time traffic and routing information. Traffic 411 incorporates  locally relevant context  (such as references to landmarks) to predict congestion and create traffic awareness. Our work extends a novel approach called  Frugal Innovation  developed at the IBM Tokyo Research Lab (TRL), where Web cameras covering only 3.5% of the roads in Nairobi are used to estimate traffic conditions. We deployed the Traffic 411 platform in Nairobi and our initial evaluation indicates that Traffic 411 enhances the driving experience and can be deployed in similar cities."
1154458,14018,8806,Implementing Java-like languages in Xtext with Xsemantics,2013,"In this paper we present Xsemantics, a DSL for writing type systems, reduction rules and in general relation rules for languages implemented in Xtext. Xsemantics aims at minimizing the gap between the formalization of a language and the actual implementation in Xtext, since it uses a syntax that resembles the typical rules in a formal setting. We present an implementation of FJ (Featherweight Java) in Xtext together with its type system and operational semantics in Xsemantics. We show how such implementation is pretty faithful to the actual formalization of FJ."
760441,14018,8806,An Active XML-based framework for integrating complex data,2012,"Data integration is a critical problem in data warehousing and decision-support systems. Traditional data integration systems are very successful in integrating structured data, but structured data represent only a small subset of interesting data that could be warehoused by many enterprises. Current data integration systems also lack of self-managing capabilities. Therefore, we propose a data integration framework for integrating complex data  actively . The purpose of our framework is twofold. Firstly, it integrates complex data using Web standards into an Active XML (AXML) repository. Secondly, beside warehousing logged events into event repository, it exploits active rules and framework events mining to self-manage, automate and activate different data integration tasks. Finally, we have implemented a prototype framework as a web application."
908090,14018,8228,Multipath-assisted maximum-likelihood indoor positioning using UWB signals,2014,"Multipath-assisted indoor positioning (using ultrawideband signals) exploits the geometric information contained in deterministic multipath components. With the help of a-priori available floorplan information, robust localization can be achieved, even in absence of a line-of-sight connection between anchor and agent. In a recent work, the Cramer-Rao lower bound has been derived for the position estimation variance using a channel model which explicitly takes into account diffuse multipath as a stochastic noise process in addition to the deterministic multipath components. In this paper, we adapt this model for position estimation via a measurement likelihood function and evaluate the performance for real channel measurements. Performance results confirm the applicability of this approach. A position accuracy better than 2.5 cm has been obtained in 90% of the estimates using only one active anchor at a bandwidth of 2GHz and robustness against non-line-of-sight situations has been demonstrated."
1464071,14018,8806,Web 3.0 in action: Vector Space Model for semantic (movie) Recommendations,2012,"In this paper we present  MORE  (acronym of   MORE  than  MO vie  RE commendation ), a  Facebook  application that semantically recommends movies to the user leveraging the knowledge within  Linked Data  and the information elicited from her profile.  MORE  exploits the power of social knowledge bases (e.g.  DBpedia ) to detect semantic similarities among movies. These similarities are computed by a Semantic version of the classical Vector Space Model (sVSM), applied to semantic datasets.  MORE  is freely available as a  Facebook  application."
1570732,14018,8806,iDetective: a persuasive application to motivate healthier behavior using smart phone,2011,"Until now, many kinds of persuasive applications have been developed, and latest persuasive applications have utilized users' contexts acquired by several sensors for more effective persuasion. Currently most applications assume that those applications run with special devices that are equipped with sensors for acquiring those contexts. The use of such special devices may deter would-be users because they do not have a habit of getting on such additional devices. In this paper, we introduce a persuasive application that uses users' contexts acquired by mobile phone. Through our experiences in the application, we clarify whether current mobile phone is enough to acquire users' contexts and to persuade users or not."
1366566,14018,8806,Philos: a sociable robot for human robot interactions and wireless health monitoring,2012,"This paper presents  Philos , a socially interactive robot designed for use in homes of those who need continual care, such as the elderly or people with disabilities. Philos is capable of daily health monitoring achieved by a wearable health monitoring device (WHMD) as well as emotional stimulation through human-robot social interactions (HRSI). This system also includes PhiloManager, a software interface for robot programming and data display. HRSIs are enabled through touch- and vision-based inputs. A behavioral framework has been developed so that Philos can be customized to exhibit a variety of behavioral characteristics and therefore achieve realistic and dynamic interactions."
1497983,14018,8806,Speaking in tongues: SQL access to NoSQL systems,2014,"Non-relational data stores, which are usually called NoSQL systems, have become an important class of data management systems. They often outperform the relational systems. Yet there is no common way to interface with NoSQL systems. The Structured Query Language (SQL) has already proven to be useful to provide a uniform query language for all the relational systems. We identify a subset of SQL for access to NoSQL systems. Our extensible middleware translates SQL queries to the query languages of the connected NoSQL systems. The migration between these systems is thereby greatly simplified as well as the comparison of the supported NoSQL systems."
1418393,14018,8806,BenchDW: a generic framework for biological data warehouse benchmarking,2013,"The rapid development of - omics  techniques have provided an unprecedented amount of data, enabling system-wide biological research. However, the success of systems biology is contingent on the ability to integrate a wide variety of types of biological data to automatically predict, assign functional annotations of proteins and perform comparative analyses. Although each biological data integration system presents to some extent a number of desirable features, none of them meets all the requirements for effective integration of system-wide data. In this paper, we present BenchDW, a generic and flexible benchmark framework that aims at facilitating the evaluation and quantification of the capabilities of those biological data warehouses. It currently comprises 22 different metrics ranging from documentation quality to accuracy and response times, which may be recorded for different hardware configurations. Each metric can be weighted to better suit the user's specific needs and compared to the gold standard. BenchDW was designed to be flexible, easy to use and offers many benefits over spreadsheets, thus presenting the characteristics required to facilitate acceptance by the scientific community. We demonstrate the utility of BenchDW by briefly reviewing three data warehouses (BioMart, BioXRT and InterMine) and by showcasing how it can be leveraged to identify the specificities of the systems of interest. BenchDW is available online at http://warehousebenchmark.fungalgenomics.ca/benchmark/benchdw/index.html under the GNU GPLv3 license."
1225916,14018,8806,Improving high-performance computations on clouds through resource underutilization,2011,"We investigate the effects of shared resources for high-performance computing in a commercial cloud environment where multiple virtual machines share a single hardware node. Although good performance is occasionally obtained, contention degrades the expected performance and introduces significant variance. Using the DGEMM kernel and the HPL benchmark, we show that the underutilization of resources considerably improves expected performance by reducing contention for the CPU and cache space. For instance, for some cluster configurations, the solution is reached almost an order of magnitude earlier on average when the available resources are underutilized. The performance benefits for single node computations are even more impressive: Underutilization improves the expected execution time by two orders of magnitude. Finally, in contrast to unshared clusters, extending underutilized clusters by adding more nodes often improves the execution time due to an increased parallelism even with a slow interconnect. In the best case, by underutilizing the nodes performance was improved enough to entirely offset the cost of an extra node in the cluster."
975397,14018,8806,Programming coordination laws of artifacts in CArtAgO,2011,"The goal of this work is the proposal of a language for programming coordination artifacts, providing a better level of abstraction than than achieved via imperative programming languages. We propose a declarative logic language based on reactive rules to define coordination artifacts implementing both well-known and novel coordination paradigms. A prototype built on top of CArtAgO, and relying on the tuProlog Prolog engine, is also presented, where different coordination paradigms realized upon the language are shown."
1097817,14018,8806,Modeling for context-aware healthcare service using ontology,2014,"Recently, as smart devices are equipped with various kinds of sensors and wireless network interface, many studies in the field of U-healthcare service have been in progress to provide smart device based medical applications to patients or healthcare providers. This paper is a variety of health-related Ontology available on the Internet and context inference-based intelligent healthcare information services. The purpose of this paper is to build context ontology for the user's healthcare service environment by making a model and to provide real-time intelligent healthcare service which has high satisfaction to the user by defining inference rules based on this."
1230681,14018,8806,Building an on-demand virtual computing market in non-commercial communities,2013,"This work describes a system that enables non-commercial communities, e.g. students and researchers, to deploy pre-configured clusters of virtual machines on arbritary cloud computing stacks implementing the Open Cloud Computing Interface (OCCI). As a key enabling technology, the virtualization and provisioning available in such infrastructure as a service clouds is leveraged in order to instantiate and setup clusters on a user's demand. A simple, web-based market system is provided as the user interface. It allows the browsing, configuring and launching of different clusters of appliances. Running virtual machines are accessible directly from the market via integrated remote desktop software."
2362844,14018,369,A Collaborative Context Prediction Technique,2011,"The prediction of contexts plays an important part in the field of context aware systems and environments for adapting services proactively to users' needs. To the best of our knowledge, most research literature on context prediction focused on forecasting a user's contexts only using his available context history. In the case of a user suddenly changing his behaviour in an unexpected way, the context history of the user does not provide future context information for the observed pattern. Hence context prediction algorithms will fail to forecast the appropriate future context. To overcome the gap of missing context information in the user's context history, we propose the Collaborative Context Prediction (CCP) approach. Our results show that the proposed CCP approach is able to give accurate predictions in the absence of needed context information and outperforms the Active LeZi method."
1353914,14018,8806,Checking the realizability of BPMN 2.0 choreographies,2012,"Choreographies allow business and service architects to specify with a global perspective the requirements of applications built over distributed and interacting software entities. While being a standard for the abstract specification of business workflows and collaboration between services, the Business Process Modeling Notation (BPMN) has only been recently extended into BPMN 2.0 to support an interaction model of choreography, which, as opposed to interconnected interface models, is better suited to top-down development processes. An important issue with choreographies is real-izability,  i.e ., whether peers obtained via projection from a choreography interact as prescribed in the choreography requirements. In this work, we propose a realizability checking approach for BPMN 2.0 choreographies. Our approach is formally grounded on a model transformation into the LOTOS NT process algebra and the use of equivalence checking. It is also completely tool-supported through interaction with the Eclipse BPMN 2.0 editor and the CADP process algebraic toolbox."
842940,14018,8806,Towards designing a tool for event reconstruction using Gladyshev Approach,2011,"Event reconstruction is the process that explains the 'how' and 'why' of an evidence in a digital investigation. Formalization of this process is essential because the evidence is of legal value which requires a logical reasoning. A finite state machine approach and formalization of the event reconstruction problem had been proposed by Gladyshev. The authors present their model for a tool for event reconstruction built upon Gladyshev formalization. The model is formulated by analyzing a case study, of hidden/deleted files, and forms the first step towards the design of a generic tool for event reconstruction."
1299785,14018,8806,Design and implementation of a personal health monitoring system with an effective SVM-based PVC detection algorithm in cardiology,2014,"In this paper, we present a bio-health monitoring system prototype specialized in capturing the Premature Ventricular Contraction (PVC) event, one of the major cardiac disorder events. The proposed bio-health system comprises three parts: (1) the Electrocardiograph (ECG) sensing hardware, (2) the Android-based processing and communication device, and (3) the expert system on the cloud to detect PVC events. The expert system on the cloud is designed and implemented based on the support vector machine (SVM). The effective identification of PVC can help patients take care of the health quickly. The main purpose of recording information is to track the patient's health status, and allow the medical team to keep tracking the recovery status."
1221962,14018,8806,Application of partial-order methods for the verification of closed-loop SDL systems,2011,"This article is concerned with the verification of closed-loop asynchronous reactive systems. Such systems, specified for instance with the industrial SDL (Specification and Description Language) language, communicate with their environment through buffers which memorize occurrences of events. Such a communication mechanism is quite interesting for specifying systems connected to several asynchronous external actors. However, it leads to a verification model possibly composed of a huge number of states (due to the state-space of the buffers). This article shows how this combinatorial explosion could be reduced by specifying the environment of the system to be verified, and by using partial-orders methods both on the system and its environment.   After presenting the formal modeling languages SDL (for the reactive system) and CDL Context Description Language (for its environment), the main points of our work are two-fold: (1) we define an independence relation between input events for a given specification  C, S , p > where  S  is the specification of the system (in SDL),  C  is the behavior of its external environment (in CDL), and p the property to verify. The key point is that this independence relation is separately computed on  S, C  and p, without building the global synchronization product of the system; (2) we apply the Mazurkiewicz theory for defining the set of scenarios (sequences of input events) which exactly covers the environment  C  and which is sufficient for verifying p on  S . We finally show on two industrial case-studies that this approach leads to an interesting reduction in verification time."
1219974,14018,8806,A proof-based approach to verifying reachability properties,2011,"This paper presents a formal approach to proving temporal reachability properties, expressed in CTL, on B systems. We are particularly interested in demonstrating that a system can reach a given state by executing a sequence of actions (or operation calls) called a path. Starting with a path, the proposed approach consists in calculating the proof obligations to discharge in order to prove that the path allows the system to evolve in order to verify the desired property. Since these proof obligations are expressed as first logic formulas without any temporal operator, they can be discharged using the prover of AtelierB. Our proposal is illustrated through a case study."
1029241,14018,8806,A particle-spring approach to geometric constraints solving,2011,"Current iterative numerical methods, such as continuation or Newton-Raphson, work only on systems for which the corresponding matrix is a square one. The geometric constraint systems need thus either to have no degrees of freedom, or to be a system the software can anchor,  i. e.  a rigid system.   In this article, we propose a new iterative numerical approach which can handle both rigid and under-rigid geometric constraint systems. It is based on the translation of the system under the form of a particle-spring system where particles correspond to the geometric entities and springs to the constraints. We show that consistently over-constrained systems are also solved.   We show that our approach is promising by giving results of a prototype implementation. We propose tracks for enhancements of the approach which could tackle its drawbacks (mainly stability)."
1226904,14018,8806,CCDR-PAID: more efficient cache-conscious PAID algorithm by data reconstruction,2012,"In this paper, we propose the CCDR-PAID algorithm which is a technique to improve CPU cache utilization of sequential pattern mining more efficiently as an extension of existing CC-PAID which is our previous work. Compared to PAID, CC-PAID improves temporal locality by changing the access pattern to data structures and processing multiple sequential patterns with a common prefix at a time to reduce the memory access latency by suppressing CPU cache misses. In this paper, we extend CC-PAID and dynamically reconstruct data structures, so that unnecessary data access to them can be avoided. The experimental results showed that CCDR-PAID executes up to 25% faster than CC-PAID because it sufficiently reduces cache misses and, therefore, provides better CPU cache utilization due to compaction of the data structure."
1527166,14018,8806,Ontology driven builder pattern: a plug and play component,2014,"We separate and encapsulate the builder pattern logic in an ontology component which increases the reusability of the pattern at the implementation level as well. By extracting the pattern logic into a separate component, it becomes easy to change the classes participating in the pattern even at runtime. The build sequence of different parts of a product can be changed even at runtime by users thus enhancing the adaptability and flexibility of the pattern. It becomes easy to add and remove the pattern to and from the application code and thus the pattern becomes a plug and play component."
2486466,14018,8806,A solution for personalized t-learning applications integrated with a web educational platform,2011,"The Interactive Digital TV may be the main source of digital inclusion in the world besides it being a means of spreading education to a greater number of people in anywhere. In this new form of interaction, the concept of learning based on interactive TV (t-learning) arises, which it will make a simple viewer in a student that would have access to video-lesson, questions, exercises, additional information, among others. However, there are barriers that still inhibits the growth process of applications based on this concept, especially by providing a personalized learning environment. Thus, it is proposed in this work to develop a solution for t-learning using a Web infrastructure showing aspects from building a personalized educational application and evidencing the integration between the iDTV and e-learning technologies by interoperability provided from the services of a framework for building interactive and semantic learning environments on the Web, Massayo-F."
982867,14018,8806,A framework for evaluating trust of service providers in cloud marketplaces,2013,"The Cloud Security Alliance (CSA) provides a framework for cloud platform providers that manages standardized self-assessments regarding security controls. The framework as it stands does not allow consumers to specify and check their own requirements, nor does it contain any means for verifying the capabilities claimed by the providers. From a customer perspective, both these aspects are essential for evaluating the trustworthiness of cloud providers and for making an informed decision. We propose a novel concept for verifying the capabilities captured in the CSA's framework, plus a decision model that checks consumer requirements against the verification results. Our capability verification combines hard trust based on rigid validation with soft trust based on evidence about past behaviour. Elaborate formal methods are applied in both fields and combined into a single concept."
2103187,14018,8806,Computing gene functional similarity using combined graphs,2012,"The Gene Ontology has been used extensively for measuring the functional similarity among genes of various organisms. All the existing gene similarity methods use either molecular function or biological process taxonomies in computing gene similarity. In this paper, we apply an algorithm for combining graphs to connect the molecular function (F) and biological process (P) taxonomies into one FP taxonomy graph. We then measure the functional similarity of two genes using the resulting FP graph with path length function. The two aspects of GO, molecular function and biological process, are combined by connecting F nodes with P nodes using gene ontology annotation, GOA, databases. By combining two GO graphs, we can have more comprehensive way to explore the functional relationships between genes. We conducted the evaluation on a dataset of OMIM disease phenotypes to estimate the similarity of disease proteins from various diseases."
1219150,14018,8806,Integrating IEEE 11073 and constrained application protocol for personal health devices,2014,"Nowadays, new challenges are presenting new opportunities for healthcare services in the Internet. The increasing availability of connected Personal Health Devices (PHDs) enables a new type of information to be available in the Internet: health information. However, most of these devices use proprietary protocols, creating vertical solutions where one device just talks to one service. In this context, this paper presents a proposal for the use of the Constrained Application Protocol (CoAP) for PHDs communication, using IEEE 11073 as base health protocol. It is discussed how to carry IEEE 11073 information over CoAP messages and, at the end, evaluation results are presented in comparison with other transport mechanisms."
1252536,14018,8806,Towards the establishment of supporting mechanisms for modeling and generating educational content,2011,"Content modeling plays a fundamental role in the development of educational modules, helping the author to determine the main concepts to be taught and providing a systematic way to structure the relevant parts of the knowledge domain. Despite its relevance, there are few approaches for modeling educational content. In this perspective, an integrated approach for content modeling, named  IMA--CID , was proposed.  IMA--CID  is composed of a set of models, each one considering specific aspects of the development of educational content; however, applying it without an automated support can be an error-prone activity. Motivated by this scenario, in this paper we describe  IMATool  -- a supporting tool for content modeling, particularly designed for helping the open and distributed construction of the EMAIL@IMA--CID models. Mechanisms for content generation are also available. We illustrate our ideas by applying  IMA--CID  and  IMATool  in the development of an educational module for the software testing domain. The preliminary results obtained provide evidences on the practical use of such mechanisms for modeling and generating content."
1872375,14018,8235,Analytics for similarity matching of IT cases with collaboratively-defined activity flows,2011,"Handling IT support cases efficiently is very important for operational excellence of IT organizations. Many IT service centers receive thousands of cases per day, some of which are similar to previously reported cases. To improve efficiency it is important to build upon lessons learned from past cases in the resolution of new cases. Therefore, a desired functionality of case management tools is finding similar previous cases to an open one, in order to leverage information about previous cases to effectively find resolution. A new generation of tools for IT case management, e.g., IT Support Conversation Manager, enables collaborative and adaptive process definition for IT case resolution. Leveraging collaborative and social networking technology makes the case information model increasingly richer and more structured compared to flat textual format case reports in traditional IT case management systems. We have developed an automated method for matching IT support cases that takes into account multiple information attributes including the collaborative flow of activities during case handling. We evaluated the system and the early evaluation results show that this method achieves a higher accuracy and comparable efficiency to text-based similarity approaches."
1585540,14018,8806,Predicting change propagation impacts in collaborative business processes,2014,"During the life cycle of a Business-to-Business (B2B) collaboration, companies may need to redesign or change parts of their service orchestrations. A change request proposed by one partner will, in most cases, result in changes to other partner orchestration. An accurate prediction of the behavior of a change request and an analysis of its impacts on the collaboration allows to avoid significant costs related to unsuccessful propagation, e.g. negotiation fail. This paper focuses on predicting the likelihood of a change request propagation as well as its ripple effects on the overall collaboration. To estimate these values, the approach analyses the collaboration structure through a priori analysis. We will show how the prediction models can be specified and implemented within a proof-of-concept prototype. Discussion will be provided on visualization possibilities and model validation."
818650,14018,8806,A novel approach for interactive debugging of dynamic dataflow embedded applications,2013,"In this paper, we propose a new approach for source-level dataflow debuggers. Going beyond their long-established ability to support sequential programming languages, we describe the functionalities a debugger should be able to provide to debug embedded and parallel dataflow applications. Then we demonstrate our solution to this problem with a proof-of-concept debugger targeting the dataflow framework used on an industrial MPSoC platform. We also explain the development challenges we faced during the implementation of this GDB-based debugger."
893163,14018,8806,An application of circumscribed circle filter in the Multi-Stencils Fast Marching method,2012,"We develop an effective method for improving the segmentation result based on the Multi-Stencils Fast Marching method (MSFM). In MSFM, the gradient information of the image plays a vital role for calculating edges. It is straightforward to obtain the edge of good quality images; however, MSFM may not have robust edge maps available for images with spurious edges. Thus, a special multi-direction circumscribed circle filter is proposed to calculate the image gradient information which is then used in the MSFM. Using the new gradient information, better image contours can be obtained with MSFM. The size of the radius used in our circle filter is constant even the standard deviation of zero-mean Gaussian noise changes while the parameters of mean filter and Canny filter for gradient computation have to be correctly selected according to different noisy images. Our proposed method shows that it is effective through the experiments of image segmentation."
1138299,14018,8806,A GUI bug finding framework for Android applications,2011,"Users increasingly rely on mobile applications for computational needs. Google Android is a popular mobile platform, hence the correctness of Android applications is becoming increasingly important. Many Android correctness issues, however, fall outside the scope of traditional verification techniques, as they are due to the novelty of the platform and its activity- and event-oriented application construction paradigm. In this paper we present an approach for verifying Android applications with a focus on GUI bugs. We present techniques for detecting GUI bugs by automatic generation of test cases, feeding the application random events, instrumenting the VM, producing log/trace files and analyzing them post-run. These techniques have helped re-discover existing bugs and finding new bugs in Android applications."
1250534,14018,8806,The role of editor in collaborative modeling,2012,"In business processes, modeling is usually a collaborative activity. In it stakeholders analyze or design business processes. One of the challenges is that group members have diverse backgrounds and conflicting interests which make it difficult to arrive at a model that represents a consensus. It is therefore important to study the way in which modeling teams are organized to overcome these problems. To approach this issue we investigated the modeling behavior of such groups with the help of a tool that supports collaborative modeling while at the same time allowing for the effective collection of data on modeling activities. Besides confirming known roles we also discovered a new one, editor, that only emerges in tool-supported sessions and that functions as a mediator between modeling experts and domain experts."
966359,14018,8806,PrefRank: fair aggregation of subjective user preferences,2014,"Ranking vast amounts of user-contributed content, such as digital photographs, is handled well through user-driven ranking, but user-driven ranking is often subjective and difficult to compare. The analytic hierarchy process helps making sense of subjective opinion, whereas finding a global ranking is a problem of rank aggregation of partially ranked lists. In this position paper, we propose a solution -- PrefRank -- based on eigenvector centrality that helps aggregating partially ranked lists. Our proposed approach can be used in other application scenarios involving qualitative judgement and ranking, such as reviewing academic papers for a conference."
2390816,14018,8806,Execution support for agenda-driven case management,2014,"In recent years, mature workflow management technology has become available to support enterprises in engineering and executing well-known business processes. But in the domain of case management, no strict process can be prescribed for tasks due to their high variability. Rather, these tasks require the expertise of case managers who work with information from many different sources. We believe that such processes will benefit from an agenda-driven case management approach. However, implementing an information system supporting such an approach is challenging since it impacts and relies on many aspects of an enterprise's way of handling heterogeneous data. In this paper, we discuss requirements and design decisions arising from the adCM concept and present the results in implementing a corresponding support system."
828255,14018,8806,A tour recommendation service for electric vehicles based on a hybrid orienteering model,2013,"This paper designs a tour recommendation scheme for electric vehicles, aiming at reducing time waste induced from long charging time and finally accelerating their penetration into our daily lives. Not just deciding the visiting and charging schedule for the user-selected tourist attractions, our scheme recommends more places having chargers as well as providing tour activities to save the waiting time. Genetic operations are tailored to create a tour plan consisting of essential selected and optional recommended spots by means of combining legacy traveling salesman problem and orienteering problem solvers. Its encoding scheme represents a visiting order, which may have variable number of tour spots, by a fixed-length integer-valued vector, while the fitness function estimates time waste considering the distance between tour places and stay time. The performance measurement result obtained from a prototype implementation discovers that our recommendation service can reduce the time waste by up to 67 % for given parameter setting."
2067384,14018,8806,A multi-resource load balancing algorithm for cloud cache systems,2013,"With the advent of cloud computing model, distributed caches have become the cornerstone for building scalable applications. Popular systems like Facebook [1] or Twitter use Memcached [5], a highly scalable distributed object cache, to speed up applications by avoiding database accesses. Distributed object caches assign objects to cache instances based on a hashing function, and objects are not moved from a cache instance to another unless more instances are added to the cache and objects are redistributed. This may lead to situations where some cache instances are overloaded when some of the objects they store are frequently accessed, while other cache instances are less frequently used.   In this paper we propose a multi-resource load balancing algorithm for distributed cache systems. The algorithm aims at balancing both CPU and Memory resources among cache instances by redistributing stored data. Considering the possible conflict of balancing multiple resources at the same time, we give CPU and Memory resources weighted priorities based on the runtime load distributions. A scarcer resource is given a higher weight than a less scarce resource when load balancing. The system imbalance degree is evaluated based on monitoring information, and the utility load of a node, a unit for resource consumption. Besides, since continuous rebalance of the system may affect the QoS of applications utilizing the cache system, our data selection policy ensures that each data migration minimizes the system imbalance degree and hence, the total reconfiguration cost can be minimized. An extensive simulation is conducted to compare our policy with other policies. Our policy shows a significant improvement in time efficiency and decrease in reconfiguration cost."
1708365,14018,8806,Delay-based incrementally mapping of virtual machines in cloud computing systems,2014,"The aim of this paper is to investigate the assignment problem of a cloud computing system where user requests to run virtual machines continuously arrive and the associated virtual machines (VMs) must be hosted on physical machines (PMs). Each PM has limited resource amounts for four kinds of resources, i.e., CPU, disk, memory, and network bandwidth. Also, each VM has the resource requirements for the four kinds resources in a PM. We propose an on-line VM-to-PM assignment algorithm, the  Delay-based Incrementally Mapping Algorithm  (DIMA), to assign VMs to PMs. We attempt to reduce resource wastes on running PMs such that the number of PMs can be minimized. The algorithm uses a user-defined variable to control the number of VMs handled at some time unit. The corresponding VM for an arrival VM request does not be host by a PM until the number of waiting VM requests reaches the value of the user-defined variable. A series of experiments were conducted to evaluate the proposed algorithm. The experimental results demonstrate that the performance of the proposed DIMA scheme is better than the greedy algorithm."
1593596,14018,8806,Predictable multithread scheduling with cycle-accurate thread progress monitor,2011,"One of the key challenges to develop applications for multicore-based embedded systems is enduring software complexity and performance requirements raised by parallel programming. Adaptive, self-aware or deterministic computing has been proposed as one method to help application developer cope with these problems. To provide these advanced features, the developer should monitor their application to know the current state of the application for optimizing or adapting to meet their goals. One solution is to make thread progress index of a program for tracking their current state. To guarantee accuracy of the tracking method, we find that the process of a thread should be repeatable and accurate. If the progress does not have the properties, it is impossible to guarantee the accuracy and determinism of the method for tracking progress of thread. Additionally, we minimize the overhead to measure and manage the progress even though providing the cycle-accurate progress tracking with inevitable runtime overhead. In this paper, we propose an efficient method to monitoring thread progress with a cycle-accurate deterministic progress counter and an efficient management technique for adaptive scheduling algorithm. As a case study, we implement 24 video decoders on a quad core system using the proposed runtime system and show the experimental result which yields a geometric mean overhead of about 2.1% to monitor each thread and these applications yield a mean performance improvement of 6% relative to normal execution when running on 4 cores."
1419088,14018,8806,A framework for semantic annotation of digital evidence,2013,"Most tools used during the forensic examination process emphasize data and metadata extraction without a formal definition of the concepts used in their outputs. These vary not only in the terminology used, but also in the way values are represented. These differences hinder the adoption of computer-assisted analysis, since the elements to be analyzed are not well-defined, requiring ad hoc parsers to process and interpret the output of each tool. A framework for semantic annotation of digital evidence is presented in this work. Semantic annotations use concepts that are defined in an ontology to describe the annotated object. They can replace raw metadata, user-defined labels and tool-specific analysis results with computer-readable, formally defined terms that can be used in semantically advanced queries. The framework's components provide means to extract, analyze and index the contents of the digital evidence. The framework allows the augmentation of a base ontology, by adding domain and case-specific concepts to it. A prototype implementation is described and a case study is conducted to illustrate its potential uses and improvements to the forensic examination process."
890243,14018,8806,Real solution formulas of cubic and quartic equations applied to generate dynamic diagrams with inequality constraints,2012,"The approach of solving geometric constraints involving inequalities proposed by Hong and others uses triangular decomposition, solution formulas, and quantifier elimination. We show that for generating dynamic diagrams automatically the performance of this approach can be enhanced, in terms of stability of numeric computation and quality of generated diagrams, when the used solution formulas of cubic and quartic equations are replaced by newly introduced real solution formulas with inequality constraints. Several examples are presented to illustrate the enhanced approach and to demonstrate the advantages and effectiveness of the new solution formulas. An implementation of the enhanced approach in Java with interface to Epsilon and QEPCAD for automated generation of dynamic diagrams is outlined and some experimental data are provided."
1161249,14018,8806,Real-time scheduling of twin stacking cranes in an automated container terminal using a genetic algorithm,2012,"We address the problem of scheduling twin automated stacking cranes (ASCs) used in automated container terminals. By extending the previous works, we show that it is important to make explicit the hidden jobs needed to prepare for the main requested jobs. Since the preparatory jobs can be done by any of the two ASCs, appropriate assignment of these jobs can help to promote cooperation and avoid interference between the two ASCs. The proposed genetic algorithm (GA) performs search within the framework of iterative rescheduling to cope with the uncertainty of ASC operation. To boost the search performance under tight real-time constraint of iterative rescheduling, our GA uses some of the solutions of the previous iteration to initialize the population of the current iteration. It has also been shown that our GA performs more robustly than other algorithm such as simulated annealing in an uncertain environment."
1595636,14018,369,A Novel Price-Based Algorithm for Spectrum Sharing in Cognitive Radio Networks,2014,Cognitive radio network is expected to use flexible radio frequency spectrum sharing techniques to achieve more efficient frequency spectrum usage. This article considers the spectrum sharing problem that one primary user (PU) can share its frequency spectrum by renting this spectrum to multiple secondary users (SUs). The pricing scheme is a key issue for spectrum sharing in cognitive radio network. We first propose a nonlinear one leader multiple followers (NLMF) sharing spectrum scheme as a multi-object optimization problem; the prices are offered by PU to SUs at the same time. This problem can be solved by using particle swarm optimization (PSO); SUs gradually and iteratively adjust their strategies respectively based on the observations on their opponents' previous strategies until Nash equilibrium is completed. We then present a general nonlinear bilevel one leader multiple followers (NBMF) optimization problem to further consider the revenue of the PU and a new optimal strategic pricing optimization technique which applies bilevel programming and swarm intelligence. A leader-follower game is formulated to obtain the stackelberg-nash equilibrium for spectrum sharing that considers not only revenue of a PU but also the SUs utility. The behaviors of the pricing model have been evaluated; and the performance results show that the proposed algorithm is well-performed to solve the spectrum sharing in a cognitive radio network.
1202428,14018,8806,Impacts of delayed replication on the key-value store,2014,"Recently, a key-value store has been used to improve the throughput and latency performance in distributed computing environments. However, some or all of the data must be processed in the memory to guarantee the performance if the data workload increases greatly. However, there is a bottleneck because of the data replication overheads incurred by the memory to guarantee availability. In this study, we implemented and evaluated the delayed replication of data in the memory to reduce the overheads of the key-value store, which stores and manages data in the memory. The results of the experiments showed that the performance was improved by 11% after applying delayed replication."
1278032,14018,8806,A structural analysis of literary fictions with social network framework,2014,"A work of literature shows different characteristics depending on its genre or author. Generally, features of literature can be revealed by linguistic analysis. However, the process of linguistic analysis is complicated and does not have a common standard. In this paper, we numerically calculate the relationship between agents that appear in literature and construct a relational network. The structure of the relational network is determined by the relationships between characters. A network that is composed of characters can be said to be a social network of a virtual world, so many existing social network analysis methods can be applied. We selected more than 20 novels including J.K. Rowling's Harry Potter and a traditional novel Three Kingdoms for an experiment. We introduce a visualization method for virtual social graphs and some useful analysis. The main contribution of this paper is that our model can be used to reveal the deep structure of a work of fiction using graph topology rather than traditional categories such as short or long novel."
2195422,14018,8806,Security and safety of assets in business processes,2012,"Business processes and service compositions are defined independent of the realizing systems. The visualization of security and safety constraints on the business process model level appears to be a promising approach to system independent specification of the security and safety requirements. Such requirements can be realized through business process annotation and used for communication or documentation, but they also can have an execution semantics that allows for automating the security and safety controls.   In this paper, we present a tool-supported framework that extends modeling and execution of business processes with specification, execution and monitoring of the security and safety constraints that are used to protect business assets. We illustrate our approach on basis of a case study modeling a supply chain for perishable goods."
910551,14018,8806,A media-based social interactions analysis procedure,2012,"There is a growing number of opportunities for users to share media using Web content providers. Moreover, in the context of Web-based Social Networks, there is also the possibility that shared media will be used in subsequent social interactions. Considering the importance of understanding the underlying collaborations, we have been investigating a human-readable technique for representing media-based social interactions. In this paper we propose a procedure for analyzing interactions among users in Social Networks, and analyze social interactions based on video and link sharing from Web providers to a Social Network. The results can be used by social application developers, e.g. in the evaluation of the user interfaces they design, or by social scientists interested in analyzing the social interactions."
1053261,14018,8806,A multimodal interaction component for digital television,2011,"In most current digital TV applications the user interaction takes place by pressing keys on a remote control. For simple applications this type of interaction is sufficient --- however, as interactive applications become more popular new input devices are demanded. After discussing motivating scenarios, this paper presents an architecture that offers to applications running on a set-top-box the possibility of receiving multimodal data (audio, video, image, ink, accelerometer, text, voice and customized data) from multiple devices (such as mobile phones, PDAs, tablet PCs, notebooks or even desktops). We validated the architecture by implementing a corresponding multimodal interaction component which extends the Brazilian Digital TV middleware, and by building applications which use the component."
1630730,14018,8806,WAVE: an architecture for predicting dropout in undergraduate courses using EDM,2014,"Predicting the academic progress of student is an issue faced by many public universities in emerging countries. Although, those institutions stores large amounts of educational data, they fail to recognize the students that are in danger to leave the system. This paper presents a novel architecture that uses EDM techniques to predict and to identify those who are at dropout risk. This approach allows academic managers to monitor the progress of the students in each academic semester, identifying the ones in difficult to fulfill their academic requirements. This paper shows initial experimental results using real world data about of three undergraduate engineering courses of one the largest Brazilian public university. According to the experiments, the classifier Naive Bayes presented the highest true positive rate for all datasets used in the experiments."
1239314,14018,8806,Towards an automatic evaluation of web applications,2012,"Evaluating the usability of computer applications using traditional laboratory-based tests is costly and time consuming. For modern Web applications, usually developed, tested and deployed in Internet Time, this approach is simply not feasible. A more effective way to evaluate the usability of Web applications consists in gathering information from the user's interactions and automatically processing this data in order to detect usability problems in the execution of predefined tasks. The reported solutions based on this approach usually fail on providing efficient tools for the definition of tasks, specially in large and dynamic Web applications. In order to tackle this problem, we developed USABILICS, a system targeted for the automatic remote evaluation of usability based on an interface model. The proposed model allows the definition of tasks using a simple and intuitive approach, which can be applied to large and dynamic Web applications. USABILICS analyzes the execution of tasks by calculating the similarity among sequence of events produced by users and those previously captured by evaluators. Based on this analysis, USABILICS provides an usability index, as well as recommendations for solving usability problems detected on the execution of each task."
953413,14018,8806,Slipstream: architecture options for real-time process analytics,2011,"Timely insight into a company's business processes is of great importance for operational efficiency. However, still today companies struggle with inflexibility of monitoring solutions and reacting to process information on time. We review the current state of the art of business process management and business activity management. Based on that we develop an architecture for event-driven business activity monitoring which is capable of standalone process analytics or can make use of complex event processing engines to filter and aggregate process events in realtime. We discuss advantages and disadvantages of the two options in terms of ease of use, flexibility, and extensibility and close by introducing on future research directions based on our experiences from a prototypical implementation using standard software."
743851,14018,8806,A common neighbour based two-way collaborative recommendation method,2012,"Traditional recommendation methods offer items, that are inanimate and one way recommendation, to users. Emerging new applications such as online dating or job recruitments require reciprocal people-to-people recommendations that are animate and two-way recommendations. In this paper, we propose a reciprocal collaborative method based on the concepts of users' similarities and common neighbors. The dataset employed for the experiment is gathered from a real life online dating network. The proposed method is compared with baseline methods that use traditional collaborative algorithms. Results show the proposed method can achieve noticeably better performance than the baseline methods."
736743,14018,8228,Maximizing revenue with dynamic cloud pricing: The infinite horizon case,2012,"We study the infinite horizon dynamic pricing problem for an infrastructure cloud provider in the emerging cloud computing paradigm. The cloud provider, such as Amazon, provides computing capacity in the form of virtual instances and charges customers a time-varying price for the period they use the instances. The provider's problem is then to find an optimal pricing policy, in face of stochastic demand arrivals and departures, so that the average expected revenue is maximized in the long run. We adopt a revenue management framework to tackle the problem. Optimality conditions and structural results are obtained for our stochastic formulation, which yield insights on the optimal pricing strategy. Numerical results verify our analysis and reveal additional properties of optimal pricing policies for the infinite horizon case."
943827,14018,8806,A probabilistic context-aware approach for quality of experience measurement in pervasive systems,2011,"In this paper, we develop a novel context-aware approach for quality of experience (QoE) modeling, reasoning and inferencing in mobile and pervasive computing environments. The proposed model is based upon a state-space approach and Bayesian networks for QoE modeling and reasoning. We further extend this context model to incorporate influence diagrams for efficient QoE inferencing. Our approach accommodates user, device and quality of service (QoS) related context parameters to determine the overall QoE of the user. This helps in user-related media, network and device adaptation. We perform experimentation to validate the proposed approach and the results verify its modeling and inferencing capabilities."
676695,14018,8806,Extracting differences between regular tree grammars,2013,"An XML document is usually stored with its schema so that the structural consistency of the document is ensured. In general, schemas are continuously updated according to changes in real world. Thus, we have to precisely know how a schema is updated to keep the validity of the XML documents. In order to know how a schema is updated, we need to extract the difference between old and new schemas. However, schemas are recently becoming larger and more complex, thus it becomes more difficult to know how a schema is updated. In this paper, we consider the problem of extracting the difference between regular tree grammars, a popular formal model of XML schema languages. We first show that the problem is NP-hard. Then we give a sufficient condition under which the problem can be solved efficiently, and present a polynomial-time algorithm for solving the problem under the sufficient condition. Finally, we show some experimental results."
1571501,14018,8806,Product-based business processes interoperability,2013,"Business Process Modeling languages can be used to specify what data artifacts each stakeholder in a collaborative environment should exchange as well as how he should exchange them (i.e. in sequence, in parallel or mixed). The use of this approach increases the coupling between stakeholders and decreases the flexibility of the collaborative environment. The reason is that stakeholders take into account the specificities of other stakeholders when designing their business processes. This constitutes a major issue in the context of dynamic networks where stakeholders can leave the network and be replaced by new ones. In this paper, we propose a conceptual framework that can be used by stakeholders to design, in isolation, interoperable business processes. This framework decreases the coupling between stakeholders while ensuring stakeholders' business processes interoperability as well as a high level of flexibility of the collaborative network."
1163565,14018,8806,Traffic classification beyond application level: identifying content types from network traces,2011,"Prior works on traffic classification mainly focus their attentions on dividing Internet traffic into different categories based on their application layer protocols (such as BitTorrent, eDonkey  etc .). Making traffic classification from another point of view, we divide Internet traffic into different content types. Our technology is an attempt to solve the classification problem of unknown and proprietary protocols. In this paper, we design a classifier which can distinguish Internet traffic into different content types using machine learning techniques, and features are the entropy of consecutive bytes and frequency of characters. The chief features of our classifier are high classification accuracy (about 81%) and small computing space (about 1K Bytes)."
2410541,14018,8806,Impact analysis for event-based systems using change patterns,2014,"Being composed of highly decoupled components, event-driven architectures are promising solutions for facilitating high flexibility, scalability, and concurrency of distributed systems. However, analyzing, maintaining, and evolving an event-based system are challenging tasks due to the intrinsic loose coupling of its components. One of the major obstacles for analyzing an event-based system is the absence of explicit information on the dependencies of its components. Furthermore, assisting techniques for analyzing the impacts of certain changes are missing, hindering the implementation the changes in event-based architectures. We presented in this paper a novel approach to supporting impact analysis based on the notion of change patterns formalized using trace semantics. A change pattern is an abstraction of the modification actions performed when evolving an event-based system. Based on this formal foundation, we introduce supporting techniques for estimating the impact and detecting undesired effects of a particular system evolution, such as dead paths, deadlocks, and livelocks. Quantitative evaluations for event-based systems with large numbers of components show that our approach is feasible and scalable for realistic application scenarios."
1686689,14018,8806,Modeling adaptation with a tuple-based coordination language,2012,"In recent years, it has been argued that systems and applications, in order to deal with their increasing complexity, should be able to adapt their behavior according to new requirements or environment conditions. In this paper, we present a preliminary investigation aiming at studying how coordination languages and formal methods can contribute to a better understanding, implementation and usage of the mechanisms and techniques for adaptation currently proposed in the literature. Our study relies on the formal coordination language Klaim as a common framework for modeling some adaptation techniques, namely the MAPE-K loop, aspect- and context-oriented programming."
1664158,14018,8806,Determining language variant in microblog messages,2013,"It is difficult to determine the country of origin of the author of a short message based only on the text. This is an even more complex problem when more than one country uses the same native language. In this paper, we address the specific problem of detecting the two main variants of the Portuguese language --- European and Brazilian --- in Twitter micro-blogging data, by proposing and evaluating a set of high-precision features. We follow an automatic classification approach using a Naive Bayes classifier, achieving 95% accuracy. We find that our system is adequate for real-time tweet classification."
1080357,14018,8806,CrowdVis: a framework for real time crowd visualization,2013,"Crowd visualization is present mostly in digital games and computer animated movies, but are also observed in simulations and virtual reality applications. In crowd simulations we should represent the behavior of agents given different scenarios, and also, such simulations can be provided by different softwares and tools. This paper presents a framework for real time crowd visualization, that no programming knowledge and modeling skills are required from the users. Our main goal is to be able to visualize previously created crowd simulations in real time, combining rendering techniques and providing easy support for managing the scene and the virtual humans."
1228411,14018,8806,Matrix correlation distance for 2D image classification,2014,"In the field of visual information processing, there have been active studies on the efficient representation of visual data, such as local feature descriptors and tensor subspace analysis. Though these methods give a representation using matrix features, current methods for classification are mainly designed for 1D vector data, which may lead to loss of information included in 2D matrix data. To solve the problem, we propose a matrix correlation distance for 2D image data by extending the correlation distance for random vectors. Through a number of computational experiments on image data with various representations, we compare the performance of the proposed measure with conventional distances."
1681810,14018,8806,Empowering automatic data-center management with machine learning,2013,"The Cloud as computing paradigm has become nowadays crucial for most Internet business models. Managing and optimizing its performance on a moment-by-moment basis is not easy given as the amount and diversity of elements involved (hardware, applications, workloads, customer needs...). Here we show how a combination of scheduling algorithms and data mining techniques helps improving the performance and profitability of a data-center running virtualized web-services. We model the data-center's main resources (CPU, memory, IO), quality of service (viewed as response time), and workloads (incoming streams of requests) from past executions. We show how these models to help scheduling algorithms make better decisions about job and resource allocation, aiming for a balance between throughput, quality of service, and power consumption."
1891956,14018,8806,Statechart-based use case requirement validation of event-driven systems,2012,"In this paper, we present an approach of validating functional requirements modeled as a set of use cases. In this approach, we perform requirement analysis both at use case level as well as system level. The approach consists of two steps. In step one, each use case is independently modeled by a UML Statechart, which is then analyzed for any state-specific anomalies. In step two, we perform systemlevel analysis by substituting each use case by its Statechart to validate all possible types of usages of the system. We demonstrate the applicability of the proposed approach using a case study."
2223202,14018,8806,Can business process modeling bridge the gap between business and information systems,2012,"Early efforts on bridging the communication gap between a business and its IT systems have resulted in several business analysis and modeling techniques. Most recently, BPMN is rapidly consolidating its position as the established standard for modeling business processes. Yet, research shows that BPMN still lacks comprehensive constructs for representing some core business concepts, including business goals, non-functional requirements and resources. The purpose of this paper is to use Zachman Framework to assess BPMN's modeling capabilities and to identify its modeling gaps. The motivation of the paper is to provide a better understanding of the suitability of BPMN as a business process modeling language for bridging the gap between business and its information systems."
1050490,14018,8806,Automatic reuse of process patterns in process modeling,2011,"Process pattern concept was proposed as a promising approach to reuse process knowledge. However, their general and practical use has been handicapped by the lack of formalization and supporting tools. In order to facilitate process patterns reuse, we aim at an (semi)automatic approach that can assure correct patterns' applications and reduce process modeling time. In our previous works, we developed a UML-based meta-model allowing pattern-based process modeling. In this paper, we present a set of reuse operators that enable automatic applications of process patterns for generating and (re)structuring process models. These operators are defined with operational semantics and implemented in a prototype of pattern-based process modeling tool."
1112152,14018,8806,On the load balancing of virtual networks in distributed clouds,2013,"The distribution of computing resources in different geographical regions and the promotion of full integration with network resources are important issues of new architectures for Cloud computing. Such scattered Cloud deployments, called Distributed Clouds (D-Clouds), can directly reach users due to their inherently distributed infrastructure and the ownership of the network. Thus, D-Clouds can comply with geographically-based requirements and network-based quality of service. One of the challenges in this area is the resource management. In this way, a clever resource allocation algorithm is needed to satisfy service requirements and an owner's management objectives. This paper proposes algorithms for allocation of computing and network resources in a D-Cloud with the objectives of balancing the load in the virtualized infrastructure and of considering constraints, such as processing power, memory, storage, and network delay. The evaluation of the algorithm shows that it is indeed adequate for link allocation across different physical networks."
2109292,14018,369,Energy Efficient Cognitive Unicast Routing for Wireless Sensor Networks,2013,Survivability is crucial in Wireless Sensor Networks (WSNs) especially when they are used for monitoring and tracking applications with limited available resources. In this paper we are proposing the use of an energy Efficient Cognitive Unicast Routing (ECUR) protocol that tries to keep a balance between the energy consumption and the packet delay in a WSN. The proposed routing protocol has a next node selection criterion to change the routing path dynamically following the network conditions and the channel availability while the energy consumption per node is also considered. Simulation results are presented that show an increase in network lifetime of up to 30% compared with geographic opportunistic routing while the packet delay remains similar.
1543779,14018,8806,Metrics for approximate query engine evaluation,2012,"The performance evaluation of the transaction processing in Database Management Systems used for Decision Support Systems is the aim of the current TPC-H standard. Decision Support Systems also include Approximate Query Answering Systems. However, the TPC-H does not define a methodology to evaluate these systems, nor does it provide useful metrics. In this paper, we address the problem related to the extension of TPC-H, in order to adjust it for the performance evaluation of the engines used in approximate query processing to provide fast and approximate responses to analytical queries."
1480841,14018,8806,Performance analysis of a rule-based SOA component for real-time applications,2013,"In this paper we consider an event based architectural component that may overcome the present reluctance to the use of Service Orientation (SOA) in military distributed real-time environments. In particular, we propose a hybrid architecture component that is decentralised in all respects, making it more reactive to real-time events, as well as being easier to analyse and adapt to changing needs. As centralised scheduling and orchestration of SOA services does not scale to distributed systems, our architecture removes this key inhibitor by distributing the data and control flow to a rule-driven Distributed Real-Time SOA (DRT-SOA) component that resides with each service. Embedded deadline driven task scheduling means each service can now dynamically adjust to changes in process priorities. We also propose a method of analysing the performance of the new architecture using the dynamics of Petri Nets and the guarantees of Real-Time Calculus. We present task level worst case analysis results using the proposed method and also present results obtained from an implementation of the proposed architecture in a naval combat system context."
1007591,14018,8806,HawkEye : a tool for collaborative business process modelling and verification,2013,"In this paper we propose a collaborative Business Process modeling approach where multiple stakeholders can be coordinate considering global and local views on Business Processes. In the modeling phase we use a standard language such as BPMN 2.0 that provides both local view, via collaboration specification, and global view, via choreography models specification. The approach provides support also for analysis activities aiming at reconciling local and global views to effectively and efficiently derive inter-organizational Business Processes. For the analysis phase we adapted well known verification approaches in order to check behavioral constraints."
1981723,14018,8806,Random rules from data streams,2013,"Existing works suggest that random inputs and random features produce good results in classification. In this paper we study the problem of generating random rule sets from data streams. One of the most interpretable and flexible models for data stream mining prediction tasks is the Very Fast Decision Rules learner (VFDR). In this work we extend the VFDR algorithm using random rules from data streams. The proposed algorithm generates several sets of rules. Each rule set is associated with a set of N  att   attributes. The proposed algorithm maintains all properties required when learning from stationary data streams: online and any-time classification, processing each example once."
1685123,14018,8806,Performance evaluation of medical imaging service,2012,"Public/private decision makers have faced challenges to improve healthcare services for supporting the increasing demand and simultaneously reducing the associated costs. Although the adoption of information and communication technologies (ICTs) are important in this context, the current service status should be firstly examined and different configurations/scenarios quantitatively evaluated before any further adjustment. Formal methods are of great importance, since they provide mathematical means for quantitative evaluation of systems and allow property analysis/verification. This paper presents an approach based on stochastic Petri nets for the performance evaluation of medical imaging service, adopting a real-world case study to demonstrate the feasibility of the proposed approach."
1566027,14018,8806,An empirical evaluation of process mining algorithms based on structural and behavioral similarities,2012,"While many process mining algorithms have been proposed recently, there exists no widely-accepted benchmark to evaluate these process mining algorithms. As a result, it can be difficult to compare different process mining algorithms especially over different application domains. This paper presents our attempt in building such a benchmark by empirically evaluating process mining algorithms using reference models, in which the quality of a discovered model is measured by the behavioral and structural similarities with its reference model. In addition to artificial reference models extracted from academic papers and SAP suites, real-life processes from a major boiler manufacturer in China are added into the benchmark."
1216733,14018,8806,Analysis of a triploid genetic algorithm over deceptive landscapes,2012,"This paper compares the performance of a canonical genetic algorithm (CGA) against that of the triploid genetic algorithm (TGA) introduced in [10], over a number of well known deceptive landscapes in order to increase our understanding of the TGA's ability to control convergence. The TGA incorporates a mechanism to control the convergence direction instead of simply increasing the population diversity. Results indicate that the TGA appears to have the highest level of difficulty in solving problems with a disordered pattern. While the disorder-mapping seems to improve the CGA's performance, it has a negative effect on the performance of the TGA. However, the results illustrate that the TGA performs better on problems with epistasis present."
907715,14018,8806,Origami axioms and circle extension,2011,"Origami, i.e. paper folding, is a powerful tool for geometrical constructions. In 1989, Humiaki Huzita introduced six folding operations based on aligning one or more combinations of points and lines [6]. Jacques Justin, in his paper of the same proceedings, also presented a list of seven distinct operations [9]. His list included, without literal description, one extra operation not in Huzita's paper. Justin's work was written in French, and was somehow unknown among researchers. This led Hatori [5] to 'discover' the same seventh operation in 2001. Alperin and Lang in 2006 [1] showed, by exhaustive enumeration of combinations of superpositions of points and lines involved, that the seven operations are complete combinations of the alignments. Huzita did not call his list of operations  axioms . However, over years, the term Huzita axioms, or Huzita-Justin or Huzita-Hatori axioms, has been widely used in origami community. From logical point of view, it is not accurate to call Huzita's original statements of folding operations as axioms, because they are not always true in plane Euclidean geometry. In this paper, we present precise statements of the folding operations, by which naming them 'axioms' is logically valid, and we make some notes about the work of Huzita and Justin."
2549415,14018,8806,Experimenting with system and Libc call interception attacks on ARM-based Linux kernel,2011,"Linux is one of the highly portable operating systems for embedded systems such as tablet PC. Recently it has been used for a smartphone operating system based on ARM processor. In ×86-based Linux, system call and standard library interception attacks using a Loadable Kernel Module (LKM) have been reported. Those attacks are usually used so that the attacker sets up a backdoor or makes a rootkit with root privilege. In this paper, we report that those attacks also are available in an ARM-based Linux which is popularly adopted for smartphone OS including Android. We also implement those attacks on Maemo platform on Nokia N900, show their empirical results, and then discuss some countermeasures thereof."
1563126,14018,8806,Proving the security of ElGamal encryption via indistinguishability logic,2011,Correctness of cryptosystems is in many cases an important prerequisite for trusting security relevant systems. Even cryptosystems with tiny specifications are often hard for humans to understand. It can be difficult to reason about them and to convince oneself that distinct security properties do indeed hold. Even mathematical proofs -- carried out with paper and pencil -- which are intended to show the strength of a cryptosystem with respect to some attacker model have turned out to be error prone.   In this paper we address the problem of establishing trusted properties of cryptosystems. We report on proving the security of the ElGamal and Hashed ElGamal encryption schemes within Coq. Security is shown with respect to Real-or-Random chosen plaintext attacks (ROR-CPA). This work is a prototypical case study for a novel approach: having defined a framework for the specification of cryptographic processes and general rules for decomposing cryptographic proofs into smaller units we use this framework to specify the involved schemes and attack model. The defined rules are used to represent the overall security proof layout. They are proven sound with respect to basic mathematical properties. To achieve a formal security proof remaining goals are proven by special tactics or in an interactive way using the basic mathematical properties.
1119862,14018,8806,Using a class abstraction technique to predict faults in OO classes: a case study through six releases of the Eclipse JDT,2011,"In this paper, we propose an innovative suite of metrics based on a class abstraction that uses a taxonomy for OO classes (CAT) to capture aspects of software complexity through combinations of class characteristics. We empirically validate their ability to predict fault prone classes using fault data for six versions of the Java-based open-source Eclipse Integrated Development Environment. We conclude that this proposed CAT metric suite, even though it treats classes in groups rather than individually, is as effective as the traditional Chidamber and Kemerer metrics in identifying fault-prone classes."
1502506,14018,8806,MobiSpatial : open source for mobile spatial interaction,2012,"This paper describes our Mobile Spatial Interaction (MSI) prototype  MobiSpatial , which benefits from location and orientation aware smartphones and existing open source spatial data initiatives to facilitate user interaction with the geospatial query process. We utilize today's ubiquitous mobile device as the central computing platform to calculate a mobile user's visibility shape at his/her current location.  MobiSpatial  uses this shape as a query window in a spatial database to perform line-of-sight, field-of-view and 360° Isovist visibility searches. These visibility based spatial queries reduce the risk of information overload by exploiting hidden query removal functionality to retrieve only those objects that a user can actually see. By incorporating open source datasets and databases, our application manages to store, index, query, retrieve, and display spatial information solely on the mobile device itself, without concern for any server side computations or internet connections."
1835062,14018,8806,Formal verification of Unreliable Failure Detectors in Partially Synchronous Systems,2012,"We formally verify four algorithms proposed in [M. Larrea, S. Arevalo and A. Fernandez, Efficient Algorithms to Implement Unreliable Failure Detectors in Partially Synchronous Systems, 1999]. Each algorithm is specified as a network of timed automata and is verified with respect to completeness and accuracy properties. Using the model-checking tool UP-PAAL, we detect and report the occurrences of deadlock (for all algorithms) between each pair of non-faulty nodes due to buffer overflow in communication channels with arbitrarily large buffers and we propose a solution. Moreover, we use one of the algorithms as a measure to compare three model-checking tools, namely, UPPAAL, mCRL2 and FDR2."
1726035,14018,8806,Dynamic guidance enhancement in workflow management systems,2012,"Today's workflow management systems have become increasingly powerful. Some prototypic approaches even tend to not patronise the users by providing a set of process steps to follow, but let them decide which step to choose next. The idea behind this approach is the impossibility to model every special case of a workflow, because a fixed process order would necessarily be inefficient or even incorrect in some cases. By admitting this freedom, the risk of confounding the users is taken. That is why we provide a qualified guidance instance through the process."
1722764,14018,8806,System-level co-simulation of integrated avionics using polychrony,2011,"The design of embedded systems from multiple views and heterogeneous models is ubiquitous in avionics as, in particular, different high-level modeling standards are adopted for specifying the structure, hardware and software components of a system. The system-level simulation of such composite models is necessary but difficult task, allowing to validate global design choices as early as possible in the system design flow. This paper presents an approach to the issue of composing, integrating and simulating heterogeneous models in a system co-design flow. First, the functional behavior of an application is modeled with synchronous data-flow and statechart diagrams using Simulink/Gene-Auto. The system architecture is modeled in the AADL standard. These highlevel, synchronous and asynchronous, models are then translated into a common model, based on a polychronous model of computation, allowing for a Globally Asynchronous Locally Synchronous (GALS) interpretation of the composed models. This translation is implemented as an automatic model transformation within Polychrony, a toolkit for embedded systems design. Simulation, including profiling and value change dump demonstration, has been carried out based on the common model within Polychrony. An avionic case study, consisting of a simplified doors and slides control system, is presented to illustrate our approach."
833442,14018,8806,Image segmentation of cervical vertebra in X-ray radiographs using the curve fitting strategy,2011,"We develop an effective method for the study of cervical vertebra maturation (CVM) for bone age evaluation. Such studies need an accurate X-ray radiographs segmentation of cervical vertebra. It is difficult to have a good segmentation on this type of images. Current segmentation methods do not work well on scanned images from analog image X-ray radiographs of cervical vertebra. A new method for analysis of cervical bone age is proposed. Two key techniques are developed in this proposed segmentation algorithm: (1) a fitting weight matrix is built to reduce the effect of subjective factors entered by the user when fast marching method is used to obtain the initial rough outline of cervical vertebra, and (2) apply a curve fitting method based on rotating and overlapping parabolic curves to derive the final segments of cervical vertebra. Furthermore, the user can calculate corresponding parameters from segmented results to assess the bone age. Experimental results using the proposed algorithm show that our algorithm is more accurate than those of fast marching method (FMM) and radiologists through repetition. It also shows that the proposed method has a higher accuracy on the correlation of the skeletal maturity indicators (SMI) and quantitative cervical vertebral maturation (QCVM)."
2401099,14018,369,Self-Organizing Adaptive Clustering for Cooperative Multipoint Transmission,2011,"Coordinated Multipoint (CoMP) transmission technique is one method to improve performance of cellular wireless systems, e.g. LTE Advanced, by cooperation of cells for reducing interference and increasing SINR of users in weak radio conditions, e.g. located at cell edge. In this paper, we present an adaptive clustering algorithm to dynamically adjust the cooperation sets of a CoMP system to the UE perceived signal strength in order to maximize the overall system performance while avoiding major system architecture modifications. We show that additional gain in SINR could be achieved compared to non UE-aware fixed cluster with limited increase of system complexity, for a practical adaptive CoMP clustering scheme performing not far from an upper bound UE-specific scheme."
1479458,14018,8806,MUSES: a corporate user-centric system which applies computational intelligence methods,2014,"This work presents the description of the architecture of a novel enterprise security system, still in development, which can prevent and deal with the security flaws derived from the users in a company. Thus, the Multiplatform Usable Endpoint Security system (MUSES) considers diverse factors such as the information distribution, the type of accesses, the context where the users are, the category of users, or the mix between personal and private data, among others. This system includes an event correlator and a risk and trust analysis engine to perform the decision process. MUSES follows a set of defined security rules, according to the enterprise security policies, but it is able to self-adapt the decisions and even create new security rules depending on the user behaviour, the specific device, and the situation or context. To this aim MUSES applies machine learning and computational intelligence techniques which can also be used to predict potential unsafe or dangerous user's behaviour."
1381614,14018,8806,Dynamic composition of coordination abstractions for pervasive systems: the case of LogOp,2012,"Originally conceived in the context of closed, parallel systems, coordination models and languages soon proved their effectiveness in the engineering of open, distributed systems [8]. Nowadays,  space-based  coordination models are developing to tackle with the issues of complex computational system such as pervasive and knowledge-intensive systems [2, 6, 3]. There, in order to deal with strong dynamicity, multiple coordination ows, physical mobility, heterogeneous knowledge sources and the like,  dynamic composition  of expressive coordination abstractions is required, involving both the information contained in the shared spaces, and the laws of coordination embedded in the coordination media."
1973205,14018,8806,An implementation study of a ghost drive: hidden file store in a filesystem,2012,"Recently, it becomes increasingly important to secure user private data in mobile devices. To protect user private data, one possible approach is to implement a secure file storage in the mobile devices based on mandatory access control (MAC), but the device manufacturers seldom implement it because of high pressure of time-to-market, frequent version upgrade of the operating system, and no unanimous agreement in the MAC standard software. In this paper, we propose an implementation study of a secure file storage, called a  ghost drive , which can facilitate the implementation of MAC in the mobile devices by unburdening the manufacturers from aggressive instrumentation of the whole operating system. Since our implementation is in form of a loadable kernel module, separated from the main kernel, it can be deployed even to commercial mobile devices already in use by installing it over the air. Our experiments show that the performance of our secure storage implementation is not worse than the original unmodified implementation."
1511991,14018,8806,Prediction of permuted super-secondary structures in β-barrel proteins,2011,"Computational structure prediction methods based on learning are poorly tractable for transmembrane β-barrel (TMB) proteins, for it is difficult to observe them with standard experimental techniques. Generally, those structures are not only a series of β-strands where each is bonded to the preceding and succeeding ones in the primary sequence, but they may contain Greek key or Jelly roll motifs as well. This may be described as a permutation on the order of the bonded segments. We model the protein folding problem with minimum energy into the search of the longest closed path in a weighted graph with respect to a given permutation. With dynamic programming, the algorithm runs in  O ( N  2 ) for an identity permutation, and at most  O ( N  4 ) for the Greek key motifs, where  N  is the number of amino acids. The prediction accuracy as well as the discrimination ability is favorably comparable with existing works."
1813252,14018,8806,A scalable communication infrastructure for smart grid applications using multicast over public networks,2013,"The transition to regenerative energy resources paves the path from a small number of large power plants to a grid of highly distributed, heterogeneous small generators. To establish a Smart Grid it requires a scalable machine-to-machine communication that allows to control and coordinate millions of energy producing and consuming devices. In this work we contribute a future deployment concept of a scalable communication infrastructure for Smart Grids. Our proposal is based on group communication utilizing hybrid multicast over public networks. We show by real-world measurements that our approach achieves high scalability while also limiting end-to-end communication delay."
1636254,14018,8806,A comparison of two metacompilation approaches to implementing a complex domain-specific language,2012,"Operational semantics and attribute grammars are examples of formalisms that can be used for generating compilers. We are interested in finding similarities and differences in how these approaches are applied to complex languages, and for generating compilers of such maturity that they have users in industry.   As a specific case, we present a comparative analysis of two compilers for Modelica, a language for physical modeling, and which contains numerous compilation challenges. The two compilers are OpenModelica, which is based on big-step operational semantics, and JModelica.org, which is based on reference attribute grammars."
1388035,14018,8806,Method reallocation to reduce energy consumption: an implementation in Android OS,2014,"Mobile applications have become ubiquitous, adopted by millions of users that register billions of downloads a day. To increase the competitiveness of the mobile software product, developers should care in a very detailed fashion about the qualities demanded by end users, execution targets and mobile markets. One important quality is the ability of the application to consume energy efficiently, as mobile devices are powered by batteries and they hold a very strong autonomy requirement. In this paper, we investigate the impact that the allocation of a software routine has in the overall energy consumption of a mobile device. We implemented software benchmarks in Java and C and we exercised them in different execution scopes of the Android OS runtime. We measured the amount of energy required to complete each job to determine the energy consumed by each routine, and to know in what cases it is advisable to reallocate the processing job from a regular application to an external execution environment."
1542573,14018,8806,A Configuration Management task ontology for semantic integration,2012,"Configuration Management (CM) is an important task for developing complex products. It is a complex task and there are many CM systems that aim to support it. However, generally, these systems work in isolation and there is a need for integrating them. In this context, ontologies have an important role, acting as an inter-lingua to help achieving a shared conceptualization that allows semantic integration. This paper presents an ontology of the CM task. This ontology was built with the purpose of supporting semantic integration of CM systems, mainly in service and process layers of integration."
1735289,14018,8806,Effective radical segmentation of offline handwritten Chinese characters by using an enhanced snake model and Genetic Algorithm,2012,"In this paper, a popular snake model is enhanced by considering the guiding image force and speeded up by incorporating Genetic Algorithm. It has been applied to segment the radicals in offline handwritten Chinese characters. Testing results show that the proposed approach can effectively decompose the radicals with overlaps and connections from the characters with various layout structures. The segmentation accuracy reaches 94.91% and the average running time is around 0.05 second per character."
1146041,14018,8806,Extracting preference terms from web browsing histories excluding pages unrelated to users' interests,2011,"Personalization is one of the most significant challenges in the World Wide Web. Extracting preference terms chiefly from Web browsing histories is a first step in personalization. However, a portion of Web pages includes information unrelated to the users' interests, and personalization results would probably be fuzzy owing to such pages. In this paper, we propose an approach to extract preference terms from Web browsing histories, excluding pages unrelated to the users' interests. Our proposed approach mainly consists of two steps. First is a page classification step, utilizing both URL expressions and keyphrase frequencies in the page, in order to eliminate keyphrases derived from pages unrelated to the users' interests. Second is a keyphrase scoring step, exploiting document frequency of terms, in order to obtain preference terms. Our empirical study for 5 participants over a period of 4 weeks reveals that the proposed approach is more effective for the users with specific Web browsing styles."
1267164,14018,8806,Evaluating the utilization of Twitter messages as a source of security alerts,2013,"The fast spread of computer security alerts, like vulnerabilities notifications, applications updates and threats of attacks, is essential to the implementation of efficient reactive measures against security incidents. This paper presents an empirical study that evaluates the efficiency of using Twitter posts, related to computer security, as notifications of security alerts. The justification to this study is the fact that posts from microblogs have a very fast spread over the Internet. By using similarity analysis and clustering between Twitter posts and alerts from specialized sites, it was verified that microblogs spread computer security alerts in a reliable way, reach a high level of dissemination and inform about security threats even before some specialized sites."
1816277,14018,8228,Predicate-tree based pretty good privacy of data,2012,"Growth of Internet has led to exponential rise in data communication over the World Wide Web. Several applications and entities such as online banking transactions, stock trading, e-commerce Web sites, etc. are at a constant risk of eavesdropping and hacking. Hence, security of data is of prime concern. Recently, vertical data have gained lot of focus because of their significant performance benefits over horizontal data in various data mining applications. In our current work, we propose a Predicate-Tree based solution for protection of data. Predicate-Trees or pTrees are compressed, data-mining-ready, vertical data structures and have been used in a plethora of data-mining research areas such as spatial association rule mining, text clustering, closed k-nearest neighbor classification, etc. We show how for data mining purposes, the scrambled pTrees would be unrevealing of the raw data to anyone except for the authorized person issuing a data mining request. In addition, we propose several techniques which come along as a benefit of using vertical pTrees. To the best of our knowledge, our approach is novel and provides sufficient speed and protection level for an effective data security."
1280419,14018,8806,Assessing handwitten digit segmentation algorithms,2012,"This work compares different segmentation algorithms for handwritten digits based on explicit segmentation. For this purpose, algorithms based on different concepts were implemented and evaluated under the same conditions. The algorithms were used to segment 2,369 pairs of touching digits of the NIST_SD19 database and were evaluated in terms of correct segmentation and computational time. We also discuss the complementarity of the segmentation algorithms. We have observed that independently of the individual performance of the algorithms, each method is able to segment samples that can not be segmented by any other method. Based on this observation, we conclude that the combination of different segmentation algorithms may be an interesting strategy to improve the correct segmentation rate."
2298539,14018,8806,Solving equations on words through boolean satisfiability,2013,"Word equations are combinatorial equalities between strings of symbols, variables and functions, which can be used to model problems in a wide range of domains. While some complexity results for the solving of specific classes of equations are known, currently there does not exist a systematic equation solver. We present in this paper a reduction of the problem of solving word equations to Boolean satisfiability, and describe the implementation of a general-purpose tool that leverages existing SAT solvers for this purpose. Our solver will prove useful in the resolution of word equations, and in the computer-based exploration of various combinatorial conjectures."
2522484,14018,8806,Amending C-net discovery algorithms,2013,"As the complexity of information systems evolves, there is a growing interest in defining suitable process models than can overcome the limitations of traditional formalisms like Petri nets or related.  Causal nets  may be one of such process models, since their declarative semantics and simple graph structure deviates from existing formalisms. Due to their novelty, few discovery algorithms exist for Causal nets. Moreover, the existing ones offer poor guarantees regarding the produced outcome. We describe an algorithm that can be applied as a second step to any discovery technique to improve the quality of the Causal net derived. We have tested the technique in combination with the existing algorithms in the literature, noticing a considerable improvement."
2200150,14018,8494,Multi-granularity dynamic analysis of complex software networks,2011,"Software systems represent one of the most complex man-made systems. In this paper, we analyze the evolution of Object-Oriented (OO) software using complex network theory from a multi-granularity perspective. First, the software networks are constructed for a multi-version software system at different levels of granularity. Then, some parameters used in complex network theory are introduced to study the topological characteristics of these software networks. By investigating the parameters' values in consecutive software networks, we have a better understanding about software evolution. A case study on an open source OO project, Azureus, is conducted as an example to illustrate our approach. It uncovers some underlying dynamic characteristics of OO systems. These results provide a different dimension to our understanding of software system dynamics and also are very useful for the design and development of OO software systems."
1684520,14018,8806,A similarity model for virtual networks negotiation,2014,"Many companies use the Internet as a basis for their services, defining Service Level Agreements (SLA) with their respective Internet Service Providers (ISP). However, the current Internet works in a best effort manner, that points toward the concept of network virtualization and Software Defined Networks (SDN) to support the Future Internet. Within this context, this work proposes a similarity model and a similarity metric that enable the client to negotiate protocols for SDN and Virtual Networks (VN). The proposed model enables the free competition among providers and allows the client to compare protocols offered by the providers to identify which ones best fulfill the requested requirements. Experiments showed the effectiveness of the proposed model."
1189894,14018,8806,Fast lists intersection with Bloom filter using graphics processing units,2011,"Intersection of sorted inverted lists is an important operation in the web search engines. Various algorithms to improve the performance of this operation have been introduced in the literature [1, 3, 5]. Previous research works mainly focused on single-core or multi-core CPU platform and did not consider the query traffic problem arises in the actual systems. Modern graphics processing units (GPUs) give a new way to solve the problem. Wu et al. [6] presented a CPU-GPU cooperative model which can dynamically switch between the asynchronous mode and the synchronous mode. Under light query traffic, asynchronous mode is triggered, each newly arriving query is serviced by an independent thread. Under heavy query traffic, synchronous mode is triggered, all active threads are blocked and a single thread takes control of query processing. Queries are grouped into batches at CPU end, and each batch is processed by GPU threads in parallel. We summarize that putting the operations on GPU has two advantages: The massive on-chip parallelism of GPU may greatly reduce the processing time of lists intersection; A great part of work on CPU is offloaded to GPU. Overall the GPU will significantly increase throughput and reduce average response time in the synchronous mode. In this paper we consider techniques for improving the performance of the GPU batched algorithm proposed in [6] assuming sufficient queries at the CPU end."
1564654,14018,8806,A semi-partitioned real-time scheduling approach for periodic task systems on multicore platforms,2012,"Semi-partitioned scheduling is regarded as a viable alternative to  partitioned  or  global  scheduling approaches. Advantage of semi-partitioned scheduling is two-folds: it has reduced runtime overhead compared to global scheduling, and improved schedulability and system utilization factor compared to partitioned scheduling. This paper proposes a new semi-partitioned scheduling algorithm for real-time periodic task systems over multicore platforms. Our proposed algorithm works in two phases. In the first phase, each task from a feasible application task set is statically assigned to a specific processor. If a task can not be partitioned on any processor in the platform, it qualifies as  migrating  task. In the second phase, processors are clustered together such that, per cluster, the unused fragmented computation power equivalent to at most one processor is available. We provide schedulability analysis and experimental evaluation to support our proposition. Moreover, simulation results show an average difference of 18-folds in the number of task preemptions and 10-folds in the number of task migrations compared to multiprocessor optimal scheduling algorithm PD."
1087387,14018,8806,A data reduction and organization approach for efficient image annotation,2013,"The labor-intensive and time-consuming process of annotating data is a serious bottleneck in many pattern recognition applications when handling massive datasets. Active learning strategies have been sought to reduce the cost on human annotation, by means of automatically selecting the most informative unlabeled samples for annotation. The critical issue lies on the selection of such samples. As an effective solution, we propose an active learning approach that preprocesses the dataset, efficiently reduces and organizes a learning set of samples and selects the most representative ones for human annotation. Experiments performed on real datasets show that the proposed approach requires only a few iterations to achieve high accuracy, keeping user involvement to a minimum."
27108,14018,23827,Securely accessing shared resources with concurrent constraint programming,2012,"We present a fine-grained security model to enforce the access control on the shared constraint store in Concurrent Constraint Programming (CCP) languages. We show the model for a nonmonotonic version of Soft CCP (SCCP), that is an extension of CCP where the constraints have a preference level associated with them. Crisp constraints can be modeled in the same framework as well. In the considered nonmonotonic soft version (NmSCCP), it is also possible to remove constraints from the store. The language can be used for coordinating agents on a common store of information that represents the set of shared resources. In such scenarios, it is clearly important to enforce the integrity and confidentiality rights on the resources, in order, for instance, to hide part of the information to some agents, or to prevent an agent to consume too many resources."
1183580,14018,8806,Considering non-functional aspects in the design of hypermedia authoring tools,2011,"Hypermedia content workflows involve many environments and actors, from the content producer to the service operator, from the editing studio to the service head-end. They all have different needs on hypermedia content creation and editing. Nowadays, even viewers are demanding tools to enrich content. But currently a single authoring tool cannot fulfill their different requirements. This paper deliberates on the importance of non-functional aspects in the development of new hypermedia authoring tools. It also proposes an architecture that enables tools to meet these different authors' requirements, relying on its extensibility, customizability, robustness and scalability."
1091476,14018,8806,A secure coordination of agents with nonmonotonic soft Concurrent Constraint Programming,2012,"We present a fine-grained security model to enforce the access control on the constraint store in Concurrent Constraint Programming (CCP) languages. We show the model for a nonmonotonic version of Soft CCP (SCCP), that is an extension of CCP which deals with soft constraints, which are constraints that have a preference level associated to them. In the considered nomonotonic version ( NMSCCP ), the language is equipped with actions that can also remove constraints from the store. The language can be used for coordinating the agents on a common store of information. Clearly, in such scenario, is important to enhance the operations that the agents may perform with security and privacy, in order to limit their behavior, e.g., to hide some information or to prevent an agent to consume too many resources."
1186082,14018,8806,Data binding for standard-based web applications,2012,"Development frameworks have proven to drive developer productivity and flourish in a variety of different application domains. For example, data binding frameworks supporting the process of associating User Interface (UI) elements and data objects can reduce implementation efforts significantly. Even so desktop application developers have access to a myriad of data binding frameworks (e.g. Eclipse JFace); Web development still lacks decent data binding framework support.   Therefore, we devised a WebSocket-based Data-binding (WebSoDa) framework which is capable of easing the cumbersome and error-prone task of coupling UI elements with their respective data objects. Besides providing a Microdata-based data binding language, the WebSoDa framework embraces the WebSocket protocol. We show that the network traffic in data binding scenarios can decrease considerably by applying a WebSocket-based framework instead of a conventional AJAX approach."
899233,14018,8806,Keyboard navigation mechanisms in tab widgets: an investigation on ARIA's conformance,2014,"This study presents an investigation on how keyboard accessibility has been delivered in RIA -  Rich Internet Applications.  We conducted an evaluation on 32 websites which contained Tab Widgets, from the 150 websites of Alexa's top most accessed websites list. The evaluation process consisted of checking if the Widgets implemented ARIA -  Accessible Rich Internet Applications  requirements, like the use of role/state semantic attributes and presentation of keyboard interaction strategies. The results showed that, even though the ARIA specification achieved the status of W3C Candidate Recommendation in 2011, few websites implemented Tab Widgets according to ARIA in the Web. The study also identified alternative keyboard navigation mechanisms that are accessible to Assistive Technologies users, despite the disadvantages they might represent."
1556589,14018,8806,Model-Driven Development with eUML-ARC,2012,"Model-Driven Development (MDD) with eUML-ARC uses a synthesis of executable UML and the ARC (Agent, Role, Coordination) programming model. An entity in the ARC model is composed of concurrent role-based agents, enabling collaboration-based design and exposing both inter-entity and intra-entity parallelism, thereby facilitating the development of software systems that execute efficiently on multi-core hardware. Concurrency in eUML-ARC is based on the Actor model, which provides a simpler and more formal treatment of concurrency than found in standard UML or other approaches to executable UML. The coordination required by collaboration-based designs is separated from other computation and enacted by coordination agents upon coordinated role-based agents. In this paper, we examine the distinguishing features of the eUML-ARC approach to MDD, including the support for hierarchical state machines, the simplified concurrency model, the structure of the ARC model, and coordination viewed as an orthogonal concern. As a case study, a benchmark system is specified as an eUML-ARC model and deployed to a multi-core computer."
1105884,14018,8806,Generalizing the like button: empowering websites with monitoring capabilities,2014,"Increasingly, a user's action in a website might have an impact in other websites. The  Like  and  ShareThis  buttons are forerunners of this tendency whereby websites strive to influence and be influenced by the actions of their users in the websphere. The term Web Radar is coined to denote software that serves to impact a website (the host) from what is happening somewhere else in the websphere (i.e. the target). Current approaches provided limited expressivity in either the reactions (e.g. the  Like  button is limited to write entries on the user's wall in Facebook), or the range of participating sites (pre-set in the Radar platform, e.g.  Ifttt ). We believe supporting Radars as configurable services might account for more domain-specific Radars, i.e. Radars where the monitoring sites, the tracking conditions and the reactions are not fixed by the Radar platform but rather determined by the Radar host. This vision is confronted with three main challenges: API heterogeneity, scalability and technical complexities. We address these matters in  RadarThis , a service that permits webmasters to set Web Radars for their websites. We capitalize on  YQL  to hide  API  complexity, and use  trigger -like syntax to specify custom radar strategies. A case study is presented using the website  Instapaper  as the Radar host."
1108964,14018,8806,ProQuPri: towards anonymity protection with privacy quantification for context-aware applications,2011,"Privacy is the most often-cited criticism of context awareness in pervasive environments and may be the utmost barrier to its enduring success. Users certainly desire to be notified of potential data capture. Context-based pervasive applications have the vulnerabilities of tracking and capturing extensive portions of users' activities. Whether such data capture is an actual threat or not, users' perceptions of such possibilities may discourage them from using and adopting pervasive applications. So far in context-based pervasive applications, location data has been the main focus to make users anonymous. However in reality, anonymity depends on all the privacy sensitive data collected by the applications. Protecting anonymity with the help of an anonymizer has the susceptibility of a single point of failure. In this poster, we propose a formal model ProQuPri ( Pr otect An o nymity and  Qu antify  Pri vacy) that preserves users' anonymity without anonymizer while quantifies the amount of privacy at the time asking for services from untrustworthy service providers. Before placing a request, each user can protect his own anonymity by collaborating with his peers."
1434935,14018,8806,Sense-respond cloud mediator architecture for services evolution,2011,"In many dynamic externally-driven enterprises, like the Government, citizens participation in the design and evolution of services is critical and essential for satisfaction. Web 2.0 features are changing the way citizens interact and participate. Here we leverage this with a sense-respond architecture between  two clouds: a)  a front-end social networks of communities and citizens, and  b)  back-end enterprise application services. We introduce an Adaptive Complex Environment (ACE) ontology and mediator architecture for complex service-intensive organizations. This mediator architecture is illustrated using Web 2.0 (e.g. Facebook services) to make  tacit  service requirements of user communities  explicit , leading to the adaptation in the use of back-end services. We also present a specific eGovernment application scenario where the services are identified, designed, delivered  and evolved  by employing ACE. We thus motivate research in management environments in which stakeholder participation, development, and operational aspects are integrated so that services can be co-engineered (i.e. collaboratively, continuously and concurrently)."
1091191,14018,8806,Lessons learned from an online open course: a Brazilian case study,2014,"This article presents some lessons learned regarding the analysis of interactional data from an online course that provided certified basic level in Spanish language (UFAL Linguas - Espanhol). The data was collected after the end of the course, and concerned the students' interactions with the learning environment's educational resources, that were represented and stored using ontologies, and used by the Pedagogical Recommendation Process. This process aims to detect pedagogical practices happening in the classroom, discover the patterns responsible for these practices, create recommendations to improve the students' performance, and monitor and evaluate if the process is working appropriately. In the end of the analysis, we identified that a considerable amount of dropouts, and other students who were very close to approval, failed. The results showed that if we had used the Pedagogical Recommendation Process during the progress of the course, we could have rescued some of these dropouts and assisted some who failed."
1160185,14018,8806,A low cost digital operating room,2014,"This paper presents a low cost solution for Digital Operating Rooms called MIR (Multimedia Integrated Room). MIR is capable of real time transmission and recording of endoscopic procedures using multiple HD cameras, integration with the hospital systems and also can be easily controlled by a wireless mobile device. The developed system main application areas are: a) medical students and specialist preceptor to follow the surgical procedure remotely; b) the surgical team to better visualize the surgery."
1709759,14018,8806,Towards a learning design authoring tool that generates personalized units of learning for CSCL,2014,"The generation of personalized Unit of Learning (UoLs), such as a course, module, or lesson, is one of the most studied topics within the field of adaptive learning environments. Nevertheless, the authoring of these UoLs in the context of Computer Supported Collaborative Learning (CSCL) using current learning design authoring tools is a complex task that requires practice and experience. Usually, practitioners (i.e. non-expert teachers) have difficulties to use these tools because they must manually define all UoL components (such as group composition, students' roles, learning activities/goals, types of interactions, and so on) taking into account the needs and preferences of each learner. This paper introduces a learning design authoring tool, referred to as  Adaptive Learning Design  (ALD), that helps instructional designers to semi-automatically generate personalized UoLs for CSCL. ALD uses a courseware web service that employs the representation of Instructional Design (ID) using CSCL Script Design Patterns as Hierarchical Task Network (HTN) Planning. We present the architecture of this courseware and others web services used by ALD, our goal is to show the facility of our approach and some of the functionalities it offers for non-expert."
2107489,14018,8806,"Hierarchical visual filtering, pragmatic and epistemic actions for database visualization",2013,"Visualization techniques of all sorts suffer from visual cluttering, the occlusion of visual information due to the overlap of graphical items; and from excessive complexity in analytical tasks due to multiple parallel perspectives. To cope with these problems, we introduce Hierarchical Visual Filtering, a novel interaction principle based on pragmatic and epistemic actions. Pragmatic actions here mean that the analyst is able to visually select and filter information, determining visual configurations that reveal different perspectives; epistemic actions mean that the analyst can record, annotate, and recall intermediate visualizations created pragmatically. To do so, we use a tree-like organization to keep multiple visualization workspaces linked according to the analytical decisions took by the user. Our goal is to promote an innovative systematization that can augment the potential for database visual inspection, and for visualization systems in general. It is our contention that Hierarchical Visual Filtering can inspire a novel scheme of visualization environments in which space limitations and complexity are treated by means of interactive tasks."
908015,14018,8806,Method for visualizing undone operations based on changes on desktop screen,2014,"The undo operation of graphical user interfaces is widely used. However, conventional implementations of undo have two problems: 1) users may not notice undone parts since the operation is instantaneous, and 2) it is difficult to know the undone contents if the user temporarily interrupts a work the user works on multiple operations in parallel. Although there have been several methods developed that improve undo operations, they are for specific applications. Here, we devised a visualization for undone operations that, is independent of the running applications and based on the idea that an undo operation causes display to change. Our method specifies the area affected by the undo operation by screens. We implementedseveral visualization methods including emphasizing the undone area, presenting the undone content, and notifying the user that the undone is not visible."
1289505,14018,8806,msocket: multiple stack support for the berkeley socket API,2012,"The  de-facto  standard for network programming, the Berkeley socket API, supports several protocol families. Unfortunately, it has a significant limitation in only allowing a single implementation for each supported protocol family. Hence, using Berkeley sockets, it is impossible to access multiple distinct networking stacks for the same protocol, e.g. multiple TCP/IP stacks. This paper defines, msocket, an extension to the Berkeley socket API which overcomes this limitation. msocket has been implemented as a feature of the View-OS project. Finally, we illustrate the utility and effectiveness of our extended API by providing some examples of its use."
980090,14018,8806,Executing and debugging UML models: an fUML extension,2013,"With the widespread of the Model-Driven Development (MDD) and surfing on the success of the Unified Modeling Language (UML), software development is shifting from being code-centric to model-centric. Models become the key artefacts in the software development process. The success of the project relies on the quality of these models. Early detection of errors by debugging and testing these models is mandatory in order to reduce development cost, ensuring quality and preventing rework at later stages. The fUML standard defines the precise semantics for executing a subset of UML models by defining a virtual machine. The models are then directly executed without transformation. However, the virtual machine is defined to execute the model as an atomic action and does not fulfil the requirements for debugging it. We highlight in this paper the limit of the current specification of fUML (v1.0) and propose an approach for extending the virtual machine with the key functionality that enables debugging of fUML models. A working UML debugger prototype has been implemented and the use and evaluation of the approach are made on a case study."
976990,14018,8806,Integrity of electronic voting systems: fallacious use of cryptography,2012,"In recent years, electronic voting systems have been deployed in all U.S. elections. Despite the fact that cryptographic integrity checks are used in most such systems, several reports have documented serious security vulnerabilities of electronic voting terminals. We present an overview of the typical security and election vulnerabilities found in most, if not all, electronic election systems, and present a case study that illustrates such vulnerabilities. Our hands-on security analysis of the AccuVote TSx voting terminal --- used by more than 12 million voters in over 350 jurisdictions in the U.S. --- demonstrates certain new integrity vulnerabilities that are present in the system. We present two attacks based on these vulnerabilities: one attack swaps the votes of two candidates and another erases the name of one candidate from the slate. These attacks do not require modification of the operating system of the voting terminal (as was the case in a number of previous attacks) and are able to circumvent the cryptographic integrity checks implemented in the terminal. The attacks can be launched in a matter of minutes and require only a computer with the capability to mount a PCMCIA card file system (a default capability in most current operating systems). The attacks presented here were discovered through direct experimentation with the voting terminal and without access to any internal documentation or the source code from the manufacturer."
1968491,14018,8806,Yet another algorithm for generalized Voronoï Diagrams,2012,"We design and implement an efficient algorithm for the computation of generalized Voronoi Diagrams (VD's) constrained to a given domain. Our framework is general and applicable to any VD-type where the distance field is given by a polynomial. We use the Bernstein form of polynomials to subdivide the domain and isolate bisector domains or domains that contain a Voronoi vertex. Efficiency is due to a filtering process, based on bounding the distance functions over the subdivided domains. The output is a polygonal description of each Voronoi cell up to any user-defined precision."
1246216,14018,8806,Practical use of static composition of refactoring operations,2013,"Refactoring tools are commonly used for remodularization tasks. Basic refactoring operations are combined to perform complex program transformations, but the resulting composed operations are rarely reused, even partially, because popular tools have few support for composition. In this paper, we recast two calculus for static composition of refactorings in a type system and we discuss their use for inferring useful properties. We illustrate the value of support for static composition in refactoring tools with a complex remodularization use case: a round-trip transformation between programs conforming to the Composite and Visitor patterns."
1103738,14018,8806,Multi-agent simulation of perception of safety from crime,2014,"Public perception of safety from crime and actual crime statistics are often mismatched. Perception of safety from crime is a social phenomenon determined and affected by (i) the mass media broadcasting news dominated by violent content, and (ii) the structural composition of the society, e.g., its socioeconomic characteristics. This paper proposes an agent-based simulation framework to analyze and study public perception of safety from crime and the effects of the mass media on safety perception. Agent-based models for (i) information sources, i.e., mass media outlets, and (ii) citizens are proposed. In addition, social interaction (and its influence on the perception of safety) is modeled by providing citizen agents with a network of acquaintances to/from which citizen agents may transmit/receive crime-related news. Experimental results show the feasibility of simulating perception of safety from crime by obtaining simulation results consistent with generally known and accepted macro-level patterns of safety perception."
844374,14018,8806,@Java: annotations in freedom,2013,"The ability to annotate code and, in general, the capability to attach arbitrary metadata to portions of a program are features that have become more and more common in programming languages. In fact, various programming techniques and tools exploit their explicit availability for a number of purposes, such as extracting documentation, guiding code profiling, enhancing the description of a data type, marking code for instrumentation (for instance, in aspect-oriented frameworks), and the list could go on.   While support to attach metadata to code is not a new concept (programming platforms as CLOS and Smalltalk have pioneered in this field), consistent, pervasive APIs to define and manage code annotations are something comparatively recent on modern platforms like the .NET and Java.   Annotations in Java make possible to attach custom, structured metadata to declarations of classes, fields and methods. With this work, we propose an extension to Java (named @Java) that has a richer annotation model, supporting code block and expression annotations. In other words, the granularity of annotations extends to the statement and expression level and does not limit to class, method and field declarations."
1668185,14018,8806,A feasibility analysis on using bathymetry for navigation of autonomous underwater vehicles,2013,"Bathymetric terrain maps generated from acoustic data offer an attractive alternative for reducing the submerged pose error estimates for autonomous underwater vehicles (AUVs). The goal of this work is to determine the extent of improvement in the navigational accuracy of an AUV equipped with an echo sounder for near-seafloor, shallow water applications. Given bathymetric variations of a certain terrain, this paper analyzes the best achievable positioning accuracy for AUVs. To counter for the strong non-linearity and the non-Gaussian nature of the problem, an optimal Bayesian estimator is initially derived. The fundamental limitations in the pose uncertainty using this approach is encompassed by the Posterior Cramer-Rao bound (PCRB), that is interpreted in terms of the sonar sensor accuracy and the bathymetric variations. The PCRB on the position error covariance is determined and it is shown that the Bayesian Bootstrap filter closely follows this bound using real inferometric sonar data."
1576967,14018,8806,A data warehouse as an infrastructure to mine molecular descriptors for virtual screening,2013,"In a Rational Drug Design (RDD) one important step is the receptor-ligand interaction evaluation through molecular docking simulations. How it is a way impossible to test all available compounds for a target receptor, there is a need to select the most promising. One possible approach for such selection is to consider characteristics like a set of molecular properties called molecular descriptors. Aiming at describing these characteristics, we introduce a Data Warehouse (DW) model that integrates molecular descriptors from different public databases of compounds, as well as relates them with Virtual Screening (VS) experiments data. With the proposed DW we are able to produce proper data sets for classification mining experiments. We performed a case study with a VS considering as receptor the HIV-1 Protease receptor and 76 compounds. The data sets produced from our DW are composed by 7 molecular descriptors as the predictive attributes, and as a target attribute the discretized Free Energy of Binding (FEB) value between the ligands and the target receptor. By performing C4.5 algorithm over the generated data sets, we got decision-trees models that indicates which molecular descriptors and their respective values are relevant to influence on good FEB results."
2214038,14018,9080,Evolutionary fabrication: a system of autonomous invention,2012,"Evolutionary algorithms have had success in designing complex objects, ranging from antennae to telescope lenses. However, evolutionary design is limited by the ability of a simulation to accurately represent the physical world. Additionally, evolved designs carry no set of specific instructions describing how to physically create such a design. Our approach, called Evolutionary Fabrication, evolves a \emph{process} rather than a product. This system of evolution can, in principle, automatically invent and build anything, from soft robots to new toys. We have implemented Evolutionary Fabrication in designing EvoFab, a machine that consists of four components: A) a genotype for printing objects, consisting of a linear set of instructions sent to a Fab@Home, an open-source 3D printer; B) a way to evaluate printed objects using custom machine vision algorithms; C) a way to automate printing by implementing a custom conveyor belt; D) a way of elaborating upon designs by implementing a genetic algorithm. In the near term, we aim to produce an evolved arch. Current results indicate increased fitness over time. Future improvements are possible through restrictions in extrusion along the Y-axis as well as refining fitness evaluation to be less exploitable."
855060,14018,8806,Efficient clustering of populations using a minimal SNP panel,2011,"The recent explosion in available SNP data requires exploration of satisfactory methods for efficiently clustering a set of individuals into their respective populations. As a practical matter, we describe a modified euclidean distance-based approach for successfully clustering 525 HapMap individuals into their 4 original populations; European, African, Japanese and Chinese. Our approach relies on the computation of the  F   st   estimator using 4 distinct methods, and shows that that the k-means clustering of the 10 highest  F   st   scoring SNPs is sufficient for producing an error-free description of the underlying population structure. A generalization of our approach represents a more faithful way of selecting the SNPs having the highest discriminating power and thus generating the most accurate population specific assignments of individuals, using the smallest possible SNP panel."
2316524,14018,8806,Operating system support for dynamic over-provisioning of solid state drives,2012,"Employing  solid state drives (SSDs)  can leverage the performance of persistent storage systems into a new dimension. However, in order to ensure a continuously high write throughput especially for small random writes, it is crucial to always maintain a substantial amount of free flash capacity. This can be achieved by additional over-provisioning or/and the TRIM command, which notifies an SSD of storage space no longer required. Since additional over-provisioning is disadvantageous as SSDs already have a higher cost per byte ratio compared to hard disk drives, using TRIM seems to be favorable. However, most intermediate software layers (e.g., filesystem encryption, software RAID drivers or a logical volume manager) but also hardware RAID controllers currently do not pass TRIM commands to the underlying devices making over-provisioning look like the only solution feasible now.   In this paper, we tackle this problem by dynamic over-provisioning, allowing the OS to use additional storage capacity for a limited amount of time to supply more storage in peak demand situations. By doing this, we accept a temporarily degraded performance to be able to supply further storage capacity. After a peak situation, the utilized SSDs are notified using the TRIM command about allocated storage capacity that is no longer needed. By presenting experimental results, we show that dynamic over-provisioning is working and can be quite effective for both single SSDs and storage systems with multiple SSDs such as SSD RAIDs."
1383069,14018,8806,DRAP: a Robust Authentication protocol to ensure survivability of computational RFID networks,2012,"The Wireless Identification and Sensing Platform (WISP) from Intel Research Seattle is an instance of Computational RFID (CRFID). Since WISP tags contain sensor data along with their  IDs , the authentication of these tags should be different than the ones found in typical RFID networks. Moreover, there can be a situation where an adversary may create more collisions at the link layer to initiate the  de-synchronization  attack. This in turn may result in DoS attack or system failure and may increase the system response time. In this paper, we propose a  De -synchronization attack resistant  R obust  A uthentication  P rotocol ( DRAP ) for WISP networks. Our proposal offers a lightweight solution to defend against major security attacks, including de-synchronization and DoS attacks. This protocol can return desynchronized tags and readers to their synchronous state. Therefore, it provides robustness and ensures survivability. We evaluate DRAP protocol in a simulated WISP environment and report the results in this paper."
2229256,14018,9080,Evolving spiking networks with variable memristors,2011,"This paper presents a spiking neuro-evolutionary system which implements memristors as neuromodulatory connections, i.e. whose weights can vary during a trial. The evolutionary design process exploits parameter self-adaptation and a constructionist approach, allowing the number of neurons, connection weights, and inter-neural connectivity pattern to be evolved for each network. Additionally, each memristor has its own conductance profile, which alters the neuromodulatory behaviour of the memristor and may be altered during the application of the GA. We demonstrate that this approach allows the evolutionary process to discover beneficial memristive behaviours at specific points in the networks. We evaluate our approach against two phenomenological real-world memristive implementations, a theoretical linear memristor, and a system containing standard connections only. Performance is evaluated on a simulated robotic navigation task."
1205034,14018,8806,Modeling the dynamics of caching in content-based publish/subscribe systems,2011,"This paper considers cache dimensioning in the context of publish/subscribe (pub/sub) systems. We assume that each broker is equipped with a limited capacity cache and it decides upon a policy for caching and prioritizing messages. By using a request mechanism defined on top of the native pub/sub communication, a client may also request earlier published information. To study the  survival time  of published messages, a Markovian system model capturing the essential dynamics is defined. The model has a modular generic form which admits a variety of different policies and thus enables the calculation of their performance. For systems without message replication between the caching brokers, the distribution of message survival time is found using matrix analytic methods for solving absorbing Markov chains. For the general problem with messages copied from caches, we propose a heuristic approximation based on estimating the mean rate of copies. The approximate model is evaluated by a discrete event simulator and it is shown that for a wide set of parameters, the approximation provides a good basis for dimensioning the caches in the content-based pub/sub systems."
1294137,14018,8806,Improving 3D navigation in multiscale environments using cubemap-based techniques,2011,"Navigation in virtual 3D environments, especially those with multiscale features, is still a problem for many users. In this regard, a good design of the navigation interfaces is critical to ensure that the users navigate with the best possible efficiency and comfort. In this paper, we present improvements made to two well-known interfaces:  fly , including support to collision treatment and automatic navigation speed adjustment in relation to scale, and  examine , with automatic pivot point. Such techniques are based on the  cubemap  structure. Usability tests have shown a significant improvement in the execution of navigation tasks."
713081,14018,8806,Development of the wireless embedded sensor network for energy-efficient flooding,2011,"In the wireless embedded sensor network (WESN), a broadcast mechanism is required for flooding to embedded computers. These embedded computers or sensor nodes are energy-constrained because they are battery-powered. In this paper, we propose an energy-efficient flooding mechanism which suits well to the static WESN by designating a subset of sensor nodes in order to forward broadcast messages. When deciding forwarding nodes, we consider the connectivity to the sink and the coverage of the entire sensor network. To validate feasibility, we implement the prototype of our mechanism on TinyOS 2.0 with Mica-2 motes. Through simulations, we show that our mechanism outperforms conventional mechanisms in terms of the network lifetime and the data delivery ratio."
779202,14018,8806,Designing a 3D widget library for WebGL enabled browsers,2013,"Emerging web technologies such as HTML5 and WebGL have rapidly altered the landscape of web application development. Indeed, it is increasingly feasible to develop interactive applications with web technologies only, with no vendor-specific plugins that require separate installation. Moreover, following the generic trend towards 3D systems, it is also becoming increasingly feasible to create 3D content inside the web. However, many of the available options build on low-level facilities, which, while well-suited for demos, complicate the development of true applications. In this paper, we aim at simplifying the development of 3D web applications by designing a 3D widget library that uses WebGL as its rendering engine, and give a sample application that uses the facilities of the library. In addition, we list lessons learned from the design and implementation process."
1009585,14018,8806,RevGlyph: a technique for reverting anaglyph stereoscopic videos,2012,"In order to visualize stereoscopic videos, it is necessary a pair of videos of a same scene horizontally displaced -- the stereo pair --, thus requiring twice the space to be stored. The anaglyph conversion is a technique in which color components of the stereo pair are removed, being the remaining color components joined to form a single video. With that, it is possible to reduce at least by the half the amount of data to be stored. Unfortunately, the anaglyphic video is not supported by other stereoscopic visualization methods -- they require the stereo pair. Our proposal is to create an anaglyph reversion technique that would enable to recreate the stereo pair from an anaglyph video, which could make stereo information available for other visualization methods. Such reversion is not straightforward, since during the anaglyph conversion some color data is lost. In this paper, we introduce RevGlyph, an approach to the anaglyph reversion by storing the removed color components in a special structure that we call Color Index Table. We then discuss the results obtained from our experiments with this technique, achieving a good compression rate of 79.64% compared to the original image. The results also showed that the process does not interfere with the stereo depth perception."
856054,14018,8806,Protecting Android applications with steganography-based software watermarking,2013,"In this paper, we propose a steganography-based software watermarking scheme to protect Android applications from software piracy. Considering the resource limitations in mobile devices, it is difficult to apply previous software watermarking schemes in mobile applications. To decrease the watermarking overhead in a watermarking scheme, such as a slowdown in the execution of applications after embedding a watermark, we proposed a different approach that applies steganography-based techniques in software watermarking. The proposed scheme embeds watermarks by reordering the sequence of instructions in the basic blocks in Dalvik executable files."
2244788,14018,8806,Bridging socially-enhanced virtual communities,2011,"Interactions spanning multiple organizations have become an important aspect in today's collaboration landscape. Organizations create alliances to fulfill strategic objectives. The dynamic nature of collaborations increasingly demands for automated techniques and algorithms to support the creation of such alliances. Our approach bases on the recommendation of potential alliances by discovery of currently relevant competence sources and the support of semi-automatic formation. The environment is service-oriented comprising humans and software services with distinct capabilities. To mediate between previously separated groups and organizations, we introduce the  broker  concept that bridges disconnected networks. We present a dynamic broker discovery approach based on interaction mining techniques and trust metrics. We evaluate our approach by using simulations in real Web services' testbeds."
1907697,14018,8806,An approach to improve code-first web services discoverability at development time,2012,"Previous efforts towards simplifying Web Service discovery have shown that avoiding some well-known WSDL specification anti-patterns yield quite good results in making more discoverable services. The anti-patterns, however, have been studied with contract-first Web Services, a service construction methodology that is much less popular in the software industry compared to code-first. We study a number of source code refactorings that can be applied at service development time to reduce the presence of anti-patterns in code-first WSDL documents. The cornerstone of these refactorings is a statistical correlation between common object-oriented (OO) metrics and the anti-patterns computed by using a data-set of real Web Services. We quantify the impact of the refactorings on Web Service discovery and show that more clear WSDL documents are generated and service discovery is greatly improved."
1037942,14018,8806,Broadcast cancellation in search mechanisms,2013,"Searching for resources over unstructured networks is usually supported by broadcast communication primitives. Ideally, the broadcast process should be cancelled as soon as possible after a successful discovery, to avoid flooding the entire network. However, cancelling an ongoing broadcast is challenging and may increase the number of exchanged messages. In this paper, we compare the cancellation mechanisms used by BERS and BERS with new proposed cancellation approaches BCIR and BCIR. The formulation of a simplified analytical model and the simulation results show that:  i ) it is possible to reduce the number of retransmitted messages, without increasing the latency observed in BERS; and  ii ) BCIR is more energy efficient, which can contribute to extend the availability of mobile battery powered devices."
769649,14018,8806,Functional requirements validation by transforming use case models into Abstract State Machines,2012,"Use cases are commonly used to structure and document functional requirements while formal methods, such as Abstract State Machines (ASMs), are helpful to specify the behavior of a system and serve to validate system requirements. Therefore, automated support for the transition from use cases to formal models would provide significant, practical help for validating system requirements. This paper proposes the framework  AsmetaRE  to automatically transform Use Cases Models into ASM executable specifications, and then validate systems requirements through  simulation  and  scenario-based simulation  of the generated ASMs with the help of the ASM analysis toolset ASMETA."
957457,14018,8806,A compilation technique to increase X3D performance and safety,2012,"As virtual worlds grow more and more complex, virtual reality browsers and engines face growing challenges. These challenges are centered on performance on one hand (an interactive framerate is always required) and complexity on the other hand (the larger and more articulated a virtual world, the more immersive the experience).   The usual implementation of an engine or browser for running virtual worlds features an object-oriented architecture of classes. This architecture is a source of often underestimated overhead in terms of dynamic dispatching, and dynamic lookups by scripts when they try to access portions of the scene are both costly and possible sources of mistakes.   In this paper we discuss how we have tackled the problem of increasing performance in X3D browsers while also making scripts safe. We have used a compilation technique that removes some overhead and which allows us to introduce safety for scripts that access the state"
1339767,14018,8806,Disciplined structured communications with consistent runtime adaptation,2013,"Session types  offer a powerful type-theoretic foundation for the analysis of structured communications, as commonly found in service-oriented systems. They are defined upon core programming calculi which offer only limited support for expressing adaptation and evolvability requirements. This is unfortunate, as service-oriented systems are increasingly being deployed upon highly dynamic infrastructures in which such requirements are central concerns. In previous work, we developed a process calculi framework of  adaptable processes , in which concurrent processes can be replaced, suspended, or discarded at runtime. In this paper, we propose a session types discipline for a calculus with adaptable processes. Our framework offers an alternative for integrating runtime adaptation mechanisms in the analysis of structured communications. We show that well-typed processes enjoy  consistency : communicating behavior is never interrupted by evolvability actions."
1559278,14018,8806,A domain specific language for process scheduling,2014,"Process scheduling is one of the main tasks of an operating system. It enables multi-tasking, ensures fairness in processing resource distribution among processes and has a significant influence on the power consumption of a system. To ensure those requirements is a complex task and results in a huge amount of code (e.g. about 13000 lines of code in the Linux kernel), which is hard to understand and maintain, not to mention to enhance and improve. Taking Linux as an example, its community tackled that issue by introducing  scheduler classes  [1]. These classes are scheduler modules where only a certain set of predefined functions need to be implemented. The Completely Fair Scheduler and the POSIX Fixed-Priority Real-Time Scheduler -- that come with every kernel since version 2.6.23 -- are examples of these scheduler classes. However, these classes are still pretty complex and it is hard to integrate new functionality like event listeners that alter the scheduling algorithm on certain events (e.g. energy consumption goes up)."
972773,14018,30,Fuzzy Logic-Based Prognostic Score for Outcome Prediction in Esophageal Cancer,2012,"Given the poor prognosis of esophageal cancer and the invasiveness of combined modality treatment, improved prognostic scoring systems are needed. We developed a fuzzy logic-based system to improve the predictive performance of a risk score based on the serum concentrations of C-reactive protein (CRP) and albumin in a cohort of 271 patients with esophageal cancer before radiotherapy. Univariate and multivariate survival analyses were employed to validate the independent prognostic value of the fuzzy risk score. To further compare the predictive performance of the fuzzy risk score with other prognostic scoring systems, time-dependent receiver operating characteristic curve analysis was used. Application of fuzzy logic to the serum values of CRP and albumin increased predictive performance for one-year overall survival (AUC = 0.773) compared with that of a single marker (AUC = 0.743 and 0.700 for CRP and albumin, respectively), where the AUC denotes the area under curve. This fuzzy logic-based approach also performed consistently better than the Glasgow prognostic score (AUC = 0.745). Thus, application of fuzzy logic to the analysis of serum markers can more accurately predict the outcome for patients with esophageal cancer."
718819,14018,8806,Patterns for configuration requirements of Software-as-a-Service,2011,"Software-as-a-Service is becoming popular in the software business, due to its rapid delivery and cost effectiveness in development and maintenance. Software-as-a-Service should be provided in single code base and operated as a single instance. To meet these constraints and requirements from various customers, Software-as-a-Service must be highly configurable. To develop configurable Software-as-a-Service, it is important to elicit and analyze configuration requirements in the early stages of development. Another issue is that on implementing configuration requirements, there are duplicated and untidy code segments. In our study, configuration requirements are identified and classified. This study introduces design patterns to remove duplicated codes for configuration."
1463195,14018,8806,An empirical study of requirements-based test generation on an automobile control system,2014,"Requirements-based test generation techniques have been widely used in industry to help practitioners generate appropriate test cases to ensure that their software systems behave according to customer expectations. These tests provide a solid foundation for functional testing -- a critical, essential step that must be performed properly for quality assurance of any software system. In this paper, we present a study, in collaboration with Hyundai Motor Company, on a real-life industrial software system used to control the driver's demand torque of an automobile. Since any hazardous incident caused by this mechanism may result in significant property loss or even fatalities, its safe and reliable operation becomes absolutely imperative. To overcome this problem, we propose a framework including different requirements-based test generation techniques such as equivalence class partitioning (ECP), boundary value analysis (BVA), a choice relation framework, and predicate testing-based BOR, BRO, and BRE strategies. In addition, a tool named C-Set has been developed to support the BOR, BRO and BRE-based automatic test generation. Results from our study indicate that weaknesses in any of the above techniques can be mitigated by exploiting the strengths of the other techniques. As a result, a set of high quality test cases for the software being studied can be efficiently generated to help Hyundai engineers improve their productivity."
1033498,14018,8806,A model for drosophila melanogaster development from a single cell to stripe pattern formation,2012,"The development of multicellular organisms, from the early forms of zygote, involves a range of phenomena that control cell growth and differentiation, making the overall process of morphogenesis highly complex. A well studied example of such a huge phenomenon is given by  Drosophila Melanogaster  morphogenesis that has been object of several models, whose main goal was to investigate the mechanisms involved in the spatial and temporal evolution of the patterning process, namely gene regulatory network, mor-phogen diffusion, synthesis and degradation.   In this paper we present a model of  Drosophila  development that considers also nuclear division and movements as basic morphogenetic mechanisms. The model is run on top of a prototype simulator which is based on a variation of an existing SSA (Stochastic Simulation Algorithm), tailored to the specific features of embryo development, including dynamicity in the topology of compartment network."
1690064,14018,8806,A comparison of metadata extraction techniques for crowdsourced bibliographic metadata management,2012,"Social research networks such as Mendeley and CiteULike offer various services for collaboratively managing bibliographic metadata and uploading textual artifacts. One core problem thereby is the extraction of bibliographic metadata from the textual artifacts. Our work investiages the use of Conditional Random Fields and Support Vector Machines, implemented in two state-of-the-art real-world systems, namely ParsCit and the Mendeley Desktop, for automatically extracting bibliographic metadata. We compare the systems' accuracy on two newly created real-world data sets gathered from Mendeley and Linked-Open-Data repositories. Our analysis shows that two-stage SVMs provide reasonable performance in solving the challenge of metadata extraction from user-provided textual artifacts."
876914,14018,8806,Time based data forensic and cross-reference analysis,2011,"Data forensics is becoming increasingly important as computer related crimes intensify. In forensic investigations, temporal evidence plays a crucial role. However, the inherent volatility of time information and the tampering of such information through anti-forensic techniques have significantly lowered the reliability of temporal evidences, and posed great challenges to simple time-based forensics. To overcome this problem, this paper proposes a cross-reference time-based forensics approach for NTFS by analyzing both the discrepancies and similarities among various temporal evidences associated with file metadata and the registry. Experiment results show that our approach can reliably identify certain intrusion activities such as malicious access, modification, copy and tampering of timestamps. Some thought about dealing with anti-forensics is also provided in our analysis."
1570870,14018,8806,A dynamically reconfigurable operating system for manycore systems,2013,"In this paper, we suggest a partitioning OS, a novel locality-aware resource management scheme for manycore systems. It manages manycore resources as a hierarchical manner and allocates cores and memory as close each other as possible while dynamically considering both multiple applications' needs and ever-changing system status. Also we can dynamically (re)configure OS features for each partition according to the needs of the application on it. We show the effectiveness of the proposed method through a real-life application, ray tracing, and achieves a scalable speedup, 13.46 times at 16 cores."
2115728,14018,9080,Modular approach for the optimal wind turbine micro siting problem through CMA-ES algorithm,2013,"Although, only in recent years, northern European countries started to install large offshore wind farms, it is expected that by 2020, several dozens of far and large offshore wind farms (FLOWFs) will be built in the Baltic, Irish and North seas. These FLOWFs will be constituted of a considerable amount of wind turbines (WTs) packed together, leading to an energy density increase. However, due to shadowing effects between WTs, power production is reduced, resulting in a revenues decrease. Therefore, when FLOWFs are considered, wake losses reduction is an important optimization goal. This work presents a modular approach to optimize the energy yield of FLOWFs through an evolutionary algorithm. In order to do so the algorithm is set to find an optimal WF layout. The method consists of a modular strategy where the site wind rose information is used in different steps, which accelerates the calculation speed of the wake losses. The results presented demonstrate the method effectiveness. A computational time decrease is observed when compared to the standard optimization strategy, without jeopardizing the quality of the optimal layouts achieved."
1517790,14018,8806,QR-code based online robot augmented reality system for education,2014,"In this paper, we propose an efficient, augmented reality system for education utilizing scenarios acquired from the metadata of quick response (QR) code. Our system enables interaction using an assembled robot in a virtual environment. First, we build a virtual environment around the robot according to the metadata. In addition, we create and manage a wide variety of content using metadata. We estimate the camera pose by using both marker-based and markerless methods. Lastly, we use a Bluetooth module and metadata of QR codes to interact with objects in the virtual environment."
1180116,14018,8806,An architecture-centered framework for developing blog crawlers,2012,"Blogs have become interesting tools for knowledge generation and sharing. As a matter of fact, the activity on blogs doubles every two hundred days. Numerous applications could make use of this massive daily information in order to find out interesting interpretations. However, the dynamic nature of the blogosphere hinders the manual information extraction from it, promoting the development of new automated approaches. In this paper, we propose a component-based framework to create blog crawlers based on software architecture. This framework provides useful services for the blog analysis, including preprocessing, indexing, content extraction, classification, and tag recommendation. In addition, we report a case study represented by a blog recommendation system, which helps student interactions in educational forums. This research work also aims to demonstrate the effort reduction when creating an application for blog analysis caused by the proposed framework. Finally other aspects of the developed application, such as the system evolution impact, reusability, and instantiation cost are qualitatively discussed."
714580,14018,8806,Secure positioning in a UAV swarm using on-board stereo cameras,2014,"The widespread use of quadcopters as unmanned aerial vehicles (UAVs) provides a number of application possibilities, mostly using a collaborative swarm of small UAVs to optimise critical missions. The common application domains for small UAV swarms are surveillance, path planning, air-bone and as relay networks. Cooperative applications make use of the UAVs' locations to make decisions. However, security vulnerabilities must be considered when the system infers rights based on the UAV's location. An attacker can cheat the system by declaring a false or inaccurate location to gain access to resources that are restricted or to undertake malicious activities without detection. In this paper, we propose the use of a UAV payload stereo camera to measure the distance between a UAV, called a  prover  node, and the set of closest  verifier  nodes. Using a multilateration algorithm to estimate the prover's position, we performed simulations for two cameras with different capabilities. The simulations showed that the proposed technique provides 98% validation accuracy for distances up to 50  m  for 1280x720-resolution cameras and more than 99% validation accuracy for distances up to 100  m  for 1920x1080-resolution cameras. For distances greater than 100  m  between the prover and the verifiers, the accuracy depends exclusively on the stereo camera with the highest capacity."
1414396,14018,8806,A formal framework for a functional language with adaptable components,2011,"We propose a component programming language called FLAC,  Functional Language for Adaptable Components , on top of a functional programming language which authorizes full adaptability of components while ensuring type safety. The langage is given together with a type system that offers a complete static type ckecking of any programs (including adaptations) to ensure error-free run-time adaptations. Dynamic adaptability and static type checking might seem at first sight paradoxical, but our approach allows it because, first, we use a single language for traditional services and control services (i.e., services for adaptations), and secondly, a specific merge operation takes care of adaptations."
1449296,14018,8806,On the equivalence of certain fault localization techniques,2011,"Software fault localization is an expensive component of program debugging, and thus, many different types of fault localization techniques have been proposed over the recent years. Such techniques aim to rank program components (such as statements, blocks, functions, etc.) in decreasing order of their likelihood of being faulty, such that programmers may then examine the ranking starting from the top, until a fault is found. However, comparisons between fault localization techniques (to see which one is more effective) have generally been based on case studies and empirical data. In this paper we propose an equivalence relation by virtue of which two or more fault localization techniques may be considered equivalent if they produce identical rankings of program components, and are therefore, equally as effective. We then make use of the proposed equivalence relation to prove that several similarity coefficient-based fault localization techniques are in fact equivalent to one another. Furthermore, no case studies and/or data were required for any of the proofs of equivalency provided in this paper."
2711404,14018,8806,Parallelizing SuperFine,2012,"The estimation of the Tree of Life, a rooted binary tree representing how all extant species evolved from a common ancestor, is one of the grand challenges of modern biology. Research groups around the world are attempting to estimate evolutionary trees on particular sets of species (typically clades, or rooted subtrees), in the hope that a final supertree can be produced from these smaller estimated trees through the addition of a scaffold tree of randomly sampled taxa from the tree of life. However, supertree estimation is itself a computationally challenging problem, because the most accurate trees are produced by running heuristics for NP-hard problems. In this paper we report on a study in which we parallelize SuperFine, the currently most accurate and efficient supertree estimation method. We explore performance of these parallel implementations on simulated data-sets with 1000 taxa and biological data-sets with up to 2,228 taxa. Our study reveals aspects of SuperFine that limit the speed-ups that are possible through the type of outer-loop parallelism we exploit."
877416,14018,8806,Accelerated robustness testing of state-based components using reverse execution,2013,"This paper presents a methodology to test the robustness of reactive state-based software components by executing defined and undefined transitions of a UML Statechart specification, and therefore simulating a usage scenario under normal and abnormal conditions. The Z3 constraint solver is used to generate corresponding test cases. Since robustness testing is time-intensive due to a large number of undefined transitions to be tested, a main advantage of our presented approach is that it accelerates robustness testing by returning to reached states of a component, using reverse execution, and then refining them as a starting point for further tests. Furthermore, the presented approach provides runtime verification capabilities to test transition guards defined in the specification and therefore increases the test significance. This is done without instrumenting the source code. Due to the high acceleration achieved, our approach is well-suited for daily regression testing during component development."
1244750,14018,8806,nuKernel: MicroKernel for multi-core DSP SoCs with load sharing and priority interrupts,2013,"The demands of modern embedded systems are hastening the adoption of multicore SoCs. Although multicore SoCs can be conceptually viewed as distributed systems, the resources on multicore SoCs including interrupts and scheduling are mostly, if not all, managed by the operating systems on general purpose CPU on SoC in a centralized manner. This approach leads to heavy overhead on the general purpose CPU and does not scale up. This paper presents the design and implementation of a microkernel for multi-core DSP SoCs, named  nμKernel , to support real-time scheduling, load sharing among DSP cores, nested priority interrupts, and predictable interrupt latency jitter. The kernel takes advantage of both the shared memory architecture on multicore DSP SoCs and pipeline real-time scheduling to support load sharing. A server-base algorithm is designed for overrun control and to reduce load sharing overhead. The developed hybrid interrupt handling framework adopts on-demand interrupt thread mechanism to reduce interrupt handling overhead and support nested priority interrupts. The experiments show that the kernel can significantly enhance application performance with least management overhead. The frame rate of a secure image display application speeds up for six times: from 2.2 frames per second to 19 frames per second while workload are shared among DSP cores. The developed interrupt handling framework shortens the interrupted latency for up to 90%, compared to two-level interrupt handling mechanism and limits the range of interrupt latency to no more than 5%."
1368694,14018,8806,An analysis of anti-micro-patterns effects on fault-proneness in large Java systems,2012,"Micro patterns are similar to design patterns, but are at a lower level of abstraction, closer to the implementation. Anti patterns are micro patterns not respecting the prescriptions of good Object Oriented programming practices. In this paper, we use the definitions introduced by Arcelli and Maggioni [3] in order to study the evolution of five particular micro patterns (anti patterns) in different releases of the Eclipse and NetBeans systems, and the correlations between anti patterns and faults. Our analysis confirms previous findings regarding the high coverage of micro patterns onto the system classes, and show that anti patterns not only represent bad Object Oriented programming practices, but may also be associated to the production of lower quality software, since they present a fault proneness significantly enhanced."
1738997,14018,8806,A Büchi automata based model checking framework for reo connectors,2012,"Reo is an exogenous coordination language for synchronizing components participating in a component-based system. In this paper we provide a verification framework for model checking of Reo connectors. The proposed framework applies an extension of Buchi automata as the operational semantic model for Reo connectors and a record-based extension of linear time temporal logic (LTL) for expressing properties. Several aspects of Reo connectors, specially synchronization, context dependencies and fairness constraints, are addressed by this model checker due to its supported underlying model. The main ideas behind this implementation are to introduce a symbolic representation for the main elements of our model checking framework, adapt some existing theories to our verification context and develop a new BDD-based model checker with efficient performance. Moreover, all above mentioned features of Reo connectors are addressed by this toolkit. This implementation is evaluated by means of some case studies and the results are reported."
645740,14018,8806,Rapid computation of distance estimators from nucleotide and amino acid alignments,2011,"Distance estimators are needed as input for popular distance based phylogenetic reconstruction methods such as UPGMA and neighbour-joining. Computation of these takes  O ( n  2  l ) time for  n  sequences with length  l  which is usually fast compared to reconstructing a phylogenetic tree of  n  taxa. However, with the introduction of fast search heuristics for distance based phylogenetic reconstruction methods, the computation of distance estimators has become a bottleneck especially for long sequences. Elias et al. have shown how distance estimators can be computed efficiently from unaligned nucleotide sequences using vectorisation of code. In this paper we extend their method to allow efficient computation of distance estimators from aligned nucleotide and amino acid sequences using vectorisation of code and parallelisation on both CPUs and GPUs. Experiments are presented which show an increase in performance of up to 36x and 8x relative to the naive approach when computing distance estimators from nucleotides and amino acids alignments respectively."
747150,14018,8806,Aspect-oriented analysis for software product lines requirements engineering,2011,"Requirements analysis and modeling for Software Product Lines demands the use of feature models, but also requires additional models to help identifying, describing, and specifying features. Traditional approaches usually perform this manually and, in general, the identification and modularization of crosscutting features is ignored, or not handled systematically. This hinders requirements change. We propose an aspect-oriented approach for SPL enriched to automatically derive feature models where crosscutting features are identified and modularized using aspect-oriented concepts and techniques. This is achieved by adapting and extending the AORA (Aspect-Oriented Requirements Analysis) approach. AORA provides templates to specify and organize requirements based on concerns and responsibilities. A set of heuristics is defined to help identifying features and their dependencies in a product line. A tool was developed to automatically generate the feature model from AORA templates."
1735428,14018,8806,Representing a bilingual lexicon with suffix trees,2011,"This paper presents a system based on generalized suffix trees that efficiently implements a set of operations over a bilingual lexicon. Besides the basic operations of adding and removing translations from the lexicon, the system provides two unique query functions that we refer to as  monolingual  and  bilingual coverage . These two functions lay the foundation for higher-level mining operations, such as identification of translation patterns, that are the subject of ongoing research. Nevertheless, the system presented here is interesting in and by itself, for the novelty of the coverage functions and the potential of the whole data structure. We compare the performance of two implementations, one based on suffix trees and the other on suffix arrays."
1717894,14018,8806,The strength of social strength: an evaluation study of algorithmic versus user-defined ranking,2014,"A family relation is generally considered to be stronger than a relation between coworkers in our society, but the strength of the relation is intrinsic and have been cumbersome to measure. The fact that we increasingly communicate electronically, such as through email, mobile phone calls or social media, has made it possible to automatically measure and analyze the relation between persons. This paper presents an evaluation study of social strength, where the social strength is defined as a metric that represents the tie strength of the relation between persons, calculated based on the frequency, duration, context and media type of the electronic communication between the persons.   The study found that the Utility Function performs better because it emphasize the communication frequency between persons. There is however a significant difference in results between the algorithms and the user-defined ranking. This indicates the inability of the algorithms to capture intrinsic knowledge (such as the importance of family bonds and non-electronic interaction). This would mean that the participants' ranking was colored by their interaction in real-life. It is however evident from the study that the functions provide more accurate results when they utilize multiple sources of communication history over only a single source.   Finally, capturing sufficient communication data from multiple data sources is very hard, as access to such data is restricted because of concerns regarding for instance business and privacy. A conclusion is that the algorithms requires a larger data set, preferably being captured continuously over a period longer than 2 weeks, to achieve a better accuracy that is closer to the ground truth. However, the study shows the feasibility of capturing social strength automatically and we believe that the results is an important step towards systems that reason about the relation between persons in order to make communications services more pervasive."
1303079,14018,8806,Extending XForms with server-side functionality,2012,"Most Web applications are based on a conventional three-tier architecture, in which the presentation, application logic, and data management are developed and maintained in separate tiers. The main disadvantage of this architecture is that it requires expertise in multiple programming languages, programming paradigms, and data models used in each tier. A single expert rarely masters all technologies involved. In this paper, we introduce a framework that allows users---namely, Web designers---to implement entire Web applications using only markup languages. In addition, all application development is performed on the client side, simplifying both development and maintenance work. The proposed framework is based on the XForms markup language and its server-side extension proposed in this paper. We derive the extension requirements from the literature and depict its function using a simple Web-based blog application. We also show how the extension can be implemented as part of a comprehensive Web application development framework called XFormsDB. Our conclusion is that expanding the presentation tier to define both application logic and data management functionality makes both the development and maintenance of small- and medium-sized Web applications easier."
2449706,14018,9080,Efficient indexing of similarity models with inequality symbolic regression,2013,"The increasing amount of available unstructured content introduced a new concept of searching for information - the  content-based retrieval . The principle behind is that the objects are compared based on their content which is far more complex than simple text or metadata based searching. Many indexing techniques arose to provide an efficient and effective similarity searching. However, these methods are restricted to a specific domain such as the metric space model. If this prerequisite is not fulfilled, indexing cannot be used, while each similarity search query degrades to sequential scanning which is unacceptable for large datasets. Inspired by previous successful results, we decided to apply the principles of  genetic programming  to the area of database indexing. We developed the GP-SIMDEX which is a universal framework that is capable of finding precise and efficient indexing methods for similarity searching for any given similarity data. For this purpose, we introduce the inequality symbolic regression principle and show how it helps the GP-SIMDEX Framework to find appropriate results that in most cases outperform the best-known indexing methods."
1149179,14018,8806,Performance analysis of the golden-SM in the V2V network,2014,"This paper shows that golden code spatial modulation (Golden-SM) improves the performance of bandwidth efficiency and reception in vehicle-to-vehicle (V2V) networks. The conventional multi-input multi-output (MIMO) scheme, space-time block code (STBC), increases the reception performance in correlated channel but cannot increase the bandwidth efficiency since it transmits two symbols during two symbol duration. SM increases the reception performance in correlated channel, and golden code increases the bandwidth efficiency since it transmits four symbols during two symbol duration. Therefore, the simulation shows Golden-SM scheme has better performance than STBC in V2V network channels which have Rician channel properties with correlation between transmit antennas."
1170642,14018,8806,Top-k service compositions: a fuzzy set-based approach,2011,"Data as a Service (DaaS) is a flexible way that allows enterprises to expose their data. Composition of DaaS services provides bridges to answer queries. User preferences are becoming increasingly important to personalizing the composition process. In this paper, we propose an approach to compose DaaS services in the context of preference queries where preferences are modeled by means of fuzzy sets that allow for a large variety of flexible terms such as 'cheap', 'affordable' and 'fairly expensive'. The proposed approach is based on RDF-based query rewritings to take into account the partial matching between individual DaaS services and parts of the user query. Matching degrees between DaaS services and fuzzy preference constraints are computed by means of different constraints inclusion methods. Such degrees express to which extent a service is relevant to the resolution of the query. A fuzzification of Pareto dominance is also proposed to better rank composite services by computing the score of services. The resulting scores are then used to compute the top- k  DaaS service compositions that cover the user query."
1039334,14018,8806,Visualization of oceanographic applications using a common data model,2014,"Due to the rapid development of ocean observation technology, the volume of ocean data is increasing exponentially. To better utilize these data, more and more computational methods have been applied to exploring, sharing, and processing such data. Although great efforts have been made, there is an urgent need for a single platform containing a set of visualization components for every kind of ocean data. This paper presents a distributed, scalable and service-oriented visualization platform. The development of the platform is based on a common data model, by which ocean data with different spatial and temporal distribution can be organized and accessed in a uniform fashion, which sets a solid foundation for designing and implementing all kinds of visualization components and facilitates collaborative oceanographic research. The platform is run in China Ocean Observation System (COOS), and enables easy and convenient development of all kinds of visualization components for professionals and the general public and greatly advances ocean data sharing and scientific research collaboration."
1025237,14018,8806,Declarative scheduling for active objects,2014,"Active objects are programming constructs that abstract distribution and help to handle concurrency. In this paper, we extend the multiactive object programming model to offer a priority specification mechanism. This mechanism allows programmers to have control on the scheduling of requests. The priority representation is based on a dependency graph which makes it very convenient to use. This article shows how to use this mechanism from the programmer side, and exposes the main properties of the dependency graph. The software architecture of our implementation is also presented, as it can be applied to various scheduling systems. Finally, we validate our approach through a microbenchmark that shows that the overhead of our priority representation is rather low. On the whole, we provide a general pattern to introduce a prioritized scheduling in active objects or in any other concurrent systems. The resulting framework is shown to be fine-grained, user-friendly, and efficient."
1327934,14018,8806,Reducing data transfer for charts on adaptive web sites,2013,Web content is consumed on devices with a significant variation in display resolution. Visualizing data is typically performed by extracting data from a database for transmission to the client and then visualizing it with a client-side Javascript library. A major challenge is to retrieve only the required data for visualization. Current approaches require programmers to manually modify their data extraction queries and do not adapt to client display characteristics. The contribution in this work is a configurable data compression method that automatically adapts the amount of data transmitted for client-side visualization based on device characteristics. We evaluate several different techniques for time series summarization and show the amount of data transmitted can be reduced by between 40% and 80% on standard data sets while preserving pixel-perfect visualization.
833824,14018,8806,Horizontal partitioning of very-large data warehouses under dynamically-changing query workloads via incremental algorithms,2013,"With the explosion of the size of data warehousing applications, the horizontal data partitioning is well adapted to reduce the cost of complex OLAP queries and the warehouse manageability. It is considered as a non redundant optimization technique. Selecting a fragmentation schema for a given data warehouse is NP-hard problem. Several studies exist and propose heuristics to select near optimal solutions. Most of these heuristics are static, since they assume the existence of a priori known set of queries. Note that in real life applications, queries may change dynamically and fragmentation heuristics need to integrate these changes. In this paper, we propose  an incremental selection of fragmentation schemes  using on genetic algorithms. Intensive experiments are conducted to validate our proposal."
1057974,14018,8806,Monitoring strategic goals in data warehouses with awareness requirements,2012,"A data warehouse (DW) system stores data from multiple data sources in integrated form and provides capabilities for monitoring business operations to ensure compliance to strategic goals. As such, DWs constitute a fundamental building block for Business Intelligence (BI) operations. In this paper, we introduce the notion of Awareness Requirements ( AwReqs ) in the requirements analysis and elicitation phase for DWs. In this context,  AwReqs  provide analysts with the means for eliciting and modeling requirements over performance measures (indicators) to appraise the success or failure of strategic goals. To demonstrate the benefit of our approach, we present a typical business example throughout the paper and show how we can establish in the early stages of DW design the adequacy of the design for BI operations."
1036899,14018,8806,An ontological basis for resource representation,2012,This paper proposes an ontology for resource representation driven by the concepts of  functionality  and  competency  depending on their type. These concepts are used to centralize resources' offer and demand to ensure their proper use and reservation. The conceptual model assumes a higher-level functional (defined here as service) dimension so that it can be used in a system development process to represent heterogeneous resources in a unified manner and develop resource-aware information systems. This helps solving interoperability issues.
925533,14018,8806,Heterogeneous device interaction using an IPv6 enabled service-oriented architecture for building automation systems,2013,"Recent research and standardization in the domain of the Internet of Things aims at providing IPv6 and Web services on the most constrained devices. The constrained application protocol (CoAP) allows the deployment of RESTful Web services on limited devices by offering a protocol similar to HTTP that uses UDP as transport layer instead of TCP. This enables more efficient communication if non-reliable, asynchronous or group communication is required. Existing and mature home and building automation technologies like KNX, BACnet or ZigBee are based on non-IP communication and define a complete custom protocol stack. State of the art integration approaches usually offer a centralized Web service interface based on oBIX, OPC UA or BACnet/WS based on SOAP or RESTful Web services. This paper presents the concept of a gateway that allows the integration of building automation systems into constrained RESTful environments by an IPv6 per-device interface based on oBIX using a novel constrained application protocol binding with efficient XML interchange message encoding. This integration approach provides a service-oriented architecture with uniform interfaces for control scenarios that span heterogeneous technologies. The evaluation results show the efficiency of novel protocol bindings compared to state of the art protocol bindings and message encodings. Performance evaluation results prove that even interactive user control scenarios can be based on these Web service interfaces."
1426035,14018,8806,A real-time operating system for manycore systems,2012,"In this paper, we suggest the partitioning RTOS, a concrete locality-aware resource management scheme for manycore systems. It manages manycore resources as a hierarchical manner and allocates cores and memory as close each other as possible while dynamically considering both multiple applications' needs and ever-changing system status. We show the effectiveness of the proposed method through a real-life application, ray tracing, and achieves a scalable speedup, 12.81 times at 16 cores."
1459480,14018,8806,Extending timestamp-based two phase commit protocol for RESTful services to meet business rules,2011,"Service Oriented Architecture allows development of software with requirements of interoperability and weak coupling. Nowadays, REST is an architectural style that has been gaining attention in the SOA domain. REST allows the development of web services based on concepts simpler than WS-*, however, REST, as an architectural style, does not provide official standards to address some nonfunctional requirements of services, such as, security, reliability, and transaction control. The Timestamp-based Two Phase Commit Protocol for RESTful Services (TS2PC4RS) algorithm proposes a REST-based technique to support the web services transactional control implementation. This paper proposes to extend the TS2PC4RS algorithm to improve the satisfaction of business rules. The goal is met in the way the clients can update their prewrites on the ongoing transactions, so that the clients do not need to start a new transaction in order to implement the desired updates. The update of prewrites takes into account the application domain business rules which guide the RESTful services behavior. Thus the business rules are also considered in the algorithm extension. An example was used to describe the TS2PC4RS extension for updates."
1204730,14018,8806,Entity matching for semistructured data in the Cloud,2012,"The rapid expansion of available information, on the Web or inside companies, is increasing. With Cloud infrastructure maturing (including tools for parallel data processing, text analytics, clustering, etc.), there is more interest in integrating data to produce higher-value content. New challenges, notably include entity matching over large volumes of heterogeneous data.   In this paper, we describe an approach for entity matching over large amounts of semistructured data in the Cloud. The approach combines ChuQL[4], a recently proposed extension of XQuery with MapReduce, and a blocking technique for entity matching which can be efficiently executed on top of MapReduce. We illustrate the proposed approach by applying it to extract automatically and enrich references in Wikipedia and report on an experimental evaluation of the approach."
1124601,14018,8806,IT evaluation in business groups: a maturity model,2013,"Ensuring the effectiveness and efficiency of IT is a substantial aim of IT evaluation which is part of the strategic IT management. Weighing costs and benefits of IT is per se a complex and difficult process but gets even more challenging in the context of business groups. Business groups are a collective of legally independent entities that are owned and managed by a holding or parent company respectively. The purpose of this paper is to develop a maturity model for IT evaluation on the group level as a governance instrument to analyze and evaluate the current setup as well as to identify possible areas for improvement. In this way, maturity models facilitate the evolutionary reengineering of IT functions as they allow benchmarking assessments and roadmap planning to be carried out. The development of the maturity model is based on design science research and evaluated through various expert interviews, a focus group workshop and a real-world implementation."
780602,14018,8806,LittleD: a SQL database for sensor nodes and embedded applications,2014,"Databases have reduced the cost associated with data management by abstracting applications from information processing challenges. There is an increasing need for managing and analyzing data in smaller embedded devices and sensor nodes. Due to resource limitations, these devices typically do not have well-defined data management APIs and standards such as the relational model and SQL. This results in increased complexity and cost. LittleD is a SQL relational database allowing ad-hoc queries on sensor devices. The novel implementation of LittleD adapts to memory and code size restrictions by streamlining query parsing and execution and implementing efficient memory management techniques. Experimental results demonstrate that LittleD executes queries with joins and selections on devices with less than 2 KB memory in a few seconds."
1765164,14018,8806,Configuration support for feature models with soft constraints,2013,"Traditional feature models may include hard domain constraints that must be upheld in all valid configurations. Soft constraints may however be used to enhance the expressiveness of feature models by allowing non-mandatory relations between features to be expressed. One advantage of including soft constraints in feature models is the opportunity to offer improved configuration support. We describe automated configuration techniques for feature models annotated with soft constraints. These techniques allow configuration suggestions to be extracted from soft constraints, even if inconsistencies among multiple soft constraints arise."
1415858,14018,8806,Multiprocessor schedulability analyser,2011,"Within the context of hard real-time systems, the  schedulability analysis  of a task set is a major issue. The problem consists in proving that the tasks always satisfy their temporal constraints for a given scheduling policy and a given platform. Extensive work has been done in the last decades for defining sufficient criteria and exact algorithms. Sufficient criteria usually have an excellent complexity but often lead to an over-dimension of the system. On the opposite, exact algorithms, especially in the case of multiprocessor platform, suffer from an exponential complexity.   In this paper, we study an exact technique: we apply a brute force search combined with a model checker (Uppaal) that determines whether the exploration is complete. We consider periodic tasks which execute on parallel platforms composed of homogeneous processors. Under these hypotheses, we have encoded four policies: fixed task priority, gEDF, gLLF and LLREF. The analyser is user friendly and provides promising performances."
2506236,14018,9080,Ants easily solve stochastic shortest path problems,2012,"The first rigorous theoretical analysis (Horoba, Sudholt (GECCO 2010)) of an ant colony optimizer for the stochastic shortest path problem suggests that ant system experience significant difficulties when the input data is prone to noise. In this work, we propose a slightly different ant optimizer to deal with noise.   We prove that under mild conditions, it finds the paths with shortest expected length efficiently, despite the fact that we do not have convergence in the classic sense. To prove our results, we introduce a stronger drift theorem that can also deal with the situation that the progress is faster when one is closer to the goal."
2459759,14018,9080,Ribosomal robots: evolved designs inspired by protein folding,2013,"The biological process of ribosomal assembly is one of the most versatile systems in nature. With only a few small building blocks, this natural process is capable of synthesizing the multitude of complex chemicals that form the basis of all organic life. This paper presents a robotics design and manufacturing scheme which seeks to capture some of the versatility of the ribosomal process. In this scheme, a custom printer folds a long ribbon of material in which control elements such as motors have been embedded into a morphology that is capable of accomplishing a pre-defined task. The evolved folding patterns are encoded with a special kind of compositional pattern producing network (CPPN), which can compactly describe patterns with regularities such as symmetry, repetition, and repetition with variation. This paper tests the efficacy of this design scheme and the effects of different ribbon lengths on the ability to produce walking robot morphologies. We show that a single strip of material can be folded into a variety of different morphologies displaying different forms of locomotion. Thus, the results presented here suggest a promising new method for the automated design and manufacturing of robotic systems."
1309218,14018,8806,Context-aware replacement operations for data cleaning,2011,"Data cleaning focuses on the identification and removal of consistency constraint violations. Existing approaches only perform statistical repair operations, i.e. inserting average or default values. This results in consistent data, but these data have no similarity with the given inconsistent data anymore. The use of an ontology-based approach allows for the detection of semantically related context-aware correction suggestions. We define metrics that can be used to calculate the similarity of such correction suggestions. We introduce measures to identify semantic distances of concepts in ontologies. This ontology enables the detection of context-aware correction suggestions and the calculation of their similarity to the invalid tuple. These suggestions can be presented to end users in data cleaning environments. We introduce this approach in a cancer registry that collects data about cancer cases. We show how the proposed approach can support domain experts in the registry in data cleaning."
172018,14018,8884,DC proposal: online analytical processing of statistical linked data,2011,"The amount of Linked Data containing statistics is increasing; and so is the need for concepts of analysing these statistics. Yet, there are challenges, e.g., discovering datasets, integrating data of different granularities, or selecting mathematical functions. To automatically, flexibly, and scalable integrate statistical Linked Data for expressive and reliable analysis, we propose to use expressive Semantic Web ontologies to build and evolve a well-interlinked conceptual model of statistical data for Online Analytical Processing."
1742904,14018,8806,Test-based SPL extraction: an exploratory study,2013,"Many software systems have been developed as single products before Software Product Lines (SPLs) have emerged. Although some promising approaches have been proposed, extracting an SPL from existing software products is still expensive and time consuming. This paper presents an exploratory study that relies on a test-based SPL extraction from systems already developed. We aim to evaluate testing as the main mean to locate feature code and different sorts of existing artifacts to support the test-based location. We conduct two case studies starting from the derivation of the SPL feature model to the feature code location. Our preliminary results indicate (i) good rates of precision for feature seed location, where seed means a small portion of the feature code that allows the identification of the remaining portion, and (ii) good rates of recall for locating the whole feature code."
759683,14018,8806,Indistinguishable regions in geographic privacy,2012,"The ubiquity of positioning devices poses a natural security challenge: users want to take advantage of location-related services as well as social sharing of their position but at the same time have security concerns about how much information should be shared about their exact position. This paper discusses different location-privacy problems, their formalization and the novel notion of indistinguishability regions that allows one to proof that a given obfuscation function provides a good trade-off between location sharing and privacy."
789620,14018,8806,Rapid development of first person serious games using the APEX platform: the asthma game,2014,"Serious games combine a ludic component with instructive and formative goals. They aim to educate and train through play. This paper explores the use of a development framework for dynamic virtual environments to develop serious games. The framework (APEX) was originally developed to prototype ubiquitous computing environments. Here it is used to develop a first person serious game: the Asthma Game. This game aims to teach children with asthma how to act to prevent attacks by drawing attention to asthma triggers in the home, and by providing information about how to avoid them. Besides the description of the game, results about the viability and utility of the approach are also discussed."
1681189,14018,8806,Application engineering of service-based software product lines,2012,"Software product lines (SPL) correspond to one of the most successful forms of reuse, since they allow the reuse of both requirements and architecture. The investment done in the SPL development can get repaid after several various products are derived from the SPL, in particular, when an application generator is used to automate the derivation of products. SPL can benefit from web service Technology, as several features can be implemented by services available on a network. This paper presents an approach in which an application generator is used for deriving members of an SPL based on services. The objective is to reduce development cost and time, increasing productivity for SPL members derivation and taking advantage of the flexibility afforded by the use of services in the SPL architecture. Additionally, this paper presents a case study where this approach was applied to a Web auction SPL."
1771361,14018,8806,Consolidating multiple requirement specifications through argumentation,2011,"The process of handling inconsistencies in software requirements is an important and challenging task. Most often in cases where multiple stakeholders interact with the requirement analysts, inconsistent, discrepant and conflicting information is gathered that needs to be understood and interpreted properly. Recent research has suggested that despite the fact that inconsistencies are not desirable by nature, they can be tolerated in order to better understand the nature of the problem domain and the stakeholders' line of thought. With this in mind, we propose an argumentative approach towards handling inconsistent requirement specifications. In our semi-formal approach, we build on Dung's abstract argumentation framework and represent requirement statements as  arguments . This way we are able to model the interaction of the requirement statements in terms of their inconsistencies and also provide a decision support process for the resolution of inconsistencies. We discuss our approach in detail through a widely used case study and introduce our Eclipse plugin tool supporting the proposed work."
2572203,14018,9080,Confronting the challenge of learning a flexible neural controller for a diversity of morphologies,2013,"The ambulatory capabilities of legged robots offer the potential for access to dangerous and uneven terrain without a risk to human life. However, while machine learning has proven effective at training such robots to walk, a significant limitation of such approaches is that controllers trained for a specific robot are likely to fail when transferred to a robot with a slightly different morphology. This paper confronts this challenge with a novel strategy: Instead of training a controller for a particular quadruped morphology, it evolves a special function (through a method called HyperNEAT) that takes morphology as input and outputs an entire neural network controller fitted to the specific morphology. Once such a relationship is learned the output controllers are able to work on a diversity of different morphologies. Highlighting the unique potential of such an approach, in this paper a neural controller evolved for three different robot morphologies, which differ in the length of their legs, can interpolate to never-seen intermediate morphologies without any further training. Thus this work suggests a new research path towards learning controllers for whole ranges of morphologies: Instead of learning controllers themselves, it is possible to learn the relationship between morphology and control."
891849,14018,8806,Collecting cloud provenance metadata with Matriohska: a case study with genomic workflows,2014,Scientific Workflows are abstractions used to model  in silico  scientific experiments. Cloud environments are still incipient in collecting and recording prospective and retrospective provenance. This paper presents an approach to support collecting metadata provenance of  in silico  scientific experiments executed in public clouds. The strategy was implemented as a distributed and modular architecture named Matriohska. This paper also presents a provenance data model compatible with PROV specification. We also show preliminary results that describe how provenance metadata was captured from the components running in the cloud.
2323168,14018,9080,GAMIV: a genetic algorithm for identifying variable-lengthmotifs in noncoding DNA,2012,"GAMI uses a genetic algorithm to identify putatively conserved motifs of a pre-selected length in noncoding DNA from diverse species. In this work, I present an extension to the system, GAMIV, that identifies putatively conserved motifs of variable length. The system begins with an initial set of very short motifs and allows them to grow through a pair of custom operators. A fitness function that rewards both motif conservation and motif length is used to evolve a population of conserved motifs of variable length. This paper describes the motivation for GAMIV, discusses the design of the system, and presents initial results for the system. Based on these initial results, GAMIV is a promising tool for the inference of variable-length motifs in noncoding DNA."
1377222,14018,8806,HMXT-GP: an information-theoretic approach to genetic programming that maintains diversity,2011,"This paper applies a recent information--theoretic approach to controlling Genetic Algorithms (GAs) called HMXT to tree--based Genetic Programming (GP). HMXT, in a GA domain, requires the setting of selection thresholds in a population and the application of high levels of crossover to thoroughly mix alleles. Applying these in a tree--based GP setting is not trivial. We present results comparing HMXT--GP to Koza--style GP for varying amounts of crossover and over three different optimisation (minimisation) problems. Results show that average fitness is better with HMXT--GP because it maintains more diversity in populations, but that the minimum fitness found was better with Koza. HMXT allows straightforward tuning of population diversity and selection pressure by altering the position of the selection thresholds."
891467,14018,8806,An intelligent building that listens to your needs,2013,"Building automation systems (BAS) are technologies which automatically control building appliances based on specialized algorithms to achieve predefined goals, such as optimising user comfort or regulating energy consumption. Normally, they are configured during deployment as a one-time effort by technicians with expert knowledge and tools. However, the dynamic nature of the environment and the preferences of occupants calls for the need of frequent reconfigurations, which is often impractical and expensive to implement. As a solution, we propose a novel BAS design which considers user-based information such as user preferences and feedback in order to continuously reconfigure itself, as opposed to static configurations. This high-level information is then adopted by a software framework based on multi-agent systems (MAS) that applies the PROSA reference model [9] to dynamically control the building. Under this framework, every room in the building is automated by a dedicated agent, acting as a software mediator between the appliances and its occupants, with the aim of maximising the result of a utility function taking into account power optimisation, user comfort, and environment dynamics. The proposed approach has been implemented in a real office environment, and we discuss our design to improve user experience a BAS deployment."
822155,14018,8806,Inverted file-based indexing for efficient multimedia information retrieval in metric spaces,2012,"Content-based similarity search is an important task in multimedia information retrieval (IR). Here, metric space access methods (MAMs) can be applied. They are purely based on the use of a metric distance. No assumption is made about the representation of the feature objects. On the one hand,  approximate  MAMs have been proposed relying on the inverted file---the de facto standard index structure for text retrieval. On the other hand, there are many  exact  hierarchical and multi-step MAMs.   We present IF4MI (Inverted Files for Metric Indexing), the first exact metric access method (MAM) based on the inverted file concept. IF4MI can outperform existing MAMs such as the M-tree and the PM-tree. In addition, the pruning power of current state-of-the-art techniques---namely the Metric Index---can be brought to inverted files without relying on an additional mechanism which maps feature objects to one-dimensional values for storing them in adequate data structures such as a B + -tree. IF4MI is conceptually appealing since it can make use of extensive knowledge in the field of inverted file-based indexing. As one example, we show how the efficient processing of textual filter queries---an important task in multimedia IR---is inherently supported."
960118,14018,8806,Improving CP-based local branching via sliced neighborhood search,2011,"In this paper we merge two problem independent search strategies, namely Local Branching and Sliced Neighborhood Search. They both integrate CP tree search with local search concepts, but while local branching is very effective in exploring small neighborhoods, its performances decrease when dealing with diversification and large neighborhood exploration. On the other hand Sliced Neighborhood Search is an effective method for exploring random slices of large neighborhoods and in moving away arbitrarily far from an incumbent solution. For this reason we obtain very good results in improving a reference solution: Local Branching obtains a 35% improvement when SNS is integrated aggressively both in the neighborhood exploration and in the diversification strategy. The tests were conducted on large instances of the Travelling Salesman Problem with Time Windows."
983242,14018,8806,A platform independent scheduler framework,2014,"One of the main task of every operating system is the scheduling of processes. A good scheduling algorithm results in reactiveness for a desktop system or ensures that deadlines are not violated in case of an real time operating system. Furthermore scheduling can assure fairness among processes and prioritisation of temporarily important ones. Besides functional requirements, the scheduler is more and more important to ensure non-functional requirements which come more into consideration these days. For example, energy and temperature aware algorithms are required in mobile devices due to restrictions from battery life and missing cooling systems."
1337949,14018,8806,MFS-Map: efficient context and content combination to annotate images,2014,"Automatic image annotation provides textual description to images based on content and context information. Since images may present large variability, image annotation methods often employ multiple extractors to represent visual content considering local and global features under different visual aspects. As result, an important aspect of image annotation is the combination of context and content representations. This paper proposes MFS-Map (Multi-Feature Space Map), a novel image annotation method that manages the problem of combining multiple content and context representations when annotating images. The advantage of MFS-Map is that it does not represent visual and textual features by a single large feature vector. Rather, MFS-Map divides the problem into feature subspaces. This approach allows MFS-Map to improve its accuracy by identifying the features relevant for each annotation. We evaluated MFS-Map using two publicly available datasets: MIR Flickr and Image CLEF 2011. MFS-Map obtained both superior precision and faster speed when compared to other widely employed annotation methods."
995769,14018,8806,The importance of geographic locality for online information diffusion,2014,"The geographic locality of users is frequently ignored as an important element for the spread of information in online social networks. This is because virtual environments cause the impression that affinities of interest are so important that other factors completely lose relevance. Through the analysis of a real network of online recommendations, this paper evidences the importance of geographic localities for information dissemination in online social networks. We analyzed a recommendation network with associated rewards for service usage of a Brazilian Internet provider. We found out that there are intrinsic characteristics of localities that have a stronger influence in the information diffusion. Our geographic localities have different usage and recommendation patterns so that they play different roles in information diffusion. Also, recommendations tend to have a strong geographical scope in our dataset."
1063797,14018,8806,Efficient data-intensive event-driven interaction in SOA,2013,"This paper presents a middleware that enables the efficient delivery of events carrying large attachments. We transparently decouple event-description from event-data, in order to avoid useless data-transfers and modifications to endpoints business logic. Our solution relieves the event-delivery system of large data transfers, by enabling direct, but transparent, publisher to subscriber data-exchange. The experiments show that we can reduce the average event delivery time by half, compared to a standard approach requiring the full mediation of the event-delivery system."
1024482,14018,8806,On the reconfiguration of software connectors,2013,"Software connectors encapsulate interaction patterns between services in complex, distributed service-oriented applications. Such patterns evolve over time, in response to faults, changes in the expected QoS levels, emergent requirements or the reassessment of contextual conditions. This paper builds up on a model for connector reconfiguration to introduce notions of reconfiguration equivalence and refinement allowing for reasoning about them. This paves the way towards a (still missing) calculus of connector reconfigurations."
1353000,14018,8806,Test intents: enhancing the semantics of requirements traceability links in test cases,2013,"Requirements traces establish links between requirements and software artifacts, such as source code, test cases and configuration files. Understanding which requirements are covered by test cases, i.e. requirements tracing in test cases, is an important concern in quality assurance of complex software systems. There is existing literature to obtain requirements traces in different software artifacts. In this work, we build upon existing requirements tracing methods for test cases and introduce  test intents , i.e. which requirements test cases aim to test. We also propose a method to identify test intents, and show that our approach can identify the intents successfully on our case studies on four realistic software systems."
817857,14018,8806,Towards a framework for relevant guidance,2014,"Relevance is a key concept nowadays. We need to search for relevant information but we do not want to spend much of our time doing it. Most times it is hard to choose what is worth looking at. To help it, this paper proposes a framework that uses ontologies to suggest relevant topics. This framework is based on a cognitive theory called Relevance Theory [4]. In this particular case, we used a domain ontology to represent a proficiency test context and an application ontology to represent the relevance method. A proof of concept was conducted in order to demonstrate the validity of the framework. Some preliminary results point that the suggestion made from the system took to slightly more focused learning paths."
1088151,14018,8806,I4Copter: an adaptable and modular quadrotor platform,2011,"Quadrotor helicopters are micro air vehicles with vertical take-off and landing capabilities controlled by varying the rotation speed of four fixed pitch propellers. Due to their rather simple mechanical design they have grown to popularity as platform for various research projects. Despite most of them being individually highly successful, they are typically tailored to a specific purpose making it hard to utilise them for further research and education.   In this paper, we present the novel design of the  I4Copter  quadrotor. It has been developed to provide a stable demonstration quadrotor platform for various kinds of research and education projects targeting cross-field challenges in real-time and embedded systems, distributed systems, robotics and cybernetics. The modular and open architecture of our platform allows an application-specific, fine-grained extension, adaption and replacement of software and hardware components. The safe extensibility is supported by strict temporal and spatial isolation between the software modules. We validated our approach by two distinct cross-field use cases: an evaluation platform for modularised control algorithms enabling trajectory tracking and an implementation that is resilient to transient hardware errors."
2373739,14018,9080,Locally geometric semantic crossover,2012,"We propose Locally Geometric Crossover (LGX) for genetic programming. For a pair of homologous loci in the parent solutions, LGX finds a semantically intermediate procedure from a previously prepared library, and uses it as replacement code. The experiments involving six symbolic regression problems show significant increase in search performance when compared to standard subtree-swapping cross-over and other control methods. This suggests that semantically geometric manipulations on subprograms propagate to entire programs and improve their fitness."
2309979,14018,9080,An evolutionary algorithm with solution archives and bounding extension for the generalized minimum spanning tree problem,2012,"We consider the recently proposed concept of enhancing an evolutionary algorithm (EA) with a complete solution archive. It stores evaluated solutions during the optimization in order to detect duplicates and to efficiently transform them into yet unconsidered solutions. For this approach we introduce the so-called bounding extension in order to identify and prune branches in the trie-based archive which only contain inferior solutions. This extension enables the EA to concentrate the search on promising areas of the solution space. Similarly to the classical branch-and-bound technique, bounds are obtained via primal and dual heuristics. As an application we consider the generalized minimum spanning tree problem where we are given a graph with nodes partitioned into clusters and exactly one node from each cluster must be connected in the cheapest way. As the EA uses operators based on two dual representations, we exploit two corresponding tries that complement each other. Test results on TSPlib instances document the strength of this concept and that it can compete with the leading metaheuristics for this problem in the literature."
1063864,14018,8806,Performance analysis of IEEE 802.11 ad hoc based broadcast,2011,"Existing IEEE 802.11 broadcast performance researchers assume that the time taken for a successful frame transmission is the same as that taken for an unsuccessful frame transmission. They do not consider the effects of an extended interframe space (EIFS); therefore, their analysis and simulation results are not consistent with each other. This paper considers the effects of EIFS, modifies the model proposed by Hu, and uses throughput as a performance benchmark. Results of our study show that our proposed method yields results that are closest to the simulation results."
1484335,14018,8806,Using ontology-based methods for implementing role-based access control in cooperative systems,2012,"The issue of security of data becomes very critical due to the federated databases that cooperative systems integrate. In this paper, we describe a role-based access control (RBAC) mechanism that can be applied to collaborative project organizations, and which uses ontology-based methods for its implementation. This eases the process of making modifications. It also brings about standardization, which is cornerstone for portability.   We test and evaluate this approach in an implementation of a data-management system for proteomic experiment data.   The primary aim of this study is, firstly, to make use of an upcoming and potentially standard technology and apply it to the domain of database security.   Our second aim is to validate the hypothesis that such a method can be effectively used in a real-world cooperative system."
985455,14018,8806,Specifying and analysing reputation systems with a coordination language,2013,"Reputation systems are nowadays widely used to support decision making in networked systems. Parties in such systems rate each other and use shared ratings to compute reputation scores that drive their interactions. The existence of reputation systems with remarkable differences calls for formal approaches to their analysis. We present a verification methodology for reputation systems that is based on the use of the coordination language Klaim and related analysis tools. First, we define a parametric Klaim specification of a reputation system that can be instantiated with different reputation models. Then, we consider stochastic specification obtained by considering actions with random (exponentially distributed) duration. The resulting specification enables quantitative analysis of properties of the considered system. Feasibility and effectiveness of our proposal is demonstrated by reporting on the analysis of two reputation models."
2031362,14018,8806,Virtual prototype generation by shockwave flash for simulating HW components of embedded system,2014,"In developers' viewpoint, it is difficult to set up the hardware and operational environment required for the system during software development. Virtual prototyping approach can be used for simulating hardware and operational environment of embedded software. However, cost and effort required for developing virtual prototyping in programming language are very high. Furthermore, related supporting tools have some limitations in the areas of detailed representation, dynamic performance, modifiability, and so on. In this paper, we propose an SWF based virtual prototyping approach for implementing virtual prototype in embedded systems. Also, we evaluate our approach through case study which shows that the technique can be applicable in real world."
1282340,14018,8806,Scalable depth map coding for 3D video using contour information,2012,"In this paper, scalable depth map coding method is proposed to accomplish better coding performance. First of all, in order to use correlation between color video and depth map, a structure in SVC is applied to 3DVC. As the depth map is mainly used to synthesize videos, corrupted contour region can damage the overall quality of video. We hereby adapt a new differential quantization method when separating the contour region. The experimental results show that the proposed method can improve video quality by 0.07~0.49dB, when compared to the reference software, JSVM 9.19."
1050679,14018,8806,BioTRON: a biological workflow management system,2011,"Bioinformatics tasks may become very complex and usually require to manually integrate both data and results from different knowledge sources and tools. In this scenario, an integrated environment for designing and executing complex biological workflows is a must. Even though several efforts are trying to cope with this aspects, they mostly focus on gene or protein sequence analysis underestimating more complex biological data such as molecular interaction data. The aim of this paper is to present the  BioTRON  system, which supports biologists in the various steps necessary to perform complex biological tasks such as biological network comparison.  BioTRON  also features a mechanism to automatically integrate even existing on-line Web services. We present the  BioTRON  architecture along with a real example, which shows the suitability of the tool."
1965049,14018,8806,On-line scheduling of real-time services with profit and penalty,2011,"In this paper, we study a new family of real-time service oriented scheduling problems. The real time tasks are scheduled non preemptively with the objective of maximizing the total utility. Different from the traditional utility accrual scheduling problem that each task is associated with only a single time utility function (TUF), two different TUFs---a profit TUF and a penalty TUF---are associated with each task, to model the real-time services that not only need to reward the early completions but also need to penalize the abortions or deadline misses. We present two scheduling heuristics to judiciously accept, schedule, and abort real-time services when necessary to maximize the accrued utility. Our extensive experimental results show that our proposed algorithms can significantly outperform the traditional scheduling algorithms such as the Earliest Deadline First (EDF), the traditional utility accrual scheduling algorithms and an earlier scheduling approach based on a similar model."
1466292,14018,8806,Process performance management: illuminating design issues through a systematic problem analysis,2011,"Business processes are the means by which organizations create value. Consequently, organizations need to continuously monitor and control their processes' performance so as to provide a consistent and predictable execution quality. A number of today's organizations, however, appear to encounter difficulties with measuring and improving their processes' performance. In this paper, we set out to identify the gap between how organizations currently approach process performance management (PPM) and what they are striving to realize in the future. The systematic gap analysis results in a set of design factors that are valuable in guiding future design efforts for useful and relevant PPM solutions."
1788218,14018,8806,Testing the reliability of web services transactions in cooperative applications,2012,"Web services provide a distributed computing environment wherein service providers and consumers can dynamically interact and cooperate on various tasks in different domains such as ebusiness, education, government and healthcare. Transaction management technology is fundamental to building automated and reliable web services applications. Various models and protocols have been developed for web services transactions. However, they give no attention to the key issue of testing the web services transactions. We propose a novel abstract model for dynamically modeling distinct web services transaction standards and test their reliability in terms of failures. The proposed approach exploits model-based testing techniques in order to automatically generate test scenarios for web service transactions."
2350824,14018,8806,Fixed-point implementation of isolated sub-word level speech recognition using hidden Markov models,2011,"This paper presents a limited vocabulary isolated-word speech recognition system based on Hidden Markov Model (HMM) that involves two stage classification and is implemented on Texas Instruments' (TI) DaVinci embedded platform for a home infotainment system. A methodology using simple metric has been proposed for segmenting the words into sub-word units and these sub-words are used in the second stage to improve recognition accuracy. Also, a simple and efficient way of handling the out-of-vocabulary words using an additional HMM model is presented. We have achieved recognition accuracy of around 89% for a fixed point implementation on the TI DaVinci platform, demonstrating its suitability for embedded systems."
1337268,14018,8806,Meta-learning based architectural and algorithmic optimization for achieving green-ness in predictive workload analytics,2013,"Predictive workload analytics for server systems has been the focus of recent research in energy-aware computing, with many algorithmic and architectural techniques proposed to analyze, predict, and optimize server workloads in order to build energy-aware IT systems. These techniques, though effective in optimizing server workloads, often ignore the green-ness aspect 'in' the technique itself and incur heavy computational costs in their operations. In this paper, we propose a meta-learning based architecture for building server workload prediction mechanism using which the computational cost of holistic predictive workload analytics can be optimized, and green-ness 'in' the analytics can be achieved. We also present an algorithmic optimization of the proposed meta-learning architecture for handling concept drift in server workloads, thereby also achieving improved greenness 'by' the analytics. Our experiments show that the proposed meta-learning based architecture substantially reduces the total computational cost of workload prediction mechanism, with a minor decrease of 0.5--1.3% in the accuracy, and the proposed algorithmic optimization significantly improves accuracy of the workload prediction mechanism in concept drift scenarios by 8.1%."
1200153,14018,8385,Cascading verification: an integrated method for domain-specific model checking,2013,"Model checking is an established method for verifying behavioral properties of system models. But model checkers tend to support low-level modeling languages that require intricate models to represent even the simplest systems. Modeling complexity arises in part from the need to encode domain knowledge at relatively low levels of abstraction.     In this paper, we demonstrate that formalized domain knowledge can be reused to raise the abstraction level of model and property specifications, and hence the effectiveness of model checking. We describe a novel method for domain-specific model checking called cascading verification that uses composite reasoning over high-level system specifications and formalized domain knowledge to synthesize both low-level system models and their behavioral properties for verification. In particular, model builders use a high-level domain-specific language (DSL) based on YAML to express system specifications that can be verified with probabilistic model checking. Domain knowledge is encoded in the Web Ontology Language (OWL), the Semantic Web Rule Language (SWRL) and Prolog, which are used in combination to overcome their individual limitations. A compiler then synthesizes models and properties for verification by the probabilistic model checker PRISM. We illustrate cascading verification for the domain of uninhabited aerial vehicles (UAVs), for which we have constructed a prototype implementation. An evaluation of this prototype reveals nontrivial reductions in the size and complexity of input specifications compared to the artifacts synthesized for PRISM."
2220822,14018,9080,On the properties of the R2 indicator,2012,"In multiobjective optimization, set-based performance indicators are commonly used to assess the quality of a Pareto front approximation. Based on the scalarization obtained by these indicators, a performance comparison of multiobjective optimization algorithms becomes possible. The R2 and the Hypervolume (HV) indicator represent two recommended approaches which have shown a correlated behavior in recent empirical studies. Whereas the HV indicator has been comprehensively analyzed in the last years, almost no studies on the R2 indicator exist. In this paper, we thus perform a comprehensive investigation of the properties of the R2 indicator in a theoretical and empirical way. The influence of the number and distribution of the weight vectors on the optimal distribution of μ solutions is analyzed. Based on a comparative analysis, specific characteristics and differences of the R2 and HV indicator are presented."
690972,14018,8806,Automatic construction of timing diagrams from UML/MARTE models for real-time embedded software,2014,"Analysis of timing constraints is an essential part in developing real-time embedded software. Performing the timing analysis during the early development phases prevents timing violations and enhances software quality. In the development of real-time embedded software, UML timing diagrams can play a significant role since they can provide not only intuitive specifications for timing constraints, but also valuable information for verifying system requirements. However, as software complexity increases, modeling timing diagrams is becoming difficult and costly. We propose an automated construction approach of timing diagrams from UML sequence diagrams and state machine diagrams with MARTE annotations. The proposed approach enables developers of RTES to save time required for modeling timing diagrams and prevents making mistakes in construction of timing diagrams."
1226506,14018,8806,Attribute implications in similarity-based databases: semantic entailment and nonredundant bases,2012,"We introduce a new type of dependencies for data over domains that are additionally equipped with similarity relations. The dependencies are expressed by if-then rules involving similarities of attribute values. Unlike strict equalities, similarities of attribute values make it possible to provide robust rules and concise descriptions of dependencies regarding attribute values, which are close to how a human expert perceives the data. In the paper, we define the rules, their semantics, entailment, and present an algorithm for computing nonredundant sets of rules, i.e., nonredundant sets of rules describing all if-then dependencies in given data. The algorithm represents basic method for extracting if-then rules from data in similarity-based databases. Due to the limited scope of the paper, all proofs are only skethced or omitted."
902587,14018,8806,A pattern-based verification approach for a multi-core system development,2011,"Recent years, with the development of multi-core processor architecture, multi-core software development is also being gradually recognized. Therefore, how to quickly develop reliable multi-core software will be a challenge. In our approach, we use  Model-driven architecture (MDA)  technology to achieve this challenge. First, we extract the experience of designing multi-core software and model the experience as a reusable pattern by using UML meta-modeling techniques. Then, we specify a set of constraints using the  Object Constraint Language (OCL)  to verify the model with profile. Finally, we provide a tool to specify the operation definition for connecting the design and implementation, so that the model transformation process can be more efficient. Furthermore, we demonstrate our approach by a case study on a real-world multi-core embedded system called  PVE (Parallel Video Encoder) , where a pattern  CommandPipeline  is designed for the development convenience."
1379587,14018,8806,A domain-specific language for managing feature models,2011,"Feature models are a popular formalism for managing variability in software product lines (SPLs). In practice, developing an SPL can involve modeling a large number of features representing different viewpoints, sub-systems or concerns of the software system. To manage complexity, there is a need to separate, relate and compose several feature models while automating the reasoning on their compositions in order to enable rigorous SPL validation and configuration. In this paper, we propose a Domain-Specific Language (DSL) that is dedicated to the management of feature models and that complements existing tool support. Rationale for this language is discussed and its main constructs are presented through examples. We show how the DSL can be used to realize a non trivial scenario in which multiple SPLs are managed."
1685486,14018,8494,Artificial immune system based methods for spam filtering,2013,"To solve the spam problem, many statistical learning methods and AIS methods have been proposed and applied. In essence, statistical learning methods and AIS methods have quite different origins, and they try to find the solutions from distinct aspects. In recent works, we proposed several hybrid methods, which combined immune theory with statistical methods in spam filtering. In this paper, we briefly review and analyze these works and possible extensions, and demonstrate the rationality of building hybrid immune models for spam filtering. In addition, a generic framework of an immune based model is presented, and online implementation strategies are given. It is well demonstrated that how to apply the immune based model to building an intelligent email server."
726620,14018,8806,Representation and management of spatiotemporal data in object-relational databases,2012,"This paper deals with the design and implementation of a data model and operations for dealing with continuously changing spatial data in object-relational DBMS. The data model relies on abstract data types but we introduce modifications to the internal structure of the spatiotemporal data representations proposed in the literature, to reduce storage requirements and to enable the reutilization of data during the execution of the queries. We show how to implement spatiotemporal operations relying on the spatial functions released by the underlying DBMS and how to use the alternative data representations to reduce the volume of temporary data created in the evaluation of spatiotemporal operations. We also discuss on the advantages and disadvantages of the proposed solutions."
792962,14018,8806,Extending JUnit 4 with Java annotations and reflection to test variant model transformation assets,2014,"Software Product Line (SPL) techniques are widely used to represent variability and commonality in reusable software assets. Similarly, model transformations are also software assets and can be reused with the same techniques. However, their applicability in the model transformations domain demands an extra effort to test the generated/adapted assets. Automated test cases should consider isolated transformations and also their combined use in a model transformation chain, that can vary according to different needs in software projects, e.g. libraries and frameworks. In order to facilitate the specification of automated test cases, this paper presents a JUnit extension to support unit and integration tests that execute dynamic SPL-based model transformation chains."
2349915,14018,9080,On neighborhood tree search,2012,"We consider the neighborhood tree induced by alternating the use of different neighborhood structures within a local search descent. We investigate the issue of designing a search strategy operating at the neighborhood tree level by exploring different paths of the tree in a heuristic way. We show that allowing the search to 'backtrack' to a previously visited solution and resuming the iterative variable neighborhood descent by 'pruning' the already explored neighborhood branches leads to the design of effective and efficient search heuristics. We describe this idea by discussing its basic design components within a generic algorithmic scheme and we propose some simple and intuitive strategies to guide the search when traversing the neighborhood tree. We conduct a throughout experimental analysis of this approach by considering two different problem domains, namely, the Total Weighted Tardiness Problem (SMTWTP), and the more sophisticated Location Routing Problem (LRP). We show that independently of the considered domain, the approach is highly competitive. In particular, we show that using different branching and backtracking strategies when exploring the neighborhood tree allows us to achieve different trade-offs in terms of solution quality and computing cost."
968329,14018,8806,Risk assessment of code injection vulnerabilities using fuzzy logic-based system,2014,"Web applications are notoriously vulnerable to code injection attacks. Given that, practitioners need to assess the risk posed by applications due to code injection attacks to plan ahead on employing necessary mitigation approaches. This paper proposes a risk assessment approach for code injection vulnerability in web applications. We are motivated by the observation that traditional risk assessment approaches work well when quantitative values of specific parameters of the risk computation model is known in advance. In practice, they are difficult to predict correctly. Moreover, one specific code injection vulnerabilities can be exploited in different ways that may result in different types of severity level. Further, diverse types of injection vulnerabilities and their implications cannot be combined in existing approaches. To address these limitations, we propose a Fuzzy Logic-based System (FLS) to assess the risk due to different types of code injection vulnerabilities. Our further contribution is a set of proposed code-level metrics that can be used to establish the linguistic terms to express vulnerability levels and their impact subjectively. We apply nested FLS to combine risk from multiple vulnerabilities to assess a single value representing the overall risk. We evaluate our approach with three real-world web applications implemented in PHP, and apply for SQL Injection (SQLI) and Cross-Site Scripting (XSS), the two most widely reported vulnerabilities in today's web applications. The initial results indicate that the proposed approach can effectively assess high risks present in vulnerable applications."
2430917,14018,9080,Automatically designing selection heuristics,2011,"In a standard evolutionary algorithm such as genetic algorithms (GAs), a selection mechanism is used to decide which individuals are to be chosen for subsequent mutation. Examples of selection mechanisms are fitness-proportional selection, in which individuals are chosen with a probability directly in proportion to their fitness value, and rank selection, in which individuals are selected with a probability in proportion to their ordinal ranking by fitness. These two human-designed selection heuristics implicitly assume that fitter individuals produce fitter offspring. Whilst one might invest human ingenuity in the construction of alternative selection heuristics, the approach adopted in this paper is to represent a generic family of selection heuristics which are applied via an algorithmic framework. We then generate instances of selection heuristics and test their performance in an evolutionary algorithm (which in this paper tackles a variety of bitstring optimization problems). The representation we use for the program space is a  register machine  (a set of real-valued registers on which a program is executed). Fitness-proportional and rank selection can be expressed as one-line programs, and more sophisticated selection heuristics may also be expressed. The result is a system which produces selection heuristics that outperform either of the original selection heuristics."
967405,14018,8806,Creating mobile ad hoc workflows with Twitter,2012,"The wide-spread adoption of powerful mobile devices allows for the design of workflows while on the move. In this paper, we leverage established SOA principles towards mobility in design and execution. For this purpose, we introduce a lightweight Service workflow model that is tailored for the needs of ad-hoc creation and mobility. We show how sensor data from mobile devices can be included during the design of Service workflows and we address crowd sourcing aspects for the deployment and execution of Service workflows. We present a proof of concept prototype application that supports the creation of Service workflows on mobile devices."
1931034,14018,8806,Anytime QoS optimization over the PlanGraph for web service composition,2012,"Automatic service composition is the generation of a business process to fulfill business goals that cannot be fulfilled by individual services. Planning algorithms are frequently used to solve this problem. In addition to satisfying functional goals, recent research is geared towards selecting the best services to optimize the QoS of the result business process. In this paper, we combine a systematic search algorithm like Dijkstra's algorithm with a planning algorithm, GraphPlan, to achieve both functional goals and QoS optimization at the same time. In addition, we make our algorithm an anytime algorithm that has the advantage of getting better solutions if it keeps running for a longer time."
1099146,14018,8806,Input-output conformance testing based on featured transition systems,2014,"We extend the theory of input-output conformance testing to the setting of software product lines. In particular, we allow for input-output featured transition systems to be used as the basis for generating test suites and test cases. We introduce refinement operators both at the level of models and at the level of test suites that allow for projecting them into a specific product configuration (or a product sub-line). We show that the two sorts of refinement are consistent and lead to the same set of test-cases."
760482,14018,8806,Convexity local contour sequences for gesture recognition,2013,"Algorithms for hand feature extraction used in gesture recognition systems have some problems such as unnecessary information gathering. This paper proposes a novel method for feature extraction in gesture recognition systems based on the Local Contour Sequence (LCS). It is called the Convexity Local Contour Sequence (CLCS) and represents the hand shape only with the most significant information. This generates a smaller output result, but capable to model an entire dynamic gesture. It is used to classify dynamic gestures with an Elman Recurrent Network and Hidden Markov Model and presents a better result compared to regular LCS."
933883,14018,8806,Using constraint-based optimization and variability to support continuous self-adaptation,2012,"Self-adaptation is one of the upcoming paradigms that accurately tackles nowadays systems complexity. In this context, Dynamic Software Product Lines model the intrinsic variability of a family of systems, and dynamically support their reconfiguration according to updated context. However, when several configurations are available for the same context, making a decision about the right one is a hard challenge: further dimensions such as QoS are needed to enrich the decision making process. In this paper, we propose to combine variability with Constraint-Satisfaction Problem techniques to face this challenge. The approach is illustrated and validated with a context-driven system used to support the control of a home through mobile devices."
678647,14018,8806,A knowledge-based framework for reference architectures,2012,"Software architectures play a major role in determining system quality, since they form the backbone of any successful software-intensive system. In this context, reference architectures refer to a special type of software architecture that capture the essence of the system architectures of a given domain, encompassing the knowledge about how to develop, standardize, and evolve systems of that domain. However, in spite of the considerable number of reference architectures available, no good understanding exists regarding knowledge contained in reference architectures. The main contribution of this work is to provide a better understanding of such knowledge and to propose a framework which presents a comprehensive panorama of this knowledge. An analysis of this framework is presented based on a relevant, well-known reference architecture widely used in industry. As its main result, this framework could provide a better understanding of the contents of such architectures, enabling the development of more complete, efficient reference architectures."
1224768,14018,8806,Evolutionary computation for the prediction of secondary protein structures,2011,"We have developed an evolutionary computation approach to predict secondary structure motifs using some main amino acid physical-chemical properties. The prediction model will consist of rules that predict both the beginning and the end of the regions corresponding to a secondary structure state conformation (α-helix or β-strand). A study about propensities of each pair of amino acids in capping regions of α-helix and β-strand are also performed with a data set of 12,830 non-homologous and non-redundant protein sequences."
1146989,14018,8806,A model-based approach to test automation for context-aware mobile applications,2014,"Current testing tools for mobile applications do not provide sufficient support for context-aware application testing. In addition to regular input vectors (e.g. touch events, text entry) context parameters must be considered (e.g. accelerometer data interpreted as shake gestures, GPS location data, etc.). A multitude of possible application faults resulting from these additional context parameters requires an appropriately selected set of test cases. In this paper, we propose a model-based approach to improve the testing of context-aware mobile applications by deducing test cases from design-time system models. Using a custom-built version of the calabash-android testing framework enhanced by an arbitrary context parameter facility, our approach to test case generation and automated execution is validated on a context-aware mobile application."
1253771,14018,8806,A novel segmentation method for convex lesions based on dynamic programming with local intra-class variance,2012,"Lesion segmentation plays an important role in medical image processing and analysis. There exist several successful dynamic programming (DP) based segmentation methods for general images. In those methods, the gradient is used as an important factor in the cost function to attract the contours to the boundaries. Since medical images have their characteristics such as low contrast, blurred edges and high noises, the gradient operator cannot work well enough to achieve a satisfactory performance for boundary detection. We define the local intra-class variance and combine it with the dynamic programming method to replace the traditional gradient operation. Experiments on synthetic and X-ray images are carried out and the results are compared with Canny and fast multilevel fuzzy edge detection (FMFED) algorithms. It is demonstrated that the proposed method performs better on medical images with low contrast, blurred edges and high noises. In addition, 483 regions of interest of mammograms randomly extracted from DDSM of the University of South Florida are used to compare our proposed method with the plane-fitting and dynamic programming method (PFDP), and the normalized cut segmentation method (Ncut). The results demonstrate that our method is more accurate and robust than PFDP and Ncut."
1133425,14018,8806,Method for fast clustering of data distributed on a sphere surface,2014,"In recent years, research for data mining is applied to many fields such as biology, geography, and environmental science. The collected data has a large size and many dimensions, so it takes a long time for researchers to cluster the data. In this paper, we introduce a method for fast clustering 3-diemsntional data whose elements are distributed mostly on the surface of a sphere by converting data to lower dimensional space and cluster this data in new space so that we may decrease the time for computation."
2494200,14018,9080,Accelerating convergence in cartesian genetic programming by using a new genetic operator,2013,"Genetic programming algorithms seek to find interpretable and good solutions for problems which are difficult to solve analytically. For example, we plan to use this paradigm to develop a car accident severity prediction model for new occupant safety functions. This complex problem will suffer from the major disadvantage of genetic programming, which is its high demand for computational effort to find good solutions. A main reason for this demand is a low rate of convergence. In this paper, we introduce a new genetic operator called  forking  to accelerate the rate of convergence. Our idea is to interpret individuals dynamically as centers of local Gaussian distributions and allow a sampling process in these distributions when populations get too homogeneous. We demonstrate this operator by extending the  Cartesian Genetic Programming  algorithm and show that on our examples convergence is accelerated by over 50% on average. We finish this paper with giving hints about parameterization of the forking operator for other problems."
1358391,14018,8806,Improving the offline clustering stage of data stream algorithms in scenarios with variable number of clusters,2012,"Many data stream clustering algorithms operate in two well-defined steps: (i) online statistical data collection stage; and (ii) offline macro-clustering stage. The well-known  k -means algorithm is often employed for performing the offline macro-clustering step. The conventional  k -means algorithm assumes that the number of clusters ( k ) is defined  a priori  by the user. Given the difficulty of defining the value of  k a priori  in real-world problems, we describe a new approach that allows estimating  k  dynamically from streams with variable number of clusters, which is a common scenario in data with a non-stationary distribution. In addition, we combine our dynamic approach with two different strategies for initializing the centroids during the offline clustering. Analysis of results suggest that, using the dynamic approach, the method  k -means++ for centroids initialization present better results."
962135,14018,8806,Massive character recognition with a large ground-truthed database,2011,"In character recognition, multiple prototype classifiers, where multiple patterns are prepared as representative patterns of each class, have often been employed to improve recognition accuracy. Our question is how we can improve the recognition accuracy by increasing prototypes massively in the multiple prototype classifier. In this paper, we will answer this question through several experimental analyses, using a simple 1-nearest neighbor (1-NN) classifier and about 550,000 manually labeled handwritten numeral patterns. The analysis results under the leave-one-out evaluation showed not only a simple fact that more prototypes provide fewer recognition errors, but also a more important fact that the error rate decreases approximately to 40% by increasing the prototypes 10 times. The analysis results also showed other phenomena in massive character recognition, such that the NN prototypes become visually closer to the input pattern by increasing the prototypes."
750000,14018,8806,A clustering-based approach for discovering flaws in requirements specifications,2012,"In this paper, we present the application of a  clustering  algorithm to exploit lexical and syntactic relationships occurring between natural language requirements. Our experiments conducted on a real-world data set highlight a correlation between clustering  outliers , i.e., requirements that are marked as noisy by the clustering algorithm, and requirements presenting flaws. Those flaws may refer to an  incomplete  explanation of the behavioral aspects, which the requirement is supposed to provide. Furthermore, flaws may also be caused by the usage of  inconsistent terminology  in the requirement specification. We evaluate the ability of our proposed algorithm to effectively discover such kind of flawed requirements. Evaluation is performed by measuring the accuracy of the algorithm in detecting a set of flaws in our testing data set, which have been previously manually-identified by a human assessor."
2226572,14018,9080,Evolving complete robots with CPPN-NEAT: the utility of recurrent connections,2011,"This paper extends prior work using Compositional Pattern Producing Networks (CPPNs) as a generative encoding for the purpose of simultaneously evolving robot morphology and control. A method is presented for translating CPPNs into complete robots including their physical topologies, sensor placements, and embedded, closed-loop, neural network control policies. It is shown that this method can evolve robots for a given task. Additionally it is demonstrated how the performance of evolved robots can be significantly improved by allowing recurrent connections within the underlying CPPNs. The resulting robots are analyzed in the hopes of answering why these recurrent connections prove to be so beneficial in this domain. Several hypotheses are discussed, some of which are refuted from the available data while others will require further examination."
2528451,14018,9080,Evolving deep unsupervised convolutional networks for vision-based reinforcement learning,2014,"Dealing with high-dimensional input spaces, like visual input, is a challenging task for reinforcement learning (RL). Neuroevolution (NE), used for continuous RL problems, has to either reduce the problem dimensionality by (1) compressing the representation of the neural network controllers or (2) employing a pre-processor (compressor) that transforms the high-dimensional raw inputs into low-dimensional features. In this paper, we are able to evolve extremely small recurrent neural network (RNN) controllers for a task that previously required networks with over a million weights. The high-dimensional visual input, which the controller would normally receive, is first transformed into a compact feature vector through a deep, max-pooling convolutional neural network (MPCNN). Both the MPCNN preprocessor and the RNN controller are evolved successfully to control a car in the TORCS racing simulator using only visual input. This is the first use of deep learning in the context evolutionary RL."
1883003,14018,9080,A study on the importance of selection pressure and low dimensional weak learners to produce robust ensembles,2013,"Ensembles of classifiers have been studied for some time. It is widely known that weak learners should be accurate and diverse. However, in the real world there are many constraints and few have been said about the robustness of ensembles and how to develop it. In the context of random subspace methods, this paper addresses the question of developing ensembles to face problems under time constraints. Experiments show that selecting weak learners based on their accuracy can be used to create robust ensembles. Thus, the selection pressure in ensembles is a key technique to create not just effective ensembles but also robust ones. Moreover, the experiments motivate further research on ensembles made of low dimensional classifiers which achieve general accurate results."
1313076,14018,8806,Gesture unit segmentation using support vector machines: segmenting gestures from rest positions,2013,"Gesture analysis has been widely used for developing new methods of human-computer interaction. The advancement reached in the gesture analysis area is also motivating its application to automate tasks related to discourse analysis, such as the gesture phases segmentation task. In this paper, we present an initiative that aims at segmenting gestures, especially considering the units -- the larger grain involved in gesture phases segmentation. Thereunto, we have captured the gestures using a Xbox Kinect™ device, modeled the problem as a classification task, and applied Support Vector Machines. Moreover, aiming at taking advantage from the temporal aspects involved in the problem, we have used several types of data pre-processing in order to consider time domain and frequency domain features."
2368684,14018,9080,Particle swarm optimisation with gradually increasing directed neighbourhoods,2011,"Particle swarm optimisation (PSO) is an intelligent random search algorithm, and the key to success is to effectively balance between the exploration of the solution space in the early stages and the exploitation of the solution space in the late stages. This paper presents a new dynamic topology called gradually increasing directed neighbourhoods (GIDN) that provides an effective way to balance between exploration and exploitation in the entire iteration process. In our model, each particle begins with a small number of connections and there are many small isolated swarms that improve the exploration ability. At each iteration, we gradually add a number of new connections between particles which improves the ability of exploitation gradually. Furthermore, these connections among particles are created randomly and have directions. We formalise this topology using random graph representations. Experiments are conducted on 31 benchmark test functions to validate our proposed topology. The results show that the PSO with GIDN performs much better than a number of the state of the art algorithms on almost all of the 31 functions."
1260770,14018,8806,Development support for QoS-aware service-adaptation in ubiquitous computing applications,2011,"In ubiquitous computing environments services may be discovered and bound dynamically. Adaptive applications may utilize such services to improve their offered functional and nonfunctional properties. Generally, the adaptation decision depends on the quality of service (QoS) of discovered services. The development of such adaptive applications is a complex, challenging task. In this paper, we present a general methodology for facilitating the development of QoS-dependent self-adaptive applications. We present several lessons learned from application case studies using the new approach."
976662,14018,8806,An approach for providing dependable self-adaptation in distributed embedded systems,2011,"Modern distributed embedded systems are reaching an extreme complexity which is very hard to master with traditional methods. Particularly the need for these systems to adapt their behavior autonomously at runtime to changing conditions is a demanding challenge. Since most industrial application domains of distributed embedded systems have high demands on reliability and safety, we need a dependable self-adaptation mechanism to apply adaptation successfully in these domains. Therefore, we propose a concept to guarantee the proper system behavior and a mechanism which preserves the predefined functional and non-functional requirements of the system."
950370,14018,8806,A model driven methodology for enabling autonomic reconfiguration of service oriented architecture,2013,"Autonomic systems are known by their abilities to manage and reconfigure themselves according to the context changes that can include the evolution of functional and/or nonfunctional requirements, without human intervention. The design and the management of such complex systems manually is a hard task since both functional and non-functional requirements should be taken into consideration. In this paper, we propose a model driven methodology which enables the dynamic reconfiguration by generating autonomic architectures from high level descriptions of functional requirements. Based on transformation and refinement rules, this methodology automates the incorporation of non-functional requirements to the initial architecture. Our work follows the Model Driven Architecture (MDA) to cover the different abstraction levels."
1393004,14018,9704,A study on time-varying partially connected topologies for the particle swarm,2013,"This paper presents a study on the effects of dynamic and partially connected 2-dimensional topologies on the performance of the particle swarm optimization (PSO). The swarm is positioned on 2-dimensional grids of nodes and the particles move through the nodes according to a simple rule. Meanwhile, the von Neumann neighborhood is used to decide which particles influence each individual. Structures with growing size are tested on a classical benchmark and compared to several configurations such as lbest, gbest and the standard von Neumann configuration. The results show that the partially connected grids with von Neumann neighborhood structure performs more consistently when compared to lbest, gbest and the standard von Neumann topology."
2410538,14018,8806,A statistical approach to simulation model validation in response-time analysis of complex real-time embedded systems,2011,"As simulation-based analysis methods make few restrictions on the system design and scale to very large and complex systems, they are widely used in, e.g., timing analysis of complex real-time embedded systems (CRTES) in industrial circles. However, before such methods are used, the analysis simulation models have to be validated in order to assess if they represent the actual system or not, which also matters to the confidence in the simulation results. This paper presents a statistical approach to validation of temporal simulation models extracted from CRTES, by introducing existing mature statistical hypothesis tests to the context. Moreover, our evaluation using simulation models depicting a fictive but representative industrial robotic control system indicates that the proposed method can successfully identify temporal differences between different simulation models, hence it has the potential to be considered as an effective simulation model validation technique."
1711531,14018,8806,A method to identify services using master data and artifact-centric modeling approach,2014,"Service identification is one of the biggest challenges in implementing a service-oriented architecture. Current service identification methods (SIMs) rely on business process descriptions to elicit business perspective. However, service identification requires a level of business process documentation only found on organizations mature on business process modeling. In this context, master data (core enterprise information concepts, needed across different business processes, organizational units and applications across the organization) can be used as alternative input to business process. This work proposes a SIM that uses master data and logical data models as inputs. The proposed method also uses artifact-centric modeling technique to detail master data lifecycle and business rules within it."
2303307,14018,9080,A honey bees mating optimization algorithm for the open vehicle routing problem,2011,"Honey Bees Mating Optimization algorithm is a relatively new nature inspired algorithm. In this paper, this nature inspired algorithm is used in a hybrid scheme with other metaheuristic algorithms for successfully solving the Open Vehicle Routing Problem. More precisely, the proposed algorithm for the solution of the Open Vehicle Routing Problem, the Honey Bees Mating Optimization (HBMOOVRP), combines a Honey Bees Mating Optimization (HBMO) algorithm and the Expanding Neighborhood Search (ENS) algorithm. Two set of benchmark instances is used in order to test the proposed algorithm. The results obtained for both sets are very satisfactory. More specifically, in the fourteen instances proposed by Christofides, the average quality is 0.35% when a hierarchical objective function is used, where, first, the number of vehicles is minimized and, afterwards, the total travel distance is minimized and the average quality is 0.42% when only the travel distance is minimized, while for the eight instances proposed by Li et al. when a hierarchical objective function is used the average quality is 0.21%."
2436797,14018,9080,The use of reputation as noise-resistant selection bias in a co-evolutionary multi-agent system,2012,"Little attention has been paid to the relationship between fitness evaluation in evolutionary algorithms and reputation mechanisms in multi-agent systems, but if these could be related it opens the way for implementation of distributed evolutionary systems via multi-agent architectures. In this paper we investigate the effectiveness with which reputation can replace direct fitness observation as the selection bias in an evolutionary multi-agent system. We do this by implementing a peer-to-peer, self-adaptive genetic algorithm, in which agents act as individual GAs that, in turn, evolve dynamically themselves in real-time. The evolution of the agents is implemented in two alternative ways: First, using the traditional approach of direct fitness observation (self-reported by each agent), and second, using a simple reputation model based on the collective past experiences of the agents. Our research shows that this simple model of distributed reputation can be successful as the evolutionary drive in such a system. Further, we discuss the effect of noise (in the form of defective agents) in both models. We show that, unlike the fitness-based model, the reputation-based model manages to identify the defective agents successfully, thus showing a level of resistance to noise."
2160218,14018,9080,Generating single and multiple cooperative heuristics for the one dimensional bin packing problem using a single node genetic programming island model,2013,"Novel deterministic heuristics are generated using Single Node Genetic Programming for application to the One Dimensional Bin Packing Problem. First a single deterministic heuristic was evolved that minimised the total number of bins used when applied to a set of 685 training instances. Following this, a set of heuristics were evolved using a form of cooperative co-evolution that collectively minimise the number of bins used across the same set of problems. Results on an unseen test set comprising a further 685 problem instances show that the single evolved heuristic outperforms existing deterministic heuristics described in the literature. The collection of heuristics evolved by cooperative co-evolution outperforms any of the single heuristics, including the newly generated ones."
1829848,14018,9080,Evolution of digital circuits,2011,"Since the early 1990's researchers have begun to apply evolutionary algorithms to design electronic circuits. Nowadays it is evident that the evolutionary design approach can automatically create efficient electronic circuits in many domains. This tutorial surveys fundamental concepts of evolutionary circuit design. It introduces relevant search algorithms and basics of digital circuit design principles. Several case studies will be presented to demonstrate strength and weakness of the method, including evolutionary synthesis of gate-level circuits, image filter evolution in FPGA and evolution of benchmark circuits for evaluation of testability analysis methods. FPGAs will be presented as accelerators for evolutionary circuit design and circuit adaptation. Finally, it will be shown how to cope with the so-called scalability problem of evolutionary design which has been identified as the most important problem from the point of view of applications."
2496981,14018,9080,Local optima networks and the performance of iterated local search,2012,"Local Optima Networks (LONs) have been recently proposed as an alternative model of combinatorial fitness landscapes. The model compresses the information given by the whole search space into a smaller mathematical object that is the graph having as vertices the local optima and as edges the possible weighted transitions between them. A new set of metrics can be derived from this model that capture the distribution and connectivity of the local optima in the underlying configuration space. This paper departs from the descriptive analysis of local optima networks, and actively studies the correlation between network features and the performance of a local search heuristic. The  NK  family of landscapes and the Iterated Local Search metaheuristic are considered. With a statistically-sound approach based on multiple linear regression, it is shown that some LONs' features strongly influence and can even partly predict the performance of a heuristic search algorithm. This study validates the expressive power of LONs as a model of combinatorial fitness landscapes."
969359,14018,8806,Towards a domain specific modeling language for agent-based models in land use science,2013,"While Multi-Agent Systems are widely used in Land Use Science, it still remains challenging for scientists to specify their models in a manner that they can be understood and used by others. Domain users require a higher-level language to promote reuse, extension and replication of model based on agents. To overcome this issue, we propose the development of a domain specific modeling language (DSML) for agent-based models in Land Use Science. In this paper, we present the first step towards the development of the DSML, which consists of a domain analysis model for agent-based in Land Use Science."
807395,14018,8806,A genetic scheduler for electric vehicle charging,2012,"This paper presents a design and evaluates the performance of a genetic scheduler for electric vehicles, aiming at reducing the peak load of a fast charging station while meeting the time constraint of all charging requests. Upon the preemptive task model consisting of actuation time, operation length, deadline, and a consumption profile, the proposed scheduler fills the allocation table, by which charging tasks can be started, suspended, and resume at each time slot. To obtain a better initial population, our heuristic approach selects time slots having the lowest power load until the previous task allocation. Then, the regular genetic operations further improve the schedule, additionally creating new chromosomes capable of satisfying all time constraints. The performance measurement result obtained from a prototype implementation shows that our scheme can reduce the peak load for the given charging task sets by up to 11.0 %, compared with a conventional scheme."
2189262,14018,9080,A genetic algorithm to enhance transmembrane helices prediction,2011,"A transmembrane helix (TMH) topology prediction is becoming a central problem in bioinformatics because the structure of TM proteins is difficult to determine by experimental means. Therefore, methods which could predict the TMHs topologies computationally are highly desired. In this paper we introduce TMHindex, a method for detecting TMH segments solely by the amino acid sequence information. Each amino acid in a protein sequence is represented by a Compositional Index deduced from a combination of the difference in amino acid appearances in TMH and non-TMH segments in training protein sequences and the amino acid composition information. Furthermore, genetic algorithm was employed to find the optimal threshold value to separate TMH segments from non-TMH segments. The method successfully predicted 376 out of the 378 TMH segments in 70 testing protein sequences. The level of accuracy achieved using TMHindex in comparison to recent methods for predicting the topology of TM proteins is a strong argument in favor of our method."
2096454,14018,9080,MOEA for clustering: comparison of mutation operators,2013,"Clustering is an important task in data mining. However, there are numerous conflicting measurements of what a good clustering solution is. Therefore, clustering is a task that is suitable for a Multi-Objective Evolutionary Algorithm. Mutation operators for these algorithms can be designed to explore a diverse range of solutions or focus upon individual solution quality. We propose using a hybrid technique that generates a wide range of solutions and then improves them with respect to the data. We create an experimental set-up to assess mutation operators with respect to Pareto front quality. Using this set-up we find that mutation operators that mutate solutions with respect to the data perform better but hybrid mutation techniques show promise."
916561,14018,8806,Hierarchical confidence-based active clustering,2012,"In this paper, we address the problem of semi-supervised hierarchical clustering by using an active clustering solution with cluster-level constraints. This active learning approach is based on a concept of merge confidence in agglomerative clustering. The proposed method was compared with an un-supervised algorithm (average-link) and a semi-supervised algorithm based on pairwise constraints. The results show that our algorithm tends to be better than the pairwise constrained algorithm and can achieve a significant improvement when compared to the unsupervised algorithm."
1731786,14018,8806,Leveraging the dynamics of learning by modeling and managing psychosocial relations and behavior by means of game theory and memetics,2011,"Despite the tremendous advances in the resources and technologies of interpersonal communication available today, when it comes to dealing with unstructured information, which covers most of the human knowledge, the effectiveness of the benefits is closely linked to the dialectical interactions between the participants. When the focus comes to distance learning process or any kind of distance intellectual or dialectical interaction, severe limitations remain in force due to the differences in the behavioral characteristics of the participants. In these cases, the effectiveness is still far from a minimally adequate standard. This paper suggests a dynamic management approach based on concepts of Memetics and Game Theory modeling and simulation, offering proposals with the potential to mitigate many of these restrictions, through the induction of so-called cooperative and altruistic behaviors in the social attitude of the participants."
1147730,14018,8806,A mediator for statistical linked data,2013,"This paper introduces a mediation architecture to help describing and consuming statistical data, exposed as RDF triples, but stored in relational databases. The architecture features a catalogue of  linked data cube descriptions , created according to the Linked Data principles. The catalogue has a standardized description for each data cube actually stored in each statistical (relational) database known to the mediation environment. The mediator offers an interface to browse the linked data cube descriptions and exports the data cubes as RDF triples, generated on demand from the underlying databases."
975501,14018,8806,ICM-Wind: semantics-empowered fluid condition monitoring of wind turbines,2014,"We present the first system, called ICM-Wind, for semantics-empowered fluid condition monitoring (FCM) in wind turbines. It monitors the condition of fluids in the wind turbine gearbox, recognizes actual and the onset of failures of FCM sensors and components installed on the gearbox, and provides knowledge-based failure diagnosis support to non-experts. For this purpose, the ICM-Wind system performs semantic sensor data analysis by applying semantic technologies for interpreting the state of turbine parts and answering questions related to their maintenance. Domain knowledge is encoded in OWL2 and with SPIN rules. Fault detection and diagnosis queries are answered by use of the semantic reasoners Fact++, STAR, TopSPIN rule engine, and SwiftOWLIM store. The system prototype was successfully tested in cooperation with the HYDAC Filter Systems GmbH based on given selected samples of a two-year recording of FCM multi-sensor and operational data for two wind turbines of a regional on-shore wind farm operated by the ABO Wind AG."
1623433,14018,8806,Examining the practical challenges of an Augmented Reality cyber-infrastructure framework,2012,"In this paper, we present issues to be addressed and practical solutions for these issues in an Augmented Reality (AR) cyber-infrastructure framework currently being developed. This framework provides services to AR application developers and users for creating, sharing, publishing, and consuming AR content. It also helps in managing different content datasets, including user-generated AR content, internal and external content repositories. The practical development challenges can be categorized into three groups: computation challenges, sensor capability challenges, and challenges associated with development and deployment of the platform. Specifically, these challenges include finding the optimal update frequency for tracking a user's motion, working with delays in the GPS, thread synchronization, adverse interactions with the hosting device, as well as debugging and testing of a pose computation in AR application. Limitations and practical solutions for both an AR mobile device and AR frameworks are discussed, along with an analysis of characteristics of AR frameworks."
1483384,14018,8806,A model-based framework for flexible safety-critical software development: a design study,2013,"This paper presents the findings from a design study of a model-based framework for safety-critical software development, called  SimPal . The objective of the study was to better understand the necessary properties of such a framework and to learn more about the challenges of realizing it. Our research approach can be labeled as design research, which means that we try to answer our research questions by developing an artifact, in our case  SimPal , and analyzing our experiences from the design of the artifact. In the paper we present what we identify as the necessary quality characteristics, using the ISO25010  quality in use  quality model, of a framework like  SimPal . These characteristics are then used to evaluate the  SimPal  framework in combination with a simple design case where we design a  soft safety controller . We show that our approach has potential considering safety-critical software development. Although, there are some concerns about its run-time performance, from our results we conclude that the ideas behind the  SimPal  framework are sound but more work is required to investigate how they can be realized. In the future more effort should be spent on increasing performance and adding more features to the framework."
1285992,14018,8806,Exploiting time predictable two-level scratchpad memory for real-time systems,2011,"In modern computer architectures, caches are time unpredictable, and thus can significantly increase the complexity of worse-case execution time (WCET) analysis for real-time systems. This paper proposes a time predictable two-level scratchpad (SPM) based architecture for VLIW (Very Long Instruction Word) processors, and an ILP (Integer Linear Programming) based static memory objects assignment algorithm is utilized in order not to harm the time predictability of SPMs. Also, both the timing and energy performance of our two-level SPM based architecture are completely evaluated in this paper. Our experimental results indicate that the timing and energy performance of our architecture is superior to the similar cache based architecture for 75% of the benchmarks we studied."
2375714,14018,8806,User centric complex event processing based on service oriented architectures,2013,"Current Complex Event Processing (CEP) systems require considerable technical efforts for event pattern definition and event stream implementation. In this paper, we present a service oriented framework that provides a user centric way to define complex event patterns and implement the patterns automatically. We extend the Business Event Modeling Notations (BEMN) to allow the business users to describe their complex events with graphical notations. Member event specifications in such event patterns refer to services that deliver them. We then transform the event pattern into a stream query, subscribe to relevant services and obtain event streams required. Finally, we evaluate the query over primitive event streams to obtain results as complex events."
1692484,14018,8806,An automatic selective color transfer algorithm for images,2011,"Most color transfer algorithms focus on how to transfer the entire color attributes between images. Rarely, do color transfer algorithms select appropriate colors to transfer on images by analyzing image color discrepancies. This article proposes an Automatic Selective Color Transfer (ASCT) algorithm between images by utilizing the image quality factor measuring skills. The ASCT algorithm enhances Reinhard's algorithm (2001) by applying covariance and weight control as well as incorporates Gaussian membership function, and the digital image quality factor. The ASCT algorithm resolves the over color transfer problem of the previous algorithms. This article uses 4 test models to demonstrate how the algorithm operates. Experimental results show that the ASCT algorithm can automatically transfer a portion of one's image color mood to another image without any user intervention."
1727952,14018,8806,An imperative language of self-modifying graphs for biological systems,2012,"The main design features of a language for the management of the dynamic reconfiguration of graphs are described. The language is domain-specific to model the behaviour of biological systems.   Nodes of graphs are biochemical components, and undirected edges between them represent biochemical bonds. Also, nodes have a finite number of binding sites, and each of them can be the end-point of an edge leading to (exactly one end-point of) another node. A pair of nodes can be connected by multiple edges, each of them representing a particular bond between the involved biochemical entities.   Spontaneous events, corresponding to the biological formation/breakage of bonds, can cause graph reconfigurations. Nodes can be programmed to react to them, and consequently change their own propensity to be involved in further events.   The event handling routines coded within nodes are executed in a concurrent fashion. This poses the usual problems related to race conditions, deadlock and the like. Language primitives are tailored to cope with these issues. They ensure, e.g., consistency of reconfigurations: each binding site of every node can be connected at most with one single binding site of another node.   This extended abstract describes the rationale behind the peculiarities of the language by showing excerpts of programmed nodes that, together with spontaneous events, can drive relevant reconfigurations of graphs."
2149359,14018,8806,Experience report on developing the Front-end client unit under the control of formal methods,2012,"Formal methods are extensively being applied to the development of control software units, of highly sophisticated X-ray machines, at Philips Healthcare. One of the early units incorporating formal methods is the Front-end client (FE-Client), which was developed under the control of formal technologies, supported by the Analytical Software Design (ASD) method. As a result, only eleven coding errors were detected during the construction of 28 thousands lines of code. Team members attribute the ultimate quality of the software to the rigor of the formal technologies supplied by the ASD method. In this paper we report about the experience of applying ASD to the development of the FE-Client, and we show how formal methods substantially enhanced its quality. We also discuss the nature of the errors found during the construction of the unit."
1471709,14018,8806,Demand response computation for future smart grids incorporating wind power,2013,"In this paper, we study supply and demand management in the presence of conventional and renewable energy sources, where the latter is represented by a single wind turbine. Total social welfare, defined in terms of consumer utility and cost of power production (both scheduled and renewable), is maximized while the probability of power shortfall due to the uncertainty in the wind power production is limited by an upper bound. We find the outage-limiting optimal production and consumption schedules, and demonstrate superiority over a competing policy which ensures that total power production meets the demand on the average."
1564281,14018,8806,The role of NFRs when transforming i* requirements models into OO-method models,2013,"In recent years the model driven software development paradigm has gained popularity. For example, initial requirements expressed in terms of i* (iStar) models have been transformed into OO-Method conceptual models that allows the complete generation of the final application. Unfortunately, the non-functional requirements (NFRs) or softgoals present in the i* models have not been considered. In this paper we outline a new process, called OOM-NFR, which relies on the softgoals for the definition of appropriate configuration of the application to be generated. The set of transformation rules is expressed in QVT."
1562622,14018,8806,Learning hybrid recommender models for heterogeneous semantic data,2013,"Recommender algorithms are key technologies supporting users to deal with information overload. Recommender Systems (RS) aim at identifying relevant items a user is unaware of. The relevance of items depends on various criteria such as item properties, the relationship to other items, user preferences, and contexts. The aggregation of different criteria is the key in computing high-quality recommendations. We propose a novel approach for automatically learning parameters for recommendations based on semantic datasets (graphs). We outline how scaling models and noise reduction models allow us to consider individual dataset properties. We evaluated the proposed methods on a semantic movie data set. The evaluation shows that each semantic relationship set requires a separate recommender model. Combining such recommender models yields much higher precision. We show the recommender ensembles outperform recommenders based on aggregated semantic graphs (block matrix recommender)."
1920122,14018,8806,A distributed intrusion detection scheme for wireless ad hoc networks,2012,"Wireless ad hoc network is an emerging technology that is gaining popularity as a cost-effective way of offering end-to-end services, such as Internet access, in an inexpensive, practical, and fast manner. However, wireless ad hoc networks are particularly vulnerable to extern and insider attacks due to their very intrinsic nature such as shared wireless medium and decentralized architecture. In this paper, we present a distributed intrusion detection approach, which is based on exchange of events and cooperation between the participating nodes. The proposed mechanism relies on non-intrusive traffic monitoring at each node, which generates network events. These events are processed by the intrusion detection system that is able to instantly signalize malicious activities, for instance, a node disseminating fake routing information through the network. We demonstrate the approach efficiency for detecting misbehaving nodes through a virtualized network environment that consists of virtual nodes interconnected which represent the network topology."
1138727,14018,8806,Introduction of a multi-layer predictive search strategy for scalable video coding,2014,"Conventional scalable solutions based on single-layer motion prediction strategies are not able to explore inter-layer redundancies. Considering that scenario, this paper introduces ESIPS, an efficient Inter-Layer Prediction (ILP) algorithm based on a multilayer predictive search strategy, which was specially designed to increase overall performance of scalable encoders. Comparative results indicate that the proposed method reaches a performance increase of around five times in relation to traditional motion prediction algorithms."
1758017,14018,8806,eXtreme enterprise architecture planning,2014,"When developing enterprise architectures, in the same way as software products, companies have to deal a constant growth on the clients demand for faster results, while facing, at the same time, a big uncertainty on the requirements surrounding the project. This paper tries to investigate the similarities between the difficulties faced in both industries of EA and software development, and propose an extension to Enterprise Architecture Planning methodology [1], by introducing agile characteristics into it as a way to address the problems identified."
948265,14018,8228,Optimal power scheduling for green smart grids with renewable sources,2013,"In this paper, we study demand response management and power scheduling in a smart grid with renewable energy sources represented by a wind farm. Total social welfare, defined in terms of consumer utility and cost of energy production (both scheduled and renewable), is maximized while the probability of energy shortfall due to the uncertainty in renewable production is limited by an upper bound. We find the outage-limiting optimal conventional energy production and consumption schedules, and investigate the impact of the statistical relationship between the turbines in a wind farm."
1510475,14018,8806,On the impact of obliviousness and quantification on model composition effort,2014,"Researchers and practitioners advocate that design properties, such as obliviousness and quantification, can improve the modularity of software systems, thereby reducing the effort of composing design models. However, there is no empirical knowledge about how these design properties impact model composition effort. This paper, therefore, performs an empirical study to understand this impact. The main contributions are: (i) quantitative indicators to evaluate to what extent such design properties impact model composition effort; (ii) an objective evaluation of the impact of such modularity properties in 26 versions of two software projects by using statistical tests; and (iii) lessons learned on whether (and how) modularity anomalies related to misuse of quantification and obliviousness in the input models can significantly increase model composition effort."
1923461,14018,8806,Double dip map-reduce for processing cross validation jobs,2012,"Cross validation is fundamental to machine learning as it provides a reliable way in which to evaluate algorithms and the overall quality of the corpora in use. In typical cross validation, the corpus is initially divided into  learning  and  training  segments, then crossed-over in successive rounds, so that each data segment is validated against the remaining ones. This process is prohibitively time and effort consuming, and often brushed off for computationally cheaper ones, such as heuristics. In this paper we introduce a cloud-based architecture for running cross validation jobs. Our solution makes heavy use of computational resources in the cloud by proposing a strategy in which there are two distinct, subsequent, map-reduce cycles: the first to perform the algorithmic target computation, and the second to provide cross validation data to retrofit the machine learning process. We demonstrate the feasibility of the proposed approach, with the implementation of a web segmentation algorithm."
1642855,14018,9080,Computational) synthetic biology,2011,"The ultimate goal of systems biology is the development of executable in silico models of cells and organisms. Systems biology attempts to provide an integrative methodology, which while able to cope with -on the one hand- the data deluge that is being generated through high throughput experimental technologies -and on the other hand- emerging technologies that produce scarce often noisy data, would allow to capture within human understandable models and simulations novel biological knowledge.   In its more modest instantiations, Systems Biology seeks to *clarify* current biological understandings by formalizing what the constitutive elements of a biological system are and how they interact with each other and also it seeks to aid in the *testing* of current understandings against experimental data. In its most ambitious incarnations, however, it aims at *predicting* the behavior of biological systems beyond current understanding and available data thus shedding light onto possible new experimental routes that could lead to better theoretical insights.   Synthetic biology, on the other hand, aims to implement, in vitro/vivo, organisms whose behavior is engineered. The field of synthetic biology holds a great promise for the design, construction and development of artificial (i.e. man-made) biological (sub systems thus offering viable new routes to genetically modified organisms, smart drugs as well as model systems to examine artificial genomes and proteomes. The informed manipulation of such biological (sub)systems could have an enormous positive impact on our societies, with its effects being felt across a range of activities such as the provision of healthcare, environmental protection and remediation, etc. The basic premise of synthetic biology is that methods commonly used to design and construct non-biological systems, such as those employed in the computational sciences and the engineering disciplines, could also be used to model and program novel synthetic biosystems. Synthetic biology thus lies at the interface of a variety of disciplines ranging from biology through chemistry, physics, computer science, mathematics and engineering.   In this tutorial I will provide an entry level understanding to Systems and Synthetic Biology, it goals, methods and limitations. Furthermore I will describe the many potential applications of evolutionary computation to these two fields. Indeed, I believe that the EC community has a beautiful new application domain in which its methods could be both valued and challenged."
1021405,14018,21102,Designing a compact Genetic fuzzy rule-based system for one-class classification,2014,"Abstract—This paper proposes a method for designing FuzzyRule-Based Classiﬁcation Systems to deal with One-Class Clas-siﬁcation, where during the training phase we have access onlyto objects originating from a single class. However, the trainedmodel must be prepared to deal with new, unseen adversarialobjects, known as outliers. We use a Genetic Algorithm forlearning the granularity, domains and fuzzy partitions of themodel and we propose an ad-hoc rule generation methodspeciﬁc for One-Class Classiﬁcation. Several datasets from UCIrepository, previously transformed to one-class problems, areused in the experiments and we compare with two of theclassical methods used in the One-Class community, one-classSupport Vector Machines and Support Vector Data Description.Our proposal of fuzzy model obtains similar results than theother methods but presents a high interpretability due itsreduced number of rules. I. I NTRODUCTION In Machine Learning, traditional methods try to clas-sify a new element considering a discrete set of cate-gories or classes. In the One-Class Classiﬁcation (OCC)paradigm[1][2], one of these categories is sufﬁciently des-cribed in the examples of the training data and usually it isnamed positive class, objective class or target class. However,for the rest of classes, those represent the negative concept,or simply the no belonging to the objective class, there areno examples or there are so few of them (not sufﬁcient forcharacterize that concept).In this contribution, we use Fuzzy Rule-Based Classiﬁca-tion Systems (FRBCSs), that present two main components:the Inference System and the Knowledge Base (KB). TheKB is composed of the Rule Base (RB) constituted by theset of fuzzy rules, and of the Data Base (DB), containingthe membership functions of the fuzzy partitions associatedto the linguistic variables. The composition of the KB ofa FRBCS directly depends on the problem being solved. Ifsome expert information about the problem under solving isnot available, it is possible to generate the KB from examplesby an automatic learning process.In previous works[3][4], we have demonstrated the highinﬂuence of the DB design in the behavior of a Fuzzy Rule-"
1162543,14018,9080,Evolutionary multiobjective optimization,2011,"Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives.   Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As randomized blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run---opposed to classical solution-based techniques from the multicriteria decision making (MCDM) field. Using EMO approaches in practice has two other advantages: they allow to learn about a problem formulation, for example, by automatically revealing common design principles among (Pareto-optimal) solutions (innovization) and it has been shown that certain single-objective problems become easier to solve with randomized search heuristics if the problem is reformulated as a multiobjective one (multiobjectivization).   This tutorial aims at giving a broad introduction to the EMO field and at presenting some of its recent research results in more detail. More specifically, I am going to (i) introduce the basic principles of EMO algorithms in comparison to classical solution-based approaches, (ii) show a few practical examples which motivate the use of EMO in terms of the mentioned innovization and multiobjectivization principles, and (iii) present a general overview of state-of-the-art algorithms and techniques. Moreover, I will present some of the most important research results in areas such as indicator-based EMO, preference articulation, and performance assessment.   Though classified as advanced, this tutorial is intended for both novices and regular users of EMO. Those without any knowledge will learn about the foundations of multiobjective optimization and the basic working principles of state-of-the-art EMO algorithms. Open questions, presented throughout the tutorial, can serve for all participants as a starting point for future research and/or discussions during the conference."
1609966,14018,9080,GECCO 2012 tutorial on evolutionary multiobjective optimization,2012,"Many optimization problems are multiobjective in nature in the sense that multiple, conflicting criteria need to be optimized simultaneously. Due to the conflict between objectives, usually, no single optimal solution exists. Instead, the optimum corresponds to a set of so-called Pareto-optimal solutions for which no other solution has better function values in all objectives.   Evolutionary Multiobjective Optimization (EMO) algorithms are widely used in practice for solving multiobjective optimization problems due to several reasons. As randomized blackbox algorithms, EMO approaches allow to tackle problems with nonlinear, nondifferentiable, or noisy objective functions. As set-based algorithms, they allow to compute or approximate the full set of Pareto-optimal solutions in one algorithm run - opposed to classical solution-based techniques from the multicriteria decision making (MCDM) field. Using EMO approaches in practice has two other advantages: they allow to learn about a problem formulation, for example, by automatically revealing common design principles among (Pareto-optimal) solutions (innovization) and it has been shown that certain single-objective problems become easier to solve with randomized search heuristics if the problem is reformulated as a multiobjective one (multiobjectivization).   This tutorial aims at giving a broad introduction to the EMO field and at presenting some of its recent research results in more detail. More specifically, we are going to (i) introduce the basic principles of EMO algorithms in comparison to classical solution-based approaches, (ii) show a few practical examples which motivate the use of EMO in terms of the mentioned innovization and multiobjectivization principles, and (iii) present a general overview of state-of-the-art algorithms and techniques. Moreover, we will present some of the most important research results in areas such as indicator-based EMO, preference articulation, surrogate-assisted EMO, and performance assessment.   Though classified as introductory, this tutorial is intended for both novices and regular users of EMO. Those without any knowledge will learn about the foundations of multiobjective optimization and the basic working principles of state-of-the-art EMO algorithms. Open questions, presented throughout the tutorial, can serve for all participants as a starting point for future research and/or discussions during the conference."
1247992,14018,9080,Theory of swarm intelligence,2011,"Social animals as found in fish schools, bird flocks, bee hives, and ant colonies are able to solve highly complex problems in nature. This includes foraging for food, constructing astonishingly complex nests, and evading or defending against predators. Remarkably, these animals in many cases use very simple, decentralized communication mechanisms that do not require a single leader. This makes the animals perform surprisingly well, even in dynamically changing environments. The collective intelligence of such animals is known as swarm intelligence and it has inspired popular and very powerful optimization paradigms, including ant colony optimization (ACO) and particle swarm optimization (PSO).   The reasons behind their success are often elusive. We are just beginning to understand when and why swarm intelligence algorithms perform well, and how to use swarm intelligence most effectively. Understanding the fundamental working principles that determine their efficiency is a major challenge.   This tutorial will give a comprehensive overview of recent theoretical results on swarm intelligence algorithms, with an emphasis on their efficiency (runtime/computational complexity). In particular, the tutorial will show how techniques for the analysis of evolutionary algorithms can be used to analyze swarm intelligence algorithms and how the performance of swarm intelligence algorithms compares to that of evolutionary algorithms.   The results shed light on the working principles of swarm intelligence algorithms, identify the impact of parameters and other design choices on performance, and thus help to use swarm intelligence more effectively.   The tutorial will be divided into a first, larger part on ACO and a second, smaller part on PSO. For ACO we will consider simple variants of the MAX-MIN ant system. Investigations of example functions in pseudo-Boolean optimization demonstrate that the choices of the pheromone update strategy and the evaporation rate have a drastic impact on the running time. We further consider the performance of ACO on illustrative problems from combinatorial optimization: constructing minimum spanning trees, solving shortest path problems with and without noise, and finding short tours for the TSP.   For particle swarm optimization, the tutorial will cover results on PSO for pseudo-Boolean optimization as well as a discussion of theoretical results in continuous spaces."
2134132,14018,21102,Automatic scene recognition for low-resource devices using evolving classifiers,2011,"In this paper an original approach is proposed which makes possible autonomous scenes recognition performed on-line by an evolving self-learning classifier. Existing approaches for scene recognition are off-line and used in intelligent albums for picture categorization/selection. The emergence of powerful mobile platforms with camera on board and sensor-based autonomous (robotic) systems is pushing forward the requirement for efficient self-learning and adaptive/evolving algorithms. Fast real-time and online algorithms for categorisation of the real world environment based on live video stream are essential for understanding and situation awareness as well as for localization and context awareness. In scene analysis the critical problem is feature extraction mechanism for a quick description of the scene. In this paper we apply a well known technique called spatial envelop or GIST. Visual scenes can be quite different but very often they can be grouped in similar types/categories. For example, pictures from different cities across the Globe, e.g. Tokyo, Vancouver, New York, Moscow, Dusseldorf, etc. bear the similar pattern of an urban scene — high rise buildings, despite the differences in the architectural style. Same applies for the beaches of Miami, Maldives, Varna, Costa del Sol, etc. One assumption based on which such automatic video classifiers can be build is to pre-train them using a large number of such images from different groups. Variety of possible scenes suggests the limitations of such an approach. Therefore, we use in this paper the recently propose evolving fuzzy rule-based classifier, simpl_eClass, which is self-learning and thus updates its rules and categories descriptions with each new image. In addition, it is fully recursive, computationally efficient and yet linguistically transparent."
1439652,14018,9704,Upload any object and evolve it: Injecting complex geometric patterns into CPPNS for further evolution,2013,"Ongoing, rapid advances in three-dimensional (3D) printing technology are making it inexpensive for lay people to manufacture 3D objects. However, the lack of tools to help nontechnical users design interesting, complex objects represents a significant barrier preventing the public from benefitting from 3D printers. Previous work has shown that an evolutionary algorithm with a generative encoding based on developmental biology-a compositional pattern-producing network (CPPN)-can automate the design of interesting 3D shapes, but users collectively had to start each act of creation from a random object, making it difficult to evolve preconceived target shapes. In this paper, we describe how to modify that algorithm to allow the further evolution of any uploaded shape. The technical insight is to inject the distance to the surface of the object as an input to the CPPN. We show that this seeded-CPPN technique reproduces the original shape to an arbitrary resolution, yet enables morphing the shape in interesting, complex ways. This technology also raises the possibility of two new, important types of science: (1) It could work equally well for CPPN-encoded neural networks, meaning neural wiring diagrams from nature, such as the mouse or human connectome, could be injected into a neural network and further evolved via the CPPN encoding. (2) The technique could be generalized to recreate any CPPN phenotype, but substituting a flat CPPN representation for the rich, originally evolved one. Any evolvability extant in the original CPPN genome can be assessed by comparing the two, a project we take first steps toward in this paper. Overall, this paper introduces a method that will enable non-technical users to modify complex, existing 3D shapes and opens new types of scientific inquiry that can catalyze research on bio-inspired artificial intelligence and the evolvability benefits of generative encodings."
966314,14018,9080,Theory of randomized search heuristics in combinatorial optimization,2011,"The rigorous mathematical analysis of randomized search heuristics(RSHs) with respect to their expected runtime is a growing research area where many results have been obtained in recent years. This class of heuristics includes well-known approaches such as Randomized Local Search (RLS), the Metropolis Algorithm (MA), Simulated Annealing (SA), and Evolutionary Algorithms (EAs) as well as more recent approaches such as Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO). Such heuristics are often applied to problems whose structure is not known or if there are not enough resources such as time, money, or knowledge to obtain good specific algorithms. It is widely acknowledged that a solid mathematical foundation for such heuristics is needed.   Most designers of RSHs, however, rather focused on mimicking processes in nature (such as evolution) rather than making the heuristics amenable to a mathematical analysis. This is different to the classical design of (randomized) algorithms which are developed with their theoretical analysis of runtime (and proof of correctness) in mind. Despite these obstacles, research from the last about 15 years has shown how to apply the methods for the probabilistic analysis of randomized algorithms to RSHs. Mostly, the expected runtime of RSHs on selected problems is analzyed. Thereby, we understand why and when RSHs are efficient optimizers and, conversely, when they cannot be efficient.   The tutorial will give an overview on the analysis of RSHs for solving combinatorial optimization problems. Starting from the first toy examples such as the OneMax function, we approach more realistic problems and arrive at analysis of the runtime and approximation quality of RSHs even for NP-hard problems. Our studies treat not only simple RLS algorithms and SA but also more complex population-based EAs. The combinatorial optimization problems that we discuss include the maximum matching problem, the partition problem and, in particular, the minimum spanning tree problem as an example where Simulated Annealing beats the Metropolis algorithm in combinatorial optimization. Important concepts of the analyses will be described as well."
2531373,14018,21102,Predicting laboratory testing in intensive care using fuzzy and neural modeling,2011,"Laboratory testing is a frequent activity for patients in intensive care units (ICU). Recent studies demonstrate that frequent laboratory testing does not necessarily relate to better outcomes. We hypothesize that unnecessary laboratory testing can be reduced by predicting which tests are unlikely to influence clinical management. Reducing unnecessary tests could reduce morbidity and hospitalization costs. We analyzed an ICU database containing 26,665 patient records at Beth Israel Deaconess Medical Center, Boston, and selected a subset of patients with gastrointestinal bleeding. Database knowledge discovery was applied involving data preprocessing, feature selection, and classification. Conventional soft computing tools such as fuzzy models and neural networks were utilized in this work, combined with statistical and mathematical tools. The input variables included bedside monitor trends, lab tests, arterial/central catheter information, urine collections, transfusions, indexes and scores calculated for the patients. The outcome variable was a binary classification based on falling levels of hematocrit. Feature selection was performed by a bottom-up strategy, maximizing the area under the ROC curve (AUC), the integrated discrimination improvement (IDI) and a multiobjective function primarily pondering the sensitivity of the models. A leave-one-out cross validation process was used to evaluate the overall models' performance, as well as the additional predictive value of the variables selected. Urine output was selected by all models as the best predictor of useful hematocrit testing. Our results show that it is possible to correctly classify the usefulness of a hematocrit lab test up to 81% of the time by using fuzzy models and neural networks."
901890,14018,23735,Integrated control method for power-assisted rehabilitation: Ellipsoid regression and impedance control,2014,"This paper proposes an integrated control method including learning with an ellipsoid function and impedance controller for rehabilitation using power-assisted robotic de- vices. The proposed controller consists of two parts of a primary algorithm, which are ellipsoid regression method for re-designing trajectory and impedance controller with pseudo mass/inertia. The ellipsoid regression method generates refer- ence impedance profiles though acquiring motion and force trajectories during rehabilitation tasks assisted by therapists. The assisted force is controlled by impedance controller during execution of rehabilitation task using a concept of pseudo mass/inertia. The proposed method offers the power-assisted rehabilitation as guided by therapist, without consistent help from the therapist or other assisters. The proposed control method is validated by experiments throughout a 2-DOF reha- bilitation robot, KULEX-2DOF(KIST Upper Limb Exoskeleton - 2DOF). I. INTRODUCTION The importance of rehabilitation robots, one of the repre- sentative applications of interactive robots, has been growing. After stroke or injuring upper limbs, the patients requires a consistent rehabilitation for recovering functions of the upper limb. Especially, approximately 50% of stroke suffers requires special care and rehabilitation as soon and regular as possible after the stroke (1). Additionally, repetitive training or implementing remedial motion are helpful to improve the performance of rehabilitation in motor system of suffers (2). As conventional researches have reported that the repetitive practice of movements and specific functional activities with external assisting force can be effective for stroke suffers (3), robot-assisted rehabilitation becomes more important and remarkable. In this sense, the robotic system can contribute to reha- bilitation for not only stroke suffers but also elderly and disabled by assisting behavior. As shown in Fig. 1. (a), the therapist or the physiatrist are demanded to guide a remedial exercise of suffers with proper therapeutic program considering physical condition of them. It means that a self-rehabilitation can be impossible or inefficient when the therapist is absent. However, the rehabilitation robot can help rehabilitation while the therapist is absent as Fig. 1. (b). For safe and effective rehabilitation, it is an interesting issue to learn and to reproduce the therapeutic behavior with training motion and assisting power for rehabilitation, simultaneously. The power-assisted exoskeleton robot is able to be used to improve performance of daily tasks for the old, infirm and"
1343160,14018,8806,Flexible cooperation in parallel local search,2014,"Constraint-Based Local Search (CBLS) consist in using Local Search methods [4] for solving Constraint Satisfaction Problems (CSP). In order to further improve the performance of Local Search, one possible option is to take advantage of the increasing availability of parallel computational resources. Parallel implementation of local search meta-heuristics has been studied since the early 90's, when multiprocessor machines started to become widely available, see [6]. One usually distinguishes between single-walk and multiple-walk methods: Single-walk methods consist in using parallelism inside a single search process, e.g. for parallelizing the exploration of the neighborhood, while multiple-walk methods (also called multi-start methods) consist in developing concurrent explorations of the search space, either independently (IW) or cooperatively (CW) with some communication between concurrent processes. Although good results can be achieved just with IW [1], a more sophisticated paradigm featuring  cooperation  between independent walks should bring better performance. We thus propose a general framework for cooperative search, which defines a flexible and parametric strategy based on the cooperative multi-walk (CW) scheme. The framework is oriented towards distributed architectures based on clusters of nodes, with the notion of teams running on nodes which group several individual search engines (e.g. multicore nodes). The idea is that teams are distributed and thus have limited  inter-node  communication. This framework allows the programmer to define aspects such as the degree of  intensification  and  diversification  present in the parallel search process. A good trade-off is essential to reach high performance. A preliminary implementation of the general CW framework has been done in the X10 programming language [5], and performance evaluation over a set of well-known benchmark CSPs shows that CW consistently outperforms IW."
1764024,14018,9080,Geometry of evolutionary algorithms,2011,"The various flavors of Evolutionary Algorithms look very similar when cleared of algorithmically irrelevant differences such as domain of application and phenotype interpretation. Representation-independent algorithmic characteristics like the selection scheme can be freely exchanged between algorithms. Ultimately, the origin of the differences of the various flavors of Evolutionary Algorithms is rooted in the solution representation and relative genetic operators. Are these differences only superficial? Is there a deeper unity encompassing all Evolutionary Algorithms beyond the specific representation? Is a general mathematical framework unifying search operators for all solution representations at all possible? The aim of the tutorial is to introduce a formal, but intuitive, unified point of view on Evolutionary Algorithms across representations based on geometric ideas, which provides a possible answer to the above questions. It also presents the benefits for both theory and practice brought by this novel perspective.   The key idea behind the geometric framework is that search operators have a dual nature. The same search operator can be defined (i) on the underlying solution representations and, equivalently, (ii) on the structure of the search space by means of simple geometric shapes, like balls and segments. These shapes are used to delimit the region of space that includes all possible offspring with respect to the location of their parents. The geometric definition of a search operator is of interest because it can be applied - unchanged - to different search spaces associated with different representations. This, in effect, allows us to define exactly the same search operator across representations in a rigorous way.   The geometric view on search operators has a number of interesting consequences of which this tutorial will give a comprehensive overview. These include (i) a straightforward view on the fitness landscape seen by recombination operators, (ii) a formal unification of many pre-existing search operators across representations, (iii) a principled way of designing crossover operators for new representations, (iv) a principled way of generalizing search algorithms from continuous to combinatorial spaces, and (v) the potential for a unified theory of evolutionary algorithms across representations."
1187622,14018,9080,Evolution of communication and cooperation,2014,"In the wild, spotted hyenas have been observed to chase lions away from a recent kill. This is a high risk, high reward behavior that requires significant teamwork and decision making skills. Modeling this behavior and creating algorithms that can improve evolutionarily may lead to more adaptable artificial systems for robotics and other cooperative artificial agents. Previous research has shown that having a lead or flag bearer hyena can significantly improve evolution. Thus, the complex social dynamics and coordination abilities required for this problem make it interesting artificial intelligence task. This also suggests that the type and encoding of the sensory inputs has a significant effect on the evolutionary trajectory and overall success at the task. Additionally, in the wild genetic diversity is driven by the migration of young males between packs, which leads to interesting evolutionary questions. To address the role of input encodings we introduce two evolutionary neural network variants, one using absolute headings as inputs/outputs and one using relative headings as inputs/outputs (headings defined relative to environmental elements). Our results show that the networks with relative inputs and outputs evolve significantly faster and result in better performance, suggesting that a critical difference is the existence of easily accessible, problem relevant, references for defining movement vectors. Our results also show that the inclusion of a leader in the team structure can improve the rate at which cooperative behaviors are evolved, but does not lead to better overall behaviors. In addition, we examine the emerging behaviors as the teams go from random behavior to a circling pattern to an aggressive charge towards the goal."
694815,14018,9080,Summary of the evolutionary origins of modularity,2013,"A long-standing, open question in biology is how populations are capable of rapidly adapting to novel environments, a trait called evolvability. A major contributor to evolvability is the fact that many biological entities are modular, especially the many biological processes and structures that can be modeled as networks, such as metabolic pathways, gene regulation, protein interactions, and animal brains. Networks are modular if they contain highly connected clusters of nodes that are sparsely connected to nodes in other clusters [4, 2]. Despite its importance and decades of research, there is no agreement on why modularity evolves [4]. Intuitively, modular systems seem more adaptable, a lesson well-known to human engineers, because it is easier to rewire a modular network with functional subunits than an entangled, monolithic network [1]. However, because this evolvability only provides a selective advantage over the long-term, such selection is at best indirect and may not be strong enough to explain the level of modularity in the natural world [4].   Modularity is likely caused by multiple forces acting to various degrees in different contexts [4], and a comprehensive understanding of the evolutionary origins of modularity involves identifying those multiple forces and their relative contributions. The leading hypothesis is that modularity mainly emerges due to rapidly changing environments that have common subproblems, but different overall problems [1]. It is unknown how much natural modularity MVG can explain, however, because it unclear if biological environments change modularly, and whether they change at a high enough frequency for this force to play a significant role.   We investigate an alternate hypothesis that has been suggested, but heretofore untested, which is that modularity evolves not because it conveys evolvability, but as a byproduct from selection to reduce connection costs in a network [3]."
1352746,14018,8806,Integrated PSO and line based representation approach for SLAM,2011,"This paper presents a novel method for integrating swarm intelligence and line-based representation of environment to solve the simultaneous localization and mapping (SLAM) problem of mobile robots. SLAM is a well-studied problem in mobile robotics. Because of stochastic nature of search strategy in swarm intelligence algorithms, they are very successful compared with other techniques in encountering SLAM problem. Line segment based representation of 2D maps is known to have advantages over raw point data or grid based representation gained from laser range scans. It contains higher geometric information that is closer to human insight and conceptual mapping, which is necessary for robust post processing. It also significantly reduces the memory and time complexity. Mobile robot reads raw laser sensor data in each step of its trajectory and converts it to a set of lines which is used to produce the last sensed map. At the next phase, the algorithm utilizes particle swarm optimization (PSO) and introduces a new evaluation function to find the actual state of the last sensed map inside a global map, which is merged into a global map by introducing a new merge method to reconstruct the global map. We use PSO's ability to run away from local extrema and converge towards an optimum point (i.e. best robot status in the map) by utilizing adaptive inertia weight strategy. We also introduce a new criterion to measure the similarity between the line pairs in the map. The experimental results on real datasets and virtual environments exhibit the algorithm's robustness, accuracy and superior performance on problems that are under consideration in SLAM such as loop closing, correspondence problem, curvature of the walls, and sensor uncertainty."
998683,14018,9080,Lessons from the black-box: fast crossover-based genetic algorithms,2013,"The recently active research area of black-box complexity revealed that for many optimization problems the best possible black-box optimization algorithm is significantly faster than all known evolutionary approaches. While it is not to be expected that a general-purpose heuristic competes with a problem-tailored algorithm, it still makes sense to look for the reasons for this discrepancy.   In this work, we exhibit one possible reason---most optimal black-box algorithms profit also from solutions that are inferior to the previous-best one, whereas evolutionary approaches guided by the survival of the fittest paradigm often ignore such solutions. Trying to overcome this shortcoming, we design a simple genetic algorithm that first creates λ offspring from a single parent by mutation with a mutation probability that is  k  times larger than the usual one. From the best of these offspring (which often is worse than the parent) and the parent itself, we generate further offspring via a uniform crossover operator that takes bits from the winner offspring with probability 1/ k  only.   A rigorous runtime analysis proves that our new algorithm for suitable parameter choices on the OneMax test function class is asymptotically faster (in terms of the number of fitness evaluations) than what has been shown for μ +, λ EAs. This is the first time that crossover is shown to give an advantage for the OneMax class that is larger than a constant factor. Using a fitness-dependent choice of  k  and λ, the optimization time can be reduced further to linear in  n .   Our experimental analysis on several test function classes shows advantages already for small problem sizes and broad parameter ranges. Also, a simple self-adaptive choice of these parameters gives surprisingly good results."
1459920,14018,9704,A hard optimisation test function with symbolic solution visualisation for fast interpretation by the human eye,2013,"We propose a class of test problems for evaluating the performance of global function optimisers based on finding an optimal spatial distribution of nonidentical particles interacting with two different potential fields. Because of the possibility of intuitive solution visualisation it can be of particular benefit during development of optimisation algorithms. An ensemble of N particles is constrained to a low-dimensional space and each particle contributes in two ways to the total potential energy: by its position on a hilly track and through repulsive neighbour potentials. The task of minimising the ensemble's total potential energy corresponds to searching an N-dimensional space with many local minima separated through higher and lower barriers; hence, it can serve as a performance measure for evolutionary algorithms (EA). The search difficulty is scalable through the number of particles and the hilliness of the track. In particular, if the particles are made nonidentical by giving them different masses or charges, the search will become very challenging because of the introduced combinatorial aspect and the “curse of dimensionality”. Among many similarly challenging optimisation problems this test function class has the advantage that solution candidates can be plotted in ways which allow humans to estimate not only relative objective function values but also DNA vector relations upon a quick glance. For the EA developer this allows a fast feedback cycle between a modification to the EA and the observed change in optimisation history behaviour. This makes experimentation with EA elements at a fundamental level easier. Furthermore, this class of real-domain search offers a wide range of difficulty and complexity levels and can be split up into a two-objective optimisation."
2431734,14018,20561,OG-Miner: An Intelligent Health Tool for Achieving Millennium Development Goals (MDGs) in m-Health Environments,2011,"The latest statistics of WHO show that approxi- mately 500, 000 women die worldwide every year - the majority of them residing in developing countries - due to pregnancy related complications. The situation is so grave that UN has set a target of reducing Maternal Mortality Rate (MMR) by 75% till the year 2015 in its millennium development goals (MDGs). Therefore, the current focus of health care researchers is to advocate the use of e-health technology in developing countries that have the capability: (1) to remotely monitor patients in their homes by semiskilled health professionals, and (2) to use data mining techniques to raise alarms about high risk patients. In this paper, we develop an intelligent health tool - Obstetrics and Gynaecology (OG) OG-Miner - that presents a novel combination of data mining techniques for accurate and effective classification of high risk pregnant women. The scheme classifies four major risk factors of mortality - hypertension, hemorrhage, septicemia and obstructed labor - in a reliable, autonomous and accurate fashion. We have collected a real world data of more than 1200 patients from tertiary care hospitals and rural areas. Our tool achieves more than 98% accuracy on the collected OG dataset. Moreover, our evaluations of OG-Miner on eight other medical datasets show that its learning paradigm can be generalized to other domains as well. Last but not least, we are using OG-Miner as an integral component of a health value chain in our m-health project to autonomously filter a significant number of low risk patients in rural areas; as a result, only high risk patients are referred to specialized obstetrician in tertiary care hospitals. As a consequence, the reduced workload enables them to provide quality care to the patients."
1384344,14018,9080,Short term wind speed forecasting with evolved neural networks,2013,"Concerns about climate change, energy security and the volatility of the price of fossil fuels has led to an increased demand for renewable energy. With wind turbines being one of the most mature renewable energy technologies available, the global use of wind power has been growing at over 20% annually, with further adoption to be expected. As a result of the inherent variability of the wind in combination with the increased uptake, demand for accurate wind forecasting, over a wide range of time scales has also increased.   We report early work as part of the EU FP7 project 'ORIGIN', which will exploit wind speed forecasting, and implement and evaluate smart-meter based energy management in 300 households in three ecovillages across Europe. The ORIGIN system will capitalise on automated weatherstation data (available cheaply) to inform predictions of the wind-turbine generated power that may be available in short term future time windows. Accurate and reliable wind-speed forecasting is essential in this enterprise.   A range of different methods for wind forecasting have been developed, ranging from relatively simple time series analysis to the use of a combination of global weather forecasting, computational fluid dynamics and machine learning methods. Here we focus on the application of neural networks, without (for the time being) the use of numerical weather predictions or expensive physical modelling methods. While work of this nature has been performed before, using past wind speeds to make predictions into the future, here we explore the use of additional recent meteorological data to improve on short-term forecasting. Specifically, we employ evolved networks and explore many configurations to assess the merits of using additional features such as cloud cover, temperature and pressure, to predict future wind speed."
2046958,14018,9080,"No entailing laws, but enablement in the evolution of the biosphere",2012,"Biological evolution is a complex blend of ever changing structural stability, variability and emergence of new phenotypes, niches, ecosystems. We wish to argue that the evolution of life marks the end of a physics world view of law entailed dynamics. Our considerations depend upon discussing the variability of the very contexts of life: the interactions between organisms, biological niches and ecosystems. These are ever changing, intrinsically indeterminate and even unprestatable: we do not know ahead of time the niches which constitute the boundary conditions on selection. More generally, by the mathematical unprestatability of the phase space (space of possibilities), no laws of motion can be formulated for evolution. We call this radical emergence, from life to life. The purpose of this paper is the integration of variation and diversity in a sound conceptual frame and situate unpredictability at a novel theoretical level, that of the very phase space. Our argument will be carried on in close comparisons with physics and the mathematical constructions of phase spaces in that discipline. The role of (theoretical) symmetries as invariant preserving transformations will allow us to understand the nature of physical phase spaces and to stress the differences required for a sound biological theoretizing. In this frame, we discuss the novel notion of ''enablement. Life lives in a web of enablement and radical emergence. This will restrict causal analyses to differential cases (a difference that causes a difference). Mutations or other causal differences will allow us to stress that ''non conservation principles are at the core of evolution, in contrast to physical dynamics, largely based on conservation principles as symmetries. Critical transitions, the main locus of symmetry changes in physics, will be discussed, and lead to ''extended criticality as a conceptual frame for a better understanding of the living state of matter."
141193,14018,422,Darwin or Lamarck? Future Challenges in Evolutionary Algorithms for Knowledge Discovery and Data Mining,2014,"Evolutionary Algorithms (EAs) are a fascinating branch of computational intelligence with much potential for use in many application areas. The fundamental principle of EAs is to use ideas inspired by the biological mechanisms observed in nature, such as selection and genetic changes, to find the best solution for a given optimization problem. Generally, EAs use iterative processes, by growing a population of solutions selected in a guided random search and using parallel processing, in order to achieve a desired result. Such population based approaches, for example particle swarm and ant colony optimization (inspired from biology), are among the most popular metaheuristic methods being used in machine learning, along with others such as the simulated annealing (inspired from thermodynamics). In this paper, we provide a short survey on the state-of-the-art of EAs, beginning with some background on the theory of evolution and contrasting the original ideas of Darwin and Lamarck; we then continue with a discussion on the analogy between biological and computational sciences, and briefly describe some fundamentals of EAs, including the Genetic Algorithms, Genetic Programming, Evolution Strategies, Swarm Intelligence Algorithms (i.e., Particle Swarm Optimization, Ant Colony Optimization, Bacteria Foraging Algorithms, Bees Algorithm, Invasive Weed Optimization), Memetic Search, Differential Evolution Search, Artificial Immune Systems, Gravitational Search Algorithm, Intelligent Water Drops Algorithm. We conclude with a short description of the usefulness of EAs for Knowledge Discovery and Data Mining tasks and present some open problems and challenges to further stimulate research."
1769514,14018,9704,An improved multi-objective optimization algorithm based on fuzzy dominance for risk minimization in biometric sensor network,2012,"Biometric system is very important for recognition in several security areas. In this paper we deal in designing biometric sensor manager by optimizing the risk. Risk is modeled as a multi-objective optimization with Global False Acceptance Rate and Global False Rejection Rate as two objectives. In practice, multiple biometric sensors are used and the decision is taken locally at each sensor and the data is passed to the sensor manager. At the sensor manager the data is fused using a fusion rule and the final decision is taken. The optimization involves designing the data fusion rule and setting the sensor thresholds. We have implemented a recent fuzzy dominance based decomposition technique for multi-objective optimization called MOEA/DFD and have compared its performance on other contemporary state-of-arts in multi-objective optimization field like MOEA/D, NSGAII. The algorithm introduces a fuzzy Pareto dominance concept to compare two solutions and uses the scalar decomposition method only when one of the solutions fails to dominate the other in terms of a fuzzy dominance level. We have simulated the algorithms on different number of sensor setups consisting of 3, 6, 8 sensors respectively. We have also varied the apriori probability of imposter from 0.1 to 0.9 to verify the performance of the system with varying threat. One of the most significant advantages of using multi-objective optimization is that with a single run just by changing the decision making logic applied to the obtained Pareto front one can find the required threshold and decision strategies for varying threat of imposter. But with single objective optimization one need to run the algorithms each time with change in threat of imposter. Thus multi-objective representation appears to be more useful and better than single objective one. In all the test instances MOEA/DFD performs better than all other algorithms."
1199592,14018,9080,Medical applications of evolutionary computation,2014,"The application of genetic and evolutionary computation to problems in medicine has increased rapidly over the past five years, but there are specific issues and challenges that distinguish it from other real-world applications. Obtaining reliable and coherent patient data, establishing the clinical need and demonstrating value in the results obtained are all aspects that require careful and detailed consideration.   This tutorial is based on research which uses genetic programming (a representation of Cartesian Genetic Programming) in the diagnosis and monitoring of Parkinson's disease, Alzheimer's disease and other neurodegenerative conditions, as well as in the early detection of breast cancer through automated assessment of mammograms. The work is supported by multiple clinical studies in progress in the UK (Leeds General Infirmary), USA (UCSF), UAE (Dubai Rashid Hospital), Australia (Monash Medical Center) and Singapore (National Neuroscience Institute). The technology is protected through three patent applications and a University spin-out company marketing four medical devices.   The tutorial considers the following topics:   Introduction to medical applications of genetic and evolutionary computation and how these differ from other real-world applications   Overview of past work in the from a medical and evolutionary computation point of view   Three case examples of medical applications: i. diagnosis and monitoring of Parkinson's disease ii. detection of beast cancer from mammograms iii. cancer screening using Raman spectroscopy   Practical advice on how to get started working on medical applications, including existing medical databases and conducting new medical studies, commercialization and protecting intellectual property.   Summary, further reading and links"
2255867,14018,9704,Formation and activation of feature hierarchies under reinforcement,2011,"Representation of knowledge through a hierarchy of re-used elements, and the discovery of intermediate terms for learning, is an area of increasing interest in artificial learning. Such a hierarchy is a recognised aspect of human visual processing and has an important role in recognition of objects. A hierarchy allows efficiency of representation, and a manner of preserving links between related concepts. The use of such an approach in an artificial system requires addressing processes for discovery of features, and for activation of features according to an observation. Learning Classifier Systems provide a means of developing a population of rules relevant to a task according to reinforcement, capturing features of the problem in a population of rules. Implementation of a hierarchical representation to define rules is examined using the Activation-Reinforcement Classifier System, acting in a game environment. Two methods of activation of fragments are examined, one using a parallel activation method allowing multiple interpretations to be active in tandem, the other based on attention to a single higher level concept at once, using a limited working memory. Attention to a high level rule provides a bias on the low level features to be activated. Trials show the system operates successfully on the game of Dots and Boxes with a large game size, and is able to extract relevant features of the game using a body of 4000 autonomously produced features. The attention-based activation method operates with a reduced memory requirement and faster processing time than the parallel method. The network of features produced shows a scale-free connectivity distribution, a common property of many human semantic networks."
1704311,14018,9704,A comparison on the search of particle swarm optimization and differential evolution on multi-objective optimization,2011,"Particle swarm optimization (PSO) and differential evolution (DE) are meta-heuristics which have been found to be successful in a wide variety of optimization tasks. The high speed of convergence and the relative simplicity of PSO make it a highly viable candidate to be used in multi-objective optimization problems (MOPs). Therefore, several PSO approaches capable to handle MOPs (MOPSOs) have appeared in the past. There are some problems, however, where PSO-based algorithms have shown a premature convergence. On the other hand, multi- objective DEs (MODE) have shown lower speed of convergence than MOPSOs but they have been successfully used in problems where MOPSO have mistakenly converged. In this work, we have developed experiments to observe the convergence behavior, the online convergence, and the diversity of solutions of both meta-heuristics in order to have a better understanding about how particles and solutions move in the search space. To this end, MOPSO and MODE algorithms under (to our best effort) similar conditions were used. Moreover, the ZDT test suite was used on all experiments since it allows to observe Pareto fronts in two-dimensional scatter plots (more details on this are presented on the experiments section). Based on the observations found, modifications to two PSO-based algorithms from the state of the art were proposed resulting in a rise on their performance. It is concluded that MOPSO presents a poor distributed scheme that leads to a more aggressive search. This aggressiveness showed to be detrimental for the selected problems. On the other hand, MODE seemed to generate better distributed points on both decision and objective space allowing it to produce better results."
1177402,14018,9704,Use of evolutionary computation techniques for exploration and prediction of helicopter loads,2012,"The development of accurate load spectra for helicopters is necessary for life cycle management and life extension efforts. This paper explores continued efforts to utilize evolutionary computation (EC) methods and machine learning techniques to estimate several helicopter dynamic loads. Estimates for the main rotor normal bending (MRNBX) on the Australian Black Hawk helicopter were generated from an input set that included thirty standard flight state and control system parameters under several flight conditions (full speed forward level flight, rolling left pullout at 1.5g, and steady 45° left turn at full speed). Multi-objective genetic algorithms (MOGA) used in combination with the Gamma test found reduced subsets of predictor variables with modeling potential. These subsets were used to estimate MRNBX using Cartesian genetic programming and neural network models trained by deterministic and evolutionary computation techniques, including particle swarm optimization (PSO), differential evolution (DE), and MOGA. PSO and DE were used alone or in combination with deterministic methods. Different error measures were explored including a fuzzy-based asymmetric error function. EC techniques played an important role in both the exploratory and modeling phase of the investigation. The results of this work show that the addition of EC techniques in the modeling stage generated more accurate and correlated models than could be obtained using only deterministic optimization."
1427153,14018,9704,Standard Particle Swarm Optimisation 2011 at CEC-2013: A baseline for future PSO improvements,2013,"In this work we benchmark, for the first time, the latest Standard Particle Swarm Optimisation algorithm (SPSO-2011) against the 28 test functions designed for the Special Session on Real-Parameter Single Objective Optimisation at CEC-2013. SPSO-2011 is a major improvement over previous PSO versions, with an adaptive random topology and rotational invariance constituting the main advancements. Results showed an outstanding performance of SPSO-2011 for the family of unimodal and separable test functions, with a fast convergence to the global optimum, while good performance was observed for four rotated multimodal functions. Conversely, SPSO-2011 showed the weakest performance for all composition problems (i.e. highly complex functions specially designed for this competition) and certain multimodal test functions. In general, a fast convergence towards the region of the global optimum was achieved, requiring less than 10E+03 function evaluations. However, for most composition and multimodal functions SPSO2011 showed a limited capability to “escape” from sub-optimal regions. Despite this limitation, a desirable feature of SPSO-2011 was its scalable behaviour, which observed up to 50-dimensional problems, i.e. keeping a similar performance across dimensions with no need for increasing the population size. Therefore, it seems advisable that future PSO improvements be focused on enhancing the algorithm's ability to solve non-separable and asymmetrical functions, with a large number of local minima and a second global minimum located far from the true optimum. This work is the first effort towards providing a baseline for a fair comparison of future PSO improvements."
1923963,14018,9704,Two encoding schemes for a multi-objective Cutting Stock Problem,2011,"This work presents a multi-objective approach to solve a Constrained Guillotine Two-Dimensional Cutting Stock Problem. The single-objective formulation of the problem has been widely studied in the related literature, so a large number of heuristics, meta-heuristics, and exact algorithms have been proposed in order to optimise the total profit obtainable from the available surface. However, in some industries, where the material is cheap enough or easily recycled, a faster generation of pieces and a minimum usage of the machinery could be more decisive aspects in determining the efficiency of the production process. For this reason, we have focused on a multi-objective formulation of the problem which seeks to maximise the total profit obtainable from the raw material, as well as minimise the number of cuts to achieve the pieces placed on the material. To solve this multi-objective problem we have applied Multi-objective Optimisation Evolutionary Algorithms given its great effectiveness with other types of real-world multi-objective problems. For the application of this kind of algorithms it has been necessary to define an encoding scheme which allows to deal with the problem intrinsic features. In this case, we have defined two encoding schemes which are based on a post-fix notation, thus simplifying the representation of guillotine patterns. The first encoding scheme controls the pieces included in the solution in order to generate valid builds. The second one generates a full solution, including all the available pieces, although the final values for the objectives are limited by the available surface. The computational results demonstrate that, in both cases, the multi-objective approach provides solutions with good compromise between the two objectives."
1878733,14018,9704,Co-evolving data driven models and test data sets with the application to forecast chaotic time series,2011,"Several approaches have been introduced for modeling and prediction of nonlinear dynamics which have chaotic characteristics. Among these methods, data driven approaches such as Auto Regressive (AR) models, Nonlinear Auto Regressive (NAR) models, Radial Basis Function (RBF) networks, and Multi Layered Perceptron (MLP) neural networks have proven themselves to be powerful approaches in modeling and prediction of chaotic dynamics. However, the structure of these models should be known before the training phase, which is a very complicated problem. In this research, we introduce a co-evolutionary approach for modeling and system identification of chaotic dynamics. The proposed algorithm is composed of two co-evolving populations: candidate data driven models, and test data sets which either extract new information from the nonlinear chaotic system or elicit desirable behavior from it. The fitness of candidate models is their ability to explain behavior of the target chaotic system observed in response to tests carried out so far by predicting the future values of these data sets; the fitness of candidate test data sets is their ability to make the models disagree in their predictions. To check the performance of this algorithm, three case studies are considered. First, we apply this method to approximate a static function which has complicated behavior near zero. Then, we use this algorithm to predict two bench mark time series in chaos literature: Sunspot Number (SSN) and Mackey-Glass (MG) time series. Simulation results depict the power of proposed method in modeling and predicting complicated nonlinear systems."
1000285,14018,9080,Evolutionary visual exploration: experimental analysis of algorithm behaviour,2013,"Recent publications in the domains of interactive evolutionary computation and data visualisation consider an emerging topic coined  Evolutionary Visual Exploration (EVE) . EVE systems combine visual analytics with stochastic optimisation to aid the exploration of complex, multidimensional datasets. In this work we present an experimental analysis of the behaviour of an EVE system that is dedicated to the visualisation of multidimensional datasets, which are generally characterised by a large number of possible views or projections. EvoGraphDice is an interactive evolutionary system that progressively evolves a small set of new dimensions, to provide new viewpoints on the dataset, in the form of linear and non-linear combinations of the original dimensions. The criteria for evolving new dimensions are not known a priori and are partially specified by the user via an interactive interface: (i) The user selects views with meaningful or interesting visual patterns and provides a satisfaction score. (ii) The system calibrates a fitness function to take into account the user input, and then calculates new views, with the help of an evolutionary engine. In previous work (an observational study), we showed that EvoGraphDice was able to facilitate exploration tasks, helping users to discover new interesting views and relationships in their data. Here, we focus on the system's convergence behavior, conducting an experiment with users who have a precise task to perform. The experimental task is set up as a geometrical game, and collected data show that EvoGraphDice is able to learn user preferences in a way that helps users fulfill their task (i.e. converge to desired solutions)."
1477908,14018,21102,A Type-2 Fuzzy Logic based system for linguistic summarization of video monitoring in indoor intelligent environments,2014,"Video monitoring can provide vital context awareness information from indoor intelligent environments where privacy is not a limitation. However, there is a need to develop linguistic summarization tools which are capable of summarizing in a layman language the information of interest within long video sequences. The key module which can enable the linguistic summarization of video monitoring is human activity/behaviour recognition. However, human behavior recognition is an important yet challenging task due to the behavior uncertainty, activity ambiguity, and uncertain factors such as position, orientation and speed, etc. In order to handle such high levels of uncertainties in activity analysis, we introduce a system based on Interval Type-2 Fuzzy Logic Systems (IT2FLSs) whose parameters are optimized by the Big Bang-Big Crunch (BB-BC) algorithm which allows for robust behaviour recognition using 3D machine vision techniques in intelligent environments. We present several experiments which were performed in real-world intelligent environments to fairly make comparisons with the state-of-the-art algorithms. The experimental results demonstrate that the proposed BB-BC paradigm is effective in tuning the parameters of the membership functions and the rule base of the IT2FLSs to improve the recognition accuracy. It will be shown through real-world experiments that the proposed IT2FLSs outperformed the Type-1 FLSs (TIFLSs) counterpart as well as other traditional non-fuzzy based systems. Based on the recognition results, higher-level applications will presented including video linguistic summarizations event searching and activity retrieval/playback."
926241,14018,21102,An interval type-2 fuzzy logic based system with user engagement feedback for customized knowledge delivery within intelligent E-learning platforms,2014,"Recent years have witnessed an expansion on realizing adaptive educational systems for intelligent E-learning platforms. Such platforms permit the development of customised learning contexts adapted to the requirements of every student by correlating the student characteristics with instructional variables. However, the vast majority of the existing adaptive educational systems do not learn from the users' behaviors to create white box models which could handle the linguistic uncertainties and could be easily read and analyzed by the lay user. Moreover, most of the existing systems ignore gauging the students' engagements levels and mapping them to suitable delivery needs which match the students' knowledge and preferred learning styles. This paper presents a novel interval type-2 fuzzy logic based system that can learn the users' preferred knowledge delivery needs and the preferred learning style based on the students' characteristics and engagement levels to generate a customized learning environment. The paper presents a novel system for gauging the students' engagement levels based on utilizing visual information to automatically calculate the engagement degree of students. This differs from traditional methods which usually employ expensive and invasive sensors. Our approach only uses a low-cost RGB-D video camera (Kinect, Microsoft) operating in a non-intrusive mode whereby the users are allowed to act and move without restrictions. The efficiency of the proposed system has been tested through various real-world experiments with the participation of 15 students. These experiments indicate the ability of the proposed type-2 fuzzy logic based system to handle the linguistic uncertainties to produce better performance in terms of improved learning and better user engagements when compared to type-1 based fuzzy systems and non-adaptive systems."
1385586,14018,23735,A novel method for capsule endoscopy video automatic segmentation,2013,"Wireless capsule endoscopy (WCE) is a recently developed revolutionary medical technology which records the video of human's digestive tract noninvasively. However, reviewing a WCE video is a tired and time-consuming task for clinicians. Thus, WCE video automatic segmentation methods are emerging to reduce the review time for clinicians. In our previous work, a two-level WCE video segmentation approach has been proposed, which provides a novel approach to localize the boundaries more exactly and efficiently. However, it has an unsatisfactory performance in the small intestine/large intestine boundary detection. In this paper, we propose new features and an improved classifier to improve the previous two-level segmentation algorithm. In the rough level, color feature is utilized to draw a dissimilarity curve and an approximate boundary has been obtained. At the same time, training data for fine level can be directly labeled and collected between the two approximate boundaries of organs to overcome the difficulty of training data acquisition. In the fine level, a novel color uniform local binary pattern (CULBP) algorithm is proposed, which includes two kinds of patterns, color norm patterns and color angle patterns. The CULBP feature is more robust to variation of illumination and more discriminative for classification. Moreover, in order to elevate the performance of SVM classifier we proposed the Ada-SVM classifier which using RBFSVMs as component of Adaboost classifier. At last, an analysis of classification results of the Ada-SVM classifier is carried out to segment the WCE video into several meaningful parts, stomach, small intestine and large intestine. The experiments demonstrate a promising performance of the proposed method. The average precision and recall are as high as 91.37% and 88.50% in stomach/small intestine classification, 90.35% and 97.28% in small intestine/ large intestine classification."
2469140,14018,9704,Using Social Networks for Exchanging Valuable Real Time Public Transport Information among Travellers,2011,"Public transport users are increasingly connected in real time through mobile devices to social networks, such as Twitter and Facebook. This allows them both to access and to provide valuable operational and emotional information from and to fellow travellers. Transport network management could benefit from this exchange, and also participate by providing rewards to valuable contributors. This paper introduces a model for such cooperative exchanges of information and proposes a valuation system for the information provided and obtained. Users and automatic systems (sensors) would provide information, such as punctuality, noise levels, and assessments of driver's skills, referenced to particular vehicles, routes and times. Then other users accessing such information would classify it on the level of correctness and usefulness, under a validation scheme operated by the transport network management. Such information could either be openly available or private in some degree within a social network, taking account of security aspects that need to be preserved. In a mature environment, more valuable information could only be made available via subscription or freely available to highly valued contributing users. The use of social networks would provide an easy way of sharing information and also provide a sense of community to the involved travellers. Transport network management benefiting from relevant information exchanges could reward users contributing with valuable data, as an incentive to enhance participation. In this context, the information exchanged would achieve a real transactional value and present a new electronic commerce paradigm. Overall, such exchange could also be seen as a serious game."
1794438,14018,9704,On the selection of surrogate models in evolutionary optimization algorithms,2011,"Since many real-world problems are related to the satisfaction of at least one goal, several optimization techniques have been proposed in the past. However, traditional optimization techniques are computationally expensive and are normally highly susceptible to some characteristics such as high dimensionality, non-differentiability, non-linearity, highly expensive function calculation, among others. Evolutionary algorithms are bio-inspired meta-heuristics that have shown flexibility, adaptability and good performance when solving these sort of problems. In order to achieve acceptable results, some problems usually require several evaluations of the optimization function. However, when each of these evaluations represents a high computational cost, these problems remain intractable even by these meta-heuristics. To reduce the computational cost in expensive optimization problems, some researchers have replaced the real optimization function with a computationally inexpensive surrogate model. Despite there are comparison studies among these techniques, these studies focused on revised the accuracy of the meta-model for the problem at hand, but neither its suitability to be used with evolutionary algorithms, nor its scalability in the variable design space. In this work, we compare four meta-modeling techniques, polynomial approximation, kriging, radial basis functions and support vector regression, in different aspects such as accuracy, robustness, efficiency, and scalability with the aim to identify advantages and disadvantages of each meta-modeling technique in order to select the most suitable one to be combined with evolutionary optimization algorithms."
1085782,14018,21102,Modeling socio-politico-economic systems with time-dependent fuzzy cognitive maps,2012,"Most of the formalisms of fuzzy cognitive maps (FCM) that had been proposed and applied in the past are relatively simple in the sense that they handle the various interacting and time-varying concepts as zeroth-order dynamical components. That is, they do not accommodate time delays and growth exponential time constants. In modeling real life systems, especially in the general domain of socio-politico-economics, the time delays are often important and need to be accounted. Also, the various sensitivities associating the system concepts may be time dependent. In the present work, a previously proposed FCM has been extended to include time delays, time constants and time-dependent sensitivities. The system has been applied to the Cyprus politico-economic dynamics, with emphasis on possible scenarios involving the finding and exploitation of oil/gas in the exclusive economic zone of Cyprus. The existence of natural gas - estimated at about 7 trillion cubic feet - has recently been preliminarily verified. In the interrelated dynamics, various important dynamical parameters have been taken into account, reflecting the interests of primarily the republic of Cyprus, as well as the interests of the Greek and Turkish Cypriot communities and other countries involved, such as Greece, Turkey, United Kingdom, USA, Russia, Israel and the European Union. The main parameters that had been considered in the interrelated dynamics are nationalism, religiousness, unemployment, external debt, oil extraction and the general interests of the countries involved and those of the two communities."
1215181,14018,9080,On the performance of evolutionary algorithms in biomedical keyword clustering,2011,"In the field of life sciences it often turns out to be a challenge to quickly find the desired information due to the huge amount of available data. The research area of information retrieval (IR) addresses this problem and tries to provide suitable solutions. One of the approaches used in IR is query extension based on keyword or document clusters.   In this paper we present a deep analysis of a keyword clustering approach using four different kinds of evolutionary algorithms, namely evolution strategy (ES), genetic algorithm (GA), genetic algorithm with strict offspring selection (OSGA), and the multi-objective elitist non-dominated sorting genetic algorithm (NSGA-II).   We have identified features that characterize solution candidates for the keyword clustering problem, e.g., the number of documents covered and how well the identified clusters of keywords match with the occurrence of keywords in the given set of documents. The use of these features and how evolutionary algorithms can be used to solve the optimization of keyword clusters is shown in this paper.   To test the here presented approach we used a real world data set provided within the TREC-9 conference; this data collection includes information about approximately 36,000 documents collected from the PubMed database.   In the results section we compare the performance of the here tested evolutionary algorithms and see that especially ES and NSGA-II produce meaningful results for this documents collection. This approach based on evolutionary algorithms shall be used further on in automated query extension for biomedical information retrieval in PubMed."
2296515,14018,9704,Evolutionary design of robust noise-specific image filters,2011,"Evolutionary design has shown as a powerful technique in solving various engineering problems. One of the areas in which this approach succeeds is digital image processing. Image filtering represents a wide topic in 2D signal processing. In this case different types of noise are considered in the filtering process to restore the image quality that has been decreased by changing values of some pixels in the image (e.g. due to the transmission through unreliable lines or in the process of acquiring the image). Impulse noise represents a basic type of non-linear noise typically affecting a single pixel in different regions of the image. In order to eliminate this type noise median filters have usually been applied. However, for higher noise intensity or wide range of the noise values this approach leads to corrupting non-noise pixels as well which results in images that are smudged or lose some details after the filtering process. Therefore, advanced filtering techniques have been developed including a concept of noise detection or iterative filtering algorithms. In case of the high noise intensity, a single filtering step is insufficient to eliminate the noise and obtain a reasonable quality of the filtered image. Therefore, iterative filters have been introduced. In this paper we apply an evolutionary algorithm combined with Cartesian Genetic Programing representation to design image filters for the impulse noise that are able to compete with some of the best conventionally used iterative filters. We consider the concept of noise detection to be designed together with the filter itself by means of the evolutionary algorithm. Finally, it will be shown that if the evolved filter is applied iteratively on the filtered image, a high-quality results can be obtained utilizing lower computational effort of the filtering process in comparison with the conventional iterative filters."
1230869,14018,8806,Generic cloud platform multi-objective optimization leveraging models@run.time,2014,"Cloud computing promises scalable hosting by offering an elastic management of virtual machines which run on top of hardware data centers. This elastic management as a cornerstone of PaaS (Platform As A Service) has to deal with trade-offs between conflicting requirements such as cost and quality of service. Solving such trade-offs is a challenging problem. Indeed, most of PaaS providers consider only one optimization axis or ad-hoc multi-objective resolution techniques using domain specific heuristics.   This paper aims at proposing a generic approach to build cloud optimization by combining modeling and search based paradigms. Our approach is two-fold: 1) To reason about a cloud environment, we use a Models@run.time approach to have an abstraction layer of a cloud configuration that supports monitoring capabilities and represents cloud intrinsic parameters like cost, load information, etc. 2) We use a search-based algorithm to navigate through cloud candidate configuration solutions in order to solve the  C loud  M ulti-objective  O ptimization  P roblem  (CMOP).    We validate our approach based on a case study that we define with our cloud provider partner EBRC as representative of a dynamic management problem of heterogeneous distributed cloud nodes. We implement a prototype of our PaaS supervision framework using Kevoree, a Models@run.time platform. The prototype shows the efficiency of our approach in terms of finding possible cloud configurations in reasonable time. The prototype is flexible since it enables an easy reconfiguration of the cloud customer optimization objectives."
2298641,14018,9704,Benefits of implicit redundant representation genetic algorithms for conceptual design and damage identification,2011,"Solving large-scale structural design and damage identification problems using genetic algorithm optimization methods requires the use of advanced representations. The flexible implicit redundant representation (IRR) provides significant benefits for inverse problems in which the solution involves determining the optimal number of design variables, in addition to their values. The IRR encodes both variables and redundant segments in each individual. The encoded variable locations and values dynamically change and self-organize through crossover and mutation during optimization. In searching for optimal structural forms in conceptual design, the IRR provides the flexibility to represent designs having different numbers and locations of members and nodes, which supports the simultaneous optimization of topology, geometry, and member sizes. Therefore a broad range of designs can be evaluated during a single trial. The set of Pareto-optimal designs evolved by the IRR define the tradeoffs that occur in optimizing the objectives as the structural topology and geometry changes. In damage detection, optimization is often used to predict the location and extent of damages based on the structural response collected from measurement data. The IRR can work with a small subset of all possible damaged elements during the search process, which allows the method to scale well with problem size. The IRR genetic algorithm representation discussed holds significant promise in solving large-scale inverse problems by providing the benefit of working with a variable number of design variables. This flexibility is leveraged to reduce the implicit size of the problem domain searched and to compare designs having markedly different forms or topologies."
1614431,14018,9080,Evolutionary computation for dynamic optimization problems,2013,"Many real-world optimization problems are subject to dynamic environments, where changes may occur over time regarding optimization objectives, decision variables, and/or constraint conditions. Such dynamic optimization problems (DOPs) are challenging problems for researchers and practitioners in decision-making due to their nature of difficulty. Yet, they are important problems that decision-makers in many domains need to face and solve. Evolutionary computation (EC) is a class of stochastic optimization methods that mimic principles from natural evolution to solve optimization and search problems. EC methods are good tools to address DOPs due to their inspiration from natural and biological evolution, which has always been subject to changing environments. EC for DOPs has attracted a lot of research effort during the last twenty years with some promising results. However, this research area is still quite young and far away from well-understood.   This tutorial aims to summarize the research area of EC for DOPs and attract potential young researchers into the important research area. It will provide an introduction to the research area of EC for DOPs and carry out an in-depth description of the state-of-the-art of research in the field regarding the following five aspects: benchmark problems and generators, performance measures, algorithmic approaches, theoretical studies, and applications. Some future research issues and directions regarding EC for DOPs will also be presented. The purpose is to (i) provide clear definition and classification of DOPs; (ii) review current approaches and provide detailed explanations on how they work; (iii) review the strengths and weaknesses of each approach; (iv) discuss the current assumptions and coverage of existing research on EC for DOPs; and (v) identify current gaps, challenges, and opportunities in EC for DOPs."
898488,14018,21102,Hypertension diagnosis: A comparative study using fuzzy expert system and neuro fuzzy system,2013,"Hypertension is called the silent killer because it has no symptoms and can cause serious trouble if left untreated for a long time. It has a major role for stroke, heart attacks, heart failure, aneurysms of the arteries, peripheral arterial diseases, chronic kidney disease etc. An intelligent and accurate diagnostic system is mandatory for better diagnosis and treatment of hypertension patients. This study develops a fuzzy expert system to diagnose the hypertension risk for different patients based on a set of symptoms and rules. Next we design a neuro fuzzy system for the same set of symptoms and rules using three different types of learning algorithms which are Levenberg-Marquardt (LM), Gradient Descent (GD) and Bayesian Resolution (BR) based learning functions. Then this paper presents a comparative study between fuzzy expert system (FES) and feed forward back propagation based neuro fuzzy system (NFS) for hypertension diagnosis. This paper also presents a comparison among the learning functions (LM, GD and BR) where Levenberg-Marquardt based learning function shows its efficiency over the others. Comparison between FES and NFS shows the effectiveness of using NFS over FES. Here, the input data set has been collected from 10 patients whose ages are between 20 and 40 years, both for male and female. The input parameters taken are age, body mass index (BMI), blood pressure (BP), and heart rate. The diagnosis process, linguistic variables and their values were modeled based on expert's knowledge and from existing database."
738185,14018,8806,Multi-agent framework for real-time processing of large and dynamic search spaces,2012,"Distributed systems based on cooperative multi-agents have been used in a wide range of application domains. However, the need for real-time processing in large and dynamic search spaces has led to new challenges. In addition to the constraints in time and computational resources, the agents have to operate under highly dynamic conditions in complex environments. Finding optimal solutions within time constraints may not be always possible. Anytime algorithms have shown great promise in providing approximate solutions. The quality of these intermediate/partial solutions depends on the amount of computational resources available for processing. The key insight that we describe in this paper is that anytime algorithms can be leveraged in a partial processing paradigm where the partial solutions can be used to quickly identify potential solutions and thereby efficiently utilize resources, even under dynamic conditions. The partial solutions can also be used for a coarse grained categorization of large search spaces that can support a mix of explorative and exploitative agent behaviors. We will describe how explicit modeling of the dynamism using a simple but unique search space model can help agents adapt to the changing information space. We describe a generic multi-agent framework that leverages our search space model while modeling various aspects of agent behavior such as candidate selection, agent interactions, etc. This framework can be used to design agents to work with partial processing in various application domains. We will develop suitable testbeds, simulation experiments, algorithms and performance metrics to validate the framework."
1666610,14018,21102,Estimating subjective assessments using a simple biosignal sensor,2012,"Given the remarkable recent progress in robotics research, we can envision the day when robots and humans coexist and robots become closely integrated into our daily lives. This means endowing robots with the ability to communicate so they perceive human emotion, adapt their behavior to humans, and sense situations even without explicit instructions. Meanwhile, affective computing, that interprets emotion or other affective phenomena from human biosignals, has emerged as an area of great interest. In addition to biosignals-brain waves, heart rate, pulse, electrical activity, and the like-affective computing is concerned with facial expressions, gestures, and a wide range of other indicators of emotion. Here we explore the latest insights of affective computing in relation to human-robot interaction (HRI). There is good reason to believe robots will soon have the ability to read human emotions, so here we investigate the feasibility of inferring human psychological states from biosensor signals. Obviously, non-invasive biosensors that don't interfere with normal everyday activities would be preferable. A number of inexpensive user-friendly brain-wave sensors have been brought to market recently, and we employ one of these devices, the NeuroSky Mindset EEG neuroheadset, in assessment trials to explore the feasibility of inferring subjective assessments. Using our experimental setup, we find that it is indeed possible to infer subjective assessments from biosignals, and this capability could prove immensely useful for future HRI applications."
2400435,14018,21102,Dynamic Profile-Selection for zSlices based type-2 fuzzy agents controlling multi-user Ambient Intelligent Environments,2012,"Ambient Intelligence (AmI) is a vision that refers to an information technology paradigm where a physical environment is ‘aware’ of its human occupants' presence/context and is sensitive, adaptive and responsive to their needs. Physical environments that are augmented with AmI are called Ambient Intelligent Environments (AIEs) which are deemed to be intelligent because the system should be able to recognise human occupants, reason with context and program itself to meet the occupants' needs by learning from their behaviour [1]. However, there is a need also to deal with real-world scenarios which involve multiple users occupying a given AIE. In order to handle multi-user AIEs and control them, there is a need to have agents that are able to learn the user(s) behaviours and handle the intra and inter-user uncertainties as people have different preferences and profiles which continuously change. In this paper, we present a zSlices based type-2 fuzzy agent which employs zSlices general type-2 fuzzy systems to learn the user(s) preferences and profiles and handle the encountered intra and inter-user uncertainties. The agent will behave according to a learned user profile that is unique to an individual user or a group of users and so the profile-selection problem manifests when the set of users in an AIE changes (i.e. when people enter/ leave an AIE). The proposed agent employs a novel strategy that we call Dynamic Profile-Selection that uses a cloud-based profile repository in order to support the agent activity in multiple AIEs. To demonstrate the proposed approach, we have conducted real-world experiments on two distinct AIEs which are the intelligent apartment (iSpace) and the intelligent Classroom (iClassroom) located at the University of Essex."
1817275,14018,9704,Simulating chemical evolution,2011,"Chemical methods such as directed evolution and some forms of the SELEX procedure implement evolutionary algorithms directly in vitro. They have a wide range of applications in detecting and targeting diseases and potential applications in other areas as well [1]. However it is relatively difficult and expensive to carry out these processes (by comparison with evolutionary computation), so that the underlying theory has seen limited development. For more complex problems, where multiple and dynamic objectives are involved, there is potential for substantial improvement in the search protocols. Simulation through the methods of evolutionary computation is one potential way to gain the necessary insights. The complex fitness functions and huge populations involved in combinatorial chemistry render detailed simulation infeasible. However detailed simulation is not needed, so long as simulations are sufficiently similar to yield qualitative insights. In this paper, we investigate whether one class of problems — those involving short-chain evolution, where stereochemical effects do not dominate — are likely to have sufficiently similar fitness landscapes to a simple problem, string matching, for useful inferences to be made. In the outcome, it appears that the differences between more detailed simulations and string matching are not sufficient to significantly alter the behaviour of evolutionary algorithms, so that string matching could be used as a realistic surrogate. This is valuable, because string matching can be implemented in GPUs, offering speed-ups to the level where populations of 10 7 , or even 10 8 , might be feasible, thus reducing the population gap between chemical and computer evolution."
1539682,14018,9704,A continuous estimation of distribution algorithm by evolving graph structures using reinforcement learning,2012,"A novel graph-based Estimation of Distribution Algorithm (EDA) named Probabilistic Model Building Genetic Network Programming (PMBGNP) has been proposed. Inspired by classical EDAs, PMBGNP memorizes the current best individuals and uses them to estimate a distribution for the generation of the new population. However, PMBGNP can evolve compact programs by representing its solutions as graph structures. Therefore, it can solve a range of problems different from conventional ones in EDA literature, such as data mining and Reinforcement Learning (RL) problems. This paper extends PMBGNP from discrete to continuous search space, which is named PMBGNP-AC. Besides evolving the node connections to determine the optimal graph structures using conventional PMBGNP, Gaussian distribution is used for the distribution of continuous variables of nodes. The mean value μ and standard deviation σ are constructed like those of classical continuous Population-based incremental learning (PBILc). However, a RL technique, i.e., Actor-Critic (AC), is designed to update the parameters (μ and σ). AC allows us to calculate the Temporal-Difference (TD) error to evaluate whether the selection of the continuous value is better or worse than expected. This scalar reinforcement signal can decide whether the tendency to select this continuous value should be strengthened or weakened, allowing us to determine the shape of the probability density functions of the Gaussian distribution. The proposed algorithm is applied to a RL problem, i.e., autonomous robot control, where the robot's wheel speeds and sensor values are continuous. The experimental results show the superiority of PMBGNP-AC comparing with the conventional algorithms."
1166817,14018,9704,Population's variance-based Adaptive Differential Evolution for real parameter optimization,2013,"Differential evolution (DE) is an evolutionary algorithm (EA) that uses a rather greedy and less stochastic approach to solve optimization problems than other evolutionary methods [1]. Like other EAs, DE is a population-based, stochastic global optimizer, capable of working reliably in nonlinear and multimodal environments. Due to several features such as simplicity, efficiency and global search capabilities, DE rapidly became a successful paradigm of evolutionary computation. However, to achieve adequate performance with DE, the process of tuning the control parameters is essential as its performance is sensitive to the choice of both mutation and crossover settings. This paper proposes a DE algorithm with adaptive tuning of scaling factor (F), crossover rate (CR) and quasi-oppositional probability based on population's variance information - Adaptive Differential Evolution (ADE). Furthermore, ADE adopts a vector called Fm in each dimension of the optimization problem instead of single variable for F as presented in the classical DE approach. The proposed optimization method is validated on the test-bed proposed for the IEEE CEC'13 (IEEE Congress on Evolutionary Computation 2013) contest for real parameter single objective optimization with 28 benchmark functions. Simulation results over the benchmark functions demonstrate the effectiveness and usefulness of the proposed ADE method. This version of paper includes the ADE's performance on the 10, 30 and 50-dimensional benchmark functions."
1973782,14018,21102,A pervasive multi-sensor data fusion for smart home healthcare monitoring,2011,"Today elderly people are the fastest growing segment of the population in developed countries, and they desire to live as independently as possible. But independent lifestyles come with risks and challenges. Medical in-home telemonitoring (and, more generally, telemedicine) is a solution to deal with these challenges and to ensure that elderly people can live safely and independently in their own homes for as long as possible. In this context we propose an automatic in-home healthcare monitoring system for several uses and to meet the needs identified above. The proposed telemonitoring system is a multimodal platform with several sensors that can be installed at home and enables us to have a full and tightly controlled universe of data sets. It integrates elderly physiological and behavioral data, the acoustical environment of the elderly, environmental conditions and medical knowledge. Each modality is processed and analyzed by specific algorithms. A data fusion approach based on fuzzy logic with a set of rules directed by medical recommendations, is used to fuse the various subsystem outputs. This multimodal fusion increases the reliability of the whole system by detecting several distress situations. In fact this fusion approach takes into account temporary sensor malfunction and increases the system reliability and the robustness in the case of environmental disturbances or material limits (Battery, RF range, etc.). The Fuzzy logic fusion methods brings high flexibility to the telemonitoring platform especially in combining modalities or adding other sensors. The proposed telemonitoring system will ensure pervasive in-home health monitoring for elderly people."
1757136,14018,9704,Training spiking neural models using cuckoo search algorithm,2011,"Several meta-heuristic algorithms have been proposed in the last years for solving a wide range of optimization problems. Cuckoo Search Algorithm (CS) is a novel meta-heuristic based on the obligate brood parasitic behaviour of some cuckoo species in combination with the Levy flight behavior of some birds and fruit flies. This algorithm has been applied in a wide range of optimization problems; nonetheless, their promising results suggest its application in the field of artificial neural networks, specially during the adjustment of the synaptic weights. On the other hand, spiking neurons are neural models that try to simulate the behavior of biological neurons when they are excited with an input current (input pattern) during a certain period time. Instead of generating a response in its output every iteration, as classical neurons do, this model generates a response (spikes or spike train) only when the model reaches a specific threshold. This response could be coded into a firing rate and perform a pattern classification task according to the firing rate generated with the input current. To perform a classification task the model ought to exhibit the next behavior: patterns from the same class must generate similar firing rates and patterns from other classes have to generate firing rates sufficiently dissimilar to differentiate among the classes. The model needs of a training phase aimed to adjust their synaptic weights and exhibit the desired behavior. In this paper, we describe how the CS algorithm can be useful to train a spiking neuron to be applied in a pattern classification task. The accuracy of the methodology is tested using several pattern recognition problems."
1769694,14018,9080,On the usefulness of linkage processing for solving MAX-SAT,2013,"Mixing of partial solutions is a key mechanism used for creating new solutions in many Genetic Algorithms (GAs). However, this mixing can be disruptive and generate improved solutions inefficiently. Exploring a problem's structure can help in establishing less disruptive operators, leading to more efficient mixing. One way of using a problem's structure is to consider variable linkage information. Once a proper linkage model for a problem is obtained, mixing becomes more efficient.   This paper focuses on exploring different methods of building family of subsets (FOS) linkage models, which are then used with the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to solve MAX-SAT problems. Individual algorithms from the GOMEA family are distinguished by how the FOS linkage models are constructed. The Linkage Tree Genetic Algorithm (LTGA) is a GOMEA instance which learns the linkage between problem variables by building a linkage tree in every generation. In this paper, we introduce SAT-GOMEA. This algorithm uses a predetermined FOS linkage model based on the SAT-problem's definition. Both algorithms use linkage information. We show that because of this information they are capable of performing significantly better than other algorithms from the GOMEA family which do not explore linkage. In a black-box (BBO) setting, LTGA performs well. We further study the use of linkage models outside of the typical BBO approach by examining the behavior of LTGA and the problem-specific SAT-GOMEA in a white-box setting, where more of the problem information is known. We show that with this white-box optimization (WBO) approach, exploring linkage information can still be beneficial.   We further compare the performance of these algorithms with a selection of non-GOMEA based algorithms. From the BBO perspective, we compare LTGA with the well-known hBOA. In the WBO setting, many very efficient problem-specific local search (LS) algorithm exist. We specifically consider Walksat and GSAT and show that combining LS with LTGA or SAT-GOMEA increases their performance."
2095061,14018,21102,An adaptive fuzzy logic based system for improved knowledge delivery within intelligent E-Learning platforms,2013,"E-learning involves the computer and network-enabled transfer of skills and knowledge. The recent years have witnessed an increased interest in intelligent E-Learning platforms that incorporate adaptive educational systems which enable the creation of personalized learning environments to suit the students' individual requirements and needs. Such systems aim to correlate the student characteristics (such as knowledge level, personality and learning style) with instructional variables, (such as the presentation of learning materials and feedback). Various artificial intelligence based methodologies have been used to realize adaptive educational systems. However, the vast majority of the existing adaptive educational systems do not learn from the users' behaviors to create white box models which could be easily read and analyzed by the lay user. This paper presents a fuzzy logic based system that can learn the users' preferred knowledge delivery based on the students characteristics to generate a personalized learning environment. The proposed methodology employs a self-learning system which enables to generate a fuzzy logic based model from data. The fuzzy model is generated from data representing various students' capabilities and their desired learning needs. The learnt fuzzy based model is then used to improve the knowledge delivery to the various students based on their individual characteristics. The proposed system is adaptive where it is continuously adapting in a lifelong learning mode to make sure that the generated models adapt to the students individual preferences. We will present experiments carried with the proposed system which involved 17 students. The experiments will show how the proposed system learnt the students' preferences and created a model which allowed providing a personalized learning environment tailored according to the students' needs and requirements. This allowed improving the knowledge delivery which resulted in improving the students' performance."
2519918,14018,9704,Evolvability as concept for the optimal design of free-form deformation control volumes,2012,"The performance of design optimizations which target the improvement of certain physical aspects of real world objects like e.g. in the automotive or aeronautical domain depends on the efficient interplay of optimization algorithm, evaluation method and shape representation. For the development of complex aerodynamic components, evolutionary algorithms as global stochastic optimization algorithms have been successfully coupled to shape morphing methods. Instead of a direct representation of the shapes' boundary, shape morphing methods like free-form deformation (FFD) apply scalable changes to a baseline prototype using a moderate number of parameters mapped to control point movements. The initial spatial arrangement of the control points influences strongly the design flexibility and the optimization performance in combination with the normal distributed mutation operator in evolutionary optimization algorithms. In the present paper, a method is proposed to support the generation process of initial FFD control volumes which is usually carried out manually in practice. The method is based on the concept of evolvability which is considered as the property of initial control volumes to generate favorable design variations within a moderate number of iterations while avoiding unfeasible mutations. We introduce mathematically translational design variability, mutational design variability and the central robust control volume as key features to compute an evolvable distribution of control points. In target shape matching experiments using an evolutionary strategy, the performance for different configurations of evolvability-tuned initial control volumes is shown empirically."
1789930,14018,9704,Adaptive Differential Evolution with Locality based Crossover for Dynamic Optimization,2013,"Real life problems which deal with time varying landscape dynamics often pose serious challenge to the mettle of researchers in the domain of Evolutionary Computation. Classified as Dynamic Optimization problems (DOPs), these deal with candidate solutions which vary their dominance over dynamic change instances. The challenge is to efficiently recapture the dominant solution or the global optimum in each varying landscape. Differential Evolution (DE) algorithm with modifications of adaptability have been widely used to deal with the complexities of a dynamic landscape, yet problems persist unless dedicated structuring is done to exclusively deal with DOPs. In Adaptive Differential Evolution with Locality based Crossover (ADE-LbX) the mutation operation has been entrusted to a locality based scheme that retains traits of Euclidean distance based closest individuals around a potential solution. Diversity maintenance is further enhanced by incorporation of local best crossover scheme that renders the landscape independent of direction and empowers the algorithm with an explorative ability. An even distribution of solutions in different regions of landscape calls for a solution retention technique that adapts this algorithm to dynamism by using its previous information in diverse search domains. To evaluate the performance of ADE-LbX, it has been tested over Dynamic Problem instance proposed as in CEC 09 and compared with State-of-the-arts. The algorithm enjoys superior performance in varied problem configurations of the problem."
1412159,14018,9080,Automated heuristic design,2011,"This tutorial will discuss state-of-the-art techniques for automating the design of heuristic search methods, in order to remove or reduce the need for a human expert in the process of designing an effective algorithm to solve a search problem. Using machine learning or meta-level search, several approaches have been proposed in computer science, artificial intelligence and operational research. The aim is to develop methodologies which can adapt to different environments without manually having to customise the search, or its parameters, for each particular problem domain. This can be seen as one of the drawbacks of many current metaheuristic and evolutionary implementations, which tend to have to be customised for a particular class of problems or even specific problem instances. We have identified two main types of approaches to this challenge: heuristic selection, and heuristic generation. In heuristic selection the idea is to automatically combine fixed pre-existing simple heuristics or neighbourhood structures to solve the problem at hand; whereas in heuristic generation the idea is to automatically create new heuristics (or heuristic components) suited to a given problem or class of problems. This latter approach is typically achieved by combining, through the use of genetic programming for example, components or building-blocks of human designed heuristics. This tutorial will go over the intellectual roots and origins of both automated heuristic selection and generation, before discussing work carried out to date in these two directions and then focusing on some observations and promising research directions."
645912,14018,9080,Variance based selection to improve test set performance in genetic programming,2011,"This paper proposes to improve the performance of Genetic Programming (GP) over unseen data by minimizing the  variance  of the output values of evolving models alongwith reducing error on the training data. Variance is a well understood, simple and inexpensive statistical measure; it is easy to integrate into a GP implementation and can be computed over arbitrary input values even when the target output is not known.   Moreover, we propose a simple  variance based selection  scheme to decide between two models (individuals). The scheme is simple because, although it uses bi-objective criteria to differentiate between two competing models, it does not rely on a multi-objective optimisation algorithm. In fact, standard multi-objective algorithms can also employ this scheme to identify good trade-offs such as those located around the  knee  of the  Pareto Front .   The results indicate that, despite some limitations, these proposals significantly improve the performance of GP over a selection of high dimensional (multi-variate) problems from the domain of symbolic regression. This improvement is manifested by superior results over test sets in three out of four problems, and by the fact that performance over the test sets does not degrade as often witnessed with standard GP; neither is this performance ever inferior to that on the training set. As with some earlier studies, these results do not find a link between expressions of small sizes and their ability to generalise to unseen data."
1708739,14018,9080,A memetic algorithm for two-dimensional multi-objective bin-packing with constraints,2011,"Over recent years, a number of independent researchers have shown that meta-heuristics are effective strategies for solving hard combinatorial optimization problems. In particular, Memetic Algorithms (MA) are population-based meta-heuristic search methods that are inspired by Darwinian principles of natural selection and Dawkins' notion of meme that have successfully been applied to single- and multi-objective optimization problems. The two-dimensional bin-packing problem (2D-BPP) [1] with rotations is an important optimization problem which has a large number of practical applications. It consists of the non-overlapping placement of a set of rectangular pieces in the lowest number of bins of a homogenous size, with the edges of these pieces always parallel to the sides of bins, and with free 90 degrees rotation. Bin-packing problems are complex combinatorial optimization problems included in the category of NP-hard problems of fundamental importance in industry, transportation, computer systems, machine scheduling, etc. The multi-objective two-dimensional bin-packing problem considers other objectives to optimize, such as the imbalance of the objects according to a centre of gravity of the bin. The balance in the bin loads has important applications in container loading, tractor trailer trucks, pallet loading and cargo airplanes. This paper analyzes the performance of a Pareto-based memetic algorithm, which operators have been specially designed to solve this problem while considering some contraints. Results obtained in some test problems show the good performance of this approach in comparison with multi-objective Particle Swarm Optimization."
1523957,14018,9080,Expressive genetic programming: tutorial: 2012 genetic and evolutionary computation conference (GECCO-2012),2012,"The language in which evolving programs are expressed can have significant impacts on the problem-solving capabilities of a genetic programming system. These impacts stem both from the absolute computational power of the languages that are used, as elucidated by formal language theory, and from the ease with which various computational structures can be produced by random code generation and by the action of genetic operators. Highly expressive languages can facilitate the evolution of programs for any computable function using, when appropriate, multiple data types, evolved subroutines, evolved control structures, evolved data structures, and evolved modular program and data architectures. In some cases expressive languages can even support the evolution of programs that express methods for their own reproduction and variation (and hence for the evolution of their offspring). This tutorial will begin with a comparative survey of approaches to the evolution of programs in expressive programming languages ranging from machine code to graphical and grammatical representations. Within this context it will then provide a detailed introduction to the Push programming language, which was designed specifically for expressiveness and specifically for use in genetic programming systems. Push programs are syntactically unconstrained but can nonetheless make use of multiple data types and express arbitrary control structures, supporting the evolution of complex, modular programs in a particularly simple and flexible way. The Push language will be described and ten years of Push-based research, including the production of human-competitive results, will be briefly surveyed. The tutorial will conclude with a discussion of recent enhancements to Push that are intended to support the evolution of complex and robust software systems."
1669630,14018,21102,An improved optimisation framework for fuzzy time-series prediction,2013,"This paper presents a hybrid identification method for Takagi-Sugeno-Kang (TSK) fuzzy model by means of a combination of optimisation techniques. First, the K-means clustering algorithm is used to get information granules (centres of clusters) which are used as the initial location of apexes of the MFs in the premise and the prototypes of the polynomial functions used in the consequent parts of the fuzzy rules. Subsequently, the initial fuzzy system is evolved iteratively by means of a hybrid learning. In particular, the premise part parameters are tuned by a combination of a Island Model Parallel Genetic Algorithm (IMPGA) and a space search Memetic Algorithm (MA) while the consequent parameters of the system are derived optimally by an improved QR Householder least square method (LSM). The optimisation search algorithm (IMPGA+MA) allows exploring the search space in multiple trajectories simultaneously to avoid getting trapped in local-optimal while the improved LSM helps minimize the occurrences of the underflow and overflow problems when dealing with floating point numbers. The proposed optimisation framework can be applied for a variety of application areas such as function approximation, time-series prediction, etc. However, in this paper, the proposed method is only evaluated using the well-known Mackey-Glass time-series prediction benchmark and has shown a better prediction accuracy than any other previous works of the same problem."
1389611,14018,9704,Behavioral study of the surrogate model-aware evolutionary search framework,2014,"The surrogate model-aware evolutionary search (SMAS) framework is an emerging model management method for surrogate model assisted evolutionary algorithms (SAEAs). SAEAs based on SMAS outperform several state-of-the-art SAEAs using other model management methods and show promising results in real-world computationally expensive op- timization problems. However, there is little behavioral study of the SMAS framework, and appropriate rules for its search strategy, training data selection and key parameter selection for different types of problems have not been provided yet. In this paper, with a newly proposed training data selection method, the SMAS framework's behaviour with different search strategies and training data selection methods is investigated. The empirical rules in terms of problem characteristics are obtained and the method to construct an SAEA based on the SMAS framework is updated. Experiments using 24 widely used benchmark test problems and the test problems in the CEC 2014 competition of computationally expensive optimization are carried out, which validate the proposed empirical rules. I. INTRODUCTION Many real-world optimization problems require compu- tationally expensive fitness function evaluations (1), (2). Directly applying evolutionary algorithms (EAs) is often not feasible because a large number of time-consuming function evaluations are unaffordable. Surrogate model assisted evolu- tionary algorithms (SAEAs) are a recent promising approach for dealing with such expensive optimization problems. In SAEA, a surrogate model is employed to replace compu- tationally expensive exact function evaluations. Surrogate models are approximation models of the fitness function that are much cheaper to evaluate than the real evaluation and the additional surrogate modeling process is often not expensive. Due to this, the computational cost can be reduced"
1519119,14018,9080,Automatic (offline) configuration of algorithms,2013,"Most optimization algorithms, including evolutionary algorithms and metaheuristics, and general-purpose solvers for integer or constraint programming, have often many parameters that need to be properly configured (i.e., tuned) for obtaining the best results on a particular problem. Automatic (offline) algorithm configuration methods help algorithm users to determine the parameter settings that optimize the performance of the algorithm before the algorithm is actually deployed. Moreover, automatic algorithm configuration methods may potentially lead to a paradigm shift in algorithm design and configuration because they enable algorithm designers to explore much larger design spaces than by traditional trial-and-error and experimental design procedures. Thus, algorithm designers can focus on inventing new algorithmic components, combine them in flexible algorithm frameworks, and let final algorithm design decisions be taken by automatic algorithm configuration techniques for specific application contexts.   This tutorial will be divided in two parts. The first part will give an overview of the algorithm configuration problem, review recent methods for automatic algorithm configuration, and illustrate the potential of these techniques using recent, notable applications from the presenters' and other researchers work. The second part of the tutorial will focus on a detailed discussion of more complex scenarios, including multi-objective problems, anytime algorithms, heterogeneous problem instances, and the automatic generation of algorithms from algorithm frameworks. The focus of this second part of the tutorial is, hence, on practical but challenging applications of automatic algorithm configuration. The second part of the tutorial will demonstrate how to tackle these configuration tasks using our irace software (http://iridia.ulb.ac.be/irace), which implements the iterated racing procedure for automatic algorithm configuration. We will provide a practical step-by-step guide on using irace for the typical algorithm configuration scenario."
1111081,14018,8806,"Stheno, a real-time fault-tolerant P 2 P middleware platform for light-train systems",2013,"Large scale information systems, such as public information systems for light-train/metro networks, must be able to fulfill contractualized Service Level Agreements (SLAs) in terms of end-to-end latencies and jitter, even in the presence of faults. Failure to do so has potential legal and financial implications for the software developers. Current middleware solutions have a hard time coping with these demands due, fundamentally, to a lack of adequate, simultaneous, support for fault-tolerance (FT) and real-time (RT) tasks. In this paper we present Stheno, a general purpose peer-to-peer (P 2 P) middleware system that builds on previous work from TAO and MEAD to provide: (a) configurable, transparent, FT support by taking advantage of the P 2 P layer topology awareness to efficiently implement Common Of The Shelf (COTS) replication algorithms and replica management strategies, and; (b) kernel-level resource reservation integrated with well-known threading strategies based on priorities to provide more robust support for soft real-time tasks. An evaluation of the first (unoptimized) prototype for the middleware shows that Stheno is able to match and often greatly exceed the SLA agreements provided by our target system, the light-train/metro information system developed and maintained by EFACEC, and currently deployed at multiple cities in Europe and Brazil."
1334921,14018,21102,A new computationally efficient mamdani interval type-2 fuzzy modelling framework,2012,"A modified center-of-sums (mCoS) type-reduction technique is proposed in this paper for constructing a data-driven Mamdani interval type-2 fuzzy modelling (MIT2FM) framework. The mCoS type-reducer is an extension of its type-1 counterpart, the center-of-sums defuzzification, which takes both the area of the scaled consequent membership function of each fired rule and its associated geometric center into account for computing the final output. Contrary to the normal center-of-sums type-reduction, the proposed approach considers the full area under the scaled consequent membership functions even if such area extends beyond the range of the output variable. This enables the commonly used Gaussian interval type-2 membership functions to be utilised in MIT2FM, the area of which has to be calculated via the improper integrals over the whole real line. Moreover, the mCoS method can make use of the mean of Gaussian membership functions directly, instead of computing the geometric center for each rule, so as to further reduce the computational burden of type-reduction. Compared with the state-of-the-art type-reducers, mCoS is shown to be more efficient and, therefore, makes the interval type-2 based fuzzy logic systems more competitive for data-driven fuzzy modelling applications. In order to test the validity of mCoS type-reduction and the elicited fuzzy modelling scheme, experiments are conducted on a benchmark problem of non-linear time series, where collected data are disturbed by noise, and on the real-world application, namely the prediction of mechanical properties of alloy steels. The mCoS based interval type-2 fuzzy modelling approach is shown to handle uncertainties very well and to provide desired generalisation capability when addressing large high-dimensional data sets."
1616646,14018,9704,Generative representations for artificial architecture and passive solar performance,2013,"This paper explores how the use of generative representations influences the quality of solutions in evolutionary design problems. A genetic programming system is developed with individuals encoded as generative representations. Two research goals motivate this work. One goal is to examine Hornby's features and measures of modularity, reuse and hierarchy in new and more complex evolutionary design problems. In particular, we consider a more difficult problem domain where the generated 3D models are no longer constrained by voxels. Experiments are carried out to generate 3D models which grow towards a set of target points. The results show that the generative representations with the three features of modularity, regularity and hierarchy performed best overall. Although the measures of these features were largely consistent with those of Hornby, a few differences were found. Our second research goal is to use the best performing encoding on some 3D modeling problems that involve passive solar performance criteria. Here, the system is challenged with generating forms that optimize exposure to the Sun. This is complicated by the fact that a model's structure can interfere with solar exposure to itself; for example, protrusions can block Sun exposure to other model elements. Furthermore, external environmental factors (geographic location, time of the day, time of the year, other buildings in the proximity) may also be relevant. Experimental results were successful, and the system was shown to scale well to the architectural problems studied."
783124,14018,21102,Evolving fuzzy rule-based classifier based on GENEFIS,2013,"This paper presents a novel evolving fuzzy rule-based classifier stemming from our recently developed algorithm for regression problem termed generic evolving neuro-fuzzy system (GENEFIS). On the one hand, the novel classifier namely GENEFIS-class is composed of two different architectures specifically zero and first orders which are dependent on the type of consequent used. On the other hand, GENEFIS-class refurbishes GENEFIS algorithm as the main learning engine to conform classification requirement. The interesting property of GENEFIS is its fully flexible rule base and its computationally efficient algorithm. GENEFIS can initiate its learning process from scratch with an empty rule base and highly narrow expert knowledge. The fuzzy rules are then flourished based on the novelty of streaming data via their statistical contribution. Conversely, the fuzzy rules, which contribute little during their lifespan, can be pruned by virtue of their contributions up to the end of training process. Meanwhile, the fuzzy rules and fuzzy sets, which are redundant, can be merged to purpose a transparent rule base. Online feature selection process coupled during the training process can be undertaken to cope with possible combinatorial rule explosion drawback. All of these are fruitful to grant significant reduction of rule base load while retaining the classification accuracy which is in line with online real-time necessity. The efficacy of GENEFIS-class was numerically validated exploiting real world and synthetic problems and compared with state-of-the-art algorithms where it generally speaking outperforms other algorithms in terms of classification performance and rule-base complexity."
1609395,14018,9704,Automatic implementation of evolutionary algorithms on GPUs using ESDL,2012,"Modern computer processing units tend towards simpler cores in greater numbers, favouring the development of data-parallel applications. Evolutionary algorithms are ideal for taking full advantage of SIMD (Single Instruction, Multiple Data) processing, which is available on both CPUs and GPUs. Creating software that runs on a GPU requires the use of specialised programming languages or styles, forcing practitioners to acquire new skills and limiting the portability of their developments. In this paper, we present an automatic translation from ESDL, a domain-specific language for composing evolutionary algorithms from arbitrary operators, to C++ AMP, a C++ extension for targeting heterogeneous hardware. Generating executable code from a simple platform-independent description allows practitioners with varying levels of programming expertise to take advantage of data-parallel execution, and enables those with strong expertise to further optimise their implementations. The automatic transformation is shown to produce code less optimal than a manual implementation but with significantly less developer effort. A secondary result is that GPU implementations require a large population, large individuals or an expensive evaluation function to achieve performance benefits over the CPU. All code developed for this paper is freely available online from http://stevedower.id.au/esdl/amp."
1670922,14018,21102,A framework for automatic modelling of survival using fuzzy inference,2012,"Survival analysis describes the analysis of data that corresponds to the time from when an individual enters a study until the occurrence of some particular event or end-point. It is most commonly used in the context of modelling survival (or disease-free interval time) in medical contexts, often concerned with the comparison of survival for different combinations of risk factors and/or treatments. Analytical methods which are transparent to the clinicians in understanding and explaining individual inference need to be considered when dealing with such medical data. In this paper, we present a framework for modelling survival utilising the application of the ANFIS fuzzy inference system. In this framework, alternative methods of partitioning the input space can be selected to define the membership functions, for example by using expert knowledge, equalizer partitioning, fuzzy c-means clustering, or the combination of these techniques. Further, the rule base can be established by enumerating all possible combinations of membership functions of all inputs. After the initialisation of the fuzzy inference structure, the replication data (until time to event) will be subject to be trained using the gradient descent and nonnegative least square algorithm to estimate the conditional event probability. This framework is validated over a novel dataset of patients following operative surgery for ovarian cancer. We demonstrate that the proposed framework can be successfully applied to estimate the hazard and survival curves between different prognostic factors, and model survival times, while providing models with explicit explanation capabilities."
848278,14018,9080,Runtime analysis of evolutionary algorithms: basic introduction,2013,"Evolutionary algorithm theory has studied the time complexity of evolutionary algorithms for more than 20 years. Different aspects of this rich and diverse research field were presented in four different advanced or specialized tutorials at last year's GECCO. This tutorial presents the foundations of this field. We introduce the most important notions and definitions used in the field and consider different evolutionary algorithms on a number of well-known and important example problems. Through a careful and thorough introduction of important analytical tools and methods, including fitness-based partitions, typical events and runs and drift analysis, by the end of the tutorial the attendees will be able to apply these techniques to derive relevant runtime results for non-trivial evolutionary algorithms. Moreover, the attendees will be fully prepared to follow the more advanced tutorials that cover more specialized aspects of the field, including the new advanced runtime analysis tutorial on realistic population-based EAs. To assure the coverage of the topics required in the specialised tutorials, this introductory tutorial will be coordinated with the presenters of the more advanced ones. In addition to custom-tailored methods for the analysis of evolutionary algorithms we also introduce the relevant tools and notions from probability theory in an accessible form. This makes the tutorial appropriate for everyone with an interest in the theory of evolutionary algorithms without the need to have prior knowledge of probability theory and analysis of randomized algorithms. The last two editions of this tutorial at GECCO 2013 and GECCO 2014 attracted over 50 participants each."
833856,14018,369,Wireless Mesh Network Planning Using Quantum Inspired Evolutionary Algorithm,2011,"The latest increase in mobile data usage and emergence of new applications such as Multimedia Online Gaming (MMOG), mobile TV and streaming contents have motivated advances in wireless broadband systems. Recently, the Long-Term Evolution (LTE) technology, which is based on the Universal Mobile Telecommunications System (UMTS) specifications, joins WiMAX as a competitor to achieve increasing demands of the broadband wireless access. Careful deployment of such a network is required to fulfill the high data rate demands with minimal cost of infrastructure and comprehensive coverage of the subscribers. In this paper, a multi-objective network planning problem is defined as utilizing the minimum number of infrastructure sites (i.e. Base Stations or eNode B in UMTS systems) while maximum number of users in service. We proposed a Quantum Inspired Evolutionary Algorithm (QIEA) in order to achieve optimized solution for this problem. The QIEA can be viewed as a probabilistic evolutionary algorithm and thus it is plausible to expect a reasonably good performance in solving combinatorial optimization problems. In this algorithm, each individual is represented by a string of Q-bits, where a Q-bit is the probabilistic representation inspired by the qubit concept in the quantum computing. Computational experiments show that our algorithm is fairly efficient to different scenarios of the network planning problem and performs better than the Genetic Algorithm (GA)."
1195156,14018,21102,Robust Load Frequency Control of multi-area interconnected system including SMES units using Type-2 Fuzzy controller,2013,"Load Frequency Control (LFC) problem plays an vital role in power systems; its main role is to maintain the system frequency and tie line flow at their scheduled values during normal period in an interconnected system. This paper proposes a new methodology to study the Load Frequency Control (LFC) problem of a three area inter-connected system including Superconducting Magnetic Energy Storage (SMES) units using Type -2 Fuzzy system (T2FS) approach. Here, the technique is applied to control systems include three areas considering Generation Rate constraint (GRC) having two steam turbines and one hydro -turbine tied together through power lines including Superconducting Magnetic Energy Storage (SMES) units. As a consequence of continually load variation, the frequency of the power system changes over time. The salient advantage of this controller is its high insensitivity to large load changes and plant parameter variations even in the presence of non-linearities. The proposed method is tested on a three-area power system to illustrate its robust performance with various area load changes. The results obtained by using Type-2 (T2)Fuzzy controller explicitly show that the performance of the proposed controller is superior to the conventional controller and Fuzzy PI Controller(Type-1 Fuzzy) controller in terms of the overshoot, settling time and robustness. Simulation results confirm the high robustness of the proposed SMES controller with small power capacity against various disturbances and system uncertainties in comparison with SMES in the previous research."
1063280,14018,9080,GECCO 2011 tutorial: cartesian genetic programming,2011,"Cartesian Genetic Programming (CGP) is an increasingly popular and efficient form of Genetic Programming. Cartesian Genetic Programming is a highly cited technique that was developed by Julian Miller in 1999 and 2000 from some earlier joint work of Julian Miller with Peter Thomson in 1997.   In its classic form, it uses a very simple integer based genetic representation of a program in the form of a directed graph. Graphs are very useful program representations and can be applied to many domains (e.g. electronic circuits, neural networks). In a number of studies, CGP has been shown to be comparatively efficient to other GP techniques. It is also very simple to program.   Since then, the classical form of CGP has been developed made more efficient in various ways. Notably by including automatically defined functions (modular CGP) and self-modification operators (self-modifying CGP). SMCGP was developed by Julian Miller, Simon Harding and Wolfgang Banzhaf. It uses functions that cause the evolved programs to change themselves as a function of time. Using this technique it is possible to find general solutions to classes of problems and mathematical algorithms (e.g. arbitrary parity, n-bit binary addition, sequences that provably compute pi and e to arbitrary precision, and so on).   This tutorial is will cover the basic technique, advanced developments and applications to a variety of problem domains. The first edited book on CGP was published by Springer in September 2011. CGP has its own dedicated website http://www.cartesiangp.co.uk"
746457,14018,9704,A new objective function to build seismic networks using differential evolution,2012,"Natural phenomena such as earthquakes have caused devastating effects in different cities around the word. To prevent a great disaster, it is necessary to construct seismic stations at strategical locations to warn population. Many Disaster Alert Systems (DAS), such as the Seismic Alert System of Mexico City (SAS) [4] or the Deep-ocean Assessment and Reporting of Tsunamis (DART II) [11], were located not based in earthquake or tsunami data, but simply by spacing the sensors more or less evenly around the contour of the Pacific Ocean. The objective of a DAS is simple: to emit an alert as fast as possible, in order to warn the population as early as possible. According to a new location of its seismic stations, the SAS could issue a longer warning time. This research focuses on designing the locations of seismic sensing stations maximizing the “warning time”; that is, the gap between the time when an earthquake is detected and the alert is launched, and the arrival time of the disaster. Since locating these stations is basically a numerical problem, in this research, the authors propose a new objective function to maximize the warning time using a differential evolution algorithm. In order to perform the experiments and validate the efficiency of the algorithm, it was considered the epicenters of recorded earthquakes located in the State of Guerrero, Mexico. This data is used in the objective function to set the fitness value of a candidate solution. The main disasters targeted in this paper are earthquakes, but this research can be extended easily to tsunamis or volcanic eruptions alert systems, locating telecommunications antennas, etc."
1985983,14018,9704,Evolutionary modeling of a blog network,2011,"A common approach to produce theory to explain the genesis and dynamics of complex networks is to create multi-agent simulations that output networks with similar characteristics to the ones derived from real data. For example, a well know explanation for the power law degree distributions found in blog (and other) networks is the agent-level endogenous mechanism of preferential attachment. However, once simplifying assumptions are dropped, finding lower level behaviors that explain global network features can become difficult. One case, explored in this paper, is that of modeling a blog network generated by human agents with heterogeneous behaviors and a priori diversity. We propose an approach based on an hybrid strategy, combining a generic behavioral template created by a human designer with a set of programs evolved using genetic programming. We present experimental results that illustrate how this approach can be successfully used to discover a set of non-trivial agent-level behaviors that generate a network that fits observed data. We then use the model to make successful testable predictions about the real data. We analyze the diversity of behaviors found in the evolved model by clustering the agents according to the execution paths their programs take during the simulation. We show that these clusters map to different behaviors, giving credence to the need for exogenous, in addition to the more conventional endogenous, explanations for the dynamics of blog networks."
901765,14018,20358,Strategic formation of credit networks,2012,"Credit networks are an abstraction for modeling trust between agents in a network. Agents who do not directly trust each other can transact through exchange of IOUs (obligations) along a chain of trust in the network. Credit networks are robust to intrusion, can enable transactions between strangers in exchange economies, and have the liquidity to support a high rate of transactions. We study the formation of such networks when agents strategically decide how much credit to extend each other. When each agent trusts a fixed set of other agents, and transacts directly only with those it trusts, the formation game is a potential game and all Nash equilibria are social optima. Moreover, the Nash equilibria of this game are equivalent in a very strong sense: the sequences of transactions that can be supported from each equilibrium credit network are identical. When we allow transactions over longer paths, the game may not admit a Nash equilibrium, and even when it does, the price of anarchy may be unbounded. Hence, we study two special cases. First, when agents have a shared belief about the trustworthiness of each agent, the networks formed in equilibrium have a star-like structure. Though the price of anarchy is unbounded, myopic best response quickly converges to a social optimum. Similar star-like structures are found in equilibria of heuristic strategies found via simulation. In addition, we simulate a second case where agents may have varying information about each others' trustworthiness based on their distance in a social network. Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare, and otherwise, credit tends to be issued over short social distances conforming to the locality of information."
1506129,14018,9080,Industrial applications of evolutionary algorithms,2013,"Industrial applications of evolutionary algorithms is intended as a resource for both experienced users of evolutionary algorithms and researchers that are beginning to approach these fascinating optimization techniques. Experienced users will find interesting details of real-world problems, advice on solving issues related to fitness computation or modeling, and suggestions on how to set the appropriate parameters to reach optimal solutions. Beginners will find a thorough introduction to evolutionary computation, and a complete presentation of several classes of evolutionary algorithms exploited to solve different problems. Inside, scholars will find useful examples on how to fill the gap between purely theoretical examples and industrial problems. The collection of case studies presented is also extremely appealing for anyone interested in Evolutionary Computation, but without direct access to extensive technical literature on the subject. After the introduction, each chapter in the book presents a test case, and is organized so that it can be read independently from the rest: all the information needed to understand the problem and the approach is reported in each part. Chapters are grouped by three themes of particular interest for real-world applications, namely prototype-based validation, reliability and test generation. The authors hope that this volume will help to expose the flexibility and efficiency of evolutionary techniques, encouraging more companies to adopt them; and that, most of all, you will enjoy your reading."
733290,14018,9080,Automatic synthesis of MEMS devices using self-adaptive hybrid metaheuristics,2011,"This paper introduces a multi-objective optimization approach for layout synthesis of MEMS components. A case study of layout synthesis of a comb-driven micro-resonator shows that the approach proposed in this paper can lead to design results accommodating two design objectives, i.e. simultaneous minimization of size and power input of a MEMS device, while investigating optimum geometrical configuration as the main concern. The major contribution of this paper is the application of self-adaptive memetic computing in MEMS design. An evolutionary multi-objective optimization (EMO) technique, in particular non-dominated sorting genetic algorithm (NSGA-II), has been applied together with a pattern recognition statistical tool, i.e. Principal Component Analysis (PCA), to find multiple trade-off solutions in an efficient manner. Following this, a gradient-based local search, i.e. sequential quadratic programming (SQP), is applied to improve and speed up the convergence of the obtained Pareto-optimal front. In order to reduce the number of function evaluations in the local search procedure, the obtained non-dominated solutions are clustered in the objective space and consequently, a post-optimality study is manually performed to find out some common design principles among those solutions. Finally, two reasonable design choices have been offered based on manufacturability issues."
1560802,14018,9704,Finding a preferred diverse set of Pareto-optimal solutions for a limited number of function calls,2012,"Evolutionary Multi-objective Optimization aims at finding a diverse set of Pareto-optimal solutions whereof the decision maker can choose the solution that fits best to her or his preferences. In case of limited time (of function evaluations) for optimization this preference information may be used to speed up the search by making the algorithm focus directly on interesting areas of the objective space. The R-NSGA-II algorithm [1] uses reference points to which the search is guided specified according to the preferences of the user. In this paper, we propose an extension to R-NSGA-II that limits the Pareto-fitness to speed up the search for a limited number of function calls. It avoids to automatically select all solutions of the first front of the candidate set into the next population. In this way non-preferred Pareto-optimal solutions are not considered thereby accelerating the search process. With focusing comes the necessity to maintain diversity. In R-NSGA-II this is achieved with the help of a clustering algorithm which keeps the found solutions above a minimum distance e. In this paper, we propose a self-adaptive e approach that autonomously provides the decision maker with a more diverse solution set if the found Pareto-set is situated further away from a reference point. Similarly, the approach also varies the diversity inside of the Pareto-set. This helps the decision maker to get a better overview of the available solutions and supports decisions about how to adapt the reference points."
2165529,14018,9704,A genotype-to-phenotype mapping for microstructured polymer optical fibres,2011,"Although glass fibres are standard in long-distance telecommunications; customised polymer microstructured optical fibres play a more significant role in many diverse new short-distance applications. Our prototyping process involves drilling an array of holes in a cylindrical preform. That preform is subsequently heated and pulled into a narrow fibre. The size and position of the holes create an effective refractive index profile, which in turn determines the optical transmission properties of the fibre. In this paper, a new variable-length genotype is introduced which controls the coordinates of the centres of ‘potential’ holes. The genotype-to-phenotype mapping carefully determines which holes are ‘activated’ and the final radius of each hole, consistent with manufacturing constraints. Two manufacturing constraints are: a minimum spacing between adjacent holes and that the drill bits are only available in a discrete range of radii. A cross-over operator is designed that works with variable-length genotypes and its effect on the distribution of genome lengths is explored in detail. An implementation of NSGA-II is used to perform a multi-objective optimisation with four objectives. One of these objectives (wanting a parabolic index profile) is the same as in our previous work. Two are new: minimising the deformability of the design and minimising the detrimental effects of surface roughness. The final objective is not optical but related to the efficiency of the GA and is to minimise the number of inactive genes. The behaviour of the genetic algorithm and a number of interesting designs are discussed."
781089,14018,9080,XCS-based versus UCS-based feature pattern classification system,2012,"Extracting features from images is an important task in order to identify (classify) the patterns contained. The Evolutionary Computation and Reinforcement Learning technique of Learning Classifier Systems (LCSs) has been successfully applied to classification tasks, but rarely to image pattern classification due to the large search space associated with pixel data. Recently, a Feature Pattern Classification System (FPCS), utilising Haar-like features has been introduced with promising results in the image recognition domain. This system used a confusion-matrix to direct learning to hard to classify classes, but due to its reinforcement learning nature was required to estimate the ground truth. The novel work presented here adopts a supervised learning (UCS-based) approach into the FPCS framework. This work is compared with the original XCS-based system, updated to include the known ground-truth of the confusion matrix to aid comparison, albeit no longer reinforcement learning. Results on the 10 class MNIST numerical digits recognition task show that the XCS-based FPCS produces better classification due to its complete mapping guiding learning. However, results on the 26 class NIST character recognition task show that the UCS-based scales better as it does not require the complete mapping. The human readable rules produced by each system, coupled with the competitive classification performance compared with similar techniques, supports future work on both the XCS and UCS-based FPCS."
2354961,14018,9704,Multi-path planning based on a NSGA-II for a fleet of robots to work on agricultural tasks,2012,"In many situations, using multiple robots in the same environment is a good strategy to handle tasks that are too complex or even too expensive for a single robot. One of these situations is the automation of tasks in the agricultural environment. In this context, one of the main problems consists of determining the best routes (multi-path plan) for the robots to minimise cost, while ensuring a fully completed treatment, i.e., the whole field is covered. The cost can be expressed by a function that considers the most relevant features of each robot in the fleet, for example, in a spray weed treatment, the tank capacity, the number of turns required or the time spent in the whole treatment. This multi-path planning problem can be expressed as a bi-objective problem. In particular, in this paper, two different objectives are taken into account: the cost in time and the cost in money. This formulation allows the analysis of situations in which it is important to distribute the robots to reduce the time of the treatment independently of the money spent and of situations where it is important to reduce the spent money independently of the time consumed. A Non-dominated Sorting Genetic Algorithm II (NSGA-II) is proposed for solving the multi-objective problem. The proposed approach has proven to offer good results in multiple situations dealing with different fields and robots with diverse features. Moreover, the results obtained show that it is possible to determine solutions very close to the optimum of each objective, even simultaneously."
2635024,14018,20332,Pattern discovery in protein networks reveals high-confidence predictions of novel interactions,2014,"Pattern discovery in protein interaction networks can reveal crucial biological knowledge on the inner workings of cellular machinery. Although far from complete, extracting meaningful patterns from proteomic networks is a non-trivial task due to their size-complexity. This paper proposes a computational framework to efficiently discover topologically-similar patterns from large proteomic networks using Particle Swarm Optimization (PSO). PSO is a robust and low-cost optimization technique that demonstrated to work effectively on the complex, mostly sparse proteomic networks. The resulting topologically-similar patterns of close proximity are utilized to systematically predict new high-confidence protein-protein interactions (PPIs). The proposed PSO-based PPI prediction method (3PI) managed to predict high-confidence PPIs, validated by more than one computational/experimental source, through a proposed PPI knowledge transfer process between topologically-similar interaction patterns of close proximity. In three case studies, over 50% of the predicted interactions for EFGR, ERBB2, ERBB3, GRB2 and UBC are overlapped with publically available interaction databases, ∼80% of the predictions are found among the Top 1% results of another PPI prediction method and their genes are significantly co-expressed across different tissues. Moreover, the only single prediction example that did not overlap with any of our validation sources was recently experimentally supported by two PubMed publications."
845055,14018,9080,Encouraging creative thinking in robots improves their ability to solve challenging problems,2014,"Evolutionary algorithms frequently get stuck on local optima--and fail to find the global optimum--when local gradients do not point the search process toward the direction of the global optimum. A recent breakthrough called Novelty Search ameliorates this problem by enabling the search process to explore in every direction by encouraging the production of novel, or not-yet-seen, phenotypes (e.g. new robot behaviors). However, a problem with Novelty Search is that it can get lost on novelty plateaus wherein novel behaviors in offspring are not immediately produced by mutation and crossover (e.g. when a sequence of specific mutations is required to produce new behaviors, but the intermediate mutations are not rewarded because they do not produce novel behaviors). In such cases, Novelty Search and related approaches that reward behavioral diversity can get stuck. Here we introduce a new approach, borrowed from human psychology, that mitigates this problem: encouraging creative thinking. In addition to rewarding novel behavior, we encourage evolving neural networks to think differently by rewarding not-yet-seen firing patterns in hidden neurons, which we call the Creative Thinking Approach. We hypothesize that encouraging novel thinking can reward stepping stones toward new behaviors. On a variety of challenging robotic control problems from previous publications we demonstrate that, as problem difficulty increases, adding the Creative Thinking Approach increasingly improves performance over simply encouraging novel behaviors. Our results suggest that the Creative Thinking Approach could help improve the scale and complexity of problems that can be solved by evolutionary algorithms."
1682892,14018,9704,Identification of dynamic equivalents based on heuristic optimization for smart grid applications,2012,"Vulnerability assessment is one of the main tasks in a Self-Healing Grid structure, since it has the function of detecting the necessity of performing global control actions in real time. Due to the short-time requirements of real time applications, the eligible vulnerability assessment methods have to consider the improvement of calculation time. Although there are several methods capable of performing quick assessment, these techniques are not fast enough to analyze real large power systems in real time. Based on the fact that vulnerability begins to develop in specific regions of the system exhibiting coherent dynamics, large interconnected power systems can be reduced through dynamic equivalence in order to reduce the calculation time. A dynamic equivalent should provide simplicity and accuracy sufficient for system dynamic simulation studies. Since the parameters of the dynamic equivalent cannot be easily derived from the mathematical models of generators and their control systems, numerical identification methods are needed. Such an identification task can be tackled as an optimization problem. This paper introduces a novel heuristic optimization algorithm, namely, the Mean-Variance Mapping Optimization (MVMO), which provides excellent performance in terms of convergence behavior and accuracy of the identified parameters. The identification procedure and the level of accuracy that can be reached are demonstrated using the Ecuadorian-Colombian interconnected system in order to obtain a dynamic equivalent representing the Colombian grid."
1450013,14018,21102,A GA-based sequential fuzzy segmentation approach for classification of remote sensing images,2012,"In this paper we suggest an integrated spectral-spatial classification scheme for handling remotely sensed images. The method combines the results of supervised pixel-based classification with spatial information from unsupervised image segmentation. Pixel wise classification is implemented here by a fuzzy rule-based classifier using spectral/textural features of pixels. In regard to the image segmentation task, the following novelties are introduced. First, we apply a robust kernelized Fuzzy c-means clustering algorithm with spatial constraints (KFCM_S) to generate a fuzzy membership map. Operating on this transformed space, a Genetic Sequential Image Segmentation (GeneSIS) algorithm is next developed to partition the image into homogeneous regions. GeneSIS follows a sequential object extraction approach whereby at each iteration, a single object is extracted by invoking a GA-based object extraction algorithm. This module evaluates the fuzzy content of candidate regions, and through an effective fitness function design provides objects with optimal balance between fuzzy region coverage and consistency. The final classification results are obtained by performing fuzzy majority voting on the fuzzy degrees derived from pixel classifier over the segments obtained by GeneSIS. The validity of the proposed method is shown on the land cover classification of an agricultural area using a high-resolution IKONOS image, in terms of image segmentation quality and accuracy improvements compared to pixel wise classifiers."
2097263,14018,9704,Refining Genetic Algorithm twin removal for high-resolution protein structure prediction,2012,"To gain a better understanding of how proteins function a process known as protein structure prediction (PSP) is carried out. However, experimental PSP methods, such as X-ray crystallography and Nuclear Magnetic Resonance (NMR), can be time-consuming and inaccurate. This has given rise to numerous computational PSP approaches to try and elicit a protein's three-dimensional conformation. A popular PSP search strategy is Genetic Algorithms (GA). GAs allow for a generic search approach, which can provide a generic improvement to alleviate the need to redefine the search strategies for separate sequences. Though GA's working principles are remarkable, a serious problem that is inherent in the GA search process is the growth of twins or identical chromosomes. Therefore, enhanced twin removal strategies are crucial for any GA search solving hard-optimisation problems like PSP. In this paper we explain our high-resolution GA feature-based resampling PSP approach and propose a twin removal strategy to further enhance its prediction accuracy. This includes investigating the optimal chromosome correlation factor (CCF) for our approach and defining a pre-built structure library for twin removal. We have also compared our GA approach with the popular Monte Carlo (MC) method for PSP. Our results indicate that out of all the CCF values we tested a CCF value of 0.8 provided the best level of diversity within our GA population. It also generated, on average, more native-like structures than any of the other CCF values, and clearly demonstrated that twin removal is needed in PSP when using GAs to obtain more accurate results."
1100165,14018,9080,A comparison of two memetic algorithms for software class modelling,2013,"Recent research has demonstrated that the problem of class modelling within early cycle object orientated software engineering can be successfully tackled by posing it as a search problem to be tackled with meta-heuristics. This Search Based Software Engineering approach has been illustrated using both Evolutionary Algorithms and Ant Colony Optimisation to perform the underlying search. Each has been shown to display strengths and weaknesses - both in terms of how easily standard algorithms can be applied to the domain, and of optimisation performance. This paper extends that work by considering the effect of incorporating Local Search. Specifically we examine the hypothesis that within a memetic framework the choice of global search heuristic does not significantly affect search performance, freeing the decision to be made on other more subjective factors.   Results show that in fact the use of local search is not always beneficial to the Ant Colony Algorithm, whereas for the Evolutionary Algorithm with order based recombination it is highly effective at improving both the quality and speed of optimisation. Across a range of parameter settings ACO found its best solutions earlier than EAs, but those solutions were of lower quality than those found by EAs. For both algorithms we demonstrated that the number of constraints present, which relates to the number of classes created, has a far bigger impact on solution quality and time than the size of the problem in terms of numbers of attributes and methods."
729475,14018,21102,Fuzzy uncertainty assessment in RBF Neural Networks using neutrosophic sets for multiclass classification,2014,"In this paper we introduce a fuzzy uncertainty assessment methodology based on Neutrosophic Sets (NS). This is achieved via the implementation of a Radial Basis Function Neural-Network (RBF-NN) for multiclass classifica- tion that is functionally equivalent to a class of Fuzzy Logic Systems (FLS). Two types of uncertainties are considered: a) fuzziness and b) ambiguity, with both uncertainty types measured in each receptive unit (RU) of the hidden layer of the RBF-NN. The use of NS assists in the quantification of the uncertainty and formation of the rulebase; the resulting RBF- NN modelling structure proves to have enhanced transparency features to interpretation that enables us to understand the influence of each system parameter thorughout the parameter identification. The presented methodology is based on firstly constructing a neutrosophic set by calculating the associated fuzziness in each rule - and then use this information to train the RBF-NN; and secondly, an ambiguity measure that is defined via the truth and falsity measures related to each normalised consequence of the fuzzy rules within the RUs. In order to evaluate the individual ambiguity in the RUs and then the average ambiguity of the whole system, a neutrosophic set is constructed. Finally, the proposed method- ology is tested against two case studies: a benchmark dataset problem and a real industrial case study. On both cases we demonstrate the effectiveness of the developed methodology in automatically creating uncertainty measures and utilising this new information to improve the quality of the trained model. Index Terms—Neutrosophic sets (NS), Fuzzy Sets (FS), RBF Neural Network (RBF-NN), Receptive Unit (RU), un- certainty/indeterminacy, fuzziness, ambiguity, Charpy test Modelling."
756371,14018,9080,Agent fitness functions for evolving coordinated sensor networks,2011,"Distributed sensor networks are an attractive area for research in agent systems. This is due primarily to the level of information available in applications where sensing technology has improved dramatically. These include energy systems and area coverage where it is desirable for sensor networks to have the ability to self-organize and be robust to changes in network structure. The challenges presented when investigating distributed sensor networks for such applications include the need for small sensor packages that are still capable of making good decisions to cover areas where multiple types of information may be present. For example in energy systems, singular areas in power plants may produce several types of valuable information, such as temperature, pressure, or chemical indicators.   The approach of the work presented in this paper provides agent fitness functions for use with a neuro-evolutionary algorithm to address some of these challenges. In particular, we show that for self-organization and robustness to network changes, it is more advantageous to evolve individual policies, rather than a shared policy that all sensor units utilize. Further, we show that using a difference objective approach to the decomposition of system-level fitness functions provides a better target for evolving these individual policies. This is because the difference evaluation for fitness provides a cleaner signal, while maintaining vital information from the system level that implicitly promotes coordination among individual sensor units in the network."
2493800,14018,9080,Incremental gaussian model-building in multi-objective EDAs with an application to deformable image registration,2012,"Estimation-of-Distribution Algorithms (EDAs) build and use probabilistic models during optimization in order to automatically discover and use an optimization problems' structure. This is especially useful for black-box optimization where no assumptions are made on the problem being solved, which is characteristic of many cases in solving complex real-world problems. In this paper we consider multi-objective optimization problems with real-valued variables. Although the vast majority of advances in EDA literature concern single-objective optimization, advances have also been made in multi-objective optimization. In this paper we bring together two recent advances, namely incremental Gaussian model building to reduce the required population size and a mixture-based multi-objective framework that has specific methods to better facilitate model-building techniques that span multiple generations. Significantly faster convergence to the optimal Pareto front is achieved on 6 out of 7 artificial benchmark problems from literature. Although results on two of these problems show that building models with higher-order interactions between variables is required, these problems are still artificial. We therefore also consider a more realistic optimization problem in image processing, namely deformable image registration. For this problem too, our results show the need for processing interactions between problem variables, stressing the importance of studying such models."
1078632,14018,9704,A dynamic archive niching differential evolution algorithm for multimodal optimization,2013,"Highly multimodal landscapes with multiple local/global optima represent common characteristics in real-world applications. Many niching algorithms have been proposed in the literature which aim to search such landscapes in an attempt to locate as many global optima as possible. However, to locate and maintain a large number of global solutions, these algorithms are substantially influenced by their parameter values, such as a large population size. Here, we propose a new niching Differential Evolution algorithm that attempts to overcome the population size influence and produce good performance almost independently of its population size. To this end, we incorporate two mechanisms into the algorithm: a control parameter adaptation technique and an external dynamic archive along with a reinitialization mechanism. The first mechanism is designed to efficiently adapt the control parameters of the algorithm, whilst the second one is responsible for enabling the algorithm to investigate unexplored regions of the search space and simultaneously keep the best solutions found by the algorithm. The proposed approach is compared with two Differential Evolution variants on a recently proposed benchmark suite. Empirical results indicate that the proposed niching algorithm is competitive and very promising. It exhibits a robust and stable behavior, whilst the incorporation of the dynamic archive seems to tackle the population size influence effectively. Moreover, it alleviates the problem of having to fine-tune the population size parameter in a niching algorithm."
1976311,14018,9704,Strengthen accuracy of feature recognition via integration of ant colony detection and adaptive contour tracking,2011,"Reliable feature recognition is necessary in broad fields of computer vision and image processing. Edges often act as primary artifacts of visual data. Edge detection is to mark sharp changes of the intensity or brightness of digital images. Canny edge detection and ant colony optimization detection are two essential edge detection approaches. The former is susceptible to noises presented on source images. The information loss occurs when Gaussian smoothing is used to improve connectivity of Canny edge detection. Edges can be also detected via other approaches. To avoid edge suppression and feature deformity, ACO has been proposed for edge and contour detection against false detection, by which more intrinsic information will be extracted. The evolutionary computation oriented ACO scheme is a promising approach for feature capturing without the necessity of smoothing filters. It is among the most effective approaches for edge detection. However, it may give rise to broken pieces of numerous true edges occasionally. To further improve accuracy, contour tracking schemes are needed to achieve stable feature recognition. Some intelligent schemes are too complex to handle in real time, so a simple adaptive contour tracking scheme has been proposed which is combined with enhanced ACO schemes. This technology integration will result in the sufficient true edge representation together with well connected linkage, which can be easily extended to contour detection of binary, grayscale and true color images. Using quantitative metrics, an objective study is made to evaluate performance outcomes based on integration of the ACO schemes and adaptive contour tracking."
1132838,14018,21102,Self-evolving parameter-free Rule-based Controller,2012,"In this paper, a new approach for Self-evolving PArameter-free fuzzy Rule-based Controller (SPARC) is proposed. Two illustrative examples are provided aiming a proof of concept. The proposed controller can start with no pre-defined fuzzy rules, and does not need to pre-define the range of the output or control variables. This SPARC learns autonomously from its own actions while performing the control of the plant. It does not use any parameters, explicit membership functions, any off-line pre-training nor the explicit model (e.g. in a form of differential equations) of the plant. It combines the relative older concept of indirect adaptive control with the newer concepts of (self-)evolving fuzzy rule-based systems (and controllers, in particular) and with the very recent concept of parameter-free, data cloud and data density based fuzzy rule based systems (and controllers in particular). It has been demonstrated that a fully autonomously and in an unsupervised manner (based only on the data density and selecting representative prototypes/focal points from the control hyper-surface acting as a data space) it is possible generate a parameter-free control structure and evolve it in on-line mode. Moreover, the results demonstrate that this autonomous controller is effective (has comparative error and performance characteristics) to other known controllers, including self-learning ones, but surpasses them with its flexibility and extremely lean structure (small number of prototypes/focal points which serve as seeds to form parameter-free and membership function-free fuzzy rules based on them). The illustrative examples aim primarily proof of concept."
1498734,14018,9080,Which algorithm should i choose at any point of the search: an evolutionary portfolio approach,2013,"Many good evolutionary algorithms have been proposed in the past. However, frequently, the question arises that given a problem, one is at a loss of which algorithm to choose. In this paper, we propose a novel algorithm portfolio approach to address the above problem. A portfolio of evolutionary algorithms is first formed. Artificial Bee Colony (ABC), Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES), Composite DE (CoDE), Particle Swarm Optimization (PSO2011) and Self adaptive Differential Evolution (SaDE) are chosen as component algorithms. Each algorithm runs independently with no information exchange. At any point in time, the algorithm with the best predicted performance is run for one generation, after which the performance is predicted again. The best algorithm runs for the next generation, and the process goes on. In this way, algorithms switch automatically as a function of the computational budget. This novel algorithm is named Multiple Evolutionary Algorithm (MultiEA). Experimental results on the full set of 25 CEC2005 benchmark functions show that MultiEA outperforms i) Multialgorithm Genetically Adaptive Method for Single Objective Optimization (AMALGAM-SO); ii) Population-based Algorithm Portfolio (PAP); and iii) a multiple algorithm approach which chooses an algorithm randomly (RandEA). The properties of the prediction measures are also studied. The portfolio approach proposed is generic. It can be applied to portfolios composed of non-evolutionary algorithms as well."
2063498,14018,9704,GART: A genetic algorithm based real-time system scheduler,2011,"Hard real-time systems require that all jobs are assigned a deadline and the system is deemed to be correct only if all jobs complete execution at or before their deadlines. Such strict timing requirements add to the complexity of the scheduling problem. This complexity is exacerbated when the system is executed on a multiprocessor platform. Even so, scheduling overheads must be kept to a minimum in order for the runtime behavior to be predictable. Thus, real-time scheduling algorithms have the dual requirement of satisfying complex requirements while using fairly simple and straightforward logic. One way an algorithm may achieve this goal is to reduce the overhead due to preemption and migration by rearranging the schedule so as to increase the duration between preemptions. Unfortunately, determining how best to rearrange the jobs is an NP-Complete problem. Hence, we need to use heuristics when scheduling such systems. This leads us to ask a couple of questions. First, what is the best heuristic? Second, is the same heuristic best for all real-time systems? This paper uses a Genetic Algorithm to help us answer these questions. Our genetic algorithm based real-time system scheduler (GART) is based on the DP-Wrap scheduling algorithm. The genetic algorithm searches through a variety of candidate heuristics to determine the best heuristic for a given task set. Experimental results demonstrate that this approach is able to efficiently identify the best heuristic for all the systems we consider. Moreover, we find that the “best” heuristic does, in fact, depend of various system parameters."
1463177,14018,9704,Messy Genetic Algorithm for evolving mathematical function evaluating variable length gene regulatory networks,2013,"Evolutionary algorithms (EAs) have been successfully used in many studies for evolving both the structure and parameters of biological networks including gene regulatory networks that demonstrate different functionalities. However, most of these studies have used only mutation as the genetic operator in the evolutionary framework, perhaps due to the difficulty of implementing the crossover operation that generates the feasible network models. Nevertheless, crossover is considered to be the most powerful operator of EA which preserves the building blocks and promote quick convergence to a global optima. In this work we propose to use a Messy Genetic Algorithm (MGA) for evolving biological reaction networks that can calculate mathematical functions. The tactful encoding of MGA for reaction networks using a variable length chromosome, allows the use of crossover as well as mutation for the problem in hand that results in a fully functional EA. Earlier MGA has been used for solving many complex problems for which solution encoding is difficult. We used the proposed MGA for evolving different types of mathematical function calculating networks and the success was very encouraging. The evolved networks were able to calculate the target functions for mutually exclusive test data sets satisfactorily. Comparing with some other existing method based on Asexual Evolution (AE), the proposed method was superior in terms of different functions it could successfully evolve and the accuracy at which it could calculate those functions."
980259,14018,9704,Towards Efficient Multiobjective Optimization: Multiobjective statistical criterions,2012,"The use of Surrogate Based Optimization (SBO) is widely spread in engineering design to reduce the number of computational expensive simulations. However, “real-world” problems often consist of multiple, conflicting objectives leading to a set of equivalent solutions (the Pareto front). The objectives are often aggregated into a single cost function to reduce the computational cost, though a better approach is to use multiobjective optimization methods to directly identify a set of Pareto-optimal solutions, which can be used by the designer to make more efficient design decisions (instead of making those decisions upfront). Most of the work in multiobjective optimization is focused on MultiObjective Evolutionary Algorithms (MOEAs). While MOEAs are well-suited to handle large, intractable design spaces, they typically require thousands of expensive simulations, which is prohibitively expensive for the problems under study. Therefore, the use of surrogate models in multiobjective optimization, denoted as MultiObjective Surrogate-Based Optimization (MOSBO), may prove to be even more worthwhile than SBO methods to expedite the optimization process. In this paper, the authors propose the Efficient Multiobjective Optimization (EMO) algorithm which uses Kriging models and multiobjective versions of the expected improvement and probability of improvement criterions to identify the Pareto front with a minimal number of expensive simulations. The EMO algorithm is applied on multiple standard benchmark problems and compared against the wellknown NSGA-II and SPEA2 multiobjective optimization methods with promising results."
755259,14018,9704,Multi-objective evolutionary algorithm for variable selection in calibration problems: A case study for protein concentration prediction,2013,"This paper presents a multi-objective formulation for variable selection in calibration problems. The prediction of protein concentration on wheat is obtained by a linear regression model using variables obtained by a spectrophotometer device. This device measure hundreds of correlated variables related with physicochemical properties and that can be used to estimate the protein concentration. The problem is the selection of a subset informative and uncorrelated variables that help the minimization of prediction error. In this work we propose the use of two objectives in this problem: the prediction error and the number of variables in the model, both related to linear equations system stability. We proposed a multi-objective formulation using two multi-objective algorithms: the NSGA-II and the SPEA-II. Additionally we propose a final decision maker method to choice the final subset of variables from the Pareto front. For the case study is used wheat data obtained by NIR spectrometry where the objective is the determination of a variable subgroup with information about protein concentration. The results of traditional techniques of multivariate calibration as the Successive Projections Algorithm (SPA), Partial Least Square (PLS) and mono-objective genetic algorithm are presents for comparisons. For NIR spectral analysis of protein concentration on wheat, the number of variables selected from 775 spectral variables was reduced for just 10 in the SPEA-II algorithm. The prediction error decreased from 0.2 in the classical methods to 0.09 in proposed approach, a reduction of 45%. The model using variables selected by SPEA-II had better prediction performance than classical algorithms and full-spectrum partial least-squares (PLS)."
1386343,14018,21102,Toxicity risk assessment from heterogeneous uncertain data with possibility-probability distribution,2013,"Due to the advance of modern computing technology, decisions can be made based on all the existing related data instances scattered across multiple data storages, such that available information has been entirely taken into consideration. Particularly in the predictive toxicology domain, because of the heterogeneity of data sources, multiple data instances with respect to the same endpoint are usually inconsistent, and the quality (or reliability) of the data instances is typically different. Also, the quantity of data instances is often not sufficient to conduct a study using conventional statistics-based methods. This paper presents a novel risk analysis approach for chemical toxicity assessment which considers all the available heterogeneous data instances in the same time, assisted by their quality (or reliability) values. The system is developed on the basis of possibility-probability distribution, where the uncertainty of the approximated probability values based on traditional statistics methods is represented by possibility. The uncertainty considered herein is led not only by the statistics on limited small number of data instances, but also by the poor quality (or reliability) of data instances. The possibility-probability distribution is automatically computed from available data instances by employing a modified diffused-interior-outer-set model (where the reliability of data is considered) based on information diffusion theory. Toxicity value for a given chemical compound is then estimated as the fuzzy expected value based on the resulted possibility-probability distribution. Toxicity risk with respect to regulatory threshold is also introduced, in order to evaluate the probability of which the toxicity may be classified into a certain regulatory range. The proposed approach is applied to a real-world dataset to illustrate the utility and the potential of the approach in risk assessment of chemical toxicity."
1730651,14018,9080,Metaheuristic approaches to tool selection optimisation,2012,"In this paper we discuss our approach to solving the tool selection problem, specifically applied to rough machining. A simulation is used to evaluate tool sequences, which provides accurate values for tool paths and a 3D model of the final machined part. This allows for a largely unrestricted search using different tool types, making this approach more useful for real world applications than previous attempts at solving the problem. An exhaustive search of every valid tool sequence is executed and shows that assumptions present in related research can prevent the optimal solution from being discovered. Metaheuristic algorithms are used to traverse the search space because of its complex combinatorial properties. Four algorithms are tested - Genetic Algorithm, Stochastic Hill Climbing, Hybrid Genetic Algorithm and Random Restart Stochastic Hill Climbing. Evaluating their performance at coping with two competing demands, finding optimal solutions and keeping the number of potentially expensive evaluations low, it is shown that RRSHC performs best in terms of solution accuracy but at the greatest computational cost. SHC finds the optimum sequence less frequently but needs far fewer evaluations and the HGA lies somewhere in between, making it a good choice if the problem domain is not well-specified."
1248184,14018,9080,EvoSpace-i: a framework for interactive evolutionary algorithms,2013,"Evolutionary art (EvoArt) encompasses a variety of research devoted to the development of evolutionary systems that can help produce artistic artifacts in an automated or semi-automated process. Given the difficulty of evaluating subjective artistic preferences, one of the main approaches used by EvoArt researchers is interactive evolution where user input guides the search. However, despite the growth of EvoArt over recent years the research area still lacks a comprehensive software tool that can help in the development of EvoArt applications. Therefore, this work presents EvoSpace-i, an open source framework for the development of collaborative-interactive evolutionary algorithms for art and design. The main components of the framework are: (i)  Evospace , a population store for the development of cloud-based evolutionary algorithms, implemented using Re-dis key-value server; and an (ii)  Interactive web application  where end-users collaborate in a social network sharing, collecting, rating and ultimately evolving individuals. Individuals can be presented as multimedia elements or artistic artifacts (images, animations, sound) using the Processing programming language, a development language specifically aimed at artists. EvoSpace-i is designed to be easy to use and setup, allowing researchers, and more importantly artists, to quickly develop distributed and collaborative EvoArt applications. This paper presents the main details of EvoSpace-i and two example applications to illustrate the potential of the tool."
1380109,14018,9704,Improved CMA-ES with Memory based Directed Individual Generation for Real Parameter Optimization,2013,"Covariance Matrix Adaptation and Evolution Strategy (CMA-ES) is an efficient method of optimization that iteratively generates new individuals around an ever-adaptive recombination point. Although it ensures speed and high rate of exploitation, CMA-ES suffers a major drawback as the scheme of generating new members scattered around an influential mean may often lead to members drawn to local minima. The result is that while precision of better solutions increases, the ability to reform is lost. In this paper we incorporate a directional feature to the generation wise perturbation of individuals in standard version of CMA-ES that utilizes potentially useful information from previous generation to retain the influence of old recombination point. Coupled with a modified population size we attempt to form an algorithm that amalgamates the effectiveness of CMA-ES along with the ability to explore. The performance is tested on IEEE CEC (Congress on Evolutionary Computation) 2013 Special Session on Real-Parameter Optimization in 10, 30 and 50 dimensions. The results obtained clearly indicates that the proposed algorithm addressed as CMA-ES with Memory based Directed Individual Generation (CMA-ES-DIG) is able to perform excessively well on majority of the test cases in a statistically meaningful way."
2255146,14018,23827,Scope as a Leading Indicator for Managing Software Development,2011,"Historically there has been a significant problem with scope on software projects. This study looks to computational intelligence to facilitate the monitoring of scope to provide useful information to project managers. Software projects are difficult to manage with an alarming number of projects ending as highly expensive failures. A recent study of complex projects sponsored by the Project Management Institute found that soft issues such as gut feelings might provide early warning signs on problem projects. That study suggested that additional measures beyond cost and time must be found to identify project problems. Adding scope as an additional constraint to ascertain project status can be done through the use of Computational Intelligence (CI) tools. Scope for information technology projects has properties of imprecision and vagueness, therefore fuzzy logic would be an appropriate CI tool for monitoring scope. Other papers have illustrated how Zadeh's fuzzy data collector could be used to gather scope status as a practical application of the computing with words paradigm. A major benefit of monitoring scope is that scope status can be a leading indicator for managing software projects. Traditional project management measures time and cost which are lagging indicators, representing resources that are unrecoverable once spent. If the status of the scope constraint is collected and correctly analyzed before the expenditure of other fixed resources, there is the potential for scope to be a predictor, or leading indicator. One analysis technique would be to adjust the status for tasks on the critical path using linguistic hedges. By adding scope as a leading indicator, a faster recognition and response to problems in software development should increase project success."
1546105,14018,9080,There and back again: gene-processing hardware for the evolution and robotic deployment of robust navigation strategies,2014,"Navigation strategies represent some of the most intriguing examples of complex and intelligent behaviors in nature. Accordingly, they have been the focus of extensive research in animal behavior and in evolutionary robotics. However, engineering successes in harnessing the evolutionary dynamics that shape sophisticated navigation strategies remain limited. Here we describe a novel gene-processing architecture for digital organisms that enables the evolution of central-place-foraging strategies, such as those seen in honeybees and striped hyena. While previous studies have evolved navigation de novo, the resulting algorithms have been relatively fragile and difficult to translate into physical systems. In contrast, the strategies evolved in this study are highly congruous with those seen in nature: a single evolved foraging strategy incorporates periods of directed travel, fixed pattern search, cue response, and reorientation when outcomes do not match expected results. Additionally, the genetic architecture enabled rapid extraction of the underlying behavioral algorithm and transference to a robotic system, proving to be robust to issues of noise and scale that commonly plague such attempts. Accordingly, we demonstrate that the flexibility and interpretability of the new gene-processing hardware readily facilitate the creation, study, and utilization of naturalistic and deployable algorithms for functionally complex behaviors."
1583463,14018,9080,A novel population-based multi-objective CMA-ES and the impact of different constraint handling techniques,2014,"The Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) is a well-known, state-of-the-art optimization algorithm for single-objective real-valued problems, especially in black-box settings. Although several extensions of CMA-ES to multi-objective (MO) optimization exist, no extension incorporates a key component of the most robust and general CMA-ES variant: the association of a population with each Gaussian distribution that drives optimization. To achieve this, we use a recently introduced framework for extending population-based algorithms from single- to multi-objective optimization. We compare, using six well-known benchmark problems, the performance of the newly constructed MO-CMA-ES with existing variants and with the estimation of distribution algorithm (EDA) known as iMAMaLGaM, that is also an instance of the framework, extending the single-objective EDA iAMaLGaM to MO. Results underline the advantages of being able to use populations. Because many real-world problems have constraints, we also study the use of four constraint-handling techniques. We find that CMA-ES is typically less robust to these techniques than iAMaLGaM. Moreover, whereas we could verify that a penalty method that was previously used in literature leads to fast convergence, we also find that it has a high risk of finding only nearly, but not entirely, feasible solutions. We therefore propose that other constraint-handling techniques should be preferred in general."
1230174,14018,9704,Multi-objective tool sequence and parameter optimization for rough milling applications,2013,"In this paper a new, evolutionary multi-objective approach is introduced to tool sequence optimization in rough milling. Previous research has focused on the optimization of either the tool sequence or associated cutting parameters. Here, the tool sequence and a machining parameter, the cutting speeds of the individual tools, are simultaneously optimized, producing a Pareto front with both discrete and continuous properties. This is the first time that a multiple-tool multi-objective approach has been taken to tool selection, offering a set of solutions to the process planner. Three objectives are considered, thickness of excess stock, machining time and tooling costs. Unconstrained NSGA-II is used as the base algorithm but several preferential search strategies are tested to attempt to deal with constraints and guide search towards the Pareto optimal front. These include the established reference point (R-NSGA-ii) and weighted objective (WO) methods, as well as two novel techniques - “Guided Elitism” (GE) and “Precedential Objective Order Ranking” (PR). While WO performs best on average when assessed using the hypervolume indicator, the algorithms behave differently in terms of the quality and diversity of solutions found. A hybrid method using GE for exploration and PR for exploitation is shown to outperform the other techniques across all performance measures."
1405813,14018,9704,Automatic method for stock trading combining technical analysis and the Artificial Bee Colony Algorithm,2013,"There are many researches on forecasting time series for building trading systems for financial markets. Some of these studies have shown that it is possible to obtain satisfactory results, thereby contradicting the theory of Efficient Markets Hypothesis (EMH) that suggests that prices are randomly generated over time. This paper proposes an intelligent system based on historical closing prices that uses technical analysis, the Artificial Bee Colony Algorithm (ABC), a selection of past values (lags), nearest neighbor classification (k-NN) and its variation, the Adaptative Classification and Nearest Neighbor (A-k-NN). A very important step for time series prediction is the correct selection of the past observations (lags). Our method uses this strategy since it uses the k-NN and A-k-NN to decide on the buy and seIl points, combined with the ABC algorithm which is used to search for the best parameter settings of system and a good set of lags. This paper compares the results obtained by the proposed method with the buy and hold strategy and with other work that performed similar experiments with the same trading model and the same stocks. The key measure for performance comparison is the profitability in the analyzed period. The proposed method generates much larger profits compared to the other method and to the buy and hold strategy. Our method outperforms the other methods in thirteen out of the fifteen stocks tested, minimizing the risk of market ex pos ure."
2033142,14018,21102,Checking orthogonal transformations and genetic algorithms for selection of fuzzy rules based on interpretability-accuracy concepts,2011,"Fuzzy modeling is one of the most known and used techniques in different areas to emulate the behavior of systems and processes. In most cases, as in data-driven fuzzy modeling, these fuzzy models reach a high performance from the point of view of accuracy, but from other points of view, such as complexity or interpretability, the models can present a poor performance. Several approaches are found in the specialized literature to reduce the complexity and improve the interpretability of the fuzzy models. Here, a post-processing approach is taken into account via the definition of the rules selection criterion that aims to choose the most relevant rules according to the well-known accuracy-interpretability trade-off. This criterion is based on Orthogonal Transformations, here the QRP transformation is taking into consideration, and its parameters are tuned genetically. The main objective is to check the true significance, drawbacks and advantages the firing matrix of the rules, that is the foundation of the most usual approaches based on orthogonal transformations for the complexity reduction of the fuzzy models. A neuro-fuzzy system, FasArt (Fuzzy Adaptive System ART based), and several case studies, data sets from the KEEL Project Repository, are used to tune and check this approach. This neuro-fuzzy system generates Mamdani fuzzy rule based systems (FRBSs), each with its own particularities and complexities from the point of view of fuzzy sets and rule generation. NSGA-II is the MOEA tool used to tune the criterion parameters based on accuracy-interpretability ideas."
1648849,14018,9080,Evolutionary feature selection for classification: a plug-in hybrid vehicle adoption application,2012,"We present a real-world application utilizing a Genetic Algorithm (GA) for exploratory multivariate association analysis of a large consumer survey designed to assess potential consumer adoption of Plug-in Hybrid Electric Vehicles (PHEVs). The GA utilizes an intersection/union crossover operator, in conjunction with high background mutation rates, to achieve rapid multivariate feature selection. We experimented with two alternative fitness measures based on classification results of a naive Bayes quadratic discriminant analysis; one fitness function rewarded only for correct classifications, and the other penalized for the degree of misclassification using a quadratic penalty function. We achieved high classification accuracy for three different survey outcome questions (with 3-, 5-, and 7- outcome classes, respectively). The quadratic penalty function yielded better overall results, returning smaller feature sets and overall more accurate contingency tables of predicted classes. Our results help to identify what consumer attributes best predict their likelihood of purchasing a PHEV. These findings will be used to better inform an existing agent-based model of PHEV market penetration, with the ultimate aim of helping auto manufacturers and policy makers identify leverage points in the system that will encourage PHEV market adoption."
1455283,14018,9080,Visualization of genetic lineages and inheritance information in genetic programming,2013,"Many studies emphasize the importance of genetic diversity and the need for an appropriate tuning of selection pressure in genetic programming. Additional important aspects are the performance and effects of the genetic operators (crossover and mutation) on the transfer and stabilization of inherited information blocks during the run of the algorithm. In this context, different ideas about the usage of lineage and genealogical information for improving genetic programming have taken shape in the last decade.   Our work builds on those ideas by introducing an evolution tracking framework for assembling genealogical and inheritance graphs of populations. The proposed approach allows detailed investigation of phenomena related to building blocks, size evolution, ancestry and diversity. We introduce the notion of genetic fragments to represent subtrees that are affected by reproductive operators (mutation and crossover) and present a methodology for tracking such fragments using flexible similarity measures. A fragment matching algorithm was designed to work on both structural and semantic levels, allowing us to gain insight into the exploratory and exploitative behavior of the evolutionary process.   The visualization part which is the subject of this paper integrates with the framework and provides an easy way of exploring the population history. The paper focuses on a case study in which we investigate the evolution of a solution to a symbolic regression benchmark problem."
1274098,14018,9704,Learning feature hierarchies under reinforcement,2012,"Learning feature hierarchies, where larger features are composed of smaller re-used features, is an important area of study in object recognition and classification, and relates to processes in the human visual system. Established techniques are able to build deep hierarchies using neural networks, such as deep learning based on Restricted Boltzmann Machines, however approaches using other machine learning techniques involving reinforcement are not well established. An approach is presented that uses a form of Learning Classifier System to build a hierarchical feature network, for classification of images using the MNIST dataset. Larger scale representations of rules are composed of re-used smaller elements, in a network of 4,000 features and 2,000 rules. The feature network is developed autonomously, according to reinforcement of rules the features participate in. An implementation is shown using the ARCS classifier system to perform classification of images, using rules based on image templates. A second implementation uses rules with image templates constructed from a hierarchical feature network. This shows effective classification performance, but not as accurate as the best neural network and kernel methods. The implementation shows the ability to construct a hierarchical feature network under reinforcement, and its application to develop a rule population used by a Learning Classifier System. An alternative method for modifying existing rules is shown to substitute for standard mutation and crossover processes, to allow exploration of the rule space more closely related to gradient descent and cognitively related processes, rather than the genetic analogy commonly used in learning classifier systems."
2144467,14018,9080,Acceleration of grammatical evolution using graphics processing units: computational intelligence on consumer games and graphics hardware,2011,"Several papers show that symbolic regression is suitable for data analysis and prediction in financial markets. Grammatical Evolution (GE), a grammar-based form of Genetic Programming (GP), has been successfully applied in solving various tasks including symbolic regression. However, often the computational effort to calculate the fitness of a solution in GP can limit the area of possible application and/or the extent of experimentation undertaken. This paper deals with utilizing mainstream graphics processing units (GPU) for acceleration of GE solving symbolic regression. GPU optimization details are discussed and the NVCC compiler is analyzed. We design an effective mapping of the algorithm to the CUDA framework, and in so doing must tackle constraints of the GPU approach, such as the PCI-express bottleneck and main memory transactions.   This is the first occasion GE has been adapted for running on a GPU. We measure our implementation running on one core of CPU Core i7 and GPU GTX 480 together with a GE library written in JAVA, GEVA.   Results indicate that our algorithm offers the same convergence, and it is suitable for a larger number of regression points where GPU is able to reach speedups of up to 39 times faster when compared to GEVA on a serial CPU code written in C. In conclusion, properly utilized, GPU can offer an interesting performance boost for GE tackling symbolic regression."
814813,14018,9080,Salient object detection using learning classifiersystems that compute action mappings,2014,"Learning classifier systems (LCSs) are rule-based online evolutionary machine learning techniques that solve a problem by interacting with an environment. LCSs have been successfully used in various applications such as data mining, robot control and computer vision systems. Salient object detection is the task of automatically localizing the objects of interests in a scene by suppressing the background information, which facilitates various machine learning applications such as object segmentation, recognition and tracking. It is a difficult problem as natural scenes can often have objects with cluttered backgrounds (making it difficult to distinguish the object from background based on its features) or other complicating factors such as multiple objects. Existing saliency learning methods learn a single weight vector emphasizing the importance of each feature/attribute for the whole image dataset, hence losing generalization in the test phase when considering unseen images. LCS technique has the ability to learn weight sets for different types of images automatically. Hence, this paper investigates the application of LCS for learning image dependent feature fusion strategies for the task of salient object detection. Our LCS approach evolves generalized rules for a well known benchmark dataset consisting of 1000 images, of various types and difficulty levels, and outperforms a genetic algorithm based system that was previously state-of-the-art."
903108,14018,9704,EvoGraphDice: Interactive evolution for visual analytics,2012,"Visualization of large and complex datasets is a research challenge, especially in frameworks like industrial design, decision making and visual analytics. Interactive Evolution, used not only as an optimisation tool, but also as an exploration tool may provide some versatile solutions to this challenge. This paper presents an attempt in this direction with the EvoGraphDice prototype, developed on top of GraphDice, a general purpose visualization freeware for multidimensional visualization based on scatterplot matrices. EvoGraphDice interactively evolves compound additional dimensions, that provide new viewpoints on a multidimensional dataset. Compound dimensions are linear combination of terms based on the initial data dimensions, they are initialised with a Principal Component Analysis (PCA), and modified progressively by the interactive evolution process. Various interactions are available to the user, either in a transparent way, via a capture of mouse-clicks, or in a fully controlled manner, where the user has the opportunity to modify or include his own compound dimension in the evolved population, control the search space, or do some interactive queries. EvoGraphDice is tested on a synthetic dataset of dimension 6, where a known dependency is rediscovered via interactive manipulation. A second example is presented, based on a real dataset of dimension 13, provided by an industrial partner. Our experiments prove the potential of this interactive approach, and allow us to sketch future directions of development for the EvoGraphDice prototype."
2415407,14018,9704,Reinforcement learning with adaptive Kanerva coding for Xpilot game AI,2011,"The Xpilot-AI video game platform allows the creation of artificially intelligent and autonomous control agents. At the same time, the Xpilot environment is highly complex, with very many state variables and action choices. Basic reinforcement learning (RL) techniques are somewhat limited in their application when dealing with such large state- and action-spaces, since the repetition of exposure that is key to their value updates can proceed very slowly. To solve this problem, state-abstractions are often generated, allowing learning to move more quickly, but often requiring the programmer to hand-craft state representations, reward functions, and action choices in an ad hoc manner. We apply an automated technique for generating useful abstractions for learning, adaptive Kanerva coding. This method employs a small sub-set of the original states as a proxy for the full environment, updating values over the abstract representative prototype states in a manner analogous to Q-learning. Over time, the set of prototypes is adjusted to provide more effective coverage and abstraction, again automatically. Our results show that this technique allows a simple learning agent to double its survival time when navigating the Xpilot environment, using only a small fraction of the full state-space as a stand-in and greatly increasing the potential for more rapid learning."
2484768,14018,21102,Improving estimation accuracy of the COCOMO II using an adaptive fuzzy logic model,2011,"Software development time and cost estimation are the process of estimating the most realistic use of time and cost required for developing a software. It is one of the biggest challenges in the area of software engineering and project management, in the last decades. The software estimates are difficult to obtain due to incomplete software information is available in the early phase of software development process. Insufficient software information causes inaccuracy in software attributes. Thus, the vagueness and uncertainty of the software attributes is the main reason of inaccuracy of software estimates. Software cost estimation models such as regression model, expert judgment, SLIM, and COCOMO require accurate software attributes and long term estimation process, which are not completely achievable in early phase of software development process. Soft computing techniques such as fuzzy logic can reduce the vagueness and uncertainty of software attributes. Therefore, it may consider as alternative to decrease the inaccuracy of software estimates. This research aims to utilise an adaptive fuzzy logic model to improve the accuracy of software time and cost estimation. Using advantages of fuzzy set and fuzzy logic can produce accurate software attributes which result in precise software estimates. The Two-Dimension Gaussian Membership Function (2-D GMF) was used in the fuzzy model to make software attributes smoother in terms of the range of values. The COCOMO I, NASA98 data sets; and four project data from a software company in Malaysia were used in the evaluation of the proposed Fuzzy Logic COCOMO II (FL-COCOMO II). The evaluation of the obtained results, using Mean of Magnitude of Relative Error (MMRE) and PRED(25%) evaluation techniques, showed that the FL-COCOMO II produced the MMRE less than the original COCOMO and the value of PRED(25%) in the Fuzzy-COCOMO II is higher than the original COCOMO. Furthermore, the FL-COCOMO II showed 8.03% improvement in terms of estimation accuracy using MMRE when compared with the original COCOMO. Using advantages of fuzzy logic such as accurate estimation; adaption; understandability, and etc., can improve the accuracy of software estimates."
1096944,14018,8806,Approximate matching over biological RDF graphs,2012,"In the last few years, the amount of biological interaction data discovered and stored in public databases (e.g., KEGG [2]) considerably increased. To this aim, RDF is a powerful representation for interactions (or pathways), since they can be modeled as directed graphs, often referred to as  biological networks , where nodes represent cellular components and the (labeled or unlabeled) edges correspond to interactions among components. Often for a given organism some components are known to be linked by well studied interactions. Such groups of components are called  modules  and they can be represented by sub-graphs in the corresponding biological network model. At today, one of the most important problems for biologists is that of querying the interaction dataset of an organism by a specific module exploited as the query. The aim is to discover if such a module is contained in the input interaction dataset. In this scenario biological variations (e.g. insertions and/or deletions of both nodes and edges) due to the evolution need to be considered in the search process, thus that  approximate matching  is more effective than  exact matching . Typically the problem is reconducted to the search of all the subgraphs in the biological network (i.e. the input graph database) that are isomorphic to the given module (i.e. the query graph). Since such a test is known to be an NP-hard problem, many proposals introduce heuristics (i.e. similarity or distance measurements) and particular indexing structures to reduce the overall complexity (e.g., [3, 4])."
981485,14018,9080,Evaluation of the performance of evolutionary algorithms for optimization of low-enthalpy geothermal heating plants,2012,"In this paper, we present the application of Evolutionary Algorithms (EAs) and linear programming for minimizing thermal impacts in the ground by operating a low-enthalpy geothermal plant with a field of multiple borehole heat exchangers (BHEs). The new methodology is demonstrated on two synthetic case studies with 36 BHEs that are grounded in reality and operated to produce given seasonal heating energy demand. We compare the performance of six different Evolutionary Algorithms (EAs) (two Differential Evolution variants, Particle Swarm Optimization, two Evolution Strategy based Algorithms, real valued Genetic Algorithm) and Monte-Carlo random search to find the optimal BHE positions. Additionally, linear programming is applied to adjust the energy extraction (loads) for the individual BHEs in the field. Both optimization steps are applied separately and in combination, and the achieved system improvements are compared to the conditions for the non-optimized case. The EAs were able to find constellations that cause less pronounced temperature changes in the subsurface (18% - 25%) than those associated with non-optimized BHE fields. Further, we could show that exclusive optimization of BHE energy extraction rates delivers slightly better results than the optimization of BHE positions. Combining both optimization approaches is the best choice and, ideally, adjusts the geothermal plant."
1982204,14018,21102,An efficient hybrid particle swarm optimization for the Job Shop Scheduling Problem,2011,"This paper proposes a hybrid particle swarm optimization algorithm for solving Job Shop Scheduling Problems (JSSP) to minimize the maximum makespan. A new hybrid heuristic, based on Particle Swarm Optimization (PSO), Tabu Search (TS) and Simulated Annealing (SA), is presented. PSO combines local search (by self-experience) with global search (by neighboring experience), achieving a high search efficiency. TS uses a memory function to avoid being trapped at a local minimum, and has emerged as an effective algorithmic approach for the JSSP. This method can also be referred to as calculation of the horizontal direction. SA employs certain probability to avoid becoming trapped in a local optimum and the search process can be controlled by the cooling schedule (also known as calculation of vertical direction). By reasonably combining these three different search algorithms, we develop a robust, fast and simply implemented hybrid optimization algorithm HPTS (Hybrid of Particle swarm optimization, Tabu search and Simulated annealing). This hybrid algorithm is applied to the standard benchmark sets and compared with other approaches. The experimental results show that the proposed algorithm could obtain the high-quality solutions within relatively short computation time. For 6 of 43 instances, new upper bounds among the unsolved problems are found in a short time in HPTS."
2136461,14018,9704,An Adaptive Strategy for Assortative Mating in Genetic Algorithm,2013,"In any traditional Genetic Algorithm (GA), recombination is a dominant search operator and capable of exploring the search space by sharing genetic information among the individuals in the population. However, a simple application of recombination alone is insufficient to guide convergence to an optimal solution. The selection of parents for recombination operation has a significant role in guiding the evolution towards the optimal solution and also for maintaining genetic diversity to avoid getting trapped in local minima. A non-random mating mimics the mechanism of reproduction in nature and is effective in maintaining diversity in population. This paper proposes a new strategy for selection of mating pairs based on a type of non-random mating called as assortative mating. The proposed mate selection scheme conserves the merits of both positive and negative assortative mating in a controlled manner by allowing mating between individuals having both similar and dissimilar phenotypes. For effective cross-over, it maintains genetic diversity in population by distributing the recombination among dissimilar individuals. Furthermore, it ensures the preservation and propagation of useful genetic information to the later stages of search by the selection of mates having similar phenotypes. Experimental results, using not only the five widely used benchmark functions but also twenty newly developed modified functions, are reported. The results show significant improvements in the convergence characteristics of the proposed mating strategy over existing nonrandom mating techniques."
1616472,14018,9704,Short adjacent repeat identification based on Chemical Reaction Optimization,2012,"The analysis of short tandem repeats (STRs) in DNA sequences has become an attractive method for determining the genetic profile of an individual. Here we focus on a more general and practical issue named short adjacent repeats identification problem (SARIP), which is extended from STR by allowing short gaps between neighboring units. Presently, the best available solution to SARIP is BASARD, which uses Markov chain Monte Carlo algorithms to determine the posterior estimate. However, the computational complexity and the tendency to get stuck in a local mode lower the efficiency of BASARD and impede its wide application. In this paper, we prove that SARIP is NP-hard, and we also solve it with Chemical Reaction Optimization (CRO), a recently developed metaheuristic approach. CRO mimics the interactions of molecules in a chemical reaction and it can explore the solution space efficiently to find the optimal or near optimal solution(s). We test the CRO algorithm with both synthetic and real data, and compare its performance in mode searching with BASARD. Simulation results show that CRO enjoys dozens of times, or even a hundred times shorter computational time compared with BASARD. It is also demonstrated that CRO can obtain the global optima most of the time. Moreover, CRO is more stable in different runs, which is of great importance in practical use. Thus, CRO is by far the best method on SARIP."
1486390,14018,9080,Novelty search creates robots with general skills for exploration,2014,"Novelty Search, a new type of Evolutionary Algorithm, has shown much promise in the last few years. Instead of selecting for phenotypes that are closer to an objective, Novelty Search assigns rewards based on how different the phenotypes are from those already generated. A common criticism of Novelty Search is that it is effectively random or exhaustive search because it tries solutions in an unordered manner until a correct one is found. Its creators respond that over time Novelty Search accumulates information about the environment in the form of skills relevant to reaching uncharted territory, but to date no evidence for that hypothesis has been presented. In this paper we test that hypothesis by transferring robots evolved under Novelty Search to new environments (here, mazes) to see if the skills they've acquired generalize. Three lines of evidence support the claim that Novelty Search agents do indeed learn general exploration skills. First, robot controllers evolved via Novelty Search in one maze and then transferred to a new maze explore significantly more of the new environment than non-evolved (randomly generated) agents. Second, a Novelty Search process to solve the new mazes works significantly faster when seeded with the transferred controllers versus randomly-generated ones. Third, no significant difference exists when comparing two types of transferred agents: those evolved in the original maze under (1) Novelty Search vs. (2) a traditional, objective-based fitness function. The evidence gathered suggests that, like traditional Evolutionary Algorithms with objective-based fitness functions, Novelty Search is not a random or exhaustive search process, but instead is accumulating information about the environment, resulting in phenotypes possessing skills needed to explore their world."
1063873,14018,9704,Reference System Architecture for Trade Promotion Management: Leveraging Business Intelligence Technologies and Decision Support Systems,2011,"Working towards gaining competitive advantage and establishing stable relationships with their supply chain intermediaries, fast moving consumer goods companies are currently focusing their attention on intelligent, goal-based funds investment. Traditional trade promotion management systems (TPMS), however, fail to provide the complex, yet flexible analytical functionality required for accurate tracing of promotional effectiveness and optimization of trade promotional spending. Most commonly encountered issues range from unreliable, inaccurate and inconsistent data and low user friendliness of the system to complicated, inflexible and time-consuming reporting. In this paper, we design the first reference system architecture integrating business intelligence technologies with trade promotion management systems, meeting the business needs and rendering high-quality reporting services to the end users. Our proposed architecture incorporates a reference trade promotion management process and is the first architecture model extending the in-built functionality of TPMS with BI tools. We outline the theoretical foundations and the design principles of the architecture and evaluate its validity with 19 interviews and one case study following the TOGAF ADM steps. Despite the time restriction for further validation, the architecture is based on extensive academic and industry literature and the evaluation confirmed its potential of solving common issues of current trade promotion management systems."
945349,14018,9080,Sociotechnical simulation and evolutionary algorithm optimization for routing siren vehicles in a water distribution contamination event,2011,"Water distribution contamination incidents occur when a poisonous chemical or pathogen is introduced intentionally or accidentally to the pipe network that delivers potable water to the residents of a municipality. These events pose a challenge to decision makers, who should quickly identify a threat and the most effective response actions for protection of public health. In these events, the dynamic interactions among consumers, utility managers, public health officials, and the water distribution pipe network affect the emergent exposure of consumers. An Agent-Based Modeling (ABM) approach is used to simulate the interactions among agents and flow conditions in the water distribution system to provide an understanding of effects of dynamic and adaptive behaviors on public health. While utility operators can protect consumers using a wide range of protective and mitigative responses, routing of siren vehicles can be effective as consumers are warned about a contaminant in the water system and respond by stopping different water activities, such as drinking water. Development of crisis management routing strategies, which are a set of routes to best warn and protect consumers from exposure, is enabled through a new simulation-optimization framework. A genetic algorithm and the ABM are coupled to find routes for siren vehicles that minimize the number of consumers who are exposed to contaminated tap water. The framework is demonstrated for an illustrative case study, a mid-sized virtual city, to identify efficient routes for protecting public health."
735901,14018,9704,Implementation of Web-based Consensus Support System for Campus Greening Project: Preliminary Results,2011,"In this paper, we present an implementation of a web-based consensus support system for the campus greening project at the Nagoya Institute of Technology. Recently, WWW technology has enabled us to share information and gather opinions on the Internet. However, on such systems, it is difficult to get a consensus because there is less focus on converging discussions and arguments. Thus, there is a lot of need to support gathering and converging of opinions to facilitate forming a consensus on the WWW. On the other hand, in the field of civil engineering, e.g., city planning and public enterprises, a lot of workshops have been held to gather opinions not only from experts but from civilians as well. The problem with such workshops is that there are concrete limitations with time, space, and cost. Thus, in this paper, we propose using a web-based system that can overcome such limitations as an alternative methodology to workshops. In this paper, we focus on support for the Campus Greening Project at the Nagoya Institute of Technology, where students and teachers are collaboratively planning to place high-tech grass panels on the campus. The system supports them in forming a consensus on where to put the grass panels. Concretely, in this system we utilize Rich Internet Application (RIA) technologies and Google Maps API so that the participants can intuitively operate the system. Here the system can effectively support creating and evaluating alternatives, and voting on the final agreement. This system enables us to clarify the evaluations of the participants and show the final agreement while considering the entire cost. Our experimentation demonstrates that our system can effectively support forming a consensus and has sufficient usability."
1134814,14018,21102,Generalization of the Fuzzy Integral for discontinuous interval- and non-convex interval fuzzy set-valued inputs,2013,"The Fuzzy Integral (FI) is a powerful approach for non-linear data aggregation. It has been used in many settings to combine evidence (typically objective) with the known “worth” (typically subjective) of each data source, where the latter is encoded in a Fuzzy Measure (FM). While initially developed for the case of numeric evidence (integrand) and numeric FM, Grabisch et al. extended the FI to the cases of continuous intervals and normal, convex fuzzy sets (i.e., fuzzy numbers). However, in many real-world applications, e.g., explosive hazard detection based on multi-sensor and/or multi-feature fusion, agreement based modeling of survey data, anthropology and forensic science, or computing with respect to linguistic descriptions of spatial relations from sensor data, discontinuous interval and/or non-convex fuzzy set data may arise. The problem is no theory and algorithm currently exists for calculating the FI for such a case. Herein, we propose an extension of the FI to discontinuous interval- and convex normal Interval Fuzzy Set (IFS)-valued integrands (with a numeric FM). Our approach arises naturally from analysis of the Extension Principle. Further, we provide a computationally efficient approach to computing the proposed extension based on the union of the FIs on the combinations of continuous sub-intervals and we demonstrate the approach using examples for both the Choquet FI (CFI) and Sugeno FI (SFI)."
775740,14018,9704,Benchmark results for a simple hybrid algorithm on the CEC 2013 benchmark set for real-parameter optimization,2013,"In this article, we benchmark a new hybrid algorithm for continuous optimization on the 28 functions for the CEC 2013 special session and competition on real-parameter optimization. Our algorithm makes a loose coupling of (i) IPOP-CMA-ES, an advanced evolution strategy with covariance matrix adaptation integrated with an occasional restart strategy and increasing population size, and (ii) an iterated local search (ILS) algorithm that repeatedly applies a different local search from CMA-ES to perturbations of previous high-quality solutions. The central idea of the hybrid algorithm is to let IPOP-CMA-ES and ILS compete in an initial competition phase and then the winner of the two algorithms is deployed for the remainder of the computation time. A cooperative element between the two algorithms is implemented through a solution exchange from IPOP-CMA-ES to ILS. Hence, one may classify this algorithm as a loosely coupled cooperative-competitive algorithm for continuous optimization. We compare the computational results of this hybrid algorithm to the default version and a tuned version of IPOP-CMA-ES to illustrate the improvement that is obtained through this hybrid algorithm. This comparison is interesting since IPOP-CMA-ES is a state-of-the-art algorithm which somehow has become a standard benchmark to compare against for any new algorithmic proposals for continuous optimization. Our computational results show that the proposed hybrid algorithm performs significantly better than the default and tuned IPOP-CMA-ES variants on the problems of dimension 30 and 50. Thus, these results also indicate that the hybrid algorithm reaches very high performance on the CEC 2013 benchmark set."
1504597,14018,9080,The SEEDS platform for evolutionary and ecological simulations,2012,"Over the past few decades, evolutionary computation (EC) has grown substantially in use for biologists and engineers alike. Its transparency makes it an indispensable tool for studying evolutionary- and ecological dynamics, and it has provided researchers with new insights that would be tremendously difficult, if not impossible, to gain using natural systems. In addition, EC has proven to be a powerful search algorithm for engineering applications, and has produced numerous novel and human-competitive solutions to complex problems. Although several well-established packages are readily available, it seems that when most users harness the power of evolutionary computation, they do so using home-grown solutions. This can likely be attributed to the ease with which simple models are created, the user's need for customization, and the sizeable learning barrier imposed by available solutions, as well as difficulties in extending them.   We present SEEDS, a modular, open-source platform for conducting evolutionary computation experiments. SEEDS provides a simple, flexible, and extensible foundation that enables users with minimal programming experience to perform complex evolutionary and ecological simulations without having to first implement core functionality. In addition, SEEDS provides the tools necessary to make sharing data and reproducing experiments both easy and convenient."
1042383,14018,9080,Multi-user detection in multi-carrier CDMA wireless broadband system using a binary adaptive differential evolution algorithm,2013,"Multi-Carrier Code Division Multiple Access (MC-CDMA) is an emerging wireless communication technology that incorporates the advantages of Orthogonal Frequency Division Multiplexing (OFDM) into the original Code Division Multiple Access (CDMA) technique. But it suffers from the inherent defect called Multiple Access Interference (MAI) due to inappropriate cross-correlation possessed by the different user codes. To reduce MAI, the multi-user detection (MUD) technique has already been proposed in which MAI is treated as noise. Due to high computational cost incorporated by the optimal MUD detector with increasing number of users, researchers are looking for sub-optimal MUD solutions. This paper proposes a binary adaptive Differential Evolution algorithm with a novel crossover strategy (MBDE_pBX) for multi-user detection in a synchronous MC-CDMA system. Since MUD detection in MC-CDMA systems is a problem in binary domain, a binary encoding rule is introduced which converts a binary domain problem of any number of dimensions into a 4-dimensional continuous domain problem. The simulation results show that this new binary Differential Evolution variant can achieve superior bit error rate (BER) performance within much lower optimum solution detection time outperforming its competitors as well as achieving 99.62% reduction in computational complexity as compared to the MUD scheme using exhaustive search."
1151522,14018,9080,Combining PSO and local search to solve scheduling problems,2011,"Intelligent manufacturing is associated with a large number of complex optimization problems and for this reason has got a considerable research attention over the last decades. Most of these problems are of combinatorial nature and have been proved to be NP-complete. This paper deals with the flow shop scheduling problem (FSSP) and the Job Shop Scheduling Problem (JSSP). The objective of these problems is to find an appropriate sequence to minimize the makespan, which are defined as the time for completing a final operation. One major challenging issue is how to obtain the high-quality global optimum. In order to refrain from the premature convergence and being easily trapped into local optimum, we are motivated to find high-quality solutions in a reasonable computation time by exploiting Particle Swarm Optimization (PSO), Tabu Search (TS) and Simulated Annealing (SA). We propose a new multi-structural hybrid evolutionary framework, and derive HPTS algorithm as its extension. Extensive experiments on different scale benchmarks validate the effectiveness of our approaches, compared with other well-established methods. The experimental results show that new upper bounds of the unsolved problems are achieved in a relatively reasonable time. For example, in 30 Tailland's and 43 OR-Library benchmarks, 7 new upper bounds and 6 new upper bounds are obtained by the HPTS algorithm, respectively."
2346870,14018,21102,Incident detection from Tweets by neural network with GPGPU,2012,"Twitter is an online social network service to supply the place to be released the short sentences as free. Recently, this service has a few hundred million users, and we can collect their Tweets easily. In this paper, we propose the climatic hazard detection method by using Twitter as a social sensor and by using neural network as a machine learning method. However the data size of the text classification is too large. So, it is required to propose the high-speed learning algorithm. On another front, GPU is the dedicated circuit to draw the graphics, so it has a characteristic that the many simple arithmetic circuits are implemented. This characteristic is hoped to apply the massive parallelism not only graphic processing. In this paper, the neural network is applied to be faster by using GPU. Some methods are considered, and the simple one is employed as comparison to compare with the proposed methods. As the result, the proposed method is 6 times faster than comparison method. This neural network learning method is used for the text classification. 35,379 Tweets were gathered and these were deconstructed to the words by using morphological analysis. The feature vectors were constructed by using the nouns and adjectives selected from the words of the Twitter. We used 860 dimensions feature vectors and classified the positive data or negative. As the result of the classification, we achieved 68 percent accuracy to classify the Tweet data."
1190884,14018,9704,Continuous game dynamics on populations with a cycle structure under weak selection,2012,"Understanding the emergence of cooperation among selfish individuals is an enduring conundrum in evolutionary biology, which has been studied using a variety of game theoretical models. Most of the previous studies presumed that interactions between individuals are discrete, but behavior in real systems can hardly be expected to have this dramatically discrete nature. In addition, existing research on continuous strategy games mostly focus on infinite well-mixed populations. Especially, there is few theoretical work on their evolutionary dynamics in structured populations. In the previous work [1], we theoretically studied the game dynamics of continuous strategies in a spatially structured population with its average degree k ≥ 3 under weak selection. Here, we study their evolutionary dynamics under weak selection on a cycle (k = 2), where each individual only interacts with its two immediate neighbors. Using the concept of fixation probability, we derive exact conditions for natural selection favoring one strategy over another for three update rules, called ‘birth-death’, ‘death-birth’, and ‘imitation’. It shows that for continuous strategy games, the same conditions are derived; especially, the simple rule b/c > k is valid as well, where b/c is the benefit-to-cost ratio of an altruistic act. In addition, we present a network gain decomposition of the game equilibrium, which might provide a new view of network reciprocity, one of five mechanisms for evolution of cooperation."
1641250,14018,9080,"Extending learning classifier system with cyclic graphs for scalability on complex, large-scale boolean problems",2013,"Evolutionary computational techniques have had limited capabilities in solving large-scale problems, due to the large search space demanding large memory and much longer training time. Recently work has begun on automously reusing learnt building blocks of knowledge to scale from low dimensional problems to large-scale ones. An XCS-based classifier system has been shown to be scalable, through the addition of tree-like code fragments, to a limit beyond standard learning classifier systems. Self-modifying cartesian genetic programming (SMCGP) can provide general solutions to a number of problems, but the obtained solutions for large-scale problems are not easily interpretable. A limitation in both techniques is the lack of a cyclic representation, which is inherent in finite state machines. Hence this work introduces a state-machine based encoding scheme into scalable XCS, for the first time, in an attempt to develop a general scalable classifier system producing easily interpretable classifier rules. The proposed system has been tested on four different Boolean problem domains, i.e. even-parity, majority-on, carry, and multiplexer problems. The proposed approach outperformed standard XCS in three of the four problem domains. In addition, the evolved machines provide general solutions to the even-parity and carry problems that are easily interpretable as compared with the solutions obtained using SMCGP."
899883,14018,9704,Large scale optimization by differential evolution with landscape modality detection and a diversity archive,2012,"In this study, the performance of Differential Evolution with landscape modality detection and a diversity archive (LMDEa) is reported on the set of benchmark functions provided for the CEC2012 Special Session on Large Scale Global Optimization. In Differential Evolution (DE), large population size, which is much larger than the number of decision variables in problem to be solved, is adopted in order to keep the diversity of search. However, it is difficult to adopt such large size to solve large scaled optimization problems because the population size will become too large and the search efficiency will degrade. In this study, we propose to solve large scale optimization problems using small population size and a large archive for diversity. Also, we propose simple control of scaling factor by observing landscape modality of search points in order to keep diversity. The landscape of a problem to be optimized is often unknown and the landscape is changing dynamically while the search process proceeds. In LMDEa, some points on a line connecting the centroid of search points and a search point are sampled. When the objective values of the sampled points are changed decreasingly and then increasingly, it is thought that one valley exists. If there exists only one valley, the landscape is unimodal and small scaling factor is adopted. Otherwise, large scaling factor is adopted. Also, the sampled points realize global search in the region spanned by all search points and realize local search near the best search point. The effect of the proposed method is shown by solving the benchmark functions."
1150215,14018,9704,Generation of realistic mobility for VANETs using genetic algorithms,2012,"The first step in the evaluation of vehicular ad hoc networks (VANETs) applications is based on simulations. The quality of those simulations not only depends on the accuracy of the network model but also on the degree of reality of the underlying mobility model. VehILux—a recently proposed vehicular mobility model, allows generating realistic mobility traces using traffic volume count data. It is based on the concept of probabilistic attraction points. However, this model does not address the question of how to select the best values of the probabilities associated with the points. Moreover, these values depend on the problem instance (i.e. geographical region). In this article we demonstrate how genetic algorithms (GAs) can be used to discover these probabilities. Our approach combined together with VehILux and a traffic simulator allows to generate realistic vehicular mobility traces for any region, for which traffic volume counts are available. The process of the discovery of the probabilities is represented as an optimisation problem. Three GAs—generational GA, steady-state GA, and cellular GA—are compared. Computational experiments demonstrate that using basic evolutionary heuristics for optimising VehILux parameters on a given problem instance permits to improve the model realism. However, in some cases, the results significantly deviate from real traffic count data. This is due to the route generation method of the VehILux model, which does not take into account specific behaviour of drivers in rush hours."
1781730,14018,21102,T-S fuzzy contact state recognition for compliant motion robotic tasks using gravitational search-based clustering algorithm,2013,"In this paper, we address the problem of contact state recognition for compliant motion robotic systems. The wrench (Cartesian forces and torques) and pose (position and orientation) of the manipulated object in different Contact Formations (CFs) are firstly captured during a certain task execution. Then for each CF, we develop an efficient Takagi-Sugeno (T-S) fuzzy inference system that can model that specific CF using the available input (wrench and pose) - output (the desired model output for each CF) data. The antecedent part parameters are computed using the Gravitational Search- based Fuzzy Clustering Algorithm (GS- FCA) and the consequent parts parameters are tuned by the Least Mean Square (LMS). Excellent mapping and hence recognition capabilities can be expected from the suggested scheme. In order to validate the approach; experimental test stand is built which is composed of a KUKA Light Weight Robot (LWR) manipulating a cube rigid object that interacts with an environment composed of three orthogonal planes. The manipulated object is rigidly attached to the robot arm. The robot is programmed, by a human operator, to move in different CFs and for each CF, the wrench and pose readings are captured via the Fast Research Interface (FRI) available at the KUKA LWR. Using the suggested approach, excellent modeling is obtained for different CFs during the robot task execution. A comparison with the available CF recognition approaches is also performed and the superiority of the suggested scheme is shown."
2515116,14018,9704,A python-based design-by-contract evolutionary algorithm framework with augmented diagnostic capabilities,2013,"Evolutionary algorithms are a class of algorithms that try to mimic natural, biological evolution a la Darwinian natural selection, to compute solutions to a given problem. They are especially useful when no well known strategies for computing solutions to such a problem exist. Evolutionary algorithms begin by creating a collection (population) of candidate solutions to the problem at hand; and through repeated application of genetic operators such as crossover and mutation, they iterate over multiple generations of this population, until they eventually converge onto an attractive solution. One important problem facing code implementing Evolutionary Algorithms is that due to the dynamic nature of the individual chromosomes in a population, simple coding errors lead to complex bugs that are difficult to both diagnose and debug. This problem is only exacerbated when attempting to develop the algorithms in a dynamically typed language such as Python. This paper presents a novel Evolutionary Algorithm framework for the Python programming language that implements design-by-contract, a paradigm in which each function and class must follow a contractual set of pre-conditions and post-conditions. Failure to follow the contract causes an error condition identifying the violated clause, thereby catching bugs earlier in the development process and in a more descriptive manner."
2092909,14018,9704,An expressive GL-2 grammar for representing story-like scenarios,2012,"This paper extends our previous work on evolving stories towards a computational platform for automatic scenario generation. In particular, we address a shortcoming of our earlier work relating to scenario representation: the regular story plot grammar. The use of this grammar, which only captures causal relationships between plot elements from the point of view of a single story character, resulted in the generation of stories that are always associated with one main character only, limiting the scalability of the approach. In addition, the regular grammar employed is not suitable for representing practical scenarios since a practical scenario may not require a character at all. To overcome these problems, we propose a new approach to scenario representation. Firstly, we introduce a set of scenario building blocks based on narrative theory. As a result, a scenario can be represented as a network of these building blocks that captures various relationships in the scenario. The task of generating scenarios is then transformed to the task of generating networks of these building blocks. Secondly, we develop two network representation languages extending Boers' GL-2 graph representation systems to describe networks with edges of more than one type and edges between two groups of nodes. Thirdly, a set of two context-free grammars is introduced to generate sentences, i.e. scenarios in these languages. Finally, we verify our approach to strategic scenario generation by employing an interactive evolution framework, which shows the proposed scenario representation scheme can facilitate the generation of coherent and novel story-like scenarios."
1910022,14018,9704,Improved multi-objective evolutionary algorithm for day-ahead thermal generation scheduling,2011,"This paper presents a multi-objective evolutionary algorithm to solve the day-ahead thermal generation scheduling problem. The objective functions considered to model the scheduling problem are: 1) minimizing the system operation cost and 2) minimizing the emission cost. In the proposed algorithm, the chromosome is formulated as a binary unit commitment matrix (UCM) which stores the generator on/off states and a real power matrix (RPM) which stores the corresponding power dispatch. Problem specific binary genetic operators act on the binary UCM and real genetic operators act on the RPM to effectively explore the large binary and real search spaces separately. Heuristics are used in the initial population by seeding the random population with two Priority list (PL) based solutions for faster convergence. Intelligent repair operator based on PL is designed to repair the solutions for load demand equality constraint violation. The ranking, selection and elitism methods are borrowed from NSGA-II. The proposed algorithm is applied to a large scale 60 generating unit power system and the simulation results are presented and compared with our earlier algorithm [26]. The presented algorithm is found to outperform our earlier algorithm in terms of both convergence and spread in the final Pareto-optimal front."
1712815,14018,9704,Differential evolution based optimization of risk budgeted Equity Market Neutral Portfolios,2012,"An Equity Market Neutral Portfolio (EMNP) is an assortment of long and short positions that ensures a riskless portfolio in terms of its exposure to the relevant market benchmark. While a naive formulation of the EMNP optimization problem can be easily solved using linear programming techniques, the inclusion of the Risk Budget constraint on the high risk assets, together with the other EMNP specific constraints of zero net market exposure, close-to-zero portfolio beta and zero financial leveraging, besides the bounding constraints imposed on the long-short positions and high risk assets, can turn the problem difficult for direct solving using traditional methods. This work aims to solve such a complex constrained EMNP optimization problem using a meta-heuristic method viz., Differential Evolution (rand/1/bin) with Hall of Fame (DE HOF). The DE HOF exploits a penalty function strategy and employs weight standardization procedures to ensure faster convergence and an efficient tackling of complex constraints to yield optimal portfolios within realistic time. Experimental studies which include a rigorous out of sample performance analysis have been undertaken on the Bombay Stock Exchange data set (BSE 200: March 1999–March 2009) which included both upturns and downturns in the global markets."
1900077,14018,9704,Tool sequence optimization using synchronous and asynchronous parallel multi-objective evolutionary algorithms with heterogeneous evaluations,2013,"Selecting the sequence of tools to use for the rough machining of components is an important task in manufacturing, which greatly affects the overall machining time and cost of the process. In this paper a multi-objective approach is presented, which supports the use of tools with different geometrical properties and offers the process planner a set of Pareto optimal solutions. An industrial simulator is employed, which allows important information to be captured in the model but has the disadvantage of being computationally expensive. A master/slave approach to parallelization is implemented, which can be used on existing grid or cloud computing infrastructures. Synchronous generational and asynchronous steady-state multi-objective algorithms are compared on their search performance and runtimes on two components. Particular attention is paid to potential problems faced by asynchronous search caused by heterogeneous evaluation times due to characteristics present in individual tool sequences. Results show that the algorithms achieve a similar search performance, with the synchronous algorithm occasionally finding a slightly more diverse spread of solutions. However, the asynchronous algorithm is considerably faster, and provides good solutions in a short runtime that means this approach could be easily and inexpensively implemented in an industrial setting."
1254230,14018,9080,Multi-objective gene-pool optimal mixing evolutionary algorithms,2014,"The recently introduced Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA), with a lean, but sufficient, linkage model and an efficient variation operator, has been shown to be a robust and efficient methodology for solving single objective (SO) optimization problems with superior performance compared to classic genetic algorithms (GAs) and estimation-of-distribution algorithms (EDAs). In this paper, we bring the strengths of GOMEAs to the multi-objective (MO) optimization realm. To this end, we modify the linkage learning procedure and the variation operator of GOMEAs to better suit the need of finding the whole Pareto-optimal front rather than a single best solution. Based on state-of-the-art studies on MOEAs, we further pinpoint and incorporate two other essential components for a scalable MO optimizer. First, the use of an elitist archive is beneficial for keeping track of non-dominated solutions when the main population size is limited. Second, clustering can be crucial if different parts of the Pareto-optimal front need to be handled differently. By combining these elements, we construct a multi-objective GOMEA (MO-GOMEA). Experimental results on various MO optimization problems confirm the capability and scalability of our MO-GOMEA that compare favorably with those of the well-known GA NSGA-II and the more recently introduced EDA mohBOA."
1152921,14018,9704,Approximating a multi-dimensional Pareto front for a land use management problem: A modified MOEA with an epigenetic silencing metaphor,2012,"Land use management is increasingly becoming complex as the public and governing bodies demand more accountability and transparency in management practices that simultaneously guarantee sustainable production of goods and continued provision of ecosystem services (i.e., public goods with no markets, such as clean air). In this paper we demonstrate a novel form of decision making that will assist in meeting some of these challenges in ensuring sustainability in land use management. We apply a modified Multi-Objective Evolutionary Algorithm (MOEA), influenced by epigenetic silencing, to a farm case study. The result is a set of time-series, farm management strategies and their related spatial arrangements of land uses that satisfy 14 incommensurable and sometimes conflicting objectives, and spatial constraints. The 14 objectives cover economic (i.e. productivity and financials) and environmental issues. Choosing a single strategy from the set for implementation will require social-ethical value judgment determined from preferences and values of multiple decision-makers. This part of the decision making process is beyond the scope of this paper, but will contribute to ongoing research which will make it possible to fully account for the Triple Bottom Line (TBL), characterised by environmental, economic and social elements."
1528063,14018,21102,A preliminary fuzzy cognitive map - based desicion support tool for geriatric depression assessment,2013,"Dramatic changes in the demographic situation across western countries suggest that new policies towards elderly healthcare should be followed. Among these, timely detection and forecasting of early signs of pathological physical, cognitive or emotional health is of paramount importance, since it will allow for certain medicative strategies to be applied. The USEFIL project as such, aims at developing services that would contribute to the prolongation of seniors' independent living and support them during their daily life activities. An unobtrusive sensor network along with several intelligent processing algorithms will result to a better understanding of the senior population health course. In this paper the modeling and preliminary analysis (first evaluation) of a Decision Support Tool-Subsystem (DSS) for geriatric depression scenario is presented. The decision support tool is based on the construction of a Fuzzy Cognitive Map (FCM) model that describes the geriatric depression scenario. First results are outlined to show the applicability of the proposed methodology. More specifically, a number of virtual senior cases (both healthy and with depressive symptoms) were built with the help of a neuropsychologist. Results of preliminary analysis are promising; however, further expansion of the model with more signs and several other pathologic conditions of seniors such as cognitive impairment and frailty shall be investigated."
777524,14018,9080,GPDL: a framework-independent problem definition language for grammar-guided genetic programming,2013,"Defining custom problem types in genetic programming (GP) software systems is a tedious task that usually involves the implementation of custom classes and methods including framework-specific code. Users who want to solve a custom problem have to know the details of the targeted framework, for instance cloning semantics, and often have to write a lot of boilerplate code in order to implement the necessary functionality correctly. This can lead to frustration and hinders new developments and the application of GP to solve interesting problems.   In this contribution we propose a framework-independent definition language for GP problems that can reduce the required effort and facilitate the integration of new problem types. We draw a parallel between the implementation of compilers for programming languages and the implementation of GP problems and reuse the well-established concept of attributed grammars with semantic actions to define computational symbols, semantics and structural constraints for GP. This goes beyond previous work in the area of context-free-grammar GP and grammatical evolution, because we also interweave the definition of symbol semantics and the target function with the definition of the grammar.   This paper describes the proposed GP problem definition language (GPDL) and exemplary definitions of two popular benchmark problems using GPDL. We also describe a reference implementation of a GPDL compiler for HeuristicLab."
1663221,14018,9080,On the performance of multiple objective evolutionary algorithms for software architecture discovery,2014,"During the design of complex systems, software architects have to deal with a tangle of abstract artefacts, measures and ideas to discover the most fitting underlying architecture. A common way to structure these systems is in terms of their interacting software components, whose composition and connections need to be properly adjusted. Its abstract and highly combinatorial nature increases the complexity of the problem. In this scenario, Search-based Software Engineering (SBSE) may serve to support this decision making process from initial analysis models, since the discovery of component-based architectures can be formulated as a challenging multiple optimisation problem, where different metrics and configurations can be applied depending on the design requirements and its specific domain. Many-objective optimisation evolutionary algorithms can provide an interesting alternative to classical multi-objective approaches. This paper presents a comparative study of five different algorithms, including an empirical analysis of their behaviour in terms of quality and variety of the returned solutions. Results are also discussed considering those aspects of concern to the expert in the decision making process, like the number and type of architectures found. The analysis of many-objectives algorithms constitutes an important challenge, since some of them have never been explored before in SBSE."
30970,14018,11187,Bio-inspired combinatorial optimization: notes on reactive and proactive interaction,2011,"Evolutionary combinatorial optimization (ECO) is a branch of evolutionary computing (EC) focused on finding optimal values for combinatorial problems. Algorithms ranging in this category require that the user defines, before the process of evolution, the fitness measure (i.e., the evaluation function) that will be used to guide the evolution of candidate solutions. However, there are many problems that possess aesthetical or psychological features and as a consequence fitness evaluation functions are difficult, or even impossible, to formulate mathematically. Interactive evolutionary computation (IEC) has recently been proposed as a part of EC to cope with this problem and its classical version basically consists of incorporating human user evaluation during the evolutionary procedure. This is however not the only way that the user can influence the evolution in IEC and currently one can find that IEC has been been successfully deployed on a number of hard combinatorial optimization problems. This work examines the application of IEC to these problems. We describe the basic fundament of IEC, present some guidelines to the design of interactive evolutionary algorithms (IEAs) to handle combinatorial optimization problems, and discuss the two main models over which IEC is constructed, namely reactive and proactive searchbased schemas. An overview of the existing literature on the topic is also provided. We conclude with some reflections on the lessons learned, and the future directions that research might take in this area."
2129910,14018,9704,Ms Pac-Man versus Ghost Team CEC 2011 competition,2011,"Games provide an ideal test bed for computational intelligence and significant progress has been made in recent years, most notably in games such as Go, where the level of play is now competitive with expert human play on smaller boards. Recently, a significantly more complex class of games has received increasing attention: real-time video games. These games pose many new challenges, including strict time constraints, simultaneous moves and open-endedness. Unlike in traditional board games, computational play is generally unable to compete with human players. One driving force in improving the overall performance of artificial intelligence players are game competitions where practitioners may evaluate and compare their methods against those submitted by others and possibly human players as well. In this paper we introduce a new competition based on the popular arcade video game Ms Pac-Man: Ms Pac-Man versus Ghost Team. The competition, to be held at the Congress on Evolutionary Computation 2011 for the first time, allows participants to develop controllers for either the Ms Pac-Man agent or for the Ghost Team and unlike previous Ms Pac-Man competitions that relied on screen capture, the players now interface directly with the game engine. In this paper we introduce the competition, including a review of previous work as well as a discussion of several aspects regarding the setting up of the game competition itself."
1694360,14018,9704,An evolutionary search paradigm that learns with past experiences,2012,"A major drawback of evolutionary optimization approaches in the literature is the apparent lack of automated knowledge transfers and reuse across problems. Particularly, evolutionary optimization methods generally start a search from scratch or ground zero state, independent of how similar the given new problem of interest is to those optimized previously. In this paper, we present a study on the transfer of knowledge in the form of useful structured knowledge or latent patterns that are captured from previous experiences of problem-solving to enhance future evolutionary search. The essential contributions of our present study include the meme learning and meme selection processes. In contrast to existing methods, which directly store and reuse specific problem solutions or problem sub-components, the proposed approach models the structured knowledge of the strategy behind solving problems belonging to similar domain, i.e., via learning the mapping from problem to its corresponding solution, which is encoded in the form of identified knowledge representation. In this manner, knowledge transfer can be conducted across problems, from differing problem size, structure to representation, etc. A demonstrating case study on the capacitated arc routing problem (CARP) is presented. Experiments on benchmark instances of CARP verified the effectiveness of the proposed new paradigm."
781043,14018,9704,Identifying overlapping communities in complex networks with multimodal optimization,2013,"The analysis of complex networks is an important research topic that helps us understand the underlying behavior of complex systems and the interactions of their components. One particularly relevant analysis is the detection of communities formed by such interactions. Most community detection algorithms work as optimization tools that minimize a given quality function, while assuming that each node belongs to a single community. However, most complex networks contain nodes that belong to two or more communities, which are called bridges. The identification of bridges is crucial to several problems, as they often play important roles in the system described by the network. By exploiting the multimodality of quality functions, it is possible to obtain distinct optimal communities where, in each solution, each bridge node belongs to a distinct community. This paper proposes a technique that tries to identify a set of (possibly) overlapping communities by combining diverse solutions contained in a pool, which correspond to disjoint community partitions of a given network. To obtain the pool of partitions, an adapted version of the immune-inspired algorithm named cob-aiNet[C] was adopted here. The proposed methodology was applied to four real-world social networks and the obtained results were compared to those reported in the literature. The comparisons have shown that the proposed approach is competitive and even capable of overcoming the best results reported for some of the problems."
1827022,14018,9704,Evolution of stochastic bio-networks using summed rank strategies,2011,"Stochastic models defined in the stochastic pi-calculus are evolved using genetic programming. The interpretation of a stochastic model results in a set of time series behaviors. Each time series denotes changing quantities of components within the modeled system. The time series are described by their statistical features. This paper uses genetic programming to reverse engineer stochastic pi-calculus models. Given the statistical characteristics of the intended model behavior, genetic programming attempts to construct a model whose statistical features closely match those of the target process. The feature objectives comprising model behavior are evaluated using a multi-objective strategy. A contribution of this research is that, rather than use conventional Pareto ranking, a summed rank scoring strategy is used instead. Summed rank scoring was originally derived for high-dimensional search spaces. This paper shows that it is likewise effective for evaluating stochastic models with low- to moderate-sized search spaces. Two models with oscillating behaviors were successfully evolved, and these results are superior to those obtained from earlier research attempts. Experiments on a larger-sized model were not successful. Reasons for its poor performance likely include inappropriate choices in feature selection, and too many selected features and channels contributing to an overly difficult search space."
1322445,14018,9080,A behavior-based analysis of modal problems,2013,"Genetic programming (GP) has proven to be a powerful tool for (semi)automated problem solving in various domains. However, while the algorithmic aspects of GP have been a primary object of study, there is a need to enhance the understanding of the problems where GP is applied. One particular goal is to categorize problems in a meaningful way, in order to select the best tools that can possibly be used to solve them. This paper studies modal problems, a conceptual class of problems recently proposed by Spector at GECCO 2012. Modal problems are those for which a solution program requires different modes of operation for different contexts. The thesis of this paper is that modality, in this sense, is better understood by analyzing program performance in behavioral space. The behavior-based perspective is seen as part of a scale of different forms of analyzing performance; with a coarse view given by a global fitness value and a highly detailed view provided by the semantics approach. On the other hand, behavioral analysis is seen as a flexible approach where the context of a program's performance is considered at in a domain-specific manner. The experimental evidence presented here suggests that behavior-based search could allow a GP to find programs with disjoint behavioral structures, that can satisfy the requirements of each mode of operation of a modal problem."
951077,14018,9080,Exploring boundaries: optimising individual class boundaries for binary classification problem,2012,"This paper explores a range of class boundary determination techniques that can be used to improve performance of Genetic Programming (GP) on binary classification tasks. These techniques involve selecting an individualised boundary threshold in order to reduce implicit bias that may be introduced through employing arbitrarily chosen values. Individuals that can chose their  own  boundaries and the manner in which they are applied, are freed from having to learn to force their outputs into a particular range or polarity and can instead concentrate their efforts on seeking a problem solution.   Our investigation suggests that while a particular boundary selection method may deliver better performance for a given problem, no single method performs best on all problems studied. We propose a new flexible combined technique which gives near optimal performance across each of the tasks undertaken. This method together with seven other techniques is tested on six benchmark binary classification data sets. Experimental results obtained suggest that the strategy can improve test fitness, produce smaller less complex individuals and reduce run times. Our approach is shown to deliver superior results when benchmarked against a standard GP system, and is very competitive when compared with a range of other machine learning algorithms."
1193917,14018,9704,Designing and characterising fitness landscapes with various operators,2013,"Stochastic optimisers such as Evolutionary Algorithms, Estimation of Distribution Algorithm are suitable methods when problems are highly complex and deterministic algorithms cannot be expected to produce acceptable results. Generally, when the search process produces the optimised solutions, there is no indication how successful the search has been. In previous work, we introduced Predictive Diagnostic Optimisation (PDO), a local-search-based solver which can predict with certain accuracy the quality of local optima and that can help decide which of the initial solutions is appropriate to optimise. The neighbourhood created by the swap operator was used in exploration of the search space and the number of predictors created is a metric for the homogeneity of the landscape. The advantage of PDO is that it provides information regarding the difficulty of the search landscape alongside the optimisation results. In this work we extend PDO by employing three more neighbourhood operators to allow a comparison between the performances of different types of local search. Each neighbourhood operator has its own group of predictors and the difficulty in predicting the local optima is quantified by a new metric, the prediction error. To provide an assessment of the characterisation ability for the algorithm, a set of landscapes with various degrees of difficulty has been designed by manipulating the matrices of the test problems instances. We show that the metric is able to identify the degree of difficulty that we expect the landscapes to pose for the employed local search operators."
883323,14018,21102,FML-based decision support system for solar energy supply and demand analysis,2013,"Because of the coming of high oil price and the trend of curbing the greenhouse gas emission, promoting the establishment of renewable energy is regarded as one of the main strategies in the world. Electricity supply in Taiwan is highly dependent on overseas imports. As a result, promotion of development and use of renewable energy not only increases the diversification of energy sources but also achieves a win-win-win situation for energy safety, environmental protection, and economic development. In Taiwan, wind power, solar energy, and bio-fuel are the three mainly promoted renewable energies, while in this paper we focus on the solar energy. This paper proposes a fuzzy markup language (FML)-based decision support system for the supply-demand analysis of the solar energy to discuss if the photovoltaic (PV)-generated electricity can supply enough one for the PV-installed household. First, the domain experts construct the ontology for solar energy supply and demand analysis (SESDA). Then, according to the power generation level from the installed PV system, appliances power consumption level from the housing loads, daily rainy probability, and today temperature forecast, the proposed system infers the power purchase possibility and then stores the results in the SESDA repository. The household could retrieve the results to try to improve his habit of electricity utilization to save electricity."
1533843,14018,9704,Resampling in Particle Swarm Optimization,2013,"Particle Swarm Optimization (PSO) is a population-based algorithm designed to find good solutions to optimization problems. Its characteristics have encouraged its adoption to tackle a variety of problems in different fields. However, when such problems are subject to noise, the performance of PSO suffers an immediate deterioration which demands the incorporation of noise handling mechanisms. One such mechanism comprises resampling methods, which re-evaluate the solutions multiple times in order to estimate their true objective values. The state-of-the-art integration with which the best results have been obtained utilizes the resampling method named Optimal Computing Budget Allocation (OCBA). This resampling method starts by estimating the objective values of all the solutions via Equal Resampling (ER), and then sequentially allocating further re-evaluations to the estimated best solutions. However, after having a first estimate via ER, we question the importance of the additional efforts to correctly select the true best solutions when a good-enough and accurate one can be selected. In this paper, we propose a new PSO algorithm based on ER in which the additional evaluations are allocated at once to the estimated best solutions, thus skipping the complexity of using OCBA. Experiments on 20 large-scale benchmark functions subject to different levels of noise show that the proposed algorithm produces similar results to PSO with OCBA in most cases."
2122103,14018,9704,Optimization of parallel Genetic Algorithms for nVidia GPUs,2011,"Led by General Purpose computing over Graphical Processing Units (GPGPUs), the parallel computing area is witnessing a rapid change in dominant parallel systems. A major hurdle in this switch is the Single Instruction Multiple Thread (SIMT) architecture of GPUs which is usually not suitable for the design of legacy parallel algorithms. Genetic Algorithms (GAs) is no exception for that. GAs are commonly parallelized due to the high demanding computational needs. Given the performance of GPGPUs, the need to best exploit them to maximize computing efficiency for parallel GAs is demandingly growing. The goal of this paper is to shed light on the challenges parallel GAs designers/programmers will likely face while trying to achieve this, and to provide some practical advice on how to maximize GPGPU exploitation as a result. To that end, this paper provides a study on adapting legacy parallel GAs on GPGPU systems. The paper exposes the design challenges of nVidia's GPU architecture to the parallel GAs community by: discussing features of GPU, reviewing design issues in GPU relevant to parallel GAs, the design and introduction of new techniques to achieve an efficient implementation for parallel GAs and observing the effect of the pivotal points that both capitalize on the strengths of GPU and limit the deficiencies/overheads of GPUs. The paper demonstrates the performance of designed-for-GPGPU parallel GAs representing the entire spectrum of legacy parallel model of GAs over nVidia Tesla C1060 workstation showing a significant improvement in performance after optimizing and tuning the algorithms for GPU."
1923451,14018,9704,Classification-assisted memetic algorithms for solving optimization problems with restricted equality constraint function mapping,2011,"The success of Memetic Algorithms (MAs) has driven many researchers to be more focused on the efficiency aspect of the algorithms such that it would be possible to effectively employ MAs to solve computationally expensive optimization problems where single evaluation of the objective and constraint functions may require minutes to hours of CPU time. One of the important design issues in MAs is the choice of the individuals upon which local search procedure should be applied. Selecting only some potential individuals lessens the demand for functional evaluations hence accelerates convergence to the global optimum. In recent years, advances have been made targeting optimization problems with single equality constraint h(x) = 0. The presence of previously evaluated candidate solutions with different signs of constraint values within some localities thus allows the estimation of the constraint boundary. An individual will undergo local search only if it is sufficiently close to the approximated boundary. Elegant as it may seem, the approach had unfortunately assumed that every constraint function maps the design variables to optimize into unbounded real values. This, however, may not always be the case in practice. In this paper, we present a strategy to efficiently solve constrained problems with a single equality constraint; the function of which maps the design variables into restricted (either strictly non-negative or strictly non-positive) real values only."
2314392,14018,8228,An Optimized Resource Allocation Scheme Based on a Multidimensional Multiple-Choice Approach with Reduced Complexity,2011,"Long Term Evolution (LTE) is considered one of the main candidate to provide wireless broadband access to mobile users. Among main LTE characteristics, flexibility and efficiency can be guaranteed by resorting to suitable resource allocation schemes, in particular by adopting adaptive OFDM schemes. This paper proposes a novel solution to the sub-carrier allocation problem for the LTE downlink that takes into account the queues length, the QoS constraints and the channel conditions. Each user has different queues, one for each QoS class, and can transmit with a different data rate depending on the propagation conditions. The proposed algorithm defines a value of each possible sub-carrier assignment as a linear combination of all the inputs following a cross-layer approach. The problem is formulated as a Multidimensional Multiple-choice Knapsack Problem (MMKP) whose optimal solution is not feasible for our purposes due to the too long computing time required to find it. Hence, a novel efficient heuristic has been proposed to solve the problem. Results shows good performance of the proposed resource allocation scheme both in terms of throughput and delay while guarantees fairness among the users. Performance has been compared also with fixed allocation scheme and round robin."
2021517,14018,9704,A hybrid local search operator for multiobjective optimization,2013,"In recent years, the development of hybrid approaches to solve multiobjective optimization problems has become an important trend in the evolutionary computation community. Despite hybrid approaches of mathematical programming techniques with multiobjective evolutionary algorithms are not very popular, when both fields are successfully coupled, results are impressive. However, the main objective of this sort of hybridization relays on the needing of several executions of the mathematical approach in order to obtain a sample of the Pareto front, raising with this, the number of fitness function evaluations. However, the use of surrogate models has become a recurrent approach to diminish the number of function evaluations. In this work, a hybrid operator that transforms the original multiobjective problem into a set of modified goal programming models is proposed. Furthermore, a local surrogate model is used instead of the real function in the hybrid operator. The goal programming model with the surrogate is optimized by a direct search method. Additionally, a standalone algorithm that uses the hybrid operator is here proposed. The new algorithm is validated using several test problems and performance measures commonly adopted in the specialized literature. Results indicate that the proposed operator gives rise to an effective algorithm, which produces results that are competitive with respect to those obtained by two well-known multiobjective evolutionary algorithms."
2458389,14018,21102,Modelling of robot attention demand in human-robot interaction using finite fuzzy state automata,2012,"Many systems have been implemented towards achieving effective human-machine interaction, but run the risk of being ignored if appropriate performance metrics are not in place. As a result, our goal becomes that of providing a foundation upon which we can assess how well the human and the robot perform as a team. Toward the efficient modelling of such metrics, we attempt to determine the true amount of time that an operator has to dedicate to the robot. Therefore, we define the robot attention demand (RAD) as a function of both direct interaction time (DIT) and indirect interaction time (IIT), where the IIT is a direct consequence of the human trust in automation. We propose a two-level fuzzy temporal model to evaluate the human trust in automation while collaborating with robots to complete some tasks. The model combines the advantages of fuzzy logic and finite state machines to best model this phenomenon, and reduces the system complexity and the size of the knowledge base by grouping perceptions into first- and second-order perceptions. The fuzzy knowledge base is further updated by implementing an application robotic platform where robots and users interact via natural language to complete tasks with varying levels of complexity. User feedback is noted and used to tune the knowledge base where needed."
2230567,14018,9704,Distributed evolutionary algorithm topologies with adaptive migration schemes,2011,"Distributed evolutionary algorithms are of increasing interest and importance for three main reasons: (i) a well designed dEA can outperform a ‘standard’ EA in terms of reliability, solution quality, and speed; (ii) they can (of course) be implemented on parallel hardware, and hence combine efficient utilization of parallel resources with very fast and reliable optimization; (iii) parallel hardware resources are increasingly common. A dEA operates as separate evolving populations with occasional interaction between them via ‘migration’. A specific dEA is characterized by the topology and nature of these interactions. The performance of alternative topologies and migration mechanisms in this field remains under-explored. In this paper we continue an investigation of two simple, novel dEA topologies, comparing with the cube-based topology that underpins Alba et al's GD-RCGA (a state of the art dEA). The focus in this paper is on testing a novel adaptive migration scheme, in which the frequency of migration events adapts dynamically in response to the current balance between exploration and exploration. We also focus on high dimensional versions of a selection of hard function optimization problems. We find that the adaptive migration scheme is promising, and that overall results marginally favour a simple three-level tree-based topology and adaptive migration with a longer window, especially as dimensionality increases."
2960032,14018,11166,Two approaches of using heavy tails in high dimensional EDA,2014,"We consider the problem of high dimensional black-box optimisation via Estimation of Distribution Algorithms (EDA). The Gaussian distribution is commonly used as a search operator in most of the EDA methods. However there are indications in the literature that heavy tailed distributions may perform better due to their higher exploration capabilities. Univariate heavy tailed distributions were already proposed for high dimensional problems. In 2D problems it has been reported that a multivariate heavy tailed (such as Cauchy) search distribution is able to blend together the strengths of multivariate modelling with a high exploration power. In this paper, we study whether a similar scheme would work well in high dimensional search problems. To get around of the difficulty of multivariate model building in high dimensions we employ a recently proposed random projections (RP) ensemble based approach which we modify to get samples from a multivariate Cauchy using the scale-mixture representation of the Cauchy distribution. Our experiments show that the resulting RP-based multivariate Cauchy EDA consistently improves on the performance of the univariate Cauchy search distribution. However, intriguingly, the RP-based multivariate Gaussian EDA has the best performance among these methods. It appears that the highly explorative nature of the multivariate Cauchy sampling is exacerbated in high dimensional search spaces and the population based search loses its focus and effectiveness as a result. Finally, we present an idea to increase exploration while maintaining exploitation and focus by using the RP-based multivariate Gaussian EDA in which the RP matrices are drawn with i.i.d. Heavy tailed entries. This achieves improved performance and is competitive with the state of the art."
1081712,14018,8806,Development and configuration of service-oriented systems families,2011,"Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders."
1890401,14018,9080,Automated generation of environments to test the general learning capabilities of AI agents,2014,"Abstract Algorithms for evolving agents that learn during their lifetime have typically been evaluated on only a handful of environments. Designing such environments is labour intensive, potentially biased, and provides only a small sample size that may prevent accurate general conclusions from being drawn. In this paper we introduce a method for automatically generating MDP environments which allows the difficulty to be scaled in several ways. We present a case study in which environments are generated that vary along three key dimensions of difficulty: the number of environment configurations, the number of available actions, and the length of each trial. The study reveals interesting differences between three neural network models -- Fixed-Weight, Plastic-Weight, and Modulated Plasticity -- that would not have been obvious without sweeping across these different dimensions. Our paper thus introduces a new way of conducting reinforcement learning science: instead of manually designing a few environments, researchers will be able to automatically generate a range of environments across key dimensions of variation. This will allow scientists to more rigorously assess the general learning capabilities of an algorithm, and may ultimately improve the rate at which we discover how to create AI with general purpose learning."
917307,14018,9080,CMA-ES: evolution strategies and covariance matrix adaptation,2011,"Evolution Strategies (ESs) and many continuous domain Estimation of Distribution Algorithms (EDAs) are stochastic optimization procedures that sample a multivariate normal (Gaussian) distribution in the continuous search space, R n . Many of them can be formulated in a unified and comparatively simple framework. This introductory tutorial focuses on the most relevant algorithmic question: how should the parameters of the sample distribution be chosen and, in particular, updated in the generation sequence? First, two common approaches for step-size control are reviewed, one-fifth success rule and path length control. Then, Covariance Matrix Adaptation (CMA) is discussed in depth: rank-one update, the evolution path, rank-mu update. Invariance properties and the interpretation as natural gradient descent are touched upon.   In the beginning, general difficulties in solving non-linear, non-convex optimization problems in continuous domain are revealed, for example non-separability, ill-conditioning and ruggedness. Algorithmic design aspects are related to these difficulties. In the end, the performance of the CMA-ES is related to other well-known evolutionary and non-evolutionary optimization algorithms, namely BFGS, DE, PSO,..."
1835125,14018,9704,Advanced genetic algorithm to solve MINLP problems over GPU,2011,"In this paper we propose a many-core implementation of evolutionary computation for GPGPU (General-Purpose Graphic Processing Unit) to solve non-convex Mixed Integer Non-Linear Programming (MINLP) and non-convex Non Linear Programming (NLP) problems using a stochastic algorithm. Stochastic algorithms being random in their behavior are difficult to implement over GPU like architectures. In this paper we not only succeed in implementation of a stochastic algorithm over GPU but show considerable speedups over CPU implementations. The stochastic algorithm considered for this paper is an adaptive resolution approach to genetic algorithm (arGA), developed by the authors of this paper. The technique uses the entropy measure of each variable to adjust the intensity of the genetic search around promising individuals. Performance is further improved by hybridization with adaptive resolution local search (arLS) operator. In this paper, we describe the challenges and design choices involved in parallelization of this algorithm to solve complex MINLPs over a commodity GPU using Compute Unified Device Architecture (CUDA) programming model. Results section shows several numerical tests and performance measurements obtained by running the algorithm over an nVidia Fermi GPU. We show that for difficult problems we can obtain a speedup of up to 20× with double precision and up to 42× with single precision."
1299831,14018,21102,Towards a general type-2 fuzzy logic approach for Computing With Words using linear adjectives,2012,"The concept of Computing With Words (CWW) was coined by Zadeh to be a methodology in which words are used instead of numbers for computing and reasoning. Since then, there have been various angles to interpret CWW. However, there is a need to tackle the problem of modeling a ‘word’ by fuzzy sets, which is one of the building blocks of the CWW concept. In this paper, we investigate the ‘word’ from the perspective of the parts of speech in English language. We point out that there exists a hierarchical analogy between the parts of speech and a linguistic variable in a fuzzy system. In other words, the linguistic variable in a fuzzy system can be interpreted to be a noun whereas the corresponding linguistic labels quantifying the linguistic variable can be classified as being ‘qualifiers + adjectives’. We propose to model the linguistic uncertainty conveyed by qualifiers as second-order word uncertainty using a general type-2 fuzzy set based approach where the qualifiers can be exploited in linear terms. In particular, we suggest a linear representation of the third dimension of a general type-2 fuzzy set where we consider only left and right shoulder membership functions. We show that the interpretation of the paradigm using linear adjectives simplifies the comprehension of the third dimension as well as offering a way to avoid the shortcomings of type-1 and interval type-2 fuzzy sets in modeling a word for CWW. For illustration, we present examples comparing the interval type-2 fuzzy labels (which are greater in number) and the linear general type-2 (LGT2) fuzzy labels which provide an efficient management of the linguistic variable by revealing a potential to reduce complexity."
2098350,14018,9704,Evolvable free-form deformation control volumes for evolutionary design optimization,2011,"Evolutionary design optimization for improving the performance of real world objects, like e.g. car shapes in the context of aerodynamic efficiency, usually depends on a well-balanced combination of representation, optimizer and design evaluation method. Shape representation requires a fair trade-off between minimum number of design parameters and design flexibility which likewise guarantees a good optimization convergence while allowing manifold design variations. Recently, shape morphing methods have gained increased attention because of their capability to represent complex shapes with a reasonable number of parameters, especially powerful if coupled with numerical simulations for measuring design performance. Free-form deformation, as prominent shape morphing representative, relies on an initial grid of control points, the control volume, which allows the modification of the embedded shape. The set-up of the control volume is a crucial process which in practice is done manually based on the experience of the human user. Here, a method for the automated construction of control volumes is suggested based on a proposed measure E CV  which relies on the concept of evolvability as a potential capacity of representations to produce successful designs in a reasonable time. It is shown for target shape matching experiments that optimizations based on E CV -tuned control volumes provide a significantly better performance in design optimization."
753694,14018,9704,Never-ending learning principles in gene ontology classification using genetic algorithms,2012,"A number of different computational approaches have been applied in many different biology application domains. When such tools are based on conventional computation techniques, they have shown limitations to approach complex biological problems. In the present study, a genetic algorithm (named GANEL) that is based on some Never-Ending Learning (NEL) principles, is proposed as a tool to extract classification rules from biological datasets. The main goal of the proposed approach is to allow the discovery of concise, yet accurate, high-level rules (from a biological database) which can be used to describe the stronger patterns present in the biological data, revealing concise and relevant information about the application domain, as well as, be used as a classification system. More than focusing only on the classification accuracy, the proposed GANEL approach aims at balancing prediction precision, interpretability and comprehensibility. The obtained results show that the proposed GANEL is promising and capable of extracting useful high-level knowledge that could not be extracted by traditional classifications methods such as Decision Trees, One R and the Single Conjunctive Rule Learner, among others. Moreover, the accuracy of GANEL results (using a small set of attributes per class) are better than Computational Evolutionary Environment (CEE) (previously proposed in the literature) which was designed to the same problem domain."
1167426,14018,9704,A relationship between network topology and search performance of PSO,2012,"Particle swarm optimization (abbr. PSO) is one of the most effective optimization algorithms. The PSO contains many control parameters, therefore, the performance of the searching ability of the PSO is significantly alternated. In order to analyze the dynamics of such PSO system rigorously, we have analyzed a deterministic PSO (abbr. D-PSO) systems which does not contain any stochastic factors, and its coordinate of the phase space is normalized. The found global best information influences the dynamics. This situation can be regarded as the full-connection state. On the other hand, there is the case where the best information in a limited population. Such information is called as lbest. How to get the lbest information from any population is equivalent to a network structure. Such network structure influences the performance of searching ability. In order to clarify a relationship between network structures of the PSO and its performance, we pay attention to the degree and the average distance used in graph theory. We consider the two cases where the D-PSO has an extended cycle structure and a Small World network structure. Our numerical simulation results indicates the searching performance of the D-PSO is depended on the average distance of the node. Especially, the long average distance exerts the search performance on the D-PSO. We confirm that the search performance properties of the D-PSO and the conventional stochastic PSO are completely different to the average distance. The search performance of the D-PSO is improved according to the average distance. On the other hand, the search performance of the conventional stochastic PSO is deteriorated according to the average distance. We consider that the slow transmission of the beneficial information leads to the diversification of the particles of the D-PSO. Also, we clarify the small perturbation of the random range of the stochastic PSO is important."
1287220,14018,9080,A hybrid evolutionary metaheuristics (HEMH) applied on 0/1 multiobjective knapsack problems,2011,"Handling Multiobjective Optimization Problems (MOOP) using Hybrid Metaheuristics represents a promising and interest area of research. In this paper, a Hybrid Evolutionary Metaheuristics (HEMH) is presented. It combines different metaheuristics integrated with each other to enhance the search capabilities. It improves both of intensification and diversification toward the preferred solutions and concentrates the search efforts to investigate the promising regions in the search space. In the proposed HEMH, the search process is divided into two phases. In the first one, the DM-GRASP is applied to obtain an initial set of high quality solutions dispersed along the Pareto front. Then, the search efforts are intensified on the promising regions around these solutions through the second phase. The greedy randomized path-relinking with local search or reproduction operators are applied to improve the quality and to guide the search to explore the non discovered regions in the search space. The two phases are combined with a suitable evolutionary framework supporting the integration and cooperation. Moreover, the efficient solutions explored over the search are collected in an external archive. The HEMH is verified and tested against some of the state of the art MOEAs using a set of MOKSP instances commonly used in the literature. The experimental results indicate that the HEMH is highly competitive and can be considered as a viable alternative."
2396464,14018,21102,Takagi-Sugeno sliding mode observer for friction compensation with application to an inverted pendulum,2013,"To improve the dynamics in mechatronic systems an active friction compensation is usually necessary. However, a poor friction compensation action in the control scheme may lead to significant tracking errors, especially at low velocities. Different kinds of friction models are commonly used to compensate for the effects of friction. However, the friction modeling problem remains a very difficult and time-consuming task. In this work, a different approach is presented: the friction force acting in a dynamic system can be viewed as an unknown but bounded input with time-varying characteristics to be estimated and compensated for using a robust sliding mode observer scheme based on the Takagi-Sugeno model structure. The TakagiSugeno fuzzy observer scheme combined with sliding mode (TS SMO) deals with nominal nonlinear dynamics, unmeasurable premise variables, bounded uncertainties in the plant and enables friction estimation based on an equivalent output error injection approach. Thus, the limitations arising from the use of an explicit friction model are removed. The stability of the nonlinear error dynamics is ensured by sufficient linear matrix inequality (LMI) conditions. The approach is illustrated using a non-linear inverted pendulum with Stribeck friction and additional uncertainties due to a slight path angle. Simulation and experimental results demonstrate the effectiveness of the proposed scheme."
1130345,14018,9704,A hybrid version of differential evolution with two differential mutation operators applied by stages,2013,"Differential Evolution (DE) is an algorithm capable of solving complex optimization problems with and without constraints. As many of the population-based algorithms, DE is based on operators that evolve a numerical population through search operators. The differential mutation, one of the basic operators in the original version of the algorithm, provides population diversity through the evolution. In this paper we propose an extended version of a previously proposed hybrid DE including know two different mutation operators, which are not applied simultaneously. The first of them, our main contribution, is based on the exploitation of feasible areas to identify promising regions of search space. The second mutation operator is the classic differential mutation and it is applied towards produce a balance between exploration and exploitation as well as to improve the individuals obtained with our operator. An experimental study was performed by considering 18 functions presented for the “Single Objective Constrained Real-Parameter Optimization” of the special session of CEC2010. The results are compared with those obtained by Takahama and Sakai, winners that CEC2010 special session with eDEag algorithm. The obtained results show that our proposed approach is capable of finding solutions of higher quality for scalable problems of dimension 30 whereas the results for dimension 10 remains competitive with eDEag."
2238432,14018,9704,Orthogonal learning particle swarm optimization for power electronic circuit optimization with free search range,2011,"Power electronic circuit (PEC) always consists of a number of components such as resistors, capacitors, and inductors which have to be optimized in order to obtain good circuit performance. In current studies, the search ranges of these components are always pre-defined carefully by expert designers, making it difficult for practical applications. In this paper, the search space is freely set to the commonly used ranges and an efficient orthogonal learning particle swarm optimization (OLPSO) is applied to optimally design the PEC with such search space. OLPSO uses an orthogonal learning (OL) strategy for PSO to discover useful information that lies in the personal historical best experience and the neighborhood's best experience via orthogonal experimental design. Therefore, OLPSO can construct a more promising and efficient exemplar to guide particle to fly better towards the global optimal region. OLPSO is implemented to optimize the design of a buck regulator in PEC. The optimized results are compared with those obtained by using a genetic algorithm (GA) approach and those obtained by using PSO with traditional learning strategy. Results show that the OLPSO algorithm is more promising in the design and optimization of the PEC with large search space. Moreover, the simulations results demonstrate the advantages of OLPSO by showing that the circuit optimized by OLPSO exhibits better startup and large-signal disturbance performance when compared with the one optimized by GA."
2270799,14018,9704,An evolutionary multiobjective optimization approach to component-based software architecture design,2011,"The design of software architecture is one of the difficult tasks in the modern component-based software development which is based on the idea that develop software systems by assembling appropriate off-the-shelf components with a well-defined software architecture. Component-based software development has achieved great success and been extensively applied to a large range of application domains from realtime embedded systems to online web-based applications. In contrast to traditional approaches, it requires software architects to address a large number of non-functional requirements that can be used to quantify the operation of system. Moreover, these quality attributes can be in conflict with each other. In practice, software designers try to come up with a set of different architectural designs and then identify good architectures among them. With the increasing scale of architecture, this process becomes time-consuming and error-prone. Consequently architects could easily end up with some suboptimal designs because of large and combinatorial search space. In this paper, we introduce AQOSA (Automated Quality-driven Optimization of Software Architecture) toolkit, which integrates modeling technologies, performance analysis techniques, and advanced evolutionary multiobjective optimization algorithms (i.e. NSGA-II, SPEA2, and SMS-EMOA) to improve non-functional properties of systems in an automated manner."
733810,14018,369,Base Station and Relay Station Broadband Network Planning Using Immune Quantum Evolutionary Algorithm,2013,"In this paper, we present simultaneous planning technique for Base Stations (BSs) and Relay Stations (RSs) for a broadband wireless network while taking data flow also known as link flow into consideration. Infrastructure cost (BS cost, RS cost and their operational costs) of a wireless network proves to be a key factor for network service providers while planning a network. The objective of this study is to help determine the set of BSs and RSs that can serve all the subscribed users and fulfill their demands at the lowest cost to the utility firm. This problem setup can be used for laying new networks as well as enhancing the already existing ones. The combinatorial optimization problem at hand is NP-hard in nature. We formulate this problem as a non-linear discrete optimization problem and compare two recent Evolutionary Algorithms (EAs) in providing approximate solution to this problem. The Quantum Inspired Evolutionary Algorithm (QEA) is a probabilistic algorithm based on quantum computing with the concept of qubits and superposition of states. The Immune theory based Immune Quantum Evolutionary Algorithm (IQEA) adopts immune operator to raise the fitness and prevent deterioration during the evolutionary process. Simulation results show better performance of IQEA as compared to QEA."
1816293,14018,9080,Passive solar building design using genetic programming,2014,"Passive solar building design considers the effect that sunlight has on energy usage. The goal is to reduce the need for artificial cooling and heating devices, thereby saving energy costs. A number of competing design objectives can arise. Window heat gain during winter requires large windows. These same windows, however, reduce energy efficiency during nights and summers. Other model requirements add further complications, which creates a challenging optimization problem. We use genetic programming for passive solar building design. The EnergyPlus system is used to evaluate energy consumption. It considers factors ranging from model construction (shape, windows, materials) to location particulars (latitude/longitude, weather, time of day/year). We use a strongly typed design language to build 3D models, and multi-objective fitness to evaluate the multiple design objectives. Experimental results showed that balancing window heat gain and total energy use is challenging, although our multi-objective strategy could find interesting compromises. Many factors (roof shape, material selection) were consistently optimized by evolution. We also found that geographic aspects of the location play a critical role in the final building design."
1345514,14018,9704,Bio-inspired in-network filtering for wireless sensor monitoring systems,2013,"In-network filtering schemes can be used for computing type-threshold functions in wireless sensor networks. Instead of relaying all data to a sink node, sensor nodes can filter measurements to provide only the set of data required to compute a given function (e.g., maximum, range). In this context, the network can progressively learn where relevant data are available and use this information to compute the function over time by only querying a subset of nodes. Trails between sink and these nodes can be obtained based on bio-inspired strategies, reducing the energy consumption and prolonging the network lifetime. The adaptive behavior of swarm intelligence allows to overcome a lot of obstacles presented in wireless communication networks. In this work, we evaluate the PhINP (Pheromone-based in Network Processing) mechanism, which drives the filtering process based on the integration of metaheuristic and learning algorithms. MAX function computation in oneand multiple-source environment monitoring is used as a case study. We show by simulation that communication cost can be significantly reduced respect to traditional mechanisms, increasing the network lifetime, while keeping a low computational error. Finally, node density requirements for efficient event detection in real applications are analyzed."
1266570,14018,9080,Some distance measures for morphological diversification in generative evolutionary robotics,2014,"Evolutionary robotics often involves optimization in large, complex search spaces, requiring good population diversity. Recently, measures to actively increase diversity or novelty have been employed in order to get sufficient exploration of the search space either as the sole optimization objective or in combination with some performance measurement. When evolving morphology in addition to the control system, it can be difficult to construct a measure that sufficiently captures the qualitative differences between individuals. In this paper we investigate four diversity measures, applied in a set of evolutionary robotics experiments using an indirect encoding for evolving robot morphology. In the experiments we optimize forward locomotion capabilities of symmetrical legged robots in a physics simulation.   Two distance measures in Cartesian phenotype feature spaces are compared with two methods operating in the space of possible morphology graphs. These measures are used for computing a diversity objective in a multi-objective evolutionary algorithm, and compared to a control case with no diversity objective.   For the given task one of the distance measures shows a clear improvement over the control case in improving the main objectives, while others display better ability to diversify, underlining the difficulty of designing good, general measures of morphological diversity."
1874563,14018,9704,Evolutionary dynamics of continuous strategy games on social networks under weak selection: A preliminary study,2011,"Cooperation is a fundamental principle of all biological systems. Most previous studies presumed that the interactions between individuals are discrete, namely, each individual offers either cooperation or defection. This discrete strategy seems unrealistic in real systems and cooperative behavior in nature should be viewed as a continuous trait. Existing research work on games with a continuous strategy mainly focuses on infinite well-mixed populations. Additionally, our previous work showed that there is a considerable difference in terms of equilibria between continuous and discrete strategy games on graphs under strong selection. This paper studies the game dynamics in finite structured populations under weak selection using the stochastic dynamics based on respectively the mutant fixation probability (ρY) and the fixation probability ratio of mutant to resident (ρY/ρX). For three update rules, called ‘birth-death’ (BD), ‘death-birth’ (DB) and ‘imitation’ (IM), we derive exact conditions for natural selection favoring one strategy over another. Comparing discrete strategy games, we find that for continuous ones (i) the rule, b/c > k, is also valid; (ii) the same selection conditions are also derived using ρY/ρX; however, (iii) the selection conditions obtained using ρY and ρY/ρX are the same instead of different; and (iv) interestingly, the ‘1/3’ rule is not observed for DB and IM updating."
1346570,14018,9704,A technique for the optimization of the parameters of technical indicators with Multi-Objective Evolutionary Algorithms,2012,"Technical indicators (TIs) are used to interpret stock market and to predict market trends. The main difficulty in the use of TIs lies in deciding which their optimal parameter values are in each moment, since constant optimal values do not seem to exist. In this work, the use of Multi-Objective Evolutionary Algorithms (MOEAs) is proposed to obtain the best values of the parameters in order to help to buy and sell shares. Those parameters are applied in real time and belong to a collection of indicators. Unlike other previous approaches, the necessity of repeating the parameter optimization process each time a new data enters the system is justified, searching for the best adjustment of the parameters (and hence the TIs) in every moment. The Moving Averages Convergence-Divergence (MACD) indicator and the Relative Strength Index (RSI) oscillator have been chosen as TIs, so the MOEAs will provide the best parameters to use them on investment decisions. Experiments compare up to nine different configurations with the Buy & Hold strategy (B & H). The obtained results show that the Multi-Objective technique proposed here can greatly improve the results of the B & H strategy even operating daily. This statement is also demonstrated by comparing the results to those previously presented in the literature."
1030216,14018,9704,Using learning classifier systems to design selective hyper-heuristics for constraint satisfaction problems,2013,"Constraint satisfaction problems (CSP) are defined by a set of variables, where each variable contains a series of values it can be instantiated with. There is a set of constraints among the variables that restrict the different values they can take simultaneously. The task is to find one assignment to all the variables without breaking any constraint. To solve a CSP instance, a search tree is created where each node represents a variable of the instance. The order in which the variables are selected for instantiation changes the form of the search tree and affects the cost of finding a solution. Many heuristics have been proposed to help to decide the next variable to instantiate during the search and they have proved to be helpful for some instances. In this paper we explore the use of learning classifier systems to construct selective hyper-heuristics that dynamically select, from a set of variable ordering heuristics for CSPs, the one that best matches the current problem state in order to perform well on a wide range of instances. During a training phase, the system constructs state-heuristic rules as it explores the search space. Heuristics with good performance at certain points are rewarded and become more likely to be applied in similar situations. The approach is tested on random instances, providing promising results with respect to the median performance of the variable ordering heuristics used in isolation."
2382725,14018,9704,Improving evolvability through novelty search and self-adaptation,2011,"A challenge for current evolutionary algorithms is to yield highly evolvable representations like those in nature. Such evolvability in natural evolution is encouraged through selection: Lineages better at molding to new niches are less susceptible to extinction. Similar selection pressure is not generally present in evolutionary algorithms; however, the first hypothesis in this paper is that novelty search, a recent evolutionary technique, also selects for evolvability because it rewards lineages able to continually radiate new behaviors. Results in experiments in a maze-navigation domain in this paper support that novelty search finds more evolvable representations than regular fitness-based search. However, though novelty search outperforms fitness-based search in a second biped locomotion experiment, it proves no more evolvable than fitness-based search because delicately balanced behaviors are more fragile in that domain. The second hypothesis is that such fragility can be mitigated through self-adaption, whereby genomes influence their own reproduction. Further experiments in fragile domains with novelty search and self-adaption indeed demonstrate increased evolvability, while, interestingly, adding self-adaptation to fitness-based search decreases evolvability. Thus, selecting for novelty may often facilitate evolvability when representations are not overly fragile; furthermore, achieving the potential of self-adaptation may often critically depend upon the reward scheme driving evolution."
1109219,14018,21102,A hybrid approach for Multi-Criteria Group Decision Making based on interval type-2 fuzzy logic and Intuitionistic Fuzzy evaluation,2012,"Multi-Criteria Decision Making (MCDM) aims to develop techniques that are able to make decisions and solve complex problems where the outcome is a factor of various conflicting criteria. Intuitionistic Fuzzy Sets (IFSs) have been shown to provide a suitable framework for dealing with decision-making systems involving membership, non-membership and hesitation which showed very good results when dealing with conflicting criteria. On the other hand, Group Decision Making (GDM) deals with decision-making systems which need to consider the opinions of a group of experts whose decisions and opinions are subject to linguistic uncertainties. Previous research has shown the power of interval type-2 fuzzy logic systems to handle the linguistic uncertainties in decision-making systems. In this paper, we propose a hybrid method combining interval type-2 fuzzy logic and IFSs to develop a Multi-Criteria Group Decision Making (MCGDM) system. The intuitionistic evaluation in interval type-2 membership functions has been derived from the proposed method which includes eight steps for the aggregation and ranking of the preference alternatives. We will present results from the proposed system deployment for the assessment of the postgraduate study where the evaluation involved 10 candidates. The proposed system was able to model the variation in the group decision-making process exhibited by the various decision-makers' opinions. In addition, the proposed system was able to provide a better agreement with human decisions compared to IFS, type-1 and interval type-2 fuzzy systems."
2097179,14018,9704,A steady state decomposition based quantum genetic algorithm for many objective optimization,2013,"Many objective optimization refers to the class of optimization problems involving four or more objectives. Optimal solutions of such problems lie on a hyper surface (the Pareto front) and the dimensionality of the hyper-surface is dependent on the number of objectives in conflict. Identifying well converged and well spread set of solutions spanning the hyper-surface is a non-trivial problem. In this paper we introduce a novel decomposition based steady state quantum genetic algorithm, wherein systematic sampling is used to generate the reference directions and a small population of quantum individuals (solutions with variables represented as Q-bits) is evolved using a simple variation operator. A solution represented using Q-bits has the ability to probabilistically represent a number of solutions defined through observation. We exploit the benefits of quantum representation within a steady state evolution scheme and illustrate the behavior of the algorithm using discrete formulations of the unconstrained DTLZ2 test problem involving 2, 3, 5, and 8 objectives. In order to illustrate the behavior for constrained optimization problems, we investigate the behavior of the algorithm using the water resource optimization problem involving 5 objectives. A quantum population of 5 individuals has been used to solve all the above problems. Preliminary results on the effects of the size of quantum population and the fidelity of representation are presented in this paper. Finally, a number of possible directions are suggested to further improve the performance of the algorithm."
1526537,14018,8806,Schedulability analysis support for automotive systems: from requirement to implementation,2014,"Modeling and analysis of precise non-functional properties, such as energy and timing constraints, is key to the correct development of automotive systems. Automotive applications development cost, in particular, is impacted by incorrect design made at the early development phases but only detected later, often after implementation. This late detection of design errors leads to additional cost. In this paper, we propose a model driven approach to perform non-functional properties verification and to enable scheduling analysis of automotive systems at the very early design level. The different phases of a design range from the requirements to a model allocated on a specific execution platform: East-adl and Marte are used together to specify the structure and energy/timing constraints of the software, as well as the hardware parts of the system. To prove the correctness of specification and perform the scheduling analysis, the semantics of the constraints is given as mapping to a formal interchange format Xfg (eXtended Function-block Graphs) language. The Xfg models are then automatically translated into priced timed automata for model checking. This later transformation is supported by a tool chain called A-BeTA. We demonstrate the applicability of our approach on the Brake-By-Wire case study."
818040,14018,9080,A comparison of different algorithms for the calculation of dominated hypervolumes,2013,"In the fields of multi- and many-objective optimization methods, the hypervolume of a set of solutions is a very useful measure for assessing the current state of the optimization process. It is also the fundamental quality criterion for the well-known SMS-EMOA (S-metric selection evolutionary multi-objective optimization), which is one of the best many objective optimization algorithms known at the moment. Unfortunately, the computation of the hypervolume for a given set of solutions is a time-consuming effort which scales unfavorably with the number of objectives and the size of the population. In this work we analyzed a number of algorithms for hypervolume computation and systematically measured their computational effort for different numbers of objectives and population size. We compared three established standard algorithms that are used in the Shark optimization library and a recent approach by While et al. We also included an approximation computation algorithm proposed by Ishibuchi et al., where we additionally evaluated the precision of the approximation computation and its impact on the selection process within an optimization run. Our findings indicate that the algorithm by While et al. outperforms the three other exact algorithms for a wide range of settings. The Ishibuchi algorithm was shown to have a slightly negative effect on the selection process, but for very large population sizes or number of objectives, the approximation method might be the only viable alternative."
1959337,14018,21102,Fuzzy complex number aided evaluation of predictive toxicology models,2012,"There is a growing interest in applying computational intelligence in the predictive toxicology (PT) domain, where a large number of predictive models are becoming available. Evaluation of such models is therefore considered to be a crucial part of their development and potential use, especially for regulatory purposes. The current evaluation approaches mainly focus on statistical measures of model performance, and few of them have taken data quality into consideration. However, it has been well recognised that datasets and models should not be considered in isolation. This paper proposes a new confidence index for evaluating PT models. A fuzzy complex number (FCN) framework is expanded in an effort to represent and evaluate dataset and regression-based model quality in a two-dimensional manner, thereby ensuring the linguistic evaluation is transparent and explainable. The utility and applicability of this research is illustrated by an experiment which evaluates 17 regression-based PT models. The experimental results have been compared and analysed against existing methods, and show that the FCN-based approach provides a consistent and interpretable means of model assessment. The proposed indexing mechanism can be used, together with customised statistical measures, in assisting PT model selection. This approach also helps to capture the relationships between datasets and models, and contributes to the development of data and model governance in PT."
1424219,14018,9704,Improving the diversity preservation of multi-objective approaches used for single-objective optimization,2013,"The maintenance of a proper diversity is an important issue for the correct behavior of Evolutionary Algorithms (EAs). The loss of diversity might lead to stagnation in suboptimal regions, producing the effect known as “premature convergence”. Several methods to avoid premature convergence have been previously proposed. Among them, the use of Multi-objective Evolutionary Algorithms (MOEAs) is a promising approach. Several ways of using MOEAs for single-objective optimization problems have been devised. The use of an additional objective based on calculating the diversity that each individual introduces in the population has been successfully applied by several researchers. Several ways of measuring the diversity have also been tested. In this work, the main weaknesses of some of the previously presented approaches are analyzed. Considering such drawbacks, a new scheme whose aim is to maintain a better diversity than previous approaches is proposed. The proposed approach is empirically validated using a set of well-known single-objective benchmark problems. Our preliminary results indicate that the proposed approach provides several advantages in terms of premature convergence avoidance. An analysis of the convergence in the average-case is also carried out. Such an analysis reveals that the better ability of our proposed approach to deal with premature convergence produces a reduction in the convergence speed in the average-case for several of the benchmark problems adopted."
1064533,14018,21102,Improving agent interoperability through a memetic ontology alignment: A comparative study,2012,"Interoperability is a key problem in agent-based systems where different interacting computational entities negotiate to achieve a common goal. In last years, this interoperability issue has been faced by exploiting the concept of ontology that enables a single agent to model its knowledge by means of a semantic description of a domain of interest. However, ontology ability to enable a full interoperability can be limited by the so-called semantic heterogeneity problem which arises when some discrepancies exist among ontologies modeling the knowledge related to different agents. As consequence, in order to enable an effective knowledge exchange, an ontology alignment process is necessary to lead proprietary ontologies to a mutual agreement. Recently, some studies have successfully investigated the suitability of memetic algorithms to solve this complex task. However, memetic algorithms are influenced by some design issues arising from the different choices that can be taken to implement them. The aim of this paper is to compare the performances yielded by different memetic ontology alignment systems in order to individuate the most suitable hybrid evolutionary approach which enables a strong agent interoperability. The comparison among the considered approaches is performed by applying a statistical multiple comparison procedure on a collection of ontologies belonging to the well-known Ontology Alignment Evaluation Initiative (OAEI) benchmarks."
1659707,14018,21102,Extracting meta-measures from data for fuzzy aggregation of crowd sourced information,2012,"Fuzzy measures (FMs) have been used to model the (typically subjective) “worth” of subsets of information sources relative to a decision making problem. The fuzzy integral (FI) is a way to fuse the information encoded in a FM with the (typically objective) confidences in the strength of a hypothesis arising from the information sources. In prior work, Yager discussed a set of aggregation functions for general FMs. However, that work is primarily focused on theoretical exploration versus application. Herein, we investigate the direct extraction of different FMs from data, one for specificity and another for agreement, in the context of crowd sourcing. In crowd sourcing, one often has a lack of a ground truth or information regarding the reliability of sources. That is, all sources must be assumed equal (in terms of knowledge, experience level, etc.). Our goal is the intelligent fusion of this data taking into account as much information as possible from the data itself. Once a set of FMs are extracted from the data, we aggregate the FMs (resulting in what we herein refer to as a meta-measure) and use it in fuzzy integration. The novel aspect of this work is the extraction of multiple FMs directly from the original pool of data and the use of the resultant meta-measure and a FI to fuse the data from which the FMs were extracted. Herein, our data is interval-valued, thus we focus on fusion with respect to the generalized interval FI."
744832,14018,9080,Stochastic tunneling transformation during selection in genetic algorithm,2014,"Genetic Algorithms (GA) combine mutational and recombination operators to then select between individuals. Thereby, competition becomes the driving force to improve solutions. Now, this naive approach to biological evolution often assumes a static fitness function, e.g., co-evolutionary effects cannot easily be leveraged.   Here, we introduce a fitness landscape transformation inspired by Monte-Carlo-based optimization schemes. In the Stochastic-Tunneling (STUN) framework fitness values are non-linearly transformed under preservation of the relative ranking of optima. The base line of the STUN-transformation can be set based on different memory mechanisms -- from current to full history.   This STUN-based GA-variant allows to include co-evolution and history into the GA. Based on analytic arguments we can show that the non-linearity of the transformation generates high population densities in areas of interest.   We numerically simulated small, controllable, and well understood test instance: replicas of Ising-spin glasses. For these systems the STUN-GAs have shown significant improvements in terms of relative error for given computational effort. In addition, we introduce an empirical measure of selection to discuss the improved convergence behavior."
1515589,14018,9704,Hybrid metaheuristic for the single vehicle routing problem with deliveries and selective pickups,2012,"This paper presents a hybrid metaheuristic for the single vehicle routing problem with deliveries and selective pickups (SVRPDSP). A vehicle departs loaded from the depot, visit every customer delivering a certain amount of goods according to their demand, and optionally pickup items from those customers, receiving a profit for each pickup realized. The vehicle has a limited capacity, which may turn impossible to attend all pickups, or make this unprofitable if it has to come back later in the customer after unloaded enough to fit the pickup demand. The objective is to find a minimal cost feasible route, the cost being the total travel costs minus the total revenue earned with pickups. Despite the many real applications, the literature is scarce. We propose an evolutionary algorithm whose crossover and mutation operators use data mining strategies to capture good characteristics from the parents and the population. Solutions are improved by a VNS algorithm during the process, and new solutions are introduced regularly to avoid premature convergence, using good constructive algorithms. The algorithm was tested with a benchmark of 68 instances, and the results compared to other publications. The results show the robustness of the method and 7 new solutions were found, including 2 new optimal solutions."
1337840,14018,9704,Differential evolution classifier with optimized distance measures from a pool of distances,2012,"In this article we propose a differential evolution based nearest prototype classifier with extension to selecting the applied distance measure from a pool of alternative measures optimally for the particular data set at hand. The proposed method extends the earlier differential evolution based nearest prototype classifier by extending the optimization process to cover also the selection of distance measure instead of optimizing only the parameters related with a preselected and fixed distance measure. Now the optimization process is seeking also for the best distance measure providing the highest classification accuracy over the selected data set. It has been clear for some time that in classification, the usual euclidean distance measure is sometimes not the best possible choice. Still usually not much has been done for it, and in many cases where some consideration to this problem is given, there has only been testing with a couple of alternative distance measures to find which one provides the highest classification accuracy over the current data set. In this paper we attempt to take one step further by not only enumerating a couple of alternative distance measures, but applying a systematic optimization process to select the best distance measure from a pool of multiple alternative distance measures. In parallel, within the same optimization process, the optimal parameter values related to each alternative distance measures are determined as well as the optimal class prototype vectors for the given data. The empirical results represented are indicating that with several data sets the optimal distance measure is some other measure than the most commonly applied euclidean distance. The results are also suggesting that from the classification accuracy point of view the proposed global optimization approach has high potential in solving classification problems of the studied type. Perhaps the most generally applicable conclusion from our results is, that emphasizing of selection of distance measure is more important to classification accuracy that it has been commonly believed so far."
2790785,14018,20332,M -unit EigenAnt: an ant algorithm to find the M best solutions,2011,"In this paper, we shed light on how powerful congestion control based on local interactions may be obtained. We show how ants can use repellent pheromones and incorporate the effect of crowding to avoid traffic congestion on the optimal path. Based on these interactions, we propose an ant algorithm, the M-unit EigenAnt algorithm, that leads to the selection of the M shortest paths. The ratio of selection of each of these paths is also optimal and regulated by an optimal amount of pheromone on each of them. To the best of our knowledge, the M-unit EigenAnt algorithm is the first ant algorithm that explicitly ensures the selection of the M shortest paths and regulates the amount of pheromone on them such that it is asymptotically optimal. Infact, it is in contrast with most ant algorithms that aim to discover just a single best path. We provide its convergence analysis and show that the steady state distribution of pheromone aligns with the eigenvectors of the cost matrix, and thus is related to its measure of quality. We also provide analysis to show that this property ensues even when the food is moved or path lengths change during foraging. We show that this behavior is robust in the presence of fluctuations and quickly reflects the change in the M optimal solutions. This makes it suitable for not only distributed applications but also dynamic ones as well. Finally, we provide simulation results for the convergence to the optimal solution under different initial biases, dynamism in lengths of paths, and discovery of new paths."
2071594,14018,21102,Decentralized fuzzy fault tolerant control for multiple satellites attitude synchronization,2011,"This paper presents a decentralized adaptive approximation design to achieve attitude tracking control for decentralized formation flying in presence of control input saturation, model uncertainties, external disturbances and reaction wheel faults. A nonsingular fast terminal sliding mode control is designed for finite time distributed cooperative attitude synchronization. In the proposed control scheme, a fuzzy logic system (FLS) is introduced to approximate unknown individual satellite attitude dynamics on-line due to the actuators fault. In order to achieve the capability of fault management without the involvement of ground station operators, the proposed control laws do not requite an explicit fault detection and isolation mechanism. In the attitude control system of each satellite four reaction wheels are placed in a pyramid configuration, numerical simulation results including actuator dynamics and initial conditions' uncertainties show that the proposed strategy with FLS can compensate for the fault and the system continues to operate satisfactorily with wheel voltage or wheel speed faults and the closed loop distributed tracking control system is stochastically stable. Several simulation examples compared with the existing decentralized fault tolerant contoller are presented for illustrating the effectiveness of the proposed fault tolerant control methodology."
1376337,14018,9080,Searching for novel clustering programs,2013,"Novelty search (NS) is an open-ended evolutionary algorithm that eliminates the need for an explicit objective function. Instead, NS focuses selective pressure on the search for novel solutions. NS has produced intriguing results in specialized domains, but has not been applied in most machine learning areas. The key component of NS is that each individual is described by the behavior it exhibits, and this description is used to determine how novel each individual is with respect to what the search has produced thus far. However, describing individuals in behavioral space is not trivial, and care must be taken to properly define a descriptor for a particular domain. This paper applies NS to a mainstream pattern analysis area: data clustering. To do so, a descriptor of clustering performance is proposed and tested on several problems, and compared with two control methods, Fuzzy C-means and K-means. Results show that NS can effectively be applied to data clustering in some circumstances. NS performance is quite poor on simple or easy problems, achieving basically random performance. Conversely, as the problems get harder NS performs better, and outperforming the control methods. It seems that the search space exploration induced by NS is fully exploited only when generating good solutions is more challenging."
2378739,14018,9080,Multiple graph edit distance: simultaneous topological alignment of multiple protein-protein interaction networks with an evolutionary algorithm,2014,"Motivation: We address the problem of multiple protein-protein interaction (PPI) network alignment. Given a set of such networks for different species we might ask how much the network topology is conserved throughout evolution. Solving this problem will help to derive a subset of interactions that is conserved over multiple species thus forming a 'core interactome'. Methods: We model the problem as Topological Multiple one-to-one Network Alignment (TMNA), where we aim to minimize the total Graph Edit Distance (GED) between pairs of the input networks. Here, the GED between two graphs is the number of deleted and inserted edges that are required to make one graph isomorphic to another. By minimizing the GED we indirectly maximize the number of edges that are aligned in multiple networks simultaneously. However, computing an optimal GED value is computationally intractable. We thus propose an evolutionary algorithm and developed a software tool, GEDEVO-M, which is able to align multiple PPI networks using topological information only. We demonstrate the power of our approach by computing a maximal common subnetwork for a set of bacterial and eukaryotic PPI networks. GEDEVO-M thus provides great potential for computing the 'core interactome' of different species. Availability: http://gedevo.mpi-inf.mpg.de/multiple-network-alignment/."
698982,14018,8806,A genetic algorithm for Hierarchical Multi-Label Classification,2012,"In Hierarchical Multi-Label Classification (HMC) problems, each example can be classified into two or more classes simultaneously, differently from standard classification. Moreover, the classes are structured in a hierarchy, in the form of either a tree or a directed acyclic graph. Therefore, an example can be assigned to two or more paths from a hierarchical structure, resulting in a complex classification problem with possibly hundreds or thousands of classes. Several methods have been proposed to deal with such problems, some of them employing a single classifier to deal with all classes simultaneously (global methods), and others employing many classifiers to decompose the original problem into a set of subproblems (local methods). In this work, we propose a novel global method called HMC-GA, which employs a genetic algorithm for solving the HMC problem. In our approach, the genetic algorithm evolves the antecedents of classification rules, in order to optimize the level of coverage of each antecedent. Then, the set of optimized antecedents is selected to build the corresponding consequent of the rules (set of classes to be predicted). Our method is compared to state-of-the-art HMC algorithms, in protein function prediction datasets. The experimental results show that our approach presents competitive predictive accuracy, suggesting that genetic algorithms constitute a promising alternative to deal with hierarchical multi-label classification of biological data."
1450080,14018,9080,Using differential evolution to optimize 'learning from signals' and enhance network security,2011,"Computer and communication network attacks are commonly orchestrated through Wireless Access Points (WAPs). This paper summarizes proof-of-concept research activity aimed at developing a physical layer Radio Frequency (RF) air monitoring capability to limit unauthorized WAP access and improve network security. This is done using Differential Evolution (DE) to optimize the performance of a Learning from Signals (LFS) classifier implemented with RF Distinct Native Attribute (RF-DNA) fingerprints. Performance of the resultant DE-optimized LFS classifier is demonstrated using 802.11a WiFi devices under the most challenging conditions of intra-manufacturer classification, i.e., using emissions of like-model devices that only differ in serial number. Using identical classifier input features, performance of the DE-optimized LFS classifier is assessed relative to a Multiple Discriminant Analysis / Maximum Likelihood (MDA/ML) classifier that has been used for previous demonstrations. The comparative assessment is made using both Time Domain (TD) and Spectral Domain (SD) fingerprint features. For all combinations of classifier type, feature type, and signal-to-noise ratio considered, results show that the DE-optimized LFS classifier with TD features is superior and provides up to 20% improvement in classification accuracy with proper selection of DE parameters."
1559325,14018,9080,Controlling tensegrity robots through evolution,2013,"Tensegrity structures (built from interconnected rods and cables) have the potential to offer a revolutionary new robotic design that is light-weight, energy-efficient, robust to failures, capable of unique modes of locomotion, impact tolerant, and compliant (reducing damage between the robot and its environment). Unfortunately robots built from tensegrity structures are difficult to control with traditional methods due to their oscillatory nature, nonlinear coupling between components and overall complexity. Fortunately this formidable control challenge can be overcome through the use of evolutionary algorithms. In this paper we show that evolutionary algorithms can be used to efficiently control a ball shaped tensegrity robot. Experimental results performed with a variety of evolutionary algorithms in a detailed soft-body physics simulator show that a centralized evolutionary algorithm performs 400% better than a hand-coded solution, while the multiagent evolution performs 800% better. In addition, evolution is able to discover diverse control solutions (both crawling and rolling) that are robust against structural failures and can be adapted to a wide range of energy and actuation constraints. These successful controls will form the basis for building high-performance tensegrity robots in the near future."
1283322,14018,9704,Multi-dimensional scaling and MODELLER-based evolutionary algorithms for protein model refinement,2014,"Protein structure prediction, i.e., computationally predicting the three-dimensional structure of a protein from its primary sequence, is one of the most important and challenging problems in bioinformatics. Model refinement is a key step in the prediction process, where improved structures are constructed based on a pool of initially generated models. Since the refinement category was added to the biennial Critical Assessment of Structure Prediction (CASP) in 2008, CASP results show that it is a challenge for existing model refinement methods to improve model quality consistently.#R##N##R##N#This paper presents three evolutionary algorithms for protein model refinement, in which multidimensional scaling(MDS), the MODELLER software, and a hybrid of both are used as crossover operators, respectively. The MDS-based method takes a purely geometrical approach and generates a child model by combining the contact maps of multiple parents. The MODELLER-based method takes a statistical and energy minimization approach, and uses the remodeling module in MODELLER program to generate new models from multiple parents. The hybrid method first generates models using the MDS-based method and then run them through the MODELLER-based method, aiming at combining the strength of both. Promising results have been obtained in experiments using CASP datasets. The MDS-based method improved the best of a pool of predicted models in terms of the global distance test score (GDT-TS) in 9 out of 16test targets."
2448664,14018,21102,Exploiting Timed Automata based Fuzzy Controllers for voltage regulation in Smart Grids,2011,"The large-scale deployment of the Smart Grid paradigm will support the evolution of conventional electrical power systems toward active, flexible and self-healing web energy networks composed of distributed and cooperative energy resources. In a Smart Grid platform, the optimal coordination of distributed voltage controllers is one of the main issues to address. In this field, the application of traditional control paradigms has some disadvantages that could hinder their application in Smart Grids where the constant growth of grid complexity and the need for massive pervasion of Distribution Generation Systems (DGSs) require more scalable, more flexible control and regulation paradigms. To try and overcome these challenges, this paper proposes the concept of a decentralized non-hierarchical voltage regulation architecture based on intelligent and cooperative smart entities. The distributed voltage controllers employ traditional sensors to acquire local bus variables and mutually coupled oscillators to assess the main variables that characterize the operation of the global Smart Grid. These variables are then amalgamated by a novel fuzzy inference engine, named Timed Automata based Fuzzy Controllers, in order to identify proper control actions aimed at improving the grid voltage profile and reducing power losses."
2197011,14018,9704,Evolutionary multi-objective optimization for the vendor-managed inventory routing problem,2011,"The class of inventory routing problems (IRPs) is present in several areas, including automotive industry and cash management for ATM networks. In the specific case of vendor-managed IRPs, in which the supplier is responsible for managing the product inventory in each client and for properly providing replenishments, the challenge is to determine which retailers should be served, the amount of product that should be delivered to each of these retailers, and which routes the distribution vehicles should follow, so that the associated costs are minimized. Although this is clearly a multi-objective optimization problem, in the literature it has been generally modeled as a single-objective problem, which limits the scope of the obtained results. Therefore, this work presents a multi-objective approach to solve one version of the IRP usually found in the scientific literature, by simultaneously minimizing both the inventory and transportation costs. The method proposed in this work is based on the well-known SPEA2 (Strength Pareto Evolutionary Algorithm) and includes innovative aspects mainly associated with the representation of candidate solutions, genetic operators and local search. The experiments were performed on a set of known benchmark IRPs from the literature, so that the obtained results could be properly compared to the best solution found for the single-objective version of each problem."
1166014,14018,21102,Feature grouping-based fuzzy-rough feature selection,2014,"Data dimensionality has become a pervasive prob- lem in many areas that require the learning of interpretable models. This has become particularly pronounced in recent years with the seemingly relentless growth in the size of datasets. Indeed, as the number of dimensions increases, the number of data instances required in order to generate accurate models increases exponentially. Feature selection has therefore become not only a useful step in the process of model learning, but rather an increasingly necessary one. Rough set and fuzzy-rough set theory have been used as such dataset pre-processors with much success, however the underlying time/space complexity of the subset evaluation metric is an obstacle to the processing of very large data. This paper proposes a general approach to this problem that employs a novel feature grouping step in order to alleviate the processing overhead for large datasets. The approach is framed within the context of (and applied to) fuzzy-rough sets, although it can be used with other subset evaluation techniques. The experimental evaluation demonstrates that considerable computational effort can be avoided, and as a result efficiency can be improved considerably for larger datasets. Index Terms—fuzzy-rough sets, feature selection, feature grouping. I. INTRODUCTION"
838050,14018,9080,Designing a novel hybrid swarm based multiobjective evolutionary algorithm for finding DNA motifs,2013,"In this paper we present a novel local search for improving the ability of multiobjective evolutionary algorithms when finding repeated patterns -motifs- in DNA sequences. In the metaheuristic design, two competing goals must be taken into account: exploration and exploitation. Exploration is needed to cover most of the optimization problem search space and provide a reliable estimation of the global optimum. In turn, exploitation is also important since normally the solutions refinement allows the achievement of better results. In this work we take advantage of both concepts by combining the exploration capabilities of a population-based evolutionary algorithm and the power of a local search, especially designed to optimize the Motif Discovery Problem (MDP). For doing this, we have implemented a new hybrid multiobjective metaheuristic based on Artificial Bee Colony (ABC). After analyzing the results achieved by this algorithm, named Hybrid-MOABC (H-MOABC), and comparing them with those achieved by three multiobjective evolutionary algorithms and thirteen well-known biological tools, we prove that the hybridization computes accurate biological predictions on real genetic instances in an optimum way. In fact, to the best of our knowledge, the results presented in this paper improve those presented in the literature."
1326289,14018,9704,Surrogate model assisted ensemble differential evolution algorithm,2012,"Differential Evolution (DE) is a simple and effective approach for solving numerical optimization problems. However, the performance of DE is sensitive to the choice of the mutation and crossover strategies and their associated control parameters. Therefore, to obtain optimal performance, time consuming parameter tuning is necessary. In DE, different mutation and crossover strategies with different parameter settings can be appropriate during different stages of the evolution. Therefore, to obtain optimal performance using DE, various adaptation and self-adaptation techniques have been proposed. Recently, a DE algorithm with an ensemble of parameters and strategies (EPSDE) was proposed. In EPSDE, a pool of distinct mutation and crossover strategies along with a pool of values for each control parameter coexists throughout the evolution process and competes to produce offspring. The performance of EPSDE degrades if the population members get struck with a combination of strategies and parameters values that produce successful offspring but lead to premature convergence in the due course of the evolution. In this paper, we try to improve the performance of the EPSDE algorithm with the help of a surrogate model that assists in generating competitive trial vectors corresponding to each parent in every generation of the evolution. The proposed algorithm is referred to as surrogate model assisted EPSDE (SMA-EPSDE) and employs a simple Kriging model to construct the surrogate. The performance of EPSDE is evaluated on a set of 17 bound-constrained problems and is compared with state-of-the-art algorithms."
1718453,14018,9080,A parallel evolutionary approach to solve the relay node placement problem in wireless sensor networks,2013,"At this time, Wireless Sensor Networks (WSNs) are widely used in many fields. This kind of network has some attractive features that have promoted their use, such as the absence of wires and the use of low-cost devices. However, WSNs also have important shortcomings that affect some features like energy cost and quality of service. In this paper, we optimize traditional static WSNs (a set of sensors and a sink node) by means of adding routers to simultaneously optimize a couple of important factors: energy consumption and average coverage. This multiobjective optimization problem was solved in a previous work using two genetic algorithms (NSGA-II and SPEA2) which had an important limitation: the computing time was very high and then, to address complex instances was difficult. In this paper, both algorithms are parallelized using OpenMP in order to reduce the computing time, and a more realistic data set is included. The results obtained are analyzed in depth from both multiobjective and parallel viewpoints. A Quite good efficiency is obtained with a wide range of processing cores, observing that NSGA-II provides the best results in small and medium instances, but in the largest ones the behavior of both algorithms is similar."
1747221,14018,9704,Restarting Particle Swarm Optimisation for deceptive problems,2012,"Particle Swarm Optimisation (PSO) has the advantage of finding, if not the optimum of a continuous problem space, at least a very good position and doing this with modest computational cost. However, as the number of possible optima increases, PSO will only explore a subset of these positions. Techniques such as niching can allow a small number of positions to be explored in parallel but by the time that a problem has become truly deceptive, has many many optima, there is little choice but to explore optima sequentially. PSO, once having converged, has no way of dispersing its particle so as to allow a further convergence, hopefully to a new optima. Random restarts are one way of providing this divergence, this paper suggest another inspired by Extremal Optimisation (EO). The technique proposed allows the particles to disperse by way of positions that are fitter than average. After a while dispersion ceases and PSO takes over again, but since it starts from better than average fitnesses the point it converges to is also better than average. This alternation of algorithms can carry on indefinitely. This paper examines the performance of sequential PSO exploration on a range of problems, some deceptive, some non-deceptive. As predicted, performance on deceptive problems tends to improve significantly with time while performance on non-deceptive problems, which do not have multiple positions with comparable fitness to spread through, does not."
1033834,14018,8806,Success factors in mobile social networking application development: case study of instagram,2014,"The distinguished features of mobile computing bring both opportunities and challenges to the development of Mobile Social Networking (MSN) applications. Learning the successful experiences from existing MSN applications is valuable to both industry and academy. The paper aims at identifying the success factors of Instagram, a well-known MSN application. By performing a historical case study, rapid development and reaction, continuous focus on system scalability on demand, and creating a good external environment are regarded as the three important success factors. To achieve those success factors, 1) using existing mature Open Source Software (OSS) extensively and consulting resident experts at websites when necessary are the secrets to develop and react to new requirements rapidly; 2) monitoring the status of the system by tools and focusing on scalability in both architecture design and detail optimization are vital to guarantee the system scalable on demand; and 3) creating appropriate social networks for different type of stakeholders using right popular social media tools is important to create a good external environment effectively. Based on these insights, the development methodology of Instagram has been extracted, that can be used to guide the MSN application development. At the end, the validation, reliability and generalizability of the information gathered from the case study are discussed thoroughly."
1785398,14018,9080,Evolutionary parameter estimation for a theory of planned behaviour microsimulation of alcohol consumption dynamics in an English birth cohort 2003 to 2010,2014,"This paper presents a new real-world application of evolutionary computation: identifying parameterisations of a theory-driven model that can reproduce alcohol consumption dynamics observed in a population over time. Population alcohol consumption is a complex system, with multiple interactions between economic and social factors and drinking behaviours, the nature and importance of which are not well-understood. Prediction of time trends in consumption is therefore difficult, but essential for robust estimation of future changes in health-related consequences of drinking and for appraising the impact of interventions aimed at changing alcohol use in society. The paper describes a microsimulation approach in which an attitude-behaviour model, Theory of Planned Behaviour, is used to describe the frequency of drinking by individuals. Consumption dynamics in the simulation are driven by changes in the social roles of individuals over time (parenthood, partnership, and paid labour). An evolutionary optimizer is used to identify parameterisations of the Theory that can describe the observed changes in drinking frequency. Niching is incorporated to enable multiple possible parameterisations to be identified, each of which can accurately recreate history but potentially encode quite different future trends. The approach is demonstrated using evidence from the 1979-1985 birth cohort in England between 2003 and 2010."
1192131,14018,9704,Optimizing risk management using NSGA-II,2012,"Companies are often susceptible to uncertainties which can disturb the achievement of their objectives. The effect of these uncertainties can be perceived as risk that will be taken. A healthful company have to anticipate undesired events by defining a process for managing risks. Risk management processes are responsible for identifying, analyzing and evaluating risky scenarios and whether they should undergo control in order to satisfy a previously defined risk criteria. Risk specialists have to consider, at the same time, many operational aspects (decision variables) and objectives to decide which and when risk treatments have to be executed. In line with that, most companies select risks to be treated by using expertise of human specialists or simple sorting heuristics based on the believed impact. Companies have limited resources (e.g. human and financial resources) and risk treatments have costs which the selection process has to deal with. Aiming to balancing the competition between risk and resource management this paper proposes a new optimization step within the standard risk management methodology created by the International Organization for Standardization (a.k.a. ISO). To test the resulted methodology, experiments based on the Non-dominated Sorting Genetic Algorithm (more specifically NSGA-II) were performed aiming to manage risk and resources of a simulated company. Results show us that the proposed approach can deal with multiple conflicting objectives reducing the risk exposure time by selecting risks to be treated according their impact and available resources."
1564233,14018,9080,Multi-objective design and analysis of robot gripper configurations using an evolutionary-classical approach,2011,"This paper is concerned with the determination of optimum forces extracted by robot grippers on the surface of a grasped rigid object -- a matter which is crucial to guarantee the stability of the grip without causing defect or damage to the grasped object. A multi-criteria optimization of robot gripper design problem is solved with two different configurations involving two conflicting objectives and a number of constraints. The objectives involve minimization of the difference between maximum and minimum gripping forces and simultaneous minimization of the transmission ratio between the applied gripper actuator force and the force experienced at the gripping ends. Two different configurations of the robot gripper are designed by a state-of-the-art algorithm (NSGA-II) and the obtained results are compared with a previous study. Due to presence of geometric constraints, the resulting optimization problem is highly non-linear and multi-modal. For both gripper configurations, the proposed methodology outperforms the results of the previous study. The Pareto-optimal solutions are thoroughly investigated to establish some meaningful relationships between the objective functions and variable values. In addition, it is observed that one of the gripper configurations completely outperforms the other one from the point of view of both objectives, thereby establishing a complete bias towards the use of one of the configurations in practice."
664468,14018,21102,Obtaining accurate TSK Fuzzy Rule-Based Systems by Multi-Objective Evolutionary Learning in high-dimensional regression problems,2013,"This paper addresses the challenging problem of fuzzy modeling in high-dimensional and large scale regression datasets. To this end, we propose a scalable two-stage method for obtaining accurate fuzzy models in high-dimensional regression problems using approximate Takagi-Sugeno-Kang Fuzzy Rule-Based Systems. In the first stage, we propose an effective Multi-Objective Evolutionary Algorithm, based on an embedded genetic Data Base learning (involved variables, granularities and a slight lateral displacement of fuzzy partitions) together with an inductive rule base learning within the same process. The second stage is a post-processing process based on a second MOEA to perform a rule selection and a fine scatter-based tuning of the Membership Functions. Moreover, it incorporates an efficient Kalman filter to estimate the coefficients of the consequent polynomial functions in the Takagi-Sugeno-Kang rules. In both stages, we include mechanisms in order to significantly improve the accuracy of the model and to ensure a fast convergence in high-dimensional regression problems. The proposed method is compared to the classical ANFIS method and to a well-known evolutionary learning algorithm for obtaining accurate TSK systems in 8 datasets with different sizes and dimensions, obtaining better results."
1428660,14018,9704,smartPATH: A hybrid ACO-GA algorithm for robot path planning,2012,"Path planning is a critical combinatorial problem essential for the navigation of a mobile robot. Several research initiatives, aiming at providing optimized solutions to this problem, have emerged. Ant Colony Optimization (ACO) and Genetic Algorithms (GA) are the two most widely used heuristics that have shown their effectiveness in solving such a problem. This paper presents, smartPATH, a new hybrid ACO-GA algorithm to solve the global robot path planning problem. The algorithm consists of a combination of an improved ACO algorithm (IACO) for efficient and fast path selection, and a modified crossover operator for avoiding falling into a local minimum. Our system model incorporates a Wireless Sensor Network (WSN) infrastructure to support the robot navigation, where sensor nodes are used as signposts that help locating the mobile robot, and guide it towards the target location. We found out smartPATH outperforms classical ACO (CACO) and GA algorithms (as defined in the literature without modification) for solving the path planning problem both and Bellman-Ford shortest path method. We demonstrate also that smartPATH reduces the execution time up to 64.9% in comparison with Bellman-Ford exact method and improves the solution quality up to 48.3% in comparison with CACO."
1459149,14018,8806,Reliable scalable symbolic computation: the design of SymGridPar2,2013,"Symbolic computation is an important area of both Mathematics and Computer Science, with many large computations that would benefit from parallel execution. Symbolic computations are, however, challenging to parallelise as they have complex data and control structures, and both dynamic and highly irregular parallelism. The SymGridPar framework has been developed to address these challenges on small-scale parallel architectures. However the multicore revolution means that the number of cores and the number of failures are growing exponentially, and that the communication topology is becoming increasingly complex. Hence an improved parallel symbolic computation framework is required.   This paper presents the design and initial evaluation of SymGrid-Par2 (SGP2), a successor to SymGridPar that is designed to provide scalability onto 10 6  cores, and hence also provide fault tolerance. We present the SGP2 design goals, principles and architecture. We describe how scalability is achieved using layering and by allowing the programmer to control task placement. We outline how fault tolerance is provided by supervising remote computations, and outline higher-level fault tolerance abstractions.   We describe the SGP2 implementation status and development plans. We report the scalability and efficiency on approximately 2000 cores, and investigate the overheads of tolerating faults for simple symbolic computations."
791661,14018,9704,Landscape characterization of numerical optimization problems using biased scattered data,2012,"The characterization of optimization problems over continuous parameter spaces plays an important role in optimization. A form of “fitness landscape” analysis is often carried out to describe the problem space in terms of modality, smoothness and variable separability. The outcomes of this analysis can then be used as a measure of problem difficulty and to predict the behaviour of a given algorithm. However, the metric value estimates of the landscape characterization are dependent upon the representation scheme adopted and the sampling method used. Consequently, the development of a complete classification of problem structure and complexity has proven to be challenging. In this paper, we continue this line of research. We present a methodology for the characterization of two dimensional numerical optimization problems. In our approach, data extracted during the search process is analyzed and the dependency of the results to the nominated sampling method are corrected. We show via computational simulations that the calculated metric values using our approach are consistent with the results from random experiments. As such, this study provides a first step towards the on-line calculation of fitness landscape characterization metrics and the development of empirical performance models of search algorithms. Advances in these areas would provide answers to the algorithm selection and portfolio configuration problems."
1331702,14018,9080,Asynchronous master/slave moeas and heterogeneous evaluation costs,2012,"Parallel master-slave evolutionary algorithms easily lead to linear speedups in the case of a small number of nodes... and homogeneous computational costs of the evaluations. However, modern computer now routinely have several hundreds of nodes - and in many real-world applications in which fitness computation involves heavy numerical simulations, the computational costs of these simulations can greatly vary from one individual to the next. A simple answer to the latter problem is to use asynchronous steady-state reproduction schemes. But the resulting algorithms then differ from the original sequential version, with two consequences: First, the linear speedup does not hold any more; Second, the convergence might be hindered by the heterogeneity of the evaluation costs. The multi-objective optimization of a diesel engine is first presented, a real-world case study where evaluations are very heterogeneous in terms of CPU cost. Both the speedup of asynchronous parallel algorithms in case of large number of nodes, and their convergence toward the Pareto Front in case of heterogeneous computation times, are then experimentally analyzed on artificial test functions. An alternative selection scheme involving the computational cost of the fitness evaluation is then proposed, that counteracts the effects of heterogeneity on convergence toward the Pareto Front."
907445,14018,9080,Evolving distributed resource sharing for cubesat constellations,2012,"Advances in miniaturization will allow for the commoditization of large numbers of tiny satellites, known as CubeSats. However, current algorithms made for small tightly-managed space missions are ill-designed to take advantage of the huge amount of resources available in a decentralized collection of these CubeSats. We believe that multiagent evolutionary algorithms are ideally suited to exploit the distributed nature of this new problem. This paper presents a solution where a customer in need of satellite observations can reliably obtain these observations at low cost, through the help of a multiagent system as an intermediary. Each agent in this system is assigned to a single CubeSat. Given a set of the customer's observational needs, and models of the CubeSats' salient properties, the agents evolve policies that attempt to purchase an appropriate set of observations at a low price. This system is especially flexible as it demands no centralized resource broker, contracts or commitments of resources. We perform a series of experiments on an Earth-observition domain. The results show that the evolutionary methods combined with multiagent techniques have three times the performance of a simple hand-coded allocation algorithm, and twice the performance of simple evolving agents."
2087173,14018,9704,An adaptive approach for solving dynamic scheduling with time-varying number of tasks — Part II,2011,"Changes in environment is common in daily activities and usually introduce new problems. To be adaptive to these changes, new solutions to the problems are to be found every time change occur. Our previous publication showed that centroid of non-dominated solutions associated with Multi-Objective Evolutionary Algorithm (MOEA) from previous changes enhances the search quality of solutions for the current change. However, the number of tasks in the test environment employed was fixed. In this two-part paper, we address the dynamic adaptation with time-varying task number. To cope with this variability, new components of the solution, corresponding to the new tasks, are inserted appropriately to all solutions of the previous changes. Then centroid of these modified solutions is recomputed. Further, to avoid confusion in solution presentation, the insertion of new tasks obliged the use of task ID number greater than the largest of the previous IDs. The first part of this paper will show that the resulting task numbering system will alter the centroid significantly which will degrade MOEA's search quality. To circumvent, task IDs are mapped to new values in order to minimize difference in IDs between adjacent solution components; an approach which significantly upgraded the search performance despite changes in task number as supported by the obtained results."
2557315,14018,9704,A Concentration-based Artificial Immune Network for combinatorial optimization,2011,"Diversity maintenance is an important aspect in population-based metaheuristics for optimization, as it tends to allow a better exploration of the search space, thus reducing the susceptibility to local optima in multimodal optimization problems. In this context, metaheuristics based on the Artificial Immune System (AIS) framework, especially those inspired by the Immune Network theory, are known to be capable of stimulating the generation of diverse sets of solutions for a given problem, even though generally implementing very simple mechanisms to control the dynamics of the network. To increase such diversity maintenance capability even further, a new immune-inspired algorithm was recently proposed, which adopted a novel concentration-based model of immune network. This new algorithm, named cob-aiNet (Concentration-based Artificial Immune Network), was originally developed to solve real-parameter single-objective optimization problems, and it was later extended (with cob-aiNet[MO]) to deal with real-parameter multi-objective optimization. Given that both cob-aiNet and cob-aiNet[MO] obtained competitive results when compared to state-of-the-art algorithms for continuous optimization and also presented significantly improved diversity maintenance mechanisms, in this work the same concentration-based paradigm was further explored, in an extension of such algorithms to deal with single-objective combinatorial optimization problems. This new algorithm, named cob-aiNet[C], was evaluated here in a series of experiments based on four Traveling Salesman Problems (TSPs), in which it was verified not only the diversity maintenance capabilities of the algorithm, but also its overall optimization performance."
1358727,14018,9704,Evolving hierarchical gene regulatory networks for morphogenetic pattern formation of swarm robots,2014,"Morphogenesis, the biological developmental process of multicellular organisms, is a robust self-organising mechanism for pattern formation governed by gene regulatory networks (GRNs). Recent findings suggest that GRNs often show the use of frequently recurring patterns termed network motifs. Inspired by these biological studies, this paper proposes a morphogenetic approach to pattern formation for swarm robots to entrap targets based on an evolving hierarchical gene regulatory network (EH-GRN). The proposed EH-GRN consists of two layers: The upper layer is for adaptive pattern generation where the GRN model is evolved by basic network motifs, and the lower layer is responsible for driving robots to the target pattern generated by the upper layer. Obstacle information is introduced as one of environmental inputs along with that of targets in order to generate patterns adaptive to unknown environmental changes. Besides, splitting or merging of multiple patterns resulting from target movement is addressed by the inherent feature of the upper layer and the k-means clustering algorithm. Numerical simulations have been performed for scenarios containing static/moving targets and obstacles to validate the effectiveness and benefit of the proposed approach for complex shape generation in dynamic environments."
1850096,14018,9704,Multi-criteria layout synthesis of MEMS devices using memetic computing,2011,"This paper introduces a multi-objective optimization approach for layout synthesis of MEMS components. A case study of layout synthesis of a comb-driven micro-resonator shows that the approach proposed in this paper can lead to design results accommodating two design objectives, i.e. simultaneous minimization of size and power input of a MEMS device, while investigating optimum geometrical configuration as the main concern. The major contribution of this paper is the application of memetic computing in MEMS design. An evolutionary multi-objective optimization (EMO) technique, in particular non-dominated sorting genetic algorithm (NSGA-II), has been applied to find multiple trade-off solutions followed by a gradient-based local search, i.e. sequential quadratic programming (SQP), to improve the convergence of the obtained Pareto-optimal front. In order to reduce the number of function evaluations in the local search procedure, the obtained non-dominated solutions are clustered in the objective space and consequently, a post-optimality study is manually performed to find out some common design principles among those solutions. Finally, two reasonable design choices have been offered based on manufacturability issues."
1939646,14018,21102,Higher order sliding fuzzy type-2 interval control for SISO uncertain nonlinear systems,2011,"A higher order sliding fuzzy type-2 controller scheme for nonlinear uncertain perturbed systems is proposed in the paper. To overcome the constraint on the knowledge of the system model, local models related to some operating points were used to synthesize a nominal fuzzy type-2 global model. The controller uses integral sliding mode concept and contains two parts. The first one leads to achieve finite time stabilization of the higher order input output dynamics without uncertainties. The second part has the object to reject bounded uncertainties throughout the entire response of the system. Two adaptive fuzzy type-2 systems have been introduced to generate the two Super Twisting signals to avoid both the chattering and the constraint on the knowledge of upper bounds of both disturbances and uncertainties. These fuzzy type-2 systems are adjusted on-line by adaptation laws deduced from the stability analysis in Lyapunov sens. The advantages of the method are that its implementation is easy, the convergence time is chosen in advance and the robustness is ensured. A robot arm actuated by a DC motor is used as illustrative example. The obtained results show the good tracking performances and the applicability of the method."
2263372,14018,9704,Eliciting Customer Wishes Using Example-Based Heuristics in E-Commerce Applications,2011,"The ubiquitous access to information via the Web has changed our daily life and business practices. E-commerce applications allow for a wide variety of vendors to compete in a world-wide market. But with a growing number of vendors, also the amount of available offers is increasing, which in turn leads to an information flood that may severely hamper the user experience. Skyline algorithms as introduced by the information systems community, promise to winnow suboptimal offers from electronic marketplaces. For the amount of Web data and the typical interaction style, however, the result sets are still too large and hard to manage. Recently, first approaches to integrate human decision processes like com-promises or trade-offs have been designed. In this paper we will build on these approaches and introduce a novel heuristic into the sky lining paradigm that not only allows for convenient Web-style user interaction, but also focuses searches on semantic clusters of offers. Thus, the view on interesting clusters is refined, whereas less interesting clusters are strongly reduced in size and strictly focused on only the outstanding items."
1025662,14018,9080,Inferring large scale genetic networks with S-system model,2013,"Gene regulatory network (GRN) reconstruction from high-throughput microarray data is an important problem in systems biology. The S-System model, a differential equation based approach, is among the mainstream approaches for modeling GRNs. It has the ability to represent GRNs accurately with precise regulatory weights. However, the current applications of S-System are limited to small and medium scale network, as inferring large network requires inhibitive computational cost. In this paper, we propose a novel S-System based framework to reconstruct biologically relevant GRNs by exploiting their special topological structure. In GRNs, the complex interactions occurring amongst transcription factors (TFs) and target genes (TGs) are unidirectional, i.e., TFs to TGs, and the vice-versa is biologically irrelevant. In addition, TFs can regulate themselves while only self-regulations may exist for TGs. As such, we decompose GRN into two sub-networks representing TF-TF and TF-TG interactions. We learn the sub-networks separately by adapting the traditional S-System model, and combining the solutions to get the entire network. Our experimental studies indicate that the proposed approach can scale up to larger networks, not achievable with other current S-System based approaches, yet with higher accuracy."
1464702,14018,21102,Data mining and modeling to predict the necessity of vasopressors for ICU patients,2013,"Shock is a life-threatening medical condition requiring the administration of powerful drugs - vasopressors. Early identification of these patients is a worthy goal in order to timely prepare them for therapy. A subset composed of the most frequently sampled and readily available variables in an intensive care unit (ICU) was used for clustering patients. Then, a data exploration process was started through the use of fuzzy clustering with the fuzzy cmeans algorithm, where four clusters were obtained and the groups characteristics were analyzed. A relationship between the clusters obtained and the use of vasopressors was found out and these results were visualized with the help of histograms. First, a single model was derived. Then, four models were trained and used for a multi model approach, one for each identified group of patients. In both cases fuzzy models were used as they are universal approximators. For the multi-model approach, two decision criteria were used. First a decision a priori based on the distance from the clusters centers to the patient characteristics was used. Lastly a decision a posteriori approach where each model was used and the final outcome used is based on the uncertainty of the output response to the threshold of each model. The multi model approach with a posteriori decision had a better performance of the two schemes tested, and also performed better than the single general model approach."
1699313,14018,9704,A modified brain storm optimization,2012,"Brain storm optimization (BSO) is a new kind of swarm intelligence algorithm inspired by human creative problem solving process. Human being is the most intelligent organism in the world and the brainstorming process popularly used by them has been demonstrated to be a significant and promising way to create great ideas for problem solving. BSO transplants the brainstorming process in human being into optimization algorithm design and gains successes. BSO generally uses the grouping, replacing, and creating operators to produce ideas as many as possible to approach the problem global optimum generation by generation. In this paper, we propose two novel designs to enhance the conventional BSO performance. The first design of the modified BSO (MBSO) is that it uses a simple grouping method (SGM) in the grouping operator instead of the clustering method to reduce the algorithm computational burden. The second design is that MBSO uses a novel idea difference strategy (IDS) in the creating operator instead of the Gaussian random strategy. The IDS not only contains open minded element to avoid the ideas being trapped by local optima, but also can match the search environment to create better new ideas for problem solving. Experiments have been conducted to illustrate the effectiveness and efficiency of the MBSO algorithm. Moreover, the contributions of SGM and IDS are investigated to show how and why MBSO can perform better than BSO."
969509,14018,21102,The M-Designer: Proficient skilled material design software,2012,"In recent year, high computer performance is required and also highly performed and downsized semiconductors are required. Multi-layered semiconductor is one of the most important technologies to enhance its performance. To bond between each layer of multi-chip package, die-bonding film is known as an effective material instead of general boding material. In this respect, we had developed a novel low-modulus die-bonding adhesive film. Properties of the films are widely changed by the ratio of epoxy resin and acrylic polymer contents. To optimize the properties of the die-bonding films, the influence of various parameters on material properties was examined. However, since die-bonding film needs multiple features, it is not so easy for researchers to discover and develop the useful characteristics. In the strategy of development, new materials should develop at a low cost and for a brief period. To solve the problem, this paper proposes the weak conditioned combinatorial linear programming method (WCCLP). By defining solution area as a function of combination index, optimum epoxy resin content, acrylic polymer content is acquired. This optimization can be done by newly developed user-friendly software. Effectiveness of the proposed method is shown with basic analyses and the result of experiments. The software is applicable not only to semiconductor related materials but also to any formulation such as paint, medicine, food."
1785593,14018,9080,Evolving morphologies and controllers for soft-bodied multicellular animats using gene regulatory networks and artificial embryogenesis,2012,"We provide a short review of our recent work on the evolution of soft-bodied animats able to swim in a two-dimensional fluid-like environment. Our Artificial Life system, called \mbox{GReaNs}, is based on a model of a gene regulatory network (GRN), encoded in a linear genome, and a model of multicellular development. Animat bodies are created by converting cells to point masses and connecting nearby cells with elastic springs. Outer cells form a skin, an external envelope subjected to fluid drag. Each cell can modify the resting length of the springs attached to it, which creates motion in the animat. We review results that we obtained through two approaches for evolvable control. In the first approach, the spring contractions and extensions follow the sine function. The frequency, phase, and amplitude associated with a particular spring are determined at the end of development, by the state of the GRN in the two cells controlling the spring. The parameters remain fixed as the animat moves. In the second approach, the GRN controls the cell's springs in real time, and cells can also communicate through diffusive signals. Various locomotion modes based on diverse morphologies emerge in both setups, including undulation and the use of primitive appendages."
1216553,14018,9080,GPU-parallel subtree interpreter for genetic programming,2014,"Genetic Programming (GP) is a computationally intensive technique but its nature is embarrassingly parallel. Graphic Processing Units (GPUs) are many-core architectures which have been widely employed to speed up the evaluation of GP. In recent years, many works have shown the high performance and efficiency of GPUs on evaluating both the individuals and the fitness cases in parallel. These approaches are known as population parallel and data parallel. This paper presents a parallel GP interpreter which extends these approaches and adds a new parallelization level based on the concurrent evaluation of the individual's subtrees. A GP individual defined by a tree structure with nodes and branches comprises different depth levels in which there are independent subtrees which can be evaluated concurrently. Threads can cooperate to evaluate different subtrees and share the results via GPU's shared memory. The experimental results show the better performance of the proposal in terms of the GP operations per second (GPops/s) that the GP interpreter is capable of processing, achieving up to 21 billion GPops/s using a NVIDIA 480 GPU. However, some issues raised due to limitations of currently available hardware are to be overcomed by the dynamic parallelization capabilities of the next generation of GPUs."
1716621,14018,9080,Artificial immune systems for optimisation,2012,"Artificial immune systems (AIS) are a class of biologically inspired algorithms which are build after different theories from immunology. While the field of AIS is a relatively new area of research, it has achieved numerous promising results in different areas of application, e.g., learning, classification, anomaly detection, and optimization. In this tutorial we focus in particular on AIS build for the purpose of optimization. From an algorithmic point of view AIS show on a high level similarities to other biologically inspired algorithms, e.g. evolutionary algorithms. Due to their different origin concrete AIS for optimization are quite different from evolutionary algorithms. They constitute an interesting alternative approach to current methods. The tutorial gives an overview over different methods in the field of AIS. It addresses everyone who wants to broaden his or her area of research within this emerging field, both practitioners and theoreticians. It enables attendees without prior knowledge of AIS to learn about a novel kind of optimization method that can be used as an alternative to other biologically inspired algorithms. Moreover, it gives researchers with prior knowledge of AIS the opportunity to deepen their understanding of the considered algorithms.   We start with an overview over the different areas of AIS, including different general approaches and some immunological background. Afterwards, we discuss several examples of AIS for optimization. We introduce concrete algorithms and their implementations and point out similarities and differences to other biologically inspired algorithms. In the last part of the tutorial, we present an overview over recent theoretical results for this kind of algorithms."
1327295,14018,9080,Evolving large scale UAV communication system,2012,"Unmanned Aerial Vehicles (UAVs) have traditionally been used for short duration missions involving surveillance or military operations. Advances in batteries, photovoltaics and electric motors though, will soon allow large numbers of small, cheap, solar powered unmanned aerial vehicles (UAVs) to fly long term missions at high altitudes. This will revolutionize the way UAVs are used, allowing them to form vast communication networks. However, to make effective use of thousands (and perhaps millions) of UAVs owned by numerous disparate institutions, intelligent and robust coordination algorithms are needed, as this domain introduces unique congestion and signal-to-noise issues. In this paper, we present a solution based on evolutionary algorithms to a specific ad-hoc communication problem, where UAVs communicate to ground-based customers over a single wide-spectrum communication channel. To maximize their bandwidth, UAVs need to optimally control their output power levels and orientation. Experimental results show that UAVs using evolutionary algorithms in combination with appropriately shaped evaluation functions can form a robust communication network and perform 180% better than a fixed baseline algorithm as well as 90% better than a basic evolutionary algorithm."
1802287,14018,9080,Applying search algorithms for optimizing stakeholders familiarity and balancing workload in requirements assignment,2014,"During the early phase of project development lifecycle of large scale cyber-physical systems, a large number of requirements are needed to be assigned to different stakeholders from different organizations or different departments of the same organization for reviewing, clarifying and checking their conformance to industry standards and government or other regulations. These requirements have different characteristics such as various extents of importance to the organization, complexity, and dependencies between each other, thereby requiring different effort (workload) to review and clarify. While working with our industrial partners in the domain of cyber-physical systems, we discovered an optimization problem, where an optimal solution is required for assigning requirements to different stakeholders by maximizing their familiarities to the assigned requirements while balancing the overall workload of each stakeholder. We propose a fitness function which was investigated with four search algorithms: (1+1) Evolutionary Algorithm (EA), Genetic Algorithm, and Alternating Variable Method, whereas Random Search is used as a comparison base line. We empirically evaluated their performance for finding an optimal solution using a large-scale industrial case study and 120 artificial problems with varying complexity. Results show that (1+1) EA gives the best results together with our proposed fitness function as compared to the other three algorithms."
709979,14018,21102,Fuzzy logic-based evaluation function for haptic tasks,2012,"This paper discusses the possibility of improving eye-hand coordination in children diagnosed with this problem, using a robotic mapping from a haptic user interface to a virtual environment. The goal is to develop, implement and refine a system that will assess and improve the eye-hand coordination and grip strength in children diagnosed with having problems with it. A detailed analysis of patters (e.g., labyrinths, letters and angles) was conducted which would yield the greatest benefit in terms of assessment of coordination and strength issues as well as in training. Support algorithms (position, force, velocity, inertia and viscosity) were also developed and incorporated into the tasks to assist the user's movements. The evaluate performance (given by % accuracy and time) of the executed tasks; a sophisticated evaluation function was designed based on image analysis and edge detection algorithms. A Fuzzy Logic-based function that takes the accuracy and time results and makes a suggestion in the next task to be performed is also described in this paper. This paper also presents the results of a study implemented at the Motor Development Clinic at Cal Poly Pomona. The purpose of this study was to test the functionality of the haptic tasks, evaluation function and Fuzzy Logic-based decision making algorithm."
1899510,14018,9704,A simple optimization method based on Backtrack and GA for delivery schedule,2011,"A delivery route optimization system greatly improves the real time delivery efficiency. To realize such an optimization, its distribution network requires solving several tens to hundreds (max. 1500–2000) cities Traveling Salesman Problems (TSP) within interactive response time (around 3 seconds) with expert-level accuracy (below 3% level of error rate). Moreover, as for the algorithms, understandability and flexibility are necessary because field experts and field engineers can understand and adjust it to satisfy the field conditions. To meet these requirements, a Backtrack and Restart Genetic Algorithm (Br-GA) is proposed. This method combines Backtracking and GA having simple heuristics such as 2-opt and NI (Nearest Insertion) so that, in case of stagflation, GA can restarts with the state of populations going back to the state in the generation before stagflation. Including these heuristics, field experts and field engineers can easily understand the way and use it. Using the tool applying their method, they can easily create/modify the solutions or conditions interactively depending on their field needs. Experimental results proved that the method meets the above-mentioned delivery scheduling requirements more than other methods from the viewpoint of optimality as well as simplicity."
1326051,14018,9080,Scaling up a hybrid genetic linear programming algorithm for statistical disclosure control,2011,"This paper looks at the real world problem of statistical disclosure control. National Statistics Agencies are required to publish detailed statistics and simultaneously guarantee the confidentiality of the contributors. When published statistical tables contain magnitude data such as turnover or health statistics the preferred method is to suppress the values of cells which may reveal confidential information. However suppressing these 'primary' cells alone will not guarantee protection due the presence of margin (row/column) totals and therefore other 'secondary' cells must also be suppressed. A previously developed algorithm that hybridizes linear programming with a genetic algorithm has been shown to protect tables with up to 40,000 cells, however Statistical Agencies are often required to protect tables with over 100,000 cells.   This algorithm's performance highly depended on the choice of mutation operator so firstly this dependency was removed. As the algorithm is unable to protect larger tables due to the time it takes for its fitness function (a linear program) to execute a series of modifications have been applied. These modifications significantly reduced its execution time which in turn greatly extend the capabilities of the hybrid algorithm to the point that it can now protect tables with up to one million cells."
1192660,14018,21102,A qualitative evaluation criterion for human-robot interaction system in achieving collective tasks,2012,"This work intends to identify common performance metrics for task-oriented human-robot interaction. We present a methodology to assess the system performance of a human-robot team in achievement of collective tasks. We propose a systematic approach that addresses the performance of both the human user and the robotic agent as a team. Toward this end, we attempt to determine the true time that an operator has to dedicate to a robot in action. We define the robot attention demand (RAD) as a function of both direct interaction time (DIT) and indirect interaction time (IIT), where the IIT is a direct consequence of the human trust in automation. We propose a two-level fuzzy temporal model to evaluate the human trust in automation while interacting with robots. Another fuzzy temporal model is presented to evaluate the human reliability during interaction time. The model is then generalized to accommodate multi-robot scenarios. Sequential and parallel robot cooperation schemes with varying levels of task dependency are considered. The fuzzy knowledge bases are further updated by implementing an application robotic platform where robots and users interact naturally to complete tasks with varying levels of complexity. User feedback is noted and used to tune the knowledge base rules where needed, to better represent a human expert's knowledge."
2672974,14018,9080,Kaizen programming,2014,"This paper presents Kaizen Programming, an evolutionary tool based on the concepts of Continuous Improvement from Kaizen Japanese methodology. One may see Kaizen Programming as a new paradigm since, as opposed to classical evolutionary algorithms where individuals are complete solutions, in Kaizen Programming each expert proposes an idea to solve part of the problem, thus a solution is composed of all ideas together. Consequently, evolution becomes a collaborative approach instead of an egocentric one. An idea's quality (analog to an individual's fitness) is not how good it fits the data, but a measurement of its contribution to the solution, which improves the knowledge about the problem. Differently from evolutionary algorithms that simply perform trial-and-error search, one can determine, exactly, parts of the solution that should be removed or improved. That property results in the reduction in bloat, number of function evaluations, and computing time. Even more important, the Kaizen Programming tool, proposed to solve symbolic regression problems, builds the solutions as linear regression models - not linear in the variables, but linear in the parameters, thus all properties and characteristics of such statistical tool are valid. Experiments on benchmark functions proposed in the literature show that Kaizen Programming easily outperforms Genetic Programming and other methods, providing high quality solutions for both training and testing sets while requiring a small number of function evaluations."
1139368,14018,9080,An EA-based approach to design optimization using evidence theory,2011,"For problems involving uncertainties in design variables and parameters, a bi-objective evolutionary algorithm (EA) based approach to design optimization using evidence theory is proposed and implemented in this paper. In addition to a functional objective, a plausibility measure of failure of constraint satisfaction is minimized. Despite some interests in classical optimization literature, such a consideration in EA is rare. Due to EA's flexibility in its operators, non-requirement of any gradient, its ability to handle multiple conflicting objectives, and ease of parallelization, evidence-based design optimization using an EA is promising. Results on a test problem and a couple of engineering design problems show that the modified evolutionary multi-objective optimization (EMO) algorithm is capable of finding a widely distributed trade-off frontier showing different optimal solutions corresponding to different levels of plausibility failure limits. Furthermore, a single-objective evidence based EA is found to produce better optimal solutions than a previously reported classical optimization procedure. Handling uncertainties of different types are getting increasingly popular in applied optimization studies and more such studies using EAs will make EAs more useful and pragmatic in practical optimization problem-solving tasks."
714339,14018,9080,Behavioral repertoire learning in robotics,2013,"Learning in robotics typically involves choosing a simple goal (e.g. walking) and assessing the performance of each controller with regard to this task (e.g. walking speed). However, learning advanced, input-driven controllers (e.g. walking in each direction) requires testing each controller on a large sample of the possible input signals. This costly process makes difficult to learn useful low-level controllers in robotics. Here we introduce BR-Evolution, a new evolutionary learning technique that generates a behavioral repertoire by taking advantage of the candidate solutions that are usually discarded. Instead of evolving a single, general controller, BR-evolution thus evolves a collection of simple controllers, one for each variant of the target behavior; to distinguish similar controllers, it uses a performance objective that allows it to produce a collection of diverse but high-performing behaviors. We evaluated this new technique by evolving gait controllers for a simulated hexapod robot. Results show that a single run of the EA quickly finds a collection of controllers that allows the robot to reach each point of the reachable space. Overall, BR-Evolution opens a new kind of learning algorithm that simultaneously optimizes all the achievable behaviors of a robot."
688663,14018,9080,GEARNet: grammatical evolution with artificial regulatory networks,2013,"The Central Dogma of Biology states that genes made proteins that made us. This principle has been revised in order to incorporate the role played by a multitude of regulatory mechanisms that are fundamental in both the processes of inheritance and development. Evolutionary Computation algorithms are inspired by the theories of evolution and development, but most of the computational models proposed so far rely on a simple genotype to phenotype mapping. During the last years some researchers advocate the need to explore computationally the new biological understanding and have proposed different gene expression models to be incorporated in the algorithms.Two examples are the Artificial Regulatory Network (ARN) model, first proposed by Wolfgang Banzhaf, and the Grammatical Evolution (GE) model, introduced by Michael O'Neill and Conor Ryan. In this paper, we show how a modified version of the ARN can be combined with the GE approach, in the context of automatic program generation. More precisely, we rely on the ARN to control the gene expression process ending in an ordered set of proteins, and on the GE to build, guided by a grammar, a computational structure from that set. As a proof of concept we apply the hybrid model to two benchmark problems and show that it is effective in solving them."
1256967,14018,9080,Efficient global optimization for combinatorial problems,2014,"Real-world optimization problems may require time consuming and expensive measurements or simulations. Recently, the application of surrogate model-based approaches was extended from continuous to combinatorial spaces. This extension is based on the utilization of suitable distance measures such as Hamming or Swap Distance. In this work, such an extension is implemented for Kriging (Gaussian Process) models. Kriging provides a measure of uncertainty when determining predictions. This can be harnessed to calculate the Expected Improvement (EI) of a candidate solution. In continuous optimization, EI is used in the Efficient Global Optimization (EGO) approach to balance exploitation and exploration for expensive optimization problems. Employing the extended Kriging model, we show for the first time that EGO can successfully be applied to combinatorial optimization problems. We describe necessary adaptations and arising issues as well as experimental results on several test problems. All surrogate models are optimized with a Genetic Algorithm (GA). To yield a comprehensive comparison, EGO and Kriging are compared to an earlier suggested Radial Basis Function Network, a linear modeling approach, as well as model-free optimization with random search and GA. EGO clearly outperforms the competing approaches on most of the tested problem instances."
827132,14018,9080,Instance-linked attribute tracking and feedback for michigan-style supervised learning classifier systems,2012,"The application of learning classifier systems (LCSs) to classification and data mining in genetic association studies has been the target of previous work. Recent efforts have focused on: (1) correctly discriminating between predictive and non-predictive attributes, and (2) detecting and characterizing epistasis (attribute interaction) and heterogeneity. While the solutions evolved by Michigan-style LCSs (M-LCSs) are conceptually well suited to address these phenomena, the explicit characterization of heterogeneity remains a particular challenge. In this study we introduce attribute tracking, a mechanism akin to memory, for supervised learning in M-LCSs. Given a finite training set, a vector of accuracy scores is maintained for each instance in the data. Post-training, we apply these scores to characterize patterns of association in the dataset. Additionally we introduce attribute feedback to the mutation and crossover mechanisms, probabilistically directing rule generalization based on an instance's tracking scores. We find that attribute tracking combined with clustering and visualization facilitates the characterization of epistasis and heterogeneity while uniquely linking individual instances in the dataset to etiologically heterogeneous subgroups. Moreover, these analyses demonstrate that attribute feedback significantly improves test accuracy, efficient generalization, run time, and the power to discriminate between predictive and non-predictive attributes in the presence of heterogeneity."
981806,14018,21102,An adaptive learning fuzzy logic system for indoor localisation using Wi-Fi in Ambient Intelligent Environments,2012,"One of the important requirements for Ambient Intelligent Environments (AIEs) is the ability to localise the whereabouts of the user in the AIE to address her/his needs. The outdoor localisation means (like GPS systems) cannot be used in indoor environments. The majority of non intrusive and non camera based indoor localisation systems require the installation of extra hardware such as ultra sound emitters/antennas, RFID antennas, etc. In this paper, we will propose a novel fuzzy logic based indoor localisation system which is based on the WiFi signals which are free to receive and they are available in abundance in the majority of domestic spaces. The proposed system receives WiFi signals from a big number of existing WiFi Access Points (up to 170 Access Points) with no prior knowledge of the access points locations and the environment. The proposed system is able to adapt online incrementally in a lifelong learning mode to deal with the uncertainties and changing conditions facing unknown indoor structures with a few days of calibration at zero-cost deployment with high accuracy. The proposed system was tested in simulated and real environments where the system has given high accuracy (that outperformed the existing techniques) to detect the user in the given AIE and the system was able also to adapt its behaviour to changes in the AIE or the WiFi signals."
892139,14018,9080,A two-leveled hybrid dendritic cell algorithm under imprecise reasoning,2014,"The Dendritic Cell Algorithm(DCA) is a bio-inspired algorithm based on the behavior of Dendritic Cells(DCs). The DCA performance relies on its data pre-processing phase where feature extraction and signal categorization are performed and which are based on the use of the Principal Component Analysis(PCA) technique. However, using PCA presents a limitation as it destroys the underlying semantics of the features after reduction. To overcome this limitation, Rough Set Theory(RST) was applied as a pre-processor; but, still the developed rough approach presents an information loss as data should be discretized beforehand. Indeed, DCA was known to be sensitive to the input class data order. This is due to the crisp separation between the two DCs contexts; semi-mature and mature. Thus, the aim of this paper is to develop a novel DCA version based on a two-leveled hybrid model handling the mentioned DCA shortcomings. In the top-level, our proposed algorithm applies a more adequate feature extraction technique based on Fuzzy Rough Set Theory(FRST) to build a solid data pre-processing phase. At the bottom level, our algorithm applies Fuzzy Set Theory to smooth the crisp separation between the DCs contexts. Results show that our proposed algorithm succeeds in obtaining significantly improved classification accuracy."
1793637,14018,9080,Grammatical evolution decision trees for trio designs,2012,"The detection of gene-gene and gene-interactions in genetic association studies is an important challenge in human genetics. The detection of such interactive models presents a difficult computational and statistical challenge, especially as advances in genotyping technology have rapidly expanded the number of potential genetic predictors in such studies. The scale of these studies makes exhaustive search approaches infeasible, inspiring the application of evolutionary computation algorithms to perform variable selection and build classification models. Recently, an application of grammatical evolution to evolve decision trees (GEDT) has been introduced for detecting interaction models. Initial results were promising, but the previous applications of GEDT have been limited to case-control studies with unrelated individuals. While this study design is popular in human genetics, other designs with related individuals offer distinct advantages. Specifically, a trio-based design (with genetic data for an affected individual and their parents collected) can be a powerful approach to mapping that is robust to population heterogeneity and other potential confounders. In the current study, we extend the GEDT approach to be able to handle trio data (trioGEDT), and demonstrate its potential in simulated data with gene-gene interactions that underlie disease risk."
1210588,14018,8806,MINI: a 3D mobile image browser with multi-dimensional datasets,2012,"Results of refined searches are often complicated because they have multi-dimensional attributes. Most of the existing retrieval systems display results in a grid or linear layout on small screens and do not visually represent the multi-dimensionality well. We think that multi-dimensional data visualization techniques can contribute to represent the distribution of the retrieval results based on multiple user-specified criteria and therefore assist the discovery of user-desired data items.   This paper presents MINI (Mobile Image Navigate Interface), a novel 3D visualization system for retrieval results adopted to run on mobile platforms such as smart phones. MINI allows users to interactively browse multi-dimensional datasets based on priority of data items calculated from multiple user criteria. It achieves the display of retrieved data items in the 3D space while avoiding overlaps and preserving adequate representation of the items' priorities. It also supports interactive orientation and provides a zooming user interface. We expect users can conveniently browse the retrieval results as well as are able to easily select the desired data items through the use of touch panel screens.   This paper also introduces the application of MINI in a recipe retrieval system, and the evaluation of effectiveness of the overlaps avoidance algorithm."
1561718,14018,9080,Dynamic multi-dimensional PSO with indirect encoding for proportional fair constrained resource allocation,2014,"Dynamic particle swarm optimization (PSO) problems are generally characterized by the exhaustively examined issues of the changing location of optima, the changing fitness of optima, and measurement noise/errors. However, the challenging issue of continuously changing problem dimensionality has not been similarly examined. Given that in anytime dynamic resource allocation it is necessary to maintain a high quality solution, we argue that, rather than restarting the PSO algorithm, a more appropriate approach is to design an algorithm that robustly handles changing problem dimensionality. Specifically, we propose an indirect particle encoding scheme specifically designed for a dynamic multi-dimensional PSO algorithm for proportional fair constrained resource allocation. This PSO algorithm is implemented for the proportional fair allocation of power and users to channels within a simulation of an Orthogonal Frequency-Division Multiple Access (OFDMA) wireless network with mobile users switching cells as they traverse the simulation environment. The proposed PSO algorithm is evaluated using simulations, which demonstrate the ability of the proposed indirect encoding scheme to maximize the overall proportional fair optimization goal, without unfairly penalizing the individual components of the solution related to newly introduced problem dimensions."
1391651,14018,9704,Toward a hybrid approach to generate Software Product Line portfolios,2013,"Software Product Line (SPL) development is a new approach to software engineering that aims at the development of a whole range of products. One of the problems which hinders the adoption of that approach is related with the management of the products of the line. Additionally, the scope of a software product line is determined by the bounds of the capabilities provided by the collection of products in the product line. This introduces new challenges related to the scope problem. One of the main three different forms of scoping is the Product Portfolio Scoping (PPS). PPS aims at defining the products that should be developed as well as their key features. While this has an impact on the actual reuse opportunities, it is usually driven from marketing aspects. Defining a product portfolio by considering costumers satisfaction and cost aspects is a NP-hard problem. This work presents a hybrid approach, which combines fuzzy inference systems and the multi-objective metaheuristics NSGAII to support product management by generating portfolios of products, based in segments of users and the development cost of the assets of the SPL. Fuzzy inference systems are used to generate development cost of an asset by using coupling, number of code lines and cyclomatic complexity and also to estimate the quality of the products generated by the optimization module of our approach. The NSGA-II metaheuristic is used to search for products minimizing the cost and maximizing the relevance of the candidate products. The results show that the proposed approach is effective in proposing the best products in terms of relevance and cost of the assets."
1206605,14018,9704,Multiobjective tactical planning under uncertainty for air traffic flow and capacity management,2013,"We investigate a method to deal with congestion of sectors and delays in the tactical phase of air traffic flow and capacity management. It relies on temporal objectives given for every point of the flight plans and shared among the controllers in order to create a collaborative environment. This would enhance the transition from the network view of the flow management to the local view of air traffic control. Uncertainty is modeled at the trajectory level with temporal information on the boundary points of the crossed sectors and then, we infer the probabilistic occupancy count. Therefore, we can model the accuracy of the trajectory prediction in the optimization process in order to fix some safety margins. On the one hand, more accurate is our prediction; more efficient will be the proposed solutions, because of the tighter safety margins. On the other hand, when uncertainty is not negligible, the proposed solutions will be more robust to disruptions. Furthermore, a multiobjective algorithm is used to find the tradeoff between the delays and congestion, which are antagonist in airspace with high traffic density. The flow management position can choose manually, or automatically with a preference-based algorithm, the adequate solution. This method is tested against two instances, one with 10 flights and 5 sectors and one with 300 flights and 16 sectors."
1595455,14018,9080,An evolutionary algorithm derived from Charles Sanders Peirce's theory of universal evolution,2013,"Historically, Evolutionary Algorithms (EAs) have been important for Evolutionary Computation (EC) community for two reasons: 1) As a simulation of evolutionary processes as they happen in nature, and 2) as a solution to hard optimization problems. With the passage of time EAs have become increasingly focused on function optimization. Given this narrowing of vision in the EC community, it is worth revisiting a paper written in 1997 by Hans-Paul Schwefel on the future challenges for EC. In that paper the author argues that the more an algorithm models natural evolution at work in the universe, the better it will perform (even in terms of function optimization). The present paper tests Schwefel's hypothesis by designing an EA based on Charles Peirce's theory of evolution. Peirce's theory not only accounts for biological evolution on earth (as other theories of evolution do) but also offers an account of global, cosmological and universal evolution. In going beyond just biological evolution, Peirce's theory of evolution meets the criteria suggested by Schewefel in his 1997 paper. The present paper mainly contributes in testing the Peircean EA on an extended set of benchmark optimization functions and compares the results with a classical EA that is based on Darwin's theory of evolution. In majority of these comparisons the performance of the Peircean EA is notably superior. This exercise provides preliminary results that support Schwefel's hypothesis. In return the experiments in evolutionary computation help provide important insights into Peirce's theory of evolution."
1984928,14018,9080,Grid data mining by means of learning classifier systems and distributed model induction,2011,"This paper introduces a distributed data mining approach suited to grid computing environments based on a supervised learning classifier system. Different methods of merging data mining models generated at different distributed sites are explored. Centralized Data Mining (CDM) is a conventional method of data mining in distributed data. In CDM, data that is stored in distributed locations have to be collected and stored in a central repository before executing the data mining algorithm. CDM method is reliable; however it is expensive (computational, communicational and implementation costs are high). Alternatively, Distributed Data Mining (DDM) approach is economical but it has limitations in combining local models. In DDM, the data mining algorithm has to be executed at each one of the sites to induce a local model. Those induced local models are collected and combined to form a global data mining model. In this work six different tactics are used for constructing the global model in DDM: Generalized Classifier Method (GCM); Specific Classifier Method (SCM); Weighed Classifier Method (WCM); Majority Voting Method (MVM); Model Sampling Method (MSM); and Centralized Training Method (CTM). Preliminary experimental tests were conducted with two synthetic data sets (eleven multiplexer and monks3) and a real world data set (intensive care medicine). The initial results demonstrate that the performance of DDM methods is competitive when compared with the CDM methods."
2289403,14018,9436,Understanding clusters of optimal solutions in multi-objective decision problems,2011,"Multi-objective decisions problems are ubiquitous in requirements engineering. A common approach to solve them is to apply search-based techniques to generate a set of non-dominated solutions, formally known as the Pareto front, that characterizes all solutions for which no other solution performs better on all objectives simultaneously. Analysing the shape of the Pareto front helps decision makers understand the solution space and possible tradeoffs among the conflicting objectives. Interpreting the optimal solutions, however, remains a significant challenge. It is in particular difficult to identify whether solutions that have similar levels of goals attainment correspond to minor variants within a same design or to very different designs involving completely different sets of decisions. Our goal is to help decision makers identify groups of strongly related solutions in a Pareto front so that they can understand more easily the range of design choices, identify areas where strongly different solutions achieve similar levels of objectives, and decide first between major groups of solutions before deciding for a particular variant within the chosen group. The benefits of the approach are illustrated on a small example and validated on a larger independently-produced example representative of industrial problems."
719120,14018,9704,Experimental analysis of the relevance of fitness landscape topographical characterization,2012,"The performance of any Evolutionary Algorithm (EA) is closely related to the topographical features of the problem fitness landscape it is applied to. It is therefore of paramount importance to determine a set of features that is useful in order to choose an appropriate algorithm for a given problem. This way, the inefficient trial and error stage that most EA users carry out until they find an EA that satisfies their objectives can be reduced. In fact, as this, usually lengthy, trial and error stage is generally carried out in an ad hoc manner, the information the user gleans from the performance of the algorithms chosen and their particular parameter sets, or lack thereof, can be very misleading or plain useless. Thus, in previous work, we analyze a set of features in synthetic fitness landscapes that can be used in order to characterize problems and relate them to the performance of EAs. The objective is to define a mechanism to reduce the trial and error stage when choosing the correct EA and, at the same time, provide more in depth knowledge on the nature of the problem. Here, in order to highlight the usefulness of the approach, this analysis is extended to real world application landscapes by means of the characterization of a horizontal axis wind turbine (HAWT) design problem, showing the relevance of the pre-processing stage in the selection of the most appropriate EA to solve it."
2286959,14018,21102,Probing performance evaluation for NPD process by using fuzzy MCDM approach,2011,"Due to the rapid changing marketplace, technology, equipment and raw materials, companies pursuit competitiveness by the new product and new product development (NPD) process for the purpose of innovation, high quality, and speed to the market. It is a gradual trend that the product life cycle is becoming shorter. The product manager intends to achieve the highest customer satisfaction, product value and product continuity. Therefore, to evaluate performance of NPD becomes a critical issue on the NPD process selection which is considered in many different uncertain aspects. Thus, this situation can be regarded as a fuzzy multiple criteria decision-making (FMCDM) problem, so the vagueness and uncertainty of subjective perception could be considered. In this paper, the non-additive (called super-additive) fuzzy integral is used to deal with evaluation of fuzzy MCDM problems particularly while there is dependence among the selected criteria. We can evaluate the quality performance of NPD process according to the result of empirical analysis. Consequently, DEMATEL is used to explore the relevance for the selected criteria of NPD process which is used to find the directions of problem-solving. The results of this study will provide NPD project team a guidance to satisfy the customer needs and creative the value of enterprise business."
895456,14018,9080,Bio-inspired and evolutionary algorithms applied to a bi-objective network design problem,2013,"Logistics network design is one of the principal parts of strategic decisions in the planning and control of production systems. It deals with determining the warehouses locations and the definition of product flow between facilities and clients. This work is focused in finding an approximation of the Pareto-optimal front for two conflicting objective functions in logistic networks design: minimize costs and maximize coverage. Since the establishing of which warehouses must be opened constitute a combinatorial optimization problem, two metaheuristic techniques, namely Improved Strength Pareto Evolutionary Algorithm - SPEA2 and a novel binary version of Bacterial Chemotaxis Multi-objective Optimization Algorithm - BCMOA, were applied. With the aim of finding the optimal flow between clients and warehouses, network flow algorithms were also used.   The performances of the above techniques were evaluated by comparative analysis of the results obtained in the solution of eight randomly generated problems by means of the dominated hypervolume metric (S-metric). The hybrid methodology here developed to solve the logistics network design problem - which combines metaheuristic techniques with a network flow algorithms - showed to be competitive regarding the Pareto Optimal Front approximation, and also displayed high efficiency in execution times."
1985121,14018,9704,"Transgenic, an operator for evolutionary algorithms",2011,"In the 1950s and the 1960s several computer scientists independently studied evolutionary systems with the idea that evolution could be used as an optimization tool for engineering problems. For these evolutionary-computation researchers, the mechanisms of evolution seem well suited for some of the most pressing computational problems in many fields. Ideas from Genetics are usually incorporated into evolutionary algorithms, such as: haploid crossover, mutation, diploid, inversion, gene doubling, deletion, and others. In the present study, we proposed an operator, named transgenic, for evolutionary algorithms, especially designed for Genetic Algorithms (GA). This operator is inspired by genetically modified organisms (GMOs), where important features are introduced into their genome artificially. The transgenic operator uses historical information to choose the best attributes, converging to better results faster than traditional GAs. The GA, used in this study, allows the discovery of concise, yet accurate, high-level rules (from a biological and synthetic database) which can be used as a classification system. The obtained results show that transgenic operator is promising at obtaining better of the the same results with a lower number of generations and smaller populations."
1812864,14018,9704,Evolution of ideas: A novel memetic algorithm based on semantic networks,2012,"This paper presents a new type of evolutionary algorithm (EA) based on the concept of “meme”, where the individuals forming the population are represented by semantic networks and the fitness measure is defined as a function of the represented knowledge. Our work can be classified as a novel memetic algorithm (MA), given that (1) it is the units of culture, or information, that are undergoing variation, transmission, and selection, very close to the original sense of memetics as it was introduced by Dawkins; and (2) this is different from existing MA, where the idea of memetics has been utilized as a means of local refinement by individual learning after classical global sampling of EA. The individual pieces of information are represented as simple semantic networks that are directed graphs of concepts and binary relations, going through variation by memetic versions of operators such as crossover and mutation, which utilize knowledge from commonsense knowledge bases. In evaluating this introductory work, as an interesting fitness measure, we focus on using the structure mapping theory of analogical reasoning from psychology to evolve pieces of information that are analogous to a given base information. Considering other possible fitness measures, the proposed representation and algorithm can serve as a computational tool for modeling memetic theories of knowledge, such as evolutionary epistemology and cultural selection theory."
2023857,14018,9704,"Biologically inspired obsolescence management in mobile agent systems: A dynamic, service oriented approach",2011,"Ubiquitous computing heralds an era marked by the increasing pervasiveness of computational hardware throughout a given environment. In order to capitalize on the increased abundance of the underlying infrastructure multi-agent systems will be required to reflect the characteristics of the ubiquitous networks upon which they operate. Due to the potential for limited communication capacity experienced by agents in the wild it will become increasingly important for mobile agents to migrate to devices in greater proximity to the problem upon which they are working or the resources they require. A result of highly mobile agents operating in potentially constrained computational and communication environments is that widely used command and coordination structures are no longer able to scale efficiently. Engineers are currently struggling with aspects of managing the physical devices which comprise such networks, particularly the obsolescence management of their constituent, highly dispersed, hardware. Analogously, the obsolescence management of deployed agents is of increasing concern. The paper examines and synthesizes several biological metaphors which may be employed in order to mitigate the inherent complexity of managing deployed mobile agent systems and presents this functionality in a service oriented manner."
2113928,14018,9080,Gaussian mixture modeling for dynamic particle swarm optimization of recurrent problems,2012,"In dynamic optimization problems, the optima location and fitness value change over time. Techniques in literature for dynamic optimization involve tracking one or more peaks moving in a sequential manner through the parameter space. However, many practical applications in, e.g., video and image processing involve optimizing a stream of recurrent problems, subject to noise. In such cases, rather than tracking one or more moving peaks, the focus is on managing a memory of solutions along with information allowing to associate these solutions with their respective problem instances. In this paper, Gaussian Mixture Modeling (GMM) of Dynamic Particle Swarm Optimization (DPSO) solutions is proposed for fast optimization of streams of recurrent problems. In order to avoid costly re-optimizations over time, a compact density representation of previously-found DPSO solutions is created through mixture modeling in the optimization space, and stored in memory. For proof of concept simulation, the proposed hybrid GMM-DPSO technique is employed to optimize embedding parameters of a bi-tonal watermarking system on a heterogeneous database of document images. Results indicate that the computational burden of this watermarking problem is reduced by up to 90.4% with negligible impact on accuracy."
1652577,14018,9080,An improved CUDA-based implementation of differential evolution on GPU,2012,"Modern GPUs enable widely affordable personal computers to carry out massively parallel computation tasks. NVIDIA's CUDA technology provides a wieldy parallel computing platform. Many state-of-the-art algorithms arising from different fields have been redesigned based on CUDA to achieve computational speedup. Differential evolution (DE), as a very promising evolutionary algorithm, is highly suitable for parallelization owing to its data-parallel algorithmic structure. However, most existing CUDA-based DE implementations suffer from excessive low-throughput memory access and less efficient device utilization. This work presents an improved CUDA-based DE to optimize memory and device utilization: several logically-related kernels are combined into one composite kernel to reduce global memory access; kernel execution configuration parameters are automatically determined to maximize device occupancy; streams are employed to enable concurrent kernel execution to maximize device utilization. Experimental results on several numerical problems demonstrate superior computational time efficiency of the proposed method over two recent CUDA-based DE and the sequential DE across varying problem dimensions and algorithmic population sizes."
1589452,14018,9704,On the use of a BSP Tree to create local surrogate models,2013,"In recent years, Evolutionary Algorithms (EAs) have been widely used to solve difficult optimization problems. However, when these problems are expensive (computationally speaking), they can remain intractable even by these approaches. The EA community has effectively used surrogate models to approximate the response of some of these expensive problems with the aim to replace with it some objective function calls. However, in order to have good results, it is important to have an accurate approach. In this regard, most of the existing approaches try to approximate the whole problem (the so-called global model). However, this may not necessary lead to a more accurate approach. The aim of the present paper is to provide a further insight into this matter through the first comparison (to the best of the authors' knowledge) between localand global-surrogate models. We investigate the performance of three different approaches, two of them have been previously used in the specialized literature, while the third is here proposed. After adjusting the single parameter of each approach, we compare their results with respect to the results produced by the global-surrogate model. The validation was performed using six test functions in three different scenarios: low-, medium-and high-dimensional problems. Results indicate our proposed approach is a viable alternative to create local-surrogate models for mediumand high-dimensional problems, while the global-surrogate model is the option for low-dimensional problems."
790207,14018,9704,A heuristic approach to greener airport ground movement,2014,"Ever increasing air traffic, rising costs and tighter environmental targets create a pressure for efficient airport#R##N#ground movement. Ground movement links other airport operations such as departure sequencing, arrival sequencing and gate/stand allocation and its operation can affect each of these. Previously, reducing taxi time was considered the main objective of the ground movement problem. However, this may conflict with efforts of airlines to minimise their fuel consumption as shorter taxi time may require higher speed and acceleration during taxiing. Therefore, in this paper a multi-objective multicomponent optimisation problem is formulated which combines two components: scheduling and routing of aircraft and speed profile optimisation. To solve this problem an integrated solution method is adopted to more accurately investigate the trade-off between the total taxi time and fuel consumption. The new heuristic which is proposed here uses observations about the characteristics of the optimised speed profiles in order to greatly improve the speed of the graph-based routing and scheduling algorithm. Current results, using real airport data, confirm that this approach can find better solutions faster, making it very promising for application within on-line applications."
2369556,14018,8806,Single and Multi Objective Genetic Programming for software development effort estimation,2012,"The idea of exploiting Genetic Programming (GP) to estimate software development effort is based on the observation that the effort estimation problem can be formulated as an optimization problem. Indeed, among the possible models, we have to identify the one providing the most accurate estimates. To this end a suitable measure to evaluate and compare different models is needed. However, in the context of effort estimation there does not exist a unique measure that allows us to compare different models but several different criteria (e.g., MMRE, Pred(25), MdMRE) have been proposed. Aiming at getting an insight on the effects of using different measures as fitness function, in this paper we analyzed the performance of GP using each of the five most used evaluation criteria. Moreover, we designed a Multi-Objective Genetic Programming (MOGP) based on Pareto optimality to simultaneously optimize the five evaluation measures and analyzed whether MOGP is able to build estimation models more accurate than those obtained using GP. The results of the empirical analysis, carried out using three publicly available datasets, showed that the choice of the fitness function significantly affects the estimation accuracy of the models built with GP and the use of some fitness functions allowed GP to get estimation accuracy comparable with the ones provided by MOGP."
1228962,14018,21102,A new fingram-based software tool for visual representation and analysis of fuzzy association rules,2013,"This paper presents an open source software tool for generation, visual representation and expert analysis of the so-called fuzzy inference-grams (fingrams in short). Fingrams are extended to fuzzy association rules in this work, expanding the scope of this methodology. The tool automatically generates fingrams and displays them to the user for expert analysis. It takes as input a simple configuration file that may be provided by whatever fuzzy modeling tool (it is actually provided by two widely known tools, GUAJE and KEEL). Fingrams graphically represent multi-dimensional fuzzy systems in 2D. The first version of this new software handles three different kinds of fuzzy systems (fuzzy rule-based systems for classification and regression, but also fuzzy association rules). Fingrams illustrate the interaction among fuzzy rules in terms of rule co-firing, i.e., paying attention to pairs of rules fired at the same time by a given input. Their visualization looks like a social network where nodes represent fuzzy rules while edges show the degree of interaction among pairs of rules. Thus, both representation and analysis are supported by social network analysis techniques. The utility of the new software tool is illustrated in a case study on visual representation and analysis of association rules."
1373186,14018,9080,A new multi-objective evolutionary algorithm based on a performance assessment indicator,2012,"An emerging trend in the design of multi-objective evolutionary algorithms (MOEAs) is to select individuals through the optimization of a quality assessment indicator. However, the most commonly adopted indicator in current use is the hypervolume which becomes very expensive (computationally speaking) as we increase the number of objectives. In this paper, we propose, instead, the use of another indicator called Δ  p  . Although the Δ  p   indicator is not Pareto compliant, we show here how it can be incorporated into the selection mechanism of an evolutionary algorithm (for that sake, we adopt differential evolution as our search engine) in order to produce a MOEA. The resulting MOEA (called Δ  p  -Differential Evolution, or DDE) is validated using standard test problems and performance indicators reported in the specialized literature. Our results are compared with respect to those obtained by both a Pareto-based MOEA (NSGA-II) and a hypervolume-based MOEA (SMS-EMOA). Our preliminary results indicate that our proposed approach is competitive with respect to these two MOEAs for continuous problems having two and three objective functions. Additionally, our proposed approach is better than NSGA-II and provides competitive results with respect to SMS-EMOA for continuous many-objective problems. However, in this case, the main advantage of our proposal is that its computational cost is significantly lower than that of SMS-EMOA."
887303,14018,9080,Parameter-less population pyramid,2014,"Real world applications of evolutionary techniques are often hindered by the need to determine problem specific parameter settings. While some previous methods have reduced or removed the need for parameter tuning, many do so by trading efficiency for general applicability. The Parameter-less Population Pyramid (P3) is an evolutionary technique that requires no parameters and is still broadly effective. P3 strikes a balance between continuous integration of diversity and exploitative elitist operators, allowing it to solve easy problems quickly and hard problems eventually. When compared with three optimally tuned, state of the art optimization techniques, P3 always finds the optimum at least a constant factor faster across four benchmarks (Deceptive Trap, Deceptive Step Trap, HIFF, Rastrigin). More importantly, on three randomized benchmarks (NK Landscapes, Ising Spin Glasses, MAX-SAT), P3 has a lower order of computational complexity as measured by evaluations. We also provide outlines for expected runtime analysis of P3, setting the stage for future theory based conclusions. Based on over 1 trillion evaluations, our results suggest P3 has wide applicability to a broad class of problems."
770833,14018,9080,Multi-objective routing optimisation for battery-powered wireless sensor mesh networks,2014,"Mesh network topologies are becoming increasingly popular in battery powered wireless sensor networks, primarily due to the extension of network range and resilience against routing failures. However, multi-hop mesh networks suffer from higher energy costs, and the routing strategy directly affects the lifetime of nodes with limited energy sources. Hence while planning routes there are trade-offs to be considered between individual and system-wide battery lifetimes. We present a novel multi-objective routing optimisation approach using evolutionary algorithms to approximate the optimal trade-off between minimum lifetime and the average lifetime of nodes in the network. In order to accomplish this combinatorial optimisation rapidly and thus permit dynamic optimisation for self-healing networks, our approach uses novel $k$-shortest paths based search space pruning in conjunction with a new edge metric, which associates the energy cost at a pair of nodes with the link between them. We demonstrate our solution on a real network, deployed in the Victoria \& Albert Museum, London. We show that this approach provides better trade-off solutions in comparison to the minimum energy option, and how a combination of solutions over the lifetime of the network can enhance the overall minimum lifetime."
1348163,14018,9080,Networks of transform-based evolvable features for object recognition,2013,"We propose an evolutionary feature creator (EFC) to explore a non-linear and offline method for generating features in image recognition tasks. Our model aims at extracting low-level features automatically when provided with an arbitrary image database. In this work, we are concerned with the addition of algorithmic depth to a genetic programming (GP) system, hypothesizing that it will improve the capacity for solving problems that require high-level, hierarchical reasoning. For this we introduce a network superstructure that co-evolves with our low-level GP representations. Two approaches are described: the first uses our previously used shallow GP system, the second presents a new deep GP system that involves this network superstructure. We evaluate these models against a benchmark object recognition database. Results show that the deep structure outperforms the shallow one in generating features that support classification, and does so without requiring significant additional computational time. Further, high accuracy is achieved on the standard ETH-80 classification task, also outperforming many existing specialized techniques. We conclude that our EFC is capable of data-driven extraction of useful features from an object recognition database."
2023727,14018,9704,A ranking method based on the R2 indicator for many-objective optimization,2013,"In recent years, the development of selection mechanisms based on performance indicators has become an important trend in algorithmic design. Hereof, the hypervolume has been the most popular choice. Multi-objective evolutionary algorithms (MOEAs) based on this indicator seem to be a good choice for dealing with many-objective optimization problems. However, their main drawback is that such algorithms are typically computationally expensive. This has motivated some recent research in which the use of other performance indicators has been explored. Here, we propose an efficient mechanism to integrate the R2 indicator to a modified version of Goldberg's nondominated sorting method, in order to rank the individuals of a MOEA. Our proposed ranking scheme is coupled to two different search engines, resulting in two new MOEAs. These MOEAs are validated using several test problems and performance measures commonly adopted in the specialized literature. Results indicate that the proposed ranking approach gives rise to effective MOEAs, which produce results that are competitive with respect to those obtained by three well-known MOEAs. Additionally, we validate our resulting MOEAs in many-objective optimization problems, in which our proposed ranking scheme shows its main advantage, since it is able to outperform a hypervolume-based MOEA, requiring a much lower computational time."
1404087,14018,9704,New heuristic and evolutionary operators for the multi-objective urban transit routing problem,2013,"The urban transit routing problem (UTRP) involves finding efficient routes in a public transport system. However, developing effective heuristics and metaheuristics for the UTRP is hugely challenging because of the vast search space and multiple constraints that make even the attainment of feasible results exceedingly difficult, as the problem size increases. Moreover, progress with academic research on the UTRP appears to be seriously hampered by: 1) a lack of benchmark data, and 2) the complex and diverse range of methods used in the literature to evaluate solution quality. It is not currently possible for researchers to effectively compare the performance of their algorithms with anyone else's. This paper presents new problem specific genetic operators within a multi-objective evolutionary framework, and furthermore proposes an effective and efficient heuristic method for seeding the population with feasible route sets. In addition new data sets are provided and made available for download, to aid future researchers. Excellent results are presented for Mandl's problem, which is currently the only benchmark available, while the results obtained for the new data sets provide a challenge for future researchers to beat."
2320985,14018,9704,Dimensioning the heterogeneous multicluster architecture via parallelism analysis and evolutionary computing,2012,"In the near future, embedded systems containing hundreds of processing elements running multiple concurrent applications will become a reality. The heterogeneous multicluster architecture enables to cope with the challenging hardware/software requirements presented by such systems. This paper shows principles and optimization of multicluster dimensioning aiming at an appropriate distribution of applications onto clusters containing different types of processing elements. The approach represents an initial exploration phase efficiently finding a suitable multicluster configuration in the large design space. Hence, results should be further refined by more accurate but less time-efficient simulation-based techniques. As the starting point, a parallelism value matrix is analytically extracted describing application mappings independently on the architecture and scheduling. A genetic algorithm (GA) and a mixed-integer linear programming (MILP) approach solving the dimensioning problem are introduced and compared. Both solutions use the parallelism value matrix as input. Scalability results show that the GA generates results faster and with a satisfactory quality relative to the found MILP solutions. Finally, the dimensioning approach is demonstrated for a realistic benchmark scenario."
2032532,14018,9080,Large network analysis for fisheries management using coevolutionary genetic algorithms,2011,"Traditionally, a genetic algorithm is used to analyze networks by maximizing the modularity (Q) measure to create a favorable community. A coevolutionary algorithm is used here to not only find the appropriate community division for a network, but to find interesting networks containing substantial changes in data within a very large network space. The network is one of the largest, if not the largest, analyzed by evolutionary computation techniques to date and is created using a real world data set consisting of fisheries catch data in the north Atlantic Ocean off the coast of Canada. This work examines the quantitative performance of two types of coevolutionary algorithms against both a standard GA that uses a natural (but not necessarily optimal) division of the data set into communities, and simulated annealing. The goal for all search algorithms was to automatically find anomalies (differences in catch) within the data. To measure practical usefulness of the system, a fisheries expert analyzed the best networks located by the search algorithms using an existing visualization software prototype. The expert indicated that a refined version of coevolutionary GA known as PAMDGA was found to most reliably locate subnetworks containing catch differences of biological relevance."
1960960,14018,21102,Educational system with the android robot SAYA and field trial,2011,"Communication robots are hard to interact with humans autonomously in everyday environment due to lack of intelligence even though robots can perform certainly only in limited environments and limited situations. However, since intelligence of robots is handled by an operator, tele-operated robots are useful to investigate functions required for robots and effects of robots in real-environment. This paper proposes an educational system with a tele-operated android robot that has human-like appearance. The developed android robot, named SAYA, is able to express human-like facial expressions and performs some communicative functions with its head and eye movements, and it is utilized as a roll of a teacher in the proposed educational system. In addition, SAYA's utterances and behaviors are controlled remotely by an operator so that it is able to not only conduct the class but also interact with students. A field trial was carried out at an elementary school, and SAYA conducted the class concerning the principle of leverage as a topic of science classes while interacting with students. Also, how it affected students' interest and motivation was investigated. As results, the trial verified that some students became interested in science class more than before the trial, and its positive effects in educational field, especially elementary schools, were verified."
1960120,14018,9080,Guided self-organization in indirectly encoded and evolving topographic maps,2014,"An important phenomenon seen in many areas of biological brains and recently in deep learning architectures is a process known as self-organization. For example, in the primary visual cortex, color and orientation maps develop based on lateral inhibitory connectivity patterns and Hebbian learning dynamics. These topographic maps, which are found in all sensory systems, are thought to be a key factor in enabling abstract cognitive representations. This paper shows for the first time that the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) method can be seeded to begin evolution with such lateral connectivity, enabling genuine self-organizing dynamics. The proposed approach draws on HyperNEAT's ability to generate a pattern of weights across the connectivity of an artificial neural network (ANN) based on a function of its geometry. Validating this approach, the afferent weights of an ANN self-organize in this paper to form a genuine topographic map of the input space for a simple line orientation task. Most interestingly, this seed can then be evolved further, providing a method to guide the self-organization of weights in a specific way, much as evolution likely guided the self-organizing trajectories of biological brains."
1096973,14018,9080,Adaptive and hybrid genetic approaches for estimating the camera motion from image point correspondences,2011,"Rigid motion estimation from image point correspondences is an overconstrained problem that can be solved by minimizing an adequate cost function. Given the unreliable nature of image point correspondences, they must be divided into two categories: inliers and outliers. Finding the correct camera motion and discarding the outliers is a coupled problem usually solved by a random search of the solution space. This article proposes adaptive and hybrid genetic approaches to improve the efficiency of this search. We build on top of the GASAC algorithm that has been recently presented for solving problems in geometric computer vision. GASAC is modified to address the specific issues of camera motion estimation such as outlier ratios above 50% due to wide-baseline image acquisition and an adequate choice of a fitness function. In order to avoid local minima, we propose three adaptive strategies: varying the mutation probability, resampling the lowest ranked individuals, and using a hybrid approach that combines GASAC with simulated annealing. Results are validated on publicly available benchmark images, and it is shown that the proposed genetic approaches outperform the standard RANSAC search used among computer vision practitioners."
2415648,14018,9704,A multiobjectivised memetic algorithm for the Frequency Assignment Problem,2011,"This work presents a set of approaches used to deal with the Frequency Assignment Problem (FAP), which is one of the key issues in the design of Global System for Mobile Communications (GSM) networks. The used formulation of the FAP is focused on aspects which are relevant for real-world GSM networks. The best up to date frequency plans for the considered version of the FAP had been obtained by using parallel memetic algorithms. However, such approaches suffer from premature convergence with some real world instances. Multiobjectivisation is a technique which transforms a mono-objective optimisation problem into a multi-objective one with the aim of avoiding stagnation. A Multiobjectivised Memetic Algorithm, based on the well-known Non-Dominated Sorting Genetic Algorithm II (NSGA-II) together with its required operators, is presented in this paper. Several multiobjectivised schemes, based on the addition of an artificial objective, are analysed. They have been combined with a novel crossover operator. Computational results obtained for two different real-world instances of the FAP demonstrate the validity of the proposed model. The new model provides benefits in terms of solution quality, and in terms of time saving. The previously known best frequency plans for both tested real-world networks have been improved."
2014663,14018,9704,Multi-objective optimization of traffic externalities using tolls,2013,"Genetic algorithms (GAs) are widely accepted by researchers as a method of solving multi-objective optimization problems (MOPs), at least for listing a high quality approximation of the Pareto front of a MOP. In traffic management, it has been long established that tolls can be used to optimally distribute traffic in a network with aim of combating some traffic externalities such as congestion, emission, noise, safety issues. Formulating the multi-objective toll problem as a one point solution problem fails to give the general overview of the objective space of the MOP. Therefore, in this paper we develop a game theoretic approach that gives the general overview of the objective space of the multiobjective problem and compare the results with those of the wellknown genetic algorithm non-dominated sorting genetic algorithm II (NSGA-II). Results show that the game theoretic approach presents a promising tool for solving multi-objective problems, since it produces similar non-dominated solutions as NSGA-II, indicating that competing objectives (or stakeholders in the game setting) can still produce Pareto optimal solutions. Most fascinating is that a range of non-dominated solutions is generated during the game, and almost all generated solutions are in the neighborhood of the Pareto set. This indicates that good solutions are generated very fast during the game."
1408572,14018,21102,A general type-2 fuzzy logic based approach for Multi-Criteria Group Decision Making,2013,"Decision making could be viewed to include Multi-Criteria Group Decision Making (MCGDM). MCGDM is a decision tool which it is able to find a unique agreement from number of decision makers/users by evaluating the uncertain judgment among them. Several fuzzy logic based approaches have been employed in MCGDM to handle the linguistic uncertainties and hesitancy. However, there is a need to handle the high level of uncertainties that exist in decision making problems involving numbers of decision makers/experts/users with varying points of view. In this paper, we present a general type-2 fuzzy logic based approach for MCGDM. The proposed system aims to handle the high levels of uncertainties which exist due to the varying Decision Makers' (DMs) judgments and the vagueness of the appraisal. The proposed method utilizes general type-2 fuzzy sets. The aggregation operation in the proposed method aggregates the various DMs opinions which allow handling the disagreements of DMs' opinions into a unique approval. We will present results from the proposed system deployment for the assessment of the postgraduate study. The proposed system was able to model the variation in the group decision making process exhibited by the various decision makers' opinions. In addition, the proposed system showed agreement between the proposed method and the real decision outputs from DMs (as quantified by the Pearson Correlation) which outperformed the MCGDM systems based on type-1 fuzzy sets, interval type-2 fuzzy sets and interval type-2 fuzzy sets with hesitation index."
2327322,14018,9704,Heterogeneous double populations based hybrid genetic algorithm design for training feedforward neural networks,2012,"Genetic algorithms (GA) has been extensively applied to address the shortcomings of gradient based leaning methods in training feedforward neural networks (NN). However, the complicated properties of NN training, such as context dependence problem between neurons and permutation problem of genetic representation, will cause difficulties in efficiently implementing conventional GAs. In the present study, a novel hybrid GA design is proposed to overcome these problems. First, for the sake of eliminating the context dependence, the new method adopts GA and least squares estimator to separately optimize the neurons in hidden and output layers. Second, in order to completely avoid the permutation problem, the proposed GA design employs two heterogeneous populations that evolve in company but respectively learn the optimal combinations and parameters of hidden neuron. Finally, experimental studies encouragingly show that, in comparison with five well-known conventional approaches, the new training method displays a much better approximation and generalization capabilities in nonlinear static and dynamic modeling, especially for the observed signals corrupted with large measurement noises."
971111,14018,21102,Collaborative rough-fuzzy clustering: An application to intensity non-uniformity correction in brain MR images,2013,"Automatic segmentation of Magnetic Resonance Images (MRI) for tissue classification becomes more challenging when the image is corrupted with noise and intensity non-uniformity (INU). Several fuzzy clustering based statistical retrospective methods exist for simultaneous correction and segmentation of images but most of them fail to be robust in presence of noise, outliers and INU artifacts. In this paper, a hybridization of rough c-means and spatial fuzzy c-means clustering is presented whose objective function has been modified to accommodate INU field as well. While the membership function of fuzzy sets enables efficient handling of overlapping partitions, the concept of lower and upper approximations of rough sets deals with uncertainty, vagueness, and incompleteness in class definition. The experiments conducted on brain MR images show promising results in terms of segmentation accuracy and class separability. The usefulness of proposed algorithm is also investigated on high field MR images. The proposed algorithm Rough-Theoretic Bias-Corrected Fuzzy C-Means Algorithm (R-BCFCM) has significant performance improvement over other similar methods from rough-fuzzy family and can be employed for MRI corrupted with high intensity non-uniformity and noise."
1126625,14018,21102,Can indices of ecological evenness be used to measure consensus,2014,"In the context of group decision making with fuzzy preferences, consensus measures are employed to provide feedback and help guide automatic or semi-automatic decision reaching processes. These measures attempt to capture the intuitive notion of how much inputs, individuals or groups agree with one another. Meanwhile, in ecological studies there has been an ongoing research effort to define measures of community evenness based on how evenly the proportional abundances of species are distributed. The question hence arises as to whether there can be any cross-fertilization from developments in these fields given their intuitive similarity. Here we investigate some of the models used in ecology toward their potential use in measuring consensus. We found that although many consensus characteristics are exhibited by evenness indices, lack of reciprocity and a tendency towards a minimum when a single input is non-zero would make them undesirable for inputs expressed on an interval scale. On the other hand, we note that some of the general frameworks could still be useful for other types of inputs like ranking profiles and that in the opposite direction consensus measures have the potential to provide new insights in ecology."
2383431,14018,9704,Training multilayer perceptrons with a Gaussian Artificial Immune System,2011,"In this paper we apply an immune-inspired approach to train Multilayer Perceptrons (MLPs) for classification problems. Our proposal, called Gaussian Artificial Immune System (GAIS), is an estimation of distribution algorithm that replaces the traditional mutation and cloning operators with a probabilistic model, more specifically a Gaussian network, representing the joint distribution of promising solutions. Subsequently, GAIS utilizes this probabilistic model for sampling new solutions. Thus, the algorithm takes into account the relationships among the variables of the problem, avoiding the disruption of already obtained high-quality partial solutions (building blocks). Besides the capability to identify and manipulate building blocks, the algorithm maintains diversity in the population, performs multimodal optimization and adjusts the size of the population automatically according to the problem. These attributes are generally absent from alternative algorithms, and all were shown to be useful attributes when optimizing the weights of MLPs, thus guiding to high-performance classifiers. GAIS was evaluated in six well-known classification problems and its performance compares favorably with that produced by contenders, such as opt-aiNet, IDEA and PSO."
2449577,14018,9704,Efficient Contracting in Cloud Service Markets with Asymmetric Information - A Screening Approach,2011,"Increasing popularity of cloud-based services has led to the emergence of cloud marketplaces where services from different providers are offered, usually in the form of a catalog. The customers' decision about buying offered services is based on idiosyncratic preferences regarding non-functional service attributes, e.g., price, provider reputation, and quality of service. The customers' preferences are not necessarily known to providers at the time the service (including pricing) is defined in the marketplace's service catalog. Thus, from a microeconomic perspective, we have to deal with information asymmetry on incomplete markets. On such markets, finding the optimal contracts (i.e., non-functional characteristics and prices) that maximize the provider's profit is challenging due to information uncertainty. This paper presents a generic economic framework based on contract theory which solves the above-mentioned optimization for cloud-based services offered at a marketplace. The contribution is threefold: (i) we analyze and select from providers' perspective non-functional attributes considered by customers when deciding which services to buy, (ii) we implement a holistic contracting framework that grants providers maximal profit through optimal combination of potential values of the chosen attributes and (iii) we present a study of a desktop service use case. The contracting framework addresses the phenomenon of adverse selection by leveraging the screening technique."
1059679,14018,9080,Exploring the evolution of internal control structure using digital enzymes,2012,"The Digital Enzyme model of control is based on the bottom-up, reactive process of signal transduction found in cells. An earlier study applied a specific instance of the this model to the foraging problem. Here, we extend the system and use it to explore a fundamental question in both biology and evolutionary computation, namely, whether environmental complexity is a driving factor for an organism''s internal control structure. To address this question, we extended the original system to allow the open-ended evolution of the unique programs, instructions, and threads within each controller. With the extended model, we were able to evolve successful foraging strategies that nearly doubled the performance of strategies found in the earlier work. In response to increasing environmental complexity, we discovered a high degree of variation for the number of programs, threads, and instructions that produced successful strategies. These results imply that environmental complexity does not require evolutionary search methods to explore regions of the search space characterized by parallel and distributed control. However, strategies found within these regions were as successful as strategies governed by a single program and thread, highlighting the importance of evolutionary search techniques that enable the open-ended evolution of key internal control components."
890969,14018,9080,Introduction to bioinformatics and computational biology,2012,"The field of biological sciences has been transformed in recent years to a domain of incredibly rich data ripe for computational exploration. High throughput technologies allow investigators to construct vast feature sets, including genetic variables, gene expression values, protein levels, biomarkers, and a multitude of other traits. These rich feature sets can be used to predict disease risk and prioritize treatment strategies, but there are incredible challenges in the analysis of such data. Features often have correlation patterns, are subject to normalization, measurement errors, and other forms of noise. Furthermore, there are far more features in a typical dataset than there are samples. Despite these issues, the analysis of complex biological data can lead to a new understanding of biological systems and human health.   This tutorial provides an introduction to the fundamental concepts of biological science. It examines the established methods for generating biological datasets, outlines online databases that contain much of this data, and introduces the newest methods for capturing high-resolution genomic sequence data. Attendees will leave this tutorial with a better understanding of the problem domains that exist in biological science."
1311184,14018,21102,A fuzzy adaptive sliding mode slip ratio controller of a HEV,2013,"Antilock braking system (ABS) is a safety measure for a vehicle essential during braking for regulating wheel slip ratio at its optimum value. Slip ratio control of a vehicle is an important concern for development of ABS to avoid skidding during road surface transitions. Sliding mode control (SMC) being a robust control paradigm is exploited for an ABS in a hybrid electric vehicle (HEV). But it yields significant amount of chattering. The dynamics of a braking system are time varying, nonlinear and uncertain. Fuzzy logic control (FLC) is popular in providing good performance for nonlinear uncertain systems. This paper exploits benefits of both fuzzy logic and SMC in designing a Fuzzy Adaptive Sliding Mode Control (FASMC) for slip ratio control of a HEV. This FASMC generates control voltage to actuators of HEV by combining two control signals namely an equivalent control, a discontinuous fuzzy control. Further to adapt the uncertainties in road conditions an adaption technique is developed in the equivalent control law. Fuzzy reaching control algorithm is introduced in the discontinuous control of SMC to mimic the reaching control. An adaptive tuning algorithm is developed to tune the fuzzy parameters. The slip ratio control performance of the proposed FASMC has been compared with that of a sliding mode controller through extensive simulations using MATLAB. From the obtained results it is observed that the proposed FASMC eliminates chattering completely and provides excellent slip control performance."
1057698,14018,8806,SEDE: state estimation-based dynamic encryption scheme for smart grid communication,2014,"The vision of smart grid relies heavily on the communication technologies as they provide a desirable infrastructure for real-time measurement, transmission, decision and control. But various attacks such as eavesdropping, information tampering and malicious control command injection that are hampering the communication in Internet, would impose great threat on the security and stability of smart grids. In this paper, a State Estimation-based Dynamic Encryption (SEDE) scheme is proposed to secure the communication in smart grid. Several states of power system are employed as the common secrets to generate a symmetric key at both sides, which are measured on the terminals and calculated on the control center using state estimation. The advantages of SEDE are 1) the common secrets, used to generate symmetric key, are never exchanged in the network due to the state estimation, that observably improves the security of SEDE; 2) the measurement and state estimation are the essential functions on the terminals and control center in power system; 3) the functions, applied to encrypt and decrypt data, are simple and easy-implemented, such as XOR, Hash, rounding, etc. Thus, SEDE is considered as an inherent, light-weight and high-security encryption scheme for smart gird. In the experiments, SEDE is simulated on a 4-bus power system to demonstrate the process of state estimation, key generation and error correction."
1998166,14018,9704,Computer Aided Threat Identification,2011,"Recently, there has been an increase of reported security threats hitting organizations. Some of them are originated from the assignments to users of inappropriate permissions on organizational sensitive data. Thus it is crucial for organizations to recognize as early as possible the risks deriving by inappropriate access right management and to identify the solutions that they need to prevent such risks. In this paper, we propose a framework to identify threats during the requirements analysis of organizations' IT systems. With respect to other works which have attempted to include security analysis into requirement engineering process (e.g., KAOS, Elahi et al., Asnar et al.), our framework does not rely on the level of expertise of the security analyst to detect threats but allows to automatically identify threats that derive from inappropriate access management. To capture the organization's setting and the system stakeholders' requirements, we adopt SI* [1], a requirement engineering framework founded on the concepts of actors, goals, tasks and resources. This framework extends SI* with a reasoning technique that identifies potential security threats on resources and relevant goals. The reasoning is based on Answer Set Programming (ASP) logic rules that take into account the relationships between resources and the delegation of permission relations between actors. We illustrate this framework using an eHealth scenario."
1024104,14018,9704,A new clustering approach based on Glowworm Swarm Optimization,2013,"High-quality clustering techniques are required for the effective analysis of the growing data. Clustering is a common data mining technique used to analyze homogeneous data instance groups based on their specifications. The clustering based nature-inspired optimization algorithms have received much attention as they have the ability to find better solutions for clustering analysis problems. Glowworm Swarm Optimization (GSO) is a recent nature-inspired optimization algorithm that simulates the behavior of the lighting worms. GSO algorithm is useful for a simultaneous search of multiple solutions, having different or equal objective function values. In this paper, a clustering based GSO is proposed (CGSO), where the GSO is adjusted to solve the data clustering problem to locate multiple optimal centroids based on the multimodal search capability of the GSO. The CGSO process ensures that the similarity between the cluster members is maximized and the similarity among members from different clusters is minimized. Furthermore, three special fitness functions are proposed to evaluate the goodness of the GSO individuals in achieving high quality clusters. The proposed algorithm is tested by artificial and real-world data sets. The better performance of our proposed algorithm over four popular clustering algorithms is demonstrated on most data sets. The results reveal that CGSO can efficiently be used for data clustering."
2489416,14018,21102,An intelligent framework for monitoring student performance using fuzzy rule-based Linguistic Summarisation,2012,"Monitoring students' activity and performance is vital to enable educators to provide effective teaching and learning in order to better engage students with the subject and improve their understanding of the material being taught. We describe the use of a fuzzy Linguistic Summarisation (LS) technique for extracting linguistically interpretable scaled fuzzy weighted rules from student data describing prominent relationships between activity / engagement characteristics and achieved performance. We propose an intelligent framework for monitoring individual or group performance during activity and problem based learning tasks. The system can be used to more effectively evaluate new teaching approaches and methodologies, identify weaknesses and provide more personalised feedback on learner's progress. We present a case study and initial experiments in which we apply the fuzzy LS technique for analysing the effectiveness of using a Group Performance Model (GPM) to deploy Activity Led Learning (ALL) in a Master-level module. Results show that the fuzzy weighted rules can identify useful relationships between student engagement and performance providing a mechanism allowing educators to transparently evaluate teaching and factors effecting student performance, which can be incorporated as part of an automated intelligent analysis and feedback system."
2339169,14018,21102,Fuzzy cognitive maps in estimating the repercussions of oil/gas exploration on politico-economic issues in Cyprus,2011,"Some important politico-economic dynamics, in relation to different scenarios involving the finding and exploitation of oil/gas in the exclusive economic zone of Cyprus, have been modeled and examined through the use of suitable fuzzy cognitive maps. In the interrelated dynamics, various important dynamical parameters have been taken into account, reflecting the interests of the republic of Cyprus, as well as the interests of the Greek and Turkish Cypriot communities. In some respects these interests are antagonistic, while in others could be cooperative. The interests of other countries involved in the Cyprus politico-economic problem have also been taken into account. These are primarily Greece, Turkey, United Kingdom, USA, Russia, Israel and the European Union. The main parameters involved in the interrelated dynamics are nationalism, religiousness, knowledge of history, level of educational development, tourism, unemployment, external debt, oil extraction, Anatolian settlers, and the general interests of the countries involved and those of the two communities. The system that has been developed can be used to study the effects of a change in any parameter or a combination of parameters, on the growth and stability of the remaining parameters. Different scenarios on the effects on economies, politics and military involvement have been implemented, observed and appraised."
1357383,14018,9704,Sample efficiency analysis of Neuroevolution algorithms on a quadruped robot,2013,"In reinforcement learning tasks with continuous state-action, parameterized policy search has been known to be a powerful method. Applying NeuroEvolution (NE) to optimizing the policy represented by artificial neural network (ANN) is a particularly active research field. In most cases, NE algorithms cost a large amount of trial-and-error (episode) to optimize policies. However, due to time and cost constraints, researchers and practitioners cannot repeat a number of episodes on physical robots. Thus, choosing an efficient NE algorithm is a key to optimize policies with limited time and cost. In this work, our goal is to help users to choose an efficient NE algorithm. We compare and analyze sample efficiency of two successful state-of-the-art NE algorithms: CMA-NeuroES and NEAT in a gait generation task of a quadruped robot. Moreover, we run both algorithms with various initial topologies in order to analyze the performance difference between each topology. From experimental results, we show CMA-NeuroES outperforms NEAT regardless of initial topologies when the limited number of episodes can be executed. Additional experiments conclude that the optimization method for connection weights in NEAT results in its inferior performance to CMA-NeuroES, while a probability-weighted averaging characteristic and self-adaptive factors make CMA-NeuroES to be advantageous."
639636,14018,9704,Evolutionary cellular automata bonsai,2013,"Cellular automata are known to be capable of Turing-complete computation and yet “programming” them to do particular tasks can be quite daunting. In this paper we use single parent crossover as a means of transferring information between successive evolving populations to create rules for cellular automata that have proscribed shapes. The proscription of regions where the automata are permitted to grow is the reason they are called bonsai automata. This work follows earlier work on apoptotic cellular automata that simply exhibit self-limited growth. The correct choice of single parents permits enormous improvement in the performance of evolutionary algorithms searching for automata that satisfy particular bonsai templates. In this study, we demonstrate that single parent techniques make meeting shape constraints on the growth of CAs possible at all in some cases. This study also introduces range niche specialization to control problems with the cloning of ancestors used for single parent crossover in an evolving population. This study demonstrates that different bonsai shapes have highly variable difficulty. It is also shown that automata evolved to satisfy one bonsai template may be needed to enable, via single parent crossover, solutions for another template. The use of bonsai techniques yields many automata not found during studies of apoptotic automata demonstrating that the technique encourages exploration of different parts of the fitness landscape."
1555013,14018,21102,A fuzzy logic based Multi-criteria Group Decision Making system for the assesement of umbilical cord acid-base balance,2012,"An interpretation of the state of health of the baby can be inferred through assessment of the umbilical cord acid-base (UAB) status. This assessment can be made based on pH and other parameters from both arterial and venous blood from the newborn umbilical cord. This can distinguish the cause of a low pH between the distinct physiological conditions of respiratory acidosis due to a short-term accumulation of CO2 and a metabolic acidosis (low pH in the tissues) due to lactic acid from a longer-term oxygen deficiency. This UAB assessment suffers the problem of high uncertainty levels between the various experts. Hence, researchers have tried to develop computer based models for the assessment of UAB. Previous research has shown the power of fuzzy logic systems to provide frameworks to handle the encountered uncertainties in real decision making models. Fuzzy Multi-criteria Group Decision Making (MCGDM) has been shown to be an efficient technique for obtaining rankings from experts' opinions. This paper presents a fuzzy logic based multi-criteria group decision making system for the assessment of umbilical cord acid-base. The proposed system models the variation in the decision making process exhibited by the various experts. We will present results which show how the proposed system can give a better agreement with the experts compared to an existing fuzzy expert system (FES)."
832249,14018,9080,Establishing integration test orders of classes with several coupling measures,2011,"During the inter-class test, a common problem, named Class Integration and Test Order (CITO) problem, involves the determination of a test class order that minimizes stub creation effort, and consequently test costs. The approach based on Multi-Objective Evolutionary Algorithms (MOEAs) has achieved promising results because it allows the use of different factors and measures that can affect the stubbing process. Many times these factors are in conflict and usually there is no a single solution for the problem. Existing works on MOEAs present some limitations. The approach was evaluated with only two coupling measures, based on the number of attributes and methods of the stubs to be created. Other MOEAs can be explored and also other coupling measures. Considering this fact, this paper investigates the performance of two evolutionary algorithms: NSGA-II and SPEA2, for the CITO problem with four coupling measures (objectives) related to: attributes, methods, number of distinct return types and distinct parameter types. An experimental study was performed with four real systems developed in Java. The obtained results point out that the MOEAs can be efficiently used to solve this problem with several objectives, achieving solutions with balanced compromise between the measures, and of minimal effort to test."
1596101,14018,9080,Automatic hippocampus localization in histological images using PSO-based deformable models,2011,"The Allen Brain Atlas (ABA) is a cellular-resolution, genome-wide map of gene expression in the mouse brain which allows users to compare gene expression patterns in neuroanatomical structures. The correct localization of the structures is the first step to carry on this comparison in an automatic way.   In this paper we present a completely automatic tool for the localization of the hippocampus that can be easily adapted also to other subcortical structures. This goal is achieved in two distinct phases.   The first phase, called best reference slice selection, is performed by comparing the image of the brain with a reference Atlas provided by ABA using a two-step affine registration. By doing so the system is able to automatically find to which brain section the image corresponds and wherein the image the hippocampus is roughly located.   The second phase, the proper hippocampus localization, is based on a method that combines Particle Swarm Optimization (PSO) and a novel technique inspired by Active Shape Models (ASMs). The hippocampus is found by adapting a deformable model derived statistically, in order to make it overlap with the hippocampus image.   Experiments on a test set of 120 images yielded a perfect or good localization in 89.2% of cases."
1266828,14018,9080,The influence of linkage-learning in the linkage-tree GA when solving multidimensional knapsack problems,2013,"Linkage Learning (LL) is an important issue concerning the development of more effective genetic algorithms (GA). It is from the identification of strongly dependent variables that crossover can be effective and an efficient search can be implemented. In the last decade many algorithms have confirmed the beneficial influence of LL when solving nearly decomposable problems. As it is a well-known fact from the no free-lunch theorem, LL can not be the best tool for all optimization problems, therefore, methods to identify those problems which could be efficiently solved by LL have become necessary. This paper investigates that nearly-decomposable problems present characteristic linkage-trees, therefore, those trees can be used as reference to infer whether or not some black-box optimization problem is a good candidate to be solved by LL. In this context, we consider the linkage-tree model from the Linkage-Tree GA (LTGA) and use the silhouette measure to expose some problems' characteristics. The  silhouette fingerprints  (SF) are defined for overlapping deceptive trap functions and compared with the SFs obtained for Multidimensional Knapsack Problems (MKP). The comparison allowed us to conclude that MKPs do not present evident linkages. This result was confirmed by experiments comparing the performance of the LTGA and the Randomized LTGA, in which both algorithms had very similar results."
1491400,14018,9080,Extended virtual loser genetic algorithm for the dynamic traveling salesman problem,2013,"The use of memory-based Evolutionary Algorithms (EAs) for dynamic optimization problems (DOPs) has proved to be efficient, namely when past environments reappear later. Memory EAs using associative approaches store the best solution and additional information about the environment. In this paper we propose a new algorithm called Extended Virtual Loser Genetic Algorithm (eVLGA) to deal with the Dynamic Traveling Salesman Problem (DTSP). In this algorithm, a matrix called extended Virtual Loser (eVL) is created and updated during the evolutionary process. This matrix contains information that reflects how much the worst individuals differ from the best, working as environmental information, which can be used to avoid past errors when new individuals are created. The matrix is stored into memory along with the current best individual of the population and, when a change is detected, this information is retrieved from memory and used to create new individuals that replace the worst of the population. eVL is also used to create immigrants that are tested in eVLGA and in other standard algorithms. The performance of the investigated eVLGAs is tested in different instances of the Dynamic Traveling Salesman Problem and compared with different types of EAs. The statistical results based on the experiments show the efficiency, robustness and adaptability of the different versions of eVLGA."
1995877,14018,9704,Comprehensive comparison of convergence performance of optimization algorithms based on nonparametric statistical tests,2012,"In evolutionary computation, statistical tests are commonly used to improve the comparative evaluation process of the performance of different algorithms. In this paper, three state-of-the-art Differential Evolution (DE) based algorithms, namely Dynamic Memetic Differential Evolution (MOS), Self-adaptive DE hybridized with modified multi-trajectory search (MMTS) algorithm (SaDE-MMTS) and Self-adaptive Differential Evolution Algorithm using Population Size Reduction and three Strategies Algorithm (jDElscop) as well as a novel algorithm called ensemble of parameters and mutation strategies in Differential Evolution with Self-adaption and MMTS (Sa-EPSDE-MMTS), are tested on the most recent LSO benchmark problems and comparatively evaluated using nonparametric statistical analysis. Instead of using the “Value-to-Reach” as the comparison criterion, comprehensive comparison over multiple evolution points are investigated on each test problem in order to quantitatively compare convergence performance of different algorithms. Our investigations demonstrate that even though all these algorithms yield the same final solutions on a large set of problems, they possess statistically significant variations during the convergence. Hence, we propose that evolutionary algorithms can be compared statistically along the evolution paths."
2246001,14018,9704,A hybrid approach based on genetic fuzzy systems for Wireless Sensor Networks,2011,"Wireless sensor networks (WSNs) are composed of sensor nodes in order to detect and transmit features from the physical environment. Generally, the sensor nodes transmit informations to a special node, called sink. The use of an unique sink represents a bottleneck in a network, especially for applications in real time. In this sense, some researches have directed studies to the use of multiple sinks. The approach proposed by this paper presents the application of Genetic Fuzzy System (GFS) for the selection of routes in WNSs, in order to make the communication between multiple sensor nodes and multiple sink nodes. Fuzzy Inference System of Mamdani are used to determine the most appropriate sink node through consideration of some characteristics of the sensors network, such as energy and number of hops. Genetic Algorithms are employed to obtain the optimal adjustment of Mamdani's fuzzy inference system parameters. By applying GAs, we intend to achieve both a fuzzy database and a fuzzy rules base to maximize performance of the application of Mamdani's inference system in the selection of routes in Wireless Sensor Networks. The proposed route selection was applied by means of computer simulations to demonstrate the feasibility of the approach implemented. The results obtained through simulations demonstrated a sensor network with a longer lifetime, through the choice of the adequate sink used for sending packets through the network in order to find the best routes."
789553,14018,21102,Fuzzy decision theory based optimization of constrained portfolios using metaheuristics,2013,"To tackle uncertainty in the financial markets, fund managers resort to market scenario generation, which are an essential component of the portfolio selection process. However, when constraints reflecting investor preferences are also included in the portfolio selection process, the problem can turn complex, for direct solving by traditional methods. In this work a portfolio selection problem that incorporates basic, bounding, cardinality and class constraints, and which employs fuzzy decision theory to tackle the uncertainty arising out of possible market scenarios in the fund managers' view point, has been discussed. The complex constrained portfolio optimization problem employs Evolution Computation based metaheuristics, augmented with appropriate weight standardization functions, to navigate the search for the optimal portfolio through feasible solution space, thereby ensuring quick convergence. The experimental simulations have been undertaken on the Bombay Stock Exchange (BSE 200 index, Period: July 2001 - July 2006) and Tokyo Stock Exchange (Nikkei225 index, Period: March 2002-March 2007) data sets. The performance efficiencies of the optimal fuzzy portfolios have been measured using Sharpe and Treynor ratios and compared with those of their crisp counterparts."
1338543,14018,9704,Differential evolution with thresheld convergence,2013,"During the search process of differential evolution (DE), each new solution may represent a new more promising region of the search space (exploration) or a better solution within the current region (exploitation). This concurrent exploitation can interfere with exploration since the identification of a new more promising region depends on finding a (random) solution in that region which is better than its target solution. Ideally, every sampled solution will have the same relative fitness with respect to its nearby local optimum - finding the best region to exploit then becomes the problem of finding the best random solution. However, differential evolution is characterized by an initial period of exploration followed by rapid convergence. Once the population starts converging, the difference vectors become shorter, more exploitation is performed, and an accelerating convergence occurs. This rapid convergence can occur well before the algorithm's budget of function evaluations is exhausted; that is, the algorithm can converge prematurely. In thresheld convergence, early exploitation is “held” back by a threshold function, allowing a longer exploration phase. This paper presents a new adaptive thresheld convergence mechanism which helps DE achieve large performance improvements in multi-modal search spaces."
2394819,14018,21102,Multiple characterisation modelling of friction stir welding using a genetic multi-objective data-driven fuzzy modelling approach,2011,"Friction Stir Welding (FSW) is a relatively new solid-state joining technique, which is versatile, environment friendly, and energy and time efficient. For a comprehensive understanding of the effects of process conditions, such as tool rotation speed and traverse speed, on characterisations of welded materials, it is essential to establish prediction models for different aspects of the materials' behaviours. Because of the high complexity of the FSW process, it is often difficult to derive accurate and yet transparent enough mathematical models. In such a situation, a systematic data-driven fuzzy modelling approach is developed and implemented in this paper to model FSW behaviour relating to AA5083 aluminium alloy, consisting of microstructural features, mechanical properties, as well as overall weld quality. This methodology allows constructing transparent fuzzy models considering both accuracy and interpretability attributes of fuzzy systems. The elicited models proved to be accurate, interpretable and robust and can be further applied to facilitate the optimal design of process parameters, with the aim of finding the optimal combinations of process parameters to achieve desired welding properties."
1750376,14018,9704,Introducing ℓ 1 -regularized logistic regression in Markov Networks based EDAs,2011,"Estimation of Distribution Algorithms evolve populations of candidate solutions to an optimization problem by introducing a statistical model, and by replacing classical variation operators of Genetic Algorithms with statistical operators, such as estimation and sampling. The choice of the model plays a key role in the evolutionary process, indeed it strongly affects the convergence to the global optimum. From this point of view, in a black-box context, especially when the interactions among variables in the objective function are sparse, it becomes fundamental for an EDA to choose the right model, able to encode such correlations. In this paper we focus on EDAs based on undirected graphical models, such as Markov Networks. To learn the topology of the graph we apply a sparse method based on l 1 -regularized logistic regression, which has been demonstrated to be efficient in the high-dimensional case, i.e., when the number of observations is much smaller than the sample space. We propose a new algorithm within the DEUM framework, called DEUM l  1 , able to learn the interactions structure of the problem without the need of prior knowledge, and we compare its performance with other popular EDAs, over a set of well known benchmarks."
1244218,14018,9080,Efficient algorithms for extracting biological key pathways with global constraints,2012,"The integrated analysis of data of different types and with various interdependencies is one of the major challenges in computational biology. Recently, we developed KeyPathwayMiner, a method that combines biological networks modeled as graphs with disease-specific genetic expression data gained from a set of cases (patients, cell lines, tissues, etc.). We aimed for finding all maximal connected sub-graphs where all nodes but $K$ are expressed in all cases but at most $L$, i.e. key pathways. Thereby, we combined biological networks with OMICS data, instead of analyzing these data sets in isolation. Here we present an alternative approach that avoids a certain bias towards hub nodes: We now aim for extracting all maximal connected sub-networks where all but at most $K$ nodes are expressed in all cases but in total (!) at most $L$, i.e. accumulated over all cases and all nodes in a solution. We call this strategy GLONE (global node exceptions); the previous problem we call INES (individual node exceptions). Since finding GLONE-components is computationally hard, we developed an Ant Colony Optimization algorithm and implemented it with the KeyPathwayMiner Cytoscape framework as an alternative to the INES algorithms. KeyPathwayMiner 3.0 now offers both the INES and the GLONE algorithms. It is available as plugin from Cytoscape and online at http://keypathwayminer.mpi-inf.mpg.de."
1577831,14018,9080,An evolutionary approach for the dubins' traveling salesman problem with neighborhoods,2012,"In this work we propose an efficient and simple three-stage evolutionary algorithm to tackle the difficult problem of planning shorter paths through regions of an environment which are feasible for a nonholonomic vehicle with curvature constraints ( e.g.  Dubins' vehicle). Our method is able to efficiently solve both the combinatorial and the continuous steps of the problem in a combined manner. In the first phase, the method varies the position of the waypoints within the boundaries of each region, it then optimizes the path orientation at each waypoint, and finally it chooses the best actual sequence of visit. Numerous trials, under different scenarios in a simulated environment, were executed providing a thorough evaluation and validation of the methodology. The results show that a substantial improvement was obtained on the search for optimal paths in the DTSPN over current works in the literature. Numerical simulations also exhibit a significant performance improvement when compared with classical solutions that use the Alternating Algorithm, and they also show that our method outperforms a random sampling based technique. Our results present a reduction on the final path length of about 25% on average when compared to paths generated by the aforementioned methods."
1667098,14018,9704,Analysing the robustness of multiobjectivisation parameters with large scale optimisation problems,2012,"Evolutionary Algorithms (EAs) are one of the most popular strategies for solving optimisation problems. To define a configuration of an EA several components and parameters must be specified. Therefore, one of the main drawbacks of EAs is the complexity of their parameter setting. Another problem is that EAs might have a tendency to converge towards local optima for many problems. For this reason, several methods to deal with local optima stagnation have been designed. Multiobjectivisation, which consists in the reformulation of mono-objective problems as multi-objective ones, is one of such methods. Some multiobjectivisation methods require the specification of parameters by the user. In some cases, the quality of the obtained solutions has been improved by these methods. However, they usually introduce more components and parameters into the optimisation scheme. The main contribution of this work is to deeply analyse the robustness of multiobjectivisation approaches with parameters. Several large scale continuous optimisation problems have been multiobjectivised in order to perform such a study. Extracted conclusions might allow designing methods which profit from multiobjectivisation with parameters, without incorporating additional parameters to the whole optimisation scheme. By this way, the parameter setting could be performed in an easier way. The experimental evaluation has provided promising results."
1302989,14018,9704,Totally disturbed chaotic Particle Swarm Optimization,2012,"Particle Swarm Optimization (PSO), classified as a swarm intelligence technique, mimics the well-informed swarming behavior of social species. A simple and effective searching strategy declares PSO as a potential member for solving various optimization problems. The present study embeds the concept of chaos at different stages of PSO, intending to enhance the convergence speed while trying to avoid stagnation and maintaining the solution quality. The proposed PSO variant is termed as “Totally disturbed PSO (TDPSO)”. The algorithm starts with a disturbed (chaotic) population, generated by considered chaotic system. Thereafter, when a certain number of iterations have elapsed and the searching process approaches equilibrium state, a relative velocity index is calculated for each particle to evaluate its present state and to decide whether or not the particle needs perturbation. The efficacy of proposed algorithm is tested against a set of benchmark problems and results are compared with existing Chaotic PSO and a standard PSO variant. Numerical results manifest that TDPSO works better over considered existing variants by effectively enhancing the searching capability and precision as well."
1294257,14018,9080,A tribal ecosystem inspired algorithm (TEA) for global optimization,2014,"Evolution mechanisms of different biological and social systems have inspired a variety of evolutionary computation (EC) algorithms. However, most existing EC algorithms simulate the evolution procedure at the individual-level. This paper proposes a new EC mechanism inspired by the evolution procedure at the tribe-level, namely tribal ecosystem inspired algorithm (TEA). In TEA, the basic evolution unit is not an individual that represents a solution point, but a tribe that covers a subarea in the search space. More specifically, a tribe represents the solution set locating in a particular subarea with a coding structure composed of three elements: tribal chief, attribute diversity, and advancing history. The tribal chief represents the locally best-so-far solution, the attribute diversity measures the range of the subarea, and the advancing history records the local search experience. This way, the new evolution unit provides extra knowledge about neighborhood profiles and search history. Using this knowledge, TEA introduces four evolution operators, reforms, self-advance, synergistic combination, and augmentation, to simulate the evolution mechanisms in a tribal ecosystem, which evolves the tribes from potentially promising subareas to the global optimum. The proposed TEA is validated on benchmark functions. Comparisons with three representative EC algorithms confirm its promising performance."
1030228,14018,21102,A type2 Fuzzy Logic System for workforce management in the telecommunications domain,2012,"Workforce management is one of the most important factors in the success of any company that provides its customers with services. Hence, in order for the company to achieve objectives like customer satisfaction and maximum resource utilization, there is a need to have a reliable means of efficiently managing the company workforce and making sure that the produced plan always gives a good choice when it comes to assigning the available technicians to the given jobs. As the quantity of services and the workforce grow, the use of an automated workforce management system becomes inevitable. However the automated workforce management system should allow full transparency to allow the user to interact with the generated plans. In addition, the workforce management systems face high levels of uncertainties when dealing with real-world scenarios, which necessitates employing systems, which are able to handle the linguistic and numerical uncertainties available in the real-world scenarios. Fuzzy Logic Systems (FLSs) are credited with providing transparent methodologies that can deal with the imprecision and uncertainties. However the vast majority of the FLSs employ the type-1 FLSs, which cannot directly handle the high levels of uncertainties. Type-2 FLSs which employ type-2 fuzzy sets can handle such high levels of uncertainties to give very good performances. In this paper, we will present a type-2 FLS based workforce management system that is being developed for a delivery unit in British Telecom (BT). We will show how the presented system was able to handle the faced uncertainties to give very good performance that outperformed the automated non-intelligent system and the type-1 FLSs based system."
2382511,14018,21102,Centroid density of interval type-2 fuzzy sets: Comparing stochastic and deterministic defuzzification,2011,"Recently, Type-2 (T2) Fuzzy Logic Systems (FLSs) gained increased attention due to their capability to better describe, model and cope with the ubiquitous dynamic uncertainties in many engineering applications. By far the most widely used type of T2 FLSs are the Interval T2 (IT2) FLSs. This paper provides a comparative analysis of two fundamentally different approaches to defuzzification of IT2 Fuzzy Sets (FSs) — the deterministic Karnik-Mendel Iterative Procedure (KMIP) and the stochastic sampling defuzzifier. As previously demonstrated by other researchers, these defuzzification algorithms do not always compute identical output values. In the presented work, the concept of centroid density of an IT2 FS is introduced in order to explain such discrepancies. It was demonstrated that the stochastic sampling defuzzification method converges towards the center of gravity of the proposed centroid density function. On the other hand, the KMIP method calculates the midpoint of the interval centroid obtained according to the extension principle. Since the information about the centroid density is removed via application of the extension principle, the two methods produce inevitably different results. As further demonstrated, this difference significantly increases in case of non-symmetric IT2 FSs."
1455222,14018,9704,Evolutionary hybrid computation in view of design information by data mining,2013,"Design Informatics has three points of view. First point is the efficient exploration in design space using evolutionary computation. Second point is the structurization and visualization of design space using data mining. Third point is the application to practical problems. In the present study, the influence of the seven pure and hybrid optimizers for design information has been investigated in order to explain the selection manner of optimizer for data mining. A single-stage hybrid rocket design problem is picked up as the present design object. As a result, mining result depends on not the number of generation (convergence) but the optimizers (diversity). Consequently, the optimizer with diversity performance should be selected in order to obtain global design information in the design space. Therefore, the diversity performance has also been explained for the seven optimization methods by using three standard mathematical test problems with/without noise. The result indicates that the hybrid method between the differential evolution and the genetic algorithm is beneficial performance for efficient exploration in the design space under the condition for large-scale design problems within 10 2  order evolution at most. Moreover, the comparison among eight crossovers indicates that the principal component analysis blended crossover is good selection on the hybrid method between the differential evolution and the genetic algorithm."
1165631,14018,9080,Forex trading using geometry sensitive neural networks,2012,"Though machine learning has been applied to the foreign exchange market for algorithmic trading for quiet some time now, and neural networks(NN) have been shown to yield positive results, in most modern approaches the NN systems are optimized through traditional methods like the backpropagation algorithm for example, and their input signals are price lists, and lists composed of other technical indicator elements. The aim of this paper is twofold: the presentation and testing of the application of topology and weight evolving artificial neural network (TWEANN) systems to automated currency trading, and to demonstrate the performance when using forex chart images as input to geometrical regularity aware indirectly encoded neural network systems, enabling them to use the patterns & trends within, when trading. This paper presents the benchmark results of NN based automated currency trading systems evolved using TWEANNs, and compares the performance and generalization capabilities of these direct encoded NNs which use the standard sliding-window based price vector inputs, and the indirect (substrate) encoded NNs which use charts as input. The TWEANN algorithm I will use in this paper to evolve these currency trading agents is the memetic algorithm based TWEANN system called Deus Ex Neural Network (DXNN) platform."
915031,14018,9080,The importance of the learning conditions in hyper-heuristics,2013,"Evolutionary Algorithms are problem solvers inspired by nature. The effectiveness of these methods on a specific task usually depends on a non trivial manual crafting of their main components and settings. Hyper-Heuristics is a recent area of research that aims to overcome this limitation by advocating the automation of the optimization algorithm design task. In this paper, we describe a Grammatical Evolution framework to automatically design evolutionary algorithms to solve the knapsack problem. We focus our attention on the evaluation of solutions that are iteratively generated by the Hyper-Heuristic. When learning optimization strategies, the hyper-method must evaluate promising candidates by executing them. However, running an evolutionary algorithm is an expensive task and the computational budget assigned to the evaluation of solutions must be limited. We present a detailed study that analyses the effect of the learning conditions on the optimization strategies evolved by the Hyper-Heuristic framework. Results show that the computational budget allocation impacts the structure and quality of the learned architectures. We also present experimental results showing that the best learned strategies are competitive with state-of-the-art hand designed algorithms in unseen instances of the knapsack problem."
2507454,14018,9704,Evolutionary computation for predicting optimal reaction knockouts and enzyme modulation strategies,2013,"One of the main purposes of Metabolic Engineering is the quantitative prediction of cell behaviour under selected genetic modifications. These methods can then be used to support adequate strain optimization algorithms in a outer layer. The purpose of the present study is to explore methods in which dynamical models provide for phenotype simulation methods, that will be used as a basis for strain optimization algorithms to indicate enzyme under/over expression or deletion of a few reactions as to maximize the production of compounds with industrial interest. This work details the developed optimization algorithms, based on Evolutionary Computation approaches, to enhance the production of a target metabolite by finding an adequate set of reaction deletions or by changing the levels of expression of a set of enzymes. To properly evaluate the strains, the ratio of the flux value associated with the target metabolite divided by the wild-type counterpart was employed as a fitness function. The devised algorithms were applied to the maximization of Serine production by Escherichia coli, using a dynamic kinetic model of the central carbon metabolism. In this case study, the proposed algorithms reached a set of solutions with higher quality, as compared to the ones described in the literature using distinct optimization techniques."
907824,14018,9080,An evolutionary data-conscious artificial immune recognition system,2013,"Artificial Immune Recognition System (AIRS) algorithm offers a promising methodology for data classification. It is an immune-inspired supervised learning algorithm that works efficiently and has shown comparable performance with respect to other classifier algorithms. For this reason, it has received escalating interests in recent years. However, the full potential of the algorithm was yet unleashed.    We proposed a novel algorithm called the evolutionary data-conscious AIRS (EDC-AIRS) algorithm that accentuates and capitalizes on 3 additional immune mechanisms observed from the natural immune system. These mechanisms are associated to the phenomena exhibited by the antibodies in response to the concentration, location and type of foreign antigens. Bio-mimicking these observations empower EDC-AIRS algorithm with the ability to robustly adapt to the different density, distribution and characteristics exhibited by each data class. This provides competitive advantages for the algorithm to better characterize and learn the underlying pattern of the data. Experiments on four widely used benchmarking datasets demonstrated promising results -- outperforming several state-of-the-art classification algorithms evaluated. This signifies the importance of integrating these immune mechanisms as part of the learning process."
1763698,14018,9704,A search for scalable evolutionary solutions to the game of MasterMind,2013,"MasterMind is a puzzle in which a hidden string of symbols must be discovered by producing query strings which are compared with the hidden one; the result of this comparison (in terms of number of correct positions and colors) is fed back to the player that is trying to crack the code (codebreaker). Methods for solving this puzzle are usually compared in terms of the number of query strings (guesses) made and the total time needed to produce those strings. In this paper we focus on the latter by trying to find a combination of parameters that is, first, uniform and independent of the problem size, and second, adequate to find a fast solution that is, at the same time, good enough. The key to this combination of parameters will be the consistent set size, that is, the maximum number of combinations that are sought before being scored and played as a guess. Having found in previous papers that the consistent set size has an influence on speed, we will concentrate on small sizes and test them through two different scoring methods from literature: most parts and entropy to find out the influence of that parameter on the outcome and which method scales better. With this we try to find out which method and size yield the best results an are effectively able to? find solutions for sizes not approached so far in a reasonable time."
2101067,14018,9080,Improving many-objective optimization performance by sequencing evolutionary algorithms,2014,"Evolutionary multiobjective optimization (EMO) has been successfully applied to various real-world scenarios with usually two or three contradicting optimization goals. However, several studies have pointed out a great deterioration of computational performance when handling more than three objectives. In order to improve the scalability of multiobjective evolutionary algorithms (MOEAs) onto higher-dimensional objective spaces, techniques using e.g. scalarizing functions and preference- or indicator-based guidance have been proposed. Most of those proposals require a-priori information or a decision maker during optimization, which increases the complexity of the algorithms. In this paper, we propose a divide and conquer method for many-objective optimization. First, we partition a problem into lower-dimensional subproblems for which standard algorithms are known to perform very well. Our key improvement is the sequential usage of MOEAs, utilizing the results of one suboptimization as initial population for another MOEA. This technique allows modular optimization phases and can be applied to common evolutionary algorithms. We test our enhanced method on the hard to solve multiobjective Quadratic Assignment Problem (mQAP), using a variety of established MOEAs."
910717,14018,21102,Improvement by sorting and thresholding in PCA based nearest neighbor search,2012,"This paper proposes a revised algorithm of the nearest neighbor searching (NNS) with the PCA based binary tree data structure by Sproull [1]. In the PCA-tree, by the successive use of principal component analysis (PCA), database is partitioned into clusters. A cluster corresponds to a node of a complete binary tree. In the search step, the algorithm first choses a leaf node, i.e., a cluster, which is likely to include the nearest neighbor (NN) point. Then the exhaustive search only in the node is done. Other leaf nodes which are also likely to include the NN point are searched by the back tracking approach. The performance is improved by sorting the data on a leaf node to leaf node basis and updating the threshold value for choosing nodes by the minimum distance found so far. Sorting the data into leaf nodes contributes greatly to the improvement in the detection time of NN points. The threshold updating in e-approximate nearest neighbors approach and a fixed threshold approach is enough efficient to cope with the deterioration of accuracy. The advantage of our revised approach is not only in the detection time but also in the computer memory usage. The k-dimensional tree approach [2], [3] used in this paper does not need large sized additional tables, whereas the popular NNS algorithms with multiple hash functions method need significantly large tables for large sized databases."
1146535,14018,9080,Dynamic segregative genetic algorithm for optimizing the variable ordering of ROBDDs,2012,"In this paper an efficient dynamic segregative genetic algorithm for optimizing variable order in Reduced Ordered Binary Decision Diagrams is presented. The approach integrates a basic genetic algorithm and uses a feature function in order to define a similarity measure between chromosomes. Subpopulations of individuals, formed by applying a clustering procedure in the feature space, are explored in parallel by multiple copies of the basic genetic algorithm. A communication protocol preserves the similarity inside each subpopulation during the evolution process. The redundant exploration of the search space is avoided by using a tabu search associative memory. Genetic material from yet unexplored regions of the search space is managed and organized in order to explicitly guide the search process to yet undiscovered local optima. The experimental evaluation of the algorithm uses classical benchmark problems, known to be very difficult. Experiments suggest that our approach has a better performance in terms of stability and quality of the solution, when compared to other heuristics, such as local search methods, basic genetic algorithms, a cellular genetic algorithm and even the static segregative genetic algorithm that was the starting point of this work. The quality of the distributed implementation and the communication protocol are thoroughly analyzed."
1279684,14018,9704,An efficient encoding for simplified protein structure prediction using genetic algorithms,2013,"Protein structure prediction is one of the most challenging problems in computational biology and remains unsolved for many decades. In a simplified version of the problem, the task is to find a self-avoiding walk with the minimum free energy assuming a discrete lattice and a given energy matrix. Genetic algorithms currently produce the state-of-the-art results for simplified protein structure prediction. However, performance of the genetic algorithms largely depends on the encodings they use in representing protein structures and the twin removal technique they use in eliminating duplicate solutions from the current population. In this paper, we present a new efficient encoding for protein structures. Our encoding is nonisomorphic in nature and results into efficient twin removal. This helps the search algorithm diversify and explore a larger area of the search space. In addition to this, we also propose an approximate matching scheme for removing near-similar solutions from the population. Our encoding algorithm is generic and applicable to any lattice type. On the standard benchmark proteins, our techniques significantly improve the state-of-the-art genetic algorithm for hydrophobic-polar (HP) energy model on face-centered-cubic (FCC) lattice."
755822,14018,9080,Single-unit pattern generators for quadruped locomotion,2013,"Legged robots can potentially venture beyond the limits of wheeled vehicles. While creating controllers for such robots by hand is possible, evolutionary algorithms are an alternative that can reduce the burden of hand-crafting robotic controllers. Although major evolutionary approaches to legged locomotion can generate oscillations through popular techniques such as continuous time recurrent neural networks (CTRNNs) or sinusoidal input, they typically face a challenge in maintaining long-term stability. The aim of this paper is to address this challenge by introducing an effective alternative based on a new type of neuron called a  single-unit pattern generator  (SUPG). The SUPG, which is indirectly encoded by a compositional pattern producing network (CPPN) evolved by HyperNEAT, produces a flexible temporal activation pattern that can be reset and repeated at any time through an explicit trigger input, thereby allowing it to dynamically recalibrate over time to maintain stability. The SUPG approach, which is compared to CTRNNs and sinusoidal input, is shown to produce natural-looking gaits that exhibit superior stability over time, thereby providing a new alternative for evolving oscillatory locomotion."
777095,14018,21102,Structural classification of proteins through amino acid sequence using interval type-2 fuzzy logic system,2014,"This paper introduces a new multi-output interval type-2 fuzzy logic system (MOIT2FLS) that is automatically constructed from unsupervised data clustering method and trained using heuristic genetic algorithm for a protein secondary structure classification. Three structure classes are distinguished including helix, strand (sheet) and coil which correspond to three outputs of the MOIT2FLS. Quantitative properties of amino acids are used to characterize the twenty amino acids rather than the widely used computationally expensive binary encoding scheme. Amino acid sequences are parsed into learnable patterns using a local moving window strategy. Three clustering tasks are performed using the adaptive vector quantization method to derive an equal number of initial rules for each type of secondary structure. Genetic algorithm is applied to optimally adjust parameters of the MOIT2FLS with the purpose of maximizing the Q3 measure. Comprehensive experimental results demonstrate the strong superiority of the proposed approach over the traditional methods including Chou-Fasman method, Garnier-Osguthorpe-Robson method, and artificial neural network models."
816380,14018,9080,Two-cornered learning classifier systems for pattern generation and classification,2012,"Classifying objects and patterns to a certain category is crucial for both humans and machines, so that learnt knowledge may be applied across similar problem instances. Although autonomous learning of patterns by machines has advanced recently, it still requires humans to set up the problem at an appropriate level for the learning technique. If the problem is too complex the system does not learn; conversely, if the problem is too simple the system does not reach its full potential to be able to classify environmental examples. In this work, an automated evolving pattern generator and pattern recognizer has been created for pattern classification problems that can be manipulated autonomously using Learning Classifier Systems (LCSs) at different levels of difficulty. Experiments confirm that both of the agents (e.g. the pattern generation and the pattern classification agent) can be evolved autonomously and co-operatively. The novel contributions in this work enable the effect of domain features on classification performance to become human readable, i.e. possibly determine what features make it difficult for the classification algorithm to learn. This work provides a foundation for a co-evolutionary approach to problem domain creation and the associated learning, such that the agents will trigger evolution when necessary."
1674832,14018,9704,Sensitivity analysis in the optimal sizing of analog ICs by evolutionary algorithms,2013,"A multi-parameter sensitivity approach based on Richardson extrapolation, and applied to the optimal sizing of analog integrated circuits (ICs), is presented. First, the multiobjective evolutionary algorithm (EA) called non-dominated sorting genetic algorithm (NSGA-II), is applied to compute the feasible sizes of analog ICs, i.e. the optimal width and length (W/L) of every metal-oxide-semiconductor field-effect-transistor (MOSFET) is found. At this stage, the simulation program with integrated circuits emphasis (SPICE) is used to evaluate the electrical characteristics of analog ICs. Second, the multiparameter sensitivity analysis based on Richardson extrapolation, is applied to approximate the partial derivatives associated to the sensitivities on the performances of the ICs with respect to W/L of every MOSFET. The cases of study are three analog ICs, namely: voltage follower (VF), positive-type second generation current conveyor (CCII+), and current-feedback operational amplifier (CFOA). The proposed approach selects W/L feasible sizes presenting the lower sensitivities that are computed from the corresponding Pareto sets. Finally, 18 feasible (W/L sizes) solutions accomplishing 18 performance objectives and guaranteeing low W/L sensitivities for a complementary metal-oxide-semiconductor (CMOS) CFOA, are listed in Table I."
2473253,14018,9704,Searching for novel regression functions,2013,"The objective function is the core element in most search algorithms that are used to solve engineering and scientific problems, referred to as the fitness function in evolutionary computation. Some researchers have attempted to bridge this difference by reducing the need for an explicit fitness function. A noteworthy example is the novelty search (NS) algorithm, that substitutes fitness with a measure of uniqueness, or novelty, that each individual introduces into the search. NS employs the concept of behavioral space, where each individual is described by a domain-specific descriptor that captures the main features of an individual's performance. However, defining a behavioral descriptor is not trivial, and most works with NS have focused on robotics. This paper is an extension of recent attempts to expand the application domain of NS. In particular, it represents the first attempt to apply NS on symbolic regression with genetic programming (GP). The relationship between the proposed NS algorithm and recent semantics-based GP algorithms is explored. Results are encouraging and consistent with recent findings, where NS achieves below average performance on easy problems, and achieves very good performance on hard problems. In summary, this paper presents the first attempt to apply NS on symbolic regression, a continuation of recent research devoted at extending the domain of competence for behavior-based search."
1819599,14018,9080,Linkage tree genetic algorithms: variants and analysis,2012,"Discovering and exploiting the linkage between genes during evolutionary search allows the Linkage Tree Genetic Algorithm (LTGA) to maximize crossover effectiveness, greatly reducing both population size and total number of evaluations required to reach success on decomposable problems. This paper presents a comparative analysis of the most prominent LTGA variants and a newly introduced variant. While the deceptive trap problem (Trap-k) is one of the canonical benchmarks for testing LTGA, when LTGA is combined with applying steepest ascent hill climbing to the initial population, as is done in all significant LTGA variations, trap-k is trivially solved. This paper introduces the deceptive step trap problem (StepTrap-k,s), which shows the novel combination of smallest first subtree ordering with global mixing (LTS-GOMEA) is effective for black box optimization, while least linked first subtree ordering (LT-GOMEA) is effective on problems where partial reevaluation is possible. Finally, nearest neighbor NK landscapes show that global mixing is not effective on problems with complex overlapping linkage structure that cannot be modeled correctly by a linkage tree, emphasizing the need to extend how LTGA stores linkage to allow the power of global mixing to be applied to these types of problems."
974229,14018,23735,Flop and Roll: Learning Robust Goal-Directed Locomotion for a Tensegrity Robot,2014,"Tensegrity robots are composed of compression elements (rods) that are connected via a network of tension elements (cables). Tensegrity robots provide many advantages over standard robots, such as compliance, robustness, and flexibility. Moreover, sphere-shaped tensegrity robots can pro- vide non-traditional modes of locomotion, such as rolling. While they have advantageous physical properties, tensegrity robots are hard to control because of their nonlinear dynamics and oscillatory nature. In this paper, we present a robust, distributed, and directional rolling algorithm, flop and roll. The algorithm uses coevolution and exploits the distributed nature and symmetry of the tensegrity structure. We validate this algorithm using the NASA Tensegrity Robotics Toolkit (NTRT) simulator, as well as the highly accurate model of the physical SUPERBall being developped under the NASA Innovative and Advanced Concepts (NIAC) program. Flop and roll improves upon previous approaches in that it provides rolling to a desired location. It is also robust to both unexpected external forces and partial hardware failures. Additionally, it handles variable terrain (hills up to 33% grade). Finally, results are compatible with the hardware since the algorithm relies on realistic sensing and actuation capabilities of the SUPERBall."
1769249,14018,9080,Automated vibrational design and natural frequency tuning of multi-material structures,2014,"Natural frequency tuning is a vital engineering problem. Every structure has natural frequencies, where vibrational loading at nearby frequencies excite the structure. This causes the structure to resonate, oscillating until energy is dissipated through friction or structural failure. Examples of fragility and distress from vibrational loading include civil structures during earthquakes or aircraft rotor blades. Tuning the structure's natural frequencies away from these vibrations increases the structure's robustness. Conversely, tuning towards the frequencies caused by vibrations can channel power into energy harvesting systems. Despite its importance, natural frequency tuning is often performed ad-hoc, by attaching external vibrational absorbers to a structure. This is usually adequate only for the lowest (fundamental) resonant frequencies, yet remains standard practice due to the unintuitive and difficult nature of the problem. Given Evolutionary Algorithms' (EA's) ability to solve these types of problems, we propose to approach this problem with the EA CPPN-NEAT to evolve multi-material structures which resonate at multiple desired natural frequencies without external damping. The EA assigns the material type of each voxel within the discretized space of the object's existing topology, preserving the object's shape and using only its material composition to shape its frequency response."
1203711,14018,8806,An agent-based architecture for supporting the workgroups creation and the detection of out-of-context conversation on problem-based learning in virtual learning environments,2011,"A computer-supported collaborative learning environment can enable the students of web-based distance education courses to interact with each other and with one or more facilitators to conduct group work. The problem- based learning (PBL) is a learning theory that emphasizes collaboration and teamwork to solve a problem. However, a problem that occurs frequently in the implementation of PBL is the out-of-context conversation, which is a situation in which the students lose focus and start talking about topics that are not related to the discussion. In presential learning, the teacher can easily detect this problem and try to avoid it in order to improve the learning process. In distance learning, however, detecting this problem is not a trivial task. That is mostly due to issues related to the students' geographic distribution and the lack of information regarding their motivation. Another noteworthy aspect is the creation of workgroups. In PBL, the members of a workgroup that is responsible for solving a problem must have certain complementary knowledge and skills related to the problem, and it might be difficult for the facilitator to assign students to workgroups, since the lack of presential contact makes it difficult to perceive important characteristics of the students' profiles. Then, this paper presents an agent-based architecture for detecting out-of-context conversation and for helping in the creation of workgroups on the PBL."
1568686,14018,9080,Open source tool for energy saving and efficient system management,2011,"In order to improve power quality (PQ) techniques, efforts are made to develop smart sensors that can report near real-time data. Proprietary software and hardware on dedicated computers or servers processes these data and shows relevant information through tables or graphics. In this situation, interoperability, compatibility and scalability are not possible because of the lack of open protocols. This paper presents a new open source solution focused on optimization of power quality and monitoring for low voltage power systems. For that, an open source platform has been developed for computing, storing and managing all of the information generated from smart sensors. We apply the most up-to-date algorithms developed for PQ, event detection, and harmonic analysis or power metering. A plugin implementing the S-transform is being developed for the system. To obtain the best input values to this plugin we are developing optimization algorithms to detect the most of well-known disturbances. Our system makes use of cutting-edge web technologies such as HTML5, CSS3 and Javascript to provide user-friendly interaction and powerful capabilities for the analysis, measurement and monitoring of power systems."
2086091,14018,9080,Real-space evolutionary annealing,2011,"Standard genetic algorithms can discover good fitness regions and later forget them due to their Markovian structure, resulting in suboptimal performance. Real-Space Evolutionary Annealing (REA) hybridizes simulated annealing and genetic algorithms into a provably convergent evolutionary algorithm for Euclidean space that relies on non-Markovian selection. REA selects any previously observed solution from an approximated Boltzmann distribution using a cooling schedule. This method enables REA to escape local optima while retaining information about prior generations. In parallel work, REA has been generalized to arbitrary measure spaces and shown to be asymptotically convergent to the global optima. This paper compares REA experimentally to six popular optimization algorithms, including Differential Evolution, Particle Swarm Optimization, Correlated Matrix Adaptation Evolution Strategies, the real-coded Bayesian Optimization Algorithm, a real-coded genetic algorithm, and simulated annealing. REA converges faster to the global optimum and succeeds more often on two out of three multimodal, non-separable benchmarks and performs strongly on all three. In particular, REA vastly outperforms the real-coded genetic algorithm and simulated annealing, proving that the hybridization is better than either algorithm alone. REA is therefore an interesting and effective algorithm for global optimization of difficult fitness functions."
694040,14018,9704,A new CSP graph-based representation for Ant Colony Optimization,2013,"Constraint Satisfaction Problems (CSP) have been widely studied in several research areas like Artificial Intelligence or Operational Research due their complexity and industrial interest. From previous research areas, heuristic (informed) search methods have been particularly active looking for feasible approaches. One of the critical problems to work with CSP is related to the exponential growth of computational resources needed to solve even the simplest problems. This paper presents a new efficient CSP graph-based representation to solve CSP by using Ant Colony Optimization (ACO) algorithms. This paper presents also a new heuristic (called Oblivion Rate), that have been designed to improve the current state-of-the-art in the application of ACO algorithms on these domains. The presented graph construction provides a strong reduction in both, the number of connections and the number of nodes needed to model the CSP. Also, the new heuristic is used to reduce the number of pheromones in the system (allowing to solve problems with an increasing complexity). This new approach has been tested, as case study, using the classical N-Queens Problem. Experimental results show how the new approach works in both, reducing the complexity of the resulting CSP graph and solving problems with increasing complexity through the utilization of the Oblivion Rate."
2068213,14018,9704,Evolving accurate and comprehensible classification rules,2011,"In this paper, Genetic Programming is used to evolve ordered rule sets (also called decision lists) for a number of benchmark classification problems, with evaluation of both predictive performance and comprehensibility. The main purpose is to compare this approach to the standard decision list algorithm JRip and also to evaluate the use of different length penalties and fitness functions for evolving this type of model. The results, using 25 data sets from the UCI repository, show that genetic decision lists with accuracy-based fitness functions outperform JRip regarding accuracy. Indeed, the best setup was significantly better than JRip. JRip, however, held a slight advantage over these models when evaluating AUC. Furthermore, all genetic decision list setups produced models that were more compact than JRip models, and thus more readily comprehensible. The effect of using different fitness functions was very clear; in essence, models performed best on the evaluation criterion that was used in the fitness function, with a worsening of the performance for other criteria. Brier score fitness provided a middle ground, with acceptable performance on both accuracy and AUC. The main conclusion is that genetic programming solves the task of evolving decision lists very well, but that different length penalties and fitness functions have immediate effects on the results. Thus, these parameters can be used to control the trade-off between different aspects of predictive performance and comprehensibility."
2299976,14018,21102,Interpreting fuzzy set operations and Multi Level Agreement in a Computing with Words context,2011,"Computing with Words (CWW) aims to investigate the possibility of imitating the unique ability of humans for approximate reasoning on the basis of approximately defined classes and concepts in the form of words. Type-2 fuzzy sets have been used to provide an adequate modeling basis for words in a fuzzy logic context. In the context of type-2 fuzzy sets employed as part of CWW, a variety of research efforts have been made to investigate approaches to model the meaning of specific words using type-2 fuzzy sets. In this paper we start by focusing on the interpretation of classical set-theoretical operations (complement, union and intersection) for crisp and type-1 fuzzy sets. We proceed by extending the interpretations to the results of the union and intersection operations of interval type-2 fuzzy sets, specifically indicating their effect on the uncertainty representation in the sets. We note the impact of the choice of t-norms and t-conorms in particular in the context of CWW applications where the interpretation of the resulting sets and its resemblance to the human intuitive meaning of the concept or word is essential. Finally, we provide the interpretation and reasoning behind the Multi Level Agreement (MLA) operation based on zSlices which was previously introduced and discuss the requirement for the selection of the right operations for the amalgamation of individual fuzzy sets and the potential for investigating this choice in particular in a CWW context."
1850525,14018,9704,A response-aware risk management framework for search-and-rescue operations,2012,"Efficient coordination among all assets participating in a response to a search-and-rescue (SAR) incident has long been a focus of many governments and organizations. Finding innovative solutions that guarantee a swift reaction to the distressed entity with a rational use of the available resources is pivotal to the success of the SAR operation. In spite of the plethora of successfully deployed SAR systems, we witness a substantial gap when it comes to the integration of risk-driven analyses into the underlying machinery of any decision support platform that leans upon the in-field SAR assets. This paper extends a recently proposed risk management framework [1] by adding automated modules for risk monitoring and response selection. An evolutionary multi-objective optimization algorithm is used to navigate across the discrete space of all available assets and their set of actions in order to present a limited number of promising responses to a SAR operator, who will ultimately decide what action must be carried out. The proposed methodology was validated in the context of a simulated nautical SAR scenario in the Canadian Atlantic coastline with nine different types of ground, maritime and aerial assets."
776534,14018,9704,A hybrid surrogate-based approach for evolutionary multi-objective optimization,2013,"Evolutionary algorithms have gained popularity as an alternative for dealing with multi-objective optimization problems. However, these algorithms require to perform a relatively high number of fitness function evaluations in order to generate a reasonably good approximation of the Pareto front. This can be a shortcoming when fitness evaluations are computationally expensive. In this paper, we propose an approach that combines an evolutionary algorithm with an ensemble of surrogate models based on support vector machines (SVM), which are used to approximate the fitness functions of a problem. The proposed approach performs a model selection process for determining the appropriate hyperparameters values for each SVM in the ensemble. The ensemble is constructed in an incremental fashion, such that the models are updated with the knowledge gained during the evolutionary process, but the information from previous evaluated regions is also preserved. A criterion based on surrogate fidelity is also proposed for determining when should the surrogates be updated. We evaluate the performance of our proposal using a benchmark of test problems widely used in the literature and we compare our results with respect to those obtained by the NSGA-II. Our proposed approach is able to significantly reduce the number of fitness function evaluations performed, while producing solutions which are close to the true Pareto front."
1107239,14018,21102,Lateral supra-acetabular external fixation for unstable pelvic ring fracture: A biomechanical assessments,2012,"Unstable pelvic ring fractures are usually affiliated with high energy impact that inflicted upon the hip such as in vehicle, industrial and extreme sport accidents. These injuries are always associated with high rate of mortality due to excessive bleeding and involvement of surrounding organs. Early stabilization of the pelvic using an external fixation can be used to reduce the pelvic volume and consequently control the hemorrhage. However, not a single currently available external fixations are capable of stabilizing the unstable fracture sufficient enough to allow patient mobilization and early weight bearing. Hence, this study was conducted to introduce a new method of fixation known as lateral supra-acetabular and subsequently compare with anterosuperior and anteroinferior fixation methods. The comparison was made by means of physical experimentation and computer simulation. The physical experiment employed four sawbone models while the simulation was performed using virtual pelvic model reconstructed from an MDCT image with Type C1 fracture. Both experiment and simulation were performed for standing and sitting position to evaluate the strength of fixation before failure of the model occurs. Lateral supra-acetabular fixation method was found to be significantly performed better than anterosuperior and anteroinferior fixation for both physical experiment and computer simulation."
2307268,14018,9704,An adaptive differential evolution with unsymmetrical mutation,2011,"Differential Evolution (DE) is one of the evolutionary algorithms under active research. It has been successfully applied to many real world problems. In this paper, an improved DE with a novel mutation scheme is proposed. The improved DE assigns a distinct scale factor for each individual mutation based on the fitness associated with each base vector involved in the mutation. With the adoption of different scale factors for mutation, DE is capable of searching more locally around superior points and explore more broadly around inferior points. Consequently, a good balance between exploration and exploitation can be achieved. Also, an adaptive base vector selection scheme is introduced to DE. This scheme is capable of estimating the complexity of objective functions based on the population variance. When the problem is simple, it will tend to select good vectors as base vector which will lead to quick convergence. When the objective function is complex, it will select base vector randomly so that the population maintains a high exploration capability and will not be trapped into local minima so easily. A suite of 12 benchmark functions are used to evaluate the performance of the proposed method. The simulation result shows that the proposed method is promising in terms of convergence speed, solution quality and stability."
1272455,14018,9080,Long-term evolutionary dynamics in heterogeneous cellular automata,2013,"In this work we study open-ended evolution through the analysis of a new model, HetCA, for heterogeneous cellular automata. Striving for simplicity, HetCA is based on classical two-dimensional CA, but differs from them in several key ways: cells include properties of age, decay, and quiescence; cells utilize a heterogeneous transition function, one inspired by genetic programming; and there exists a notion of genetic transfer between adjacent cells. The cumulative effect of these changes is the creation of an evolving ecosystem of competing cell colonies. To evaluate the results of our new model, we define a measure of phenotypic diversity on the space of cellular automata. Via this measure, we contrast HetCA to several controls known for their emergent behaviours---homogeneous CA and the Game of Life---and several variants of our model. This analysis demonstrates that HetCA has a capacity for long-term phenotypic dynamics not readily achieved in other models. Runs exceeding one million time steps do not exhibit stagnation or even cyclic behaviour. Further, we show that the design choices are well motivated, as the exclusion of any one of them disrupts the long-term dynamics."
852046,14018,9080,Darwinian rivers: evolving stream topographies to match hyporheic residence time distributions,2012,"We employed genetic algorithms to investigate the relationship between stream topographies and their associated hyporheic residence time distributions. A hyporheic residence time is the time it takes a water particle to enter the sediments below a stream, travel through the sediment, and re-enter the surface water of the stream. This subsurface journey affects stream chemistry and water quality, and increased knowledge of this process could be helpful in addressing the environmental problems caused by excess nutrients and waterborne pollutants in riverine ecosystems. We used a multi-scale two-dimensional model, lightly adapted from three previous models, to calculate residence time distributions from system characteristics. Our primary goal is the investigation of the RTD inverse problem - discovering stream topographies that would generate a specified target residence time distribution (RTD). We used genetic algorithms to evolve the shape of stream topographies (represented by Fourier series) to discover shapes that yield RTDs that closely match the target RTD. Our contributions are: a) the specification of the RTD inverse problem, b) evidence that genetic algorithms provide an effective method for approaching this problem, and c) the discovery of some unanticipated patterns among the evolved topographies. This early work seems promising and should encourage further applications of evolutionary computing in this area, with eventual application to stream restoration projects."
2477146,14018,9704,Boosting Cultural Algorithms with an incongruous layered social fabric influence function,2011,"In this paper we investigate the emergence and power of a complex social system based upon principles of cultural evolution. Cultural Algorithms employ a basic set of knowledge sources, each related to knowledge observed in various social species. Here we extend the influence and integration function in Cultural Algorithms by adding a mechanism by which knowledge sources can spread their influence throughout a population in the presence of heterogeneous layered social network. The interaction (overlapping) of the knowledge sources, represented as bounding boxes on the landscape, at the right level projects how efficient the cooperation is between the agents in the resultant Social Network. The inter-related structures that emerge with this approach are critical to the effective functioning of the approach. We view these structures as constituting a normal form for Cultures within these real-valued optimization landscapes. Our goal will be to identify the minimum social structure needed to solve problems of certain complexities. If this can be accomplished, it means that there will be a correspondence between the social structure and the problem environment in which it emerged. An escalating sequence of complex benchmark problems to our system will be presented. We conclude by suggesting the emergent features are what give cultural systems their power to learn and adapt."
1736789,14018,8806,Software effort prediction: a hyper-heuristic decision-tree based approach,2013,"Software effort prediction is an important task within software engineering. In particular, machine learning algorithms have been widely-employed to this task, bearing in mind their capability of providing accurate predictive models for the analysis of project stakeholders. Nevertheless, none of these algorithms has become the  de facto  standard for metrics prediction given the particularities of different software projects. Among these intelligent strategies, decision trees and evolutionary algorithms have been continuously employed for software metrics prediction, though mostly independent from each other. A recent work has proposed evolving decision trees through an evolutionary algorithm, and applying the resulting tree in the context of software maintenance effort prediction. In this paper, we raise the search-space level of an evolutionary algorithm by proposing the evolution of a decision-tree  algorithm  instead of the decision tree itself --- an approach known as hyper-heuristic. Our findings show that the decision-tree algorithm automatically generated by a hyper-heuristic is capable of statistically outperforming state-of-the-art top-down and evolution-based decision-tree algorithms, as well as traditional logistic regression. The ability of generating a highly-accurate comprehensible predictive model is crucial in software projects, considering that it allows the stakeholder to properly manage the team's resources with an improved confidence in the model predictions."
822581,14018,9704,Evolutionary detection of community structures in complex networks: A new fitness function,2012,"The discovery and analysis of communities in networks is a topic of high interest in sociology, biology and computer science. Complex networks in nature and society range from the immune system and the brain to social, communication and transport networks. The key issue in the development of algorithms able to automatically detect communities in complex networks refers to a meaningful quality evaluation of a community structure. Given a certain grouping of nodes into communities, a good measure is needed to evaluate the quality of the community structure based on the definition that a strong community has dense intra-connections and sparse outside-community links. We propose a new fitness function for the assessment of community structures quality which is based on the number of nodes and their links inside a community versus the community size further reported to the size of the network. A novel aspect of the proposed fitness function refers to considering the way nodes connect to other nodes inside the same community making this second level of links contribute to the strength of the community. The introduced fitness function is tested inside a collaborative evolutionary algorithm specifically designed for the problem of community detection in complex networks. Computational experiments are performed for several real-world complex networks which have a known real community structure. This allows the direct verification of the quality of evolved communities via the proposed fitness function emphasizing extremely promising numerical results."
1985900,14018,9704,Using a genetic algorithm as a decision support tool for the deployment of Fiber Optic Networks,2012,"Fiber optics is a relatively new technology, one which has not yet been extensively used, because of its high cost. In order to evaluate the viability of such a costly investment, techno-economic models are employed. These models evaluate the investment from both technical (e.g., optimal network design) and economical (e.g., profitability) perspectives. However, an area that has not received much attention is the deployment plans of a given fiber optic investment. Existing works usually compare manually predefined deployment plans that are considered profitable, and then apply techno-economic analysis. While this indeed offers valuable information, it does not guarantee that the examined plans are the optimal ones. This should be considered as a major disadvantage, because there could be other deployment plans that could offer significantly higher profit. This paper offers a first attempt at looking for the optimal deployment plan of fiber optics, based on profit. Our method can be considered as a framework that wraps around existing techno-economic models. We employ a Genetic Algorithm (GA), which creates a population of deployment plans. These plans can then be evaluated through the usual techno-economic approach. The GA then evolves the population of these plans and at the end of the process acts as a decision support tool that advises on the optimal deployment plan, without the need for any human interference in the decision-making process. For comparison purposes, we compare the GA's results with results under other profitable plans. Results show that the introduction of the use of the GA is very advantageous and leads to a significant increase in profit."
2107752,14018,21102,Presence expression using eye robot for computer go and system,2011,"Friendly user interface is desired for robots and computer systems which support human in all areas of daily life, e.g., information service, educational system, support service for elderly, etc. Presence expressions by eye robot is proposed, where the eye robot expressions help human to more deeply understand robot, and then effectively provide their service in its situation e.g., a personal learning program and an entertainment program. The proposed presence expression is applied a personal education system that involves the eye robot, learning contents on a laptop PC with a web camera, a wii remote (Nintendo Co., Ltd.) for measuring attention span of a subject based on 3D accelerometer information. To estimate effects of presence expression by the eye robot, subjective questioner is performed in the personal education system. Three type of leaning programs are conducted by the system for 10 subjects with subjective questionnaires evaluated with the 3-poin scale. The results show that the proposed expressions by the eye robot help the subject to feel like one is watching and to have an uncanny impression. As a result, the system has enabled the subjects to take the program with feeling of tension. This research plans to apply computer go system, another personal educational and entertainment system, where the system realizes casual communication in the games."
1378266,14018,9704,Opposition-based adaptive differential evolution,2012,"Differential evolution (DE) is a simple and efficient evolutionary algorithm. It contains three parameters which need to be predefined by users. These parameters are sensitive to specific problems and difficult to set. Opposition-based computing (OBC) is a new scheme for computational intelligence. OBC is helpful to existing techniques by making better decisions through simultaneous consideration of entities and opposite entities. The opposition phenomenon exists in the literature concerning parameter control of DE. In this paper, OBC is employed to assist with the solving of parameter control problem in DE. Employing OBC to parameter control problem in DE has not been reported previously to our knowledge. The proposed approach is called opposition-based adaptive DE (OADE). It uses two pools to respectively store parameters and opposite parameters. The parameters and their opposites are used at the same time to generate trial vectors in DE. During the evolutionary process, fitness improvement at a generation serves as a filter to detect proper parameters for optimization problems. The detected proper parameters and their opposites are stored in pools, whereas the improper parameters and their opposites are replaced by new randomly generated ones. The utilization of parameters and their opposites can balance the exploration and exploitation behavior of DE in one generation. The performance of OADE is compared with three other DE algorithms. The experimental results show that OADE significantly outperforms the benchmark algorithms. Moreover, OADE is not sensitive to the pool size."
1497671,14018,21102,GPU based parallel cooperative Particle Swarm Optimization using C-CUDA: A case study,2013,"The applications requiring massive computations may get benefit from the Graphics Processing Units (GPUs) with Compute Unified Device Architecture (CUDA) by reducing the execution time. Since the introduction of CUDA, applications from different areas have been benefited. Evolutionary algorithms are one such potential area where CUDA implementation proves to be beneficial not only in terms of the speedups obtained but also the improvement in convergence time. In this paper we present a detailed study of parallel implementation of one of the existing variants of Particle Swarm Optimization which is Cooperative Particle Swarm Optimization (CPSO). We also present a comparative study on CPSO implemented in C and C-CUDA. The algorithm was tested on a set of standard benchmark optimization functions. In this process, some interesting results related to the speedup and improvements in the time in convergence were obtained. The differences in randomizing procedures used in CUDA seem to contribute towards the diversity in population leading to better solution in contrast with the serial implementation. It also provides motivation for further research on neural network architecture and weight optimization using CUDA implementation. The results obtained in this paper therefore re-emphasize the utility of CUDA based implementation for complex and computationally intensive applications."
1694431,14018,8806,Runtime enforcement of regular timed properties,2014,"Runtime enforcement is a verification/validation technique aiming at correcting (possibly incorrect) executions of a system of interest. In this paper, we consider enforcement monitoring for systems with timed specifications (modeled as timed automata). We consider runtime enforcement of any regular timed property specified by a timed automaton. To ease their design and their correctness-proof, enforcement mechanisms are described at several levels: enforcement functions that specify the input-output behavior, constraints that should be satisfied by such functions, enforcement monitors that implement an enforcement function as a transition system, and enforcement algorithms that describe the implementation of enforcement monitors. The feasibility of enforcement monitoring for timed properties is validated by prototyping the synthesis of enforcement monitors."
2511818,14018,8806,Subjective review-based reputation,2012,"The choice of a product or a service is often influenced by its reputation, which is usually calculated from existing reviews of this product or service. A review can be either  objective , for instance when referring to concrete features of a product, or  subjective , for instance when referring to the feeling of the reviewer about one aspect. Subjective reviews are potentially biased by the characteristics of the reviewers, and therefore two subjective reviews should not be treated equally. We propose in this paper a model of reputation compensating the subjective bias of di?erent categories of reviewers. We firstly calculate this bias by analyzing the ratio between reviews coming from different categories, and then we project a subjective reputation for a given category of reviewer. We demonstrate the accuracy of our bias calculation with an experimentation on public reviews for hotels, and two specific categories of users."
1380661,14018,8806,Using Hierarchical Edge Bundles to visualize complex ontologies in GLOW,2012,"In the past decade, much effort has been put into the visual representation of ontologies. However, present visualization strategies are not equipped to handle complex ontologies with many relations, leading to visual clutter and inefficient use of space. In this paper, we propose GLOW, a method for ontology visualization based on Hierarchical Edge Bundles. Hierarchical Edge Bundles is a new visually attractive technique for displaying relations in hierarchical data, such as concept structures formed by 'subclass-of' and 'type-of' relations. We have developed a visualization library based on OWL API, as well as a plug-in for Protege, a well-known ontology editor. The displayed adjacency relations can be selected from an ontology using a set of common configurations, allowing for intuitive discovery of information. Our evaluation demonstrates that the GLOW visualization provides better visual clarity, and displays relations and complex ontologies better than the existing Protege visualization plug-in Jambalaya."
2147540,14018,8806,Modeling of privacy-aware business processes in BPMN to protect personal data,2014,"This paper proposes a privacy-aware business process modeling framework supporting reasoning and enforcement of privacy constraints. The BPMN notation is extended to incorporate visual constructs for modeling privacy requirements. Moreover, we describe Semantic Web Rule Language constructs to represent the semantics of the privacy-aware extensions to BPMN and enable the use of reasoning tools that support the verification and enforcement of privacy constraints during run-time. To analyze the potential applicability of the proposed framework we describe an airport emergency system scenario where we illustrate how the privacy-aware extension facilitates the modeling of privacy of personal data requirements in a challenging application domain."
2521494,14018,8806,Feasibility of a privacy preserving collaborative filtering scheme on the Google App Engine: a performance case study,2012,"The cloud is a utility computing infrastructure that has caused a paradigm shift in the way organisations requisition, allocate, and use IT resources. One big challenge is to preserve the confidentiality of information on the cloud. Most typical solutions use cryptographic techniques without considering how well suited they are to the cloud. This paper presents a performance case-study on implementing the building blocks of a privacy preserving collaborative filtering (PPCF) scheme in Java on the Google App Engine (GAE/J) cloud platform. The results show that the GAE/J in its current state exhibits serious performance bottlenecks for the chosen application scenario. This case study highlights the need for better performance from the GAE/J. It also informs the need for validating theoretical cloud security algorithms on real cloud computing platforms in which many performance expectations do not hold."
1482384,14018,8806,A provenance approach to assess the quality of geospatial data,2012,"Geographic information is present in our daily lives. This pervasiveness is also at the origin of several problems, including heterogeneity and trustworthiness - of the data sources, of the data providers, and of the data products derived from the original sources. Most efforts to improve this situation concentrate on establishing data collection and cura-tion standards, and quality metadata. This paper extends these efforts by presenting an approach to assess quality of geospatial data based on provenance."
1370393,14018,8806,Choosing my partners based on how they will evaluate my behavior,2014,"In nowadays reputation systems, future partners are selected based solely on their reputation values calculated according to their behavior in providing services. However, an agent that intends to participate in future interactions may concern not only about the quality of the services it provides, but also about how its partners evaluate its behavior since such evaluation may be transmitted to other agents that will use it to select their partners. In this context, this paper proposes two mechanisms able (i) to help an agent to identify how a colleague evaluates the behavior of its partners and (ii) to estimate the reputation the agent using the approach will probably receive after interacting with such colleague. The agent then puts together the reputation value associated with the quality of the services provided by the colleague and the reputation it will receive if interacts with such colleague in order to evaluate if such colleague may be selected or not as a future partner. The approach was evaluated by using an e-commerce scenario."
1661291,14018,8806,Enhancing social matrix factorization with privacy,2013,"Within the course of this manuscript we present a privacy-preserving collaborative filtering recommender system which aims at alleviating the concern with privacy of user profiles within the context of sparse social trust data. While problem of sparsity in social trust is often addressed by taking similarity driven trust measures through a probabilistic matrix factorization technique, we address the issue of privacy by proposing a dynamic privacy inference model. The privacy inference model exploits the underlying inter-entity trust information in order to build a personalized privacy perspective for each individual within the social network. This is followed by our evaluation of the proposed solution by adopting an off-the-shelf collaborative filtering recommender library, in order to generate predictions using this personalized view."
1467996,14018,8806,Verification of data pattern for interactive privacy preservation model,2011,"The research problem of privacy-preserving data publishing is to release microdata in an aggregated form using distinguished techniques that will effectively conceal sensitive and private information but can be used by external users to exercise data mining. These techniques are often studied in interactive and non-interactive settings. While non-interactive setting mainly deals with the data publication using anonymization or noise addition approaches, interactive models are based on noisy response of queries. Most of the data pattern verification and classification accuracy determination approaches exist for non-interactively published microdata. In this paper, we verify the data pattern and determine classification accuracy on an interactive privacy preservation model called  differential privacy . The contributions of this paper are: (1) We present a concise literature review of non-interactive and interactive models and technologies. (2) We propose an approach of retrieving information along with investigating, understanding and comparing the data classification accuracy experimentally on  Privacy Integrated Queries . (3) We verify data pattern by comparing the correlation and classification accuracy of the differentially private data with non-interactive  k -anonymous data."
2036161,14018,8806,Real-time visual analytics for event data streams,2012,"Real-time analysis of data streams has become an important factor for success in many domains such as server and system administration, news analysis and finance to name just a few. Introducing real-time visual analytics into such application areas promises a lot of benefits since the rate of new incoming information often exceeds human perceptual limits when displayed linearly in raw formats such as textual lines and automatic aggregation often hides important details. This paper presents a system to tackle some of the visualization challenges when analyzing such dynamic event data streams. In particular, we introduce the Event Visualizer, which is a loosely coupled modular system for collecting, processing, analyzing and visualizing dynamic real-time event data streams. Due to the variety of different analysis tasks the system provides an extensible framework with several interactive linked visualizations to focus on different aspects of the event data stream. Data streams with logging data from a computer network are used as a case study to demonstrate the advantages of visual exploration."
1542233,14018,8806,Off-line (Optimal) multiprocessor scheduling of dependent periodic tasks,2012,"This paper addresses the global scheduling of constrained deadline periodic dependent task sets on multiprocessor platforms composed of identical processors. We propose two orthogonal approaches: (1) an off-line computation of a valid fixed priority assignment (2) a computation of an off-line schedule. The method in both cases is based on the efficient exploration of a finite automaton encoding all the possible executions. Even if the problems are NP-complete, we obtain rather reasonable performances as illustrated in the benchmarks."
1613924,14018,8806,Towards better manageability of database clusters on cloud computing platforms,2014,"Cloud computing (CC) has become a very popular computing model in the last decade, as it has proved to be a cost-effective alternative to large-scale computers hosted in conventional Information Technologies (IT) department. The ability to provide on-demand IT-related resources either through the internet (public clouds) or private networks (private clouds) based on customer's needs, and the scalability they have to deliver resources horizontally, can be considered the main reasons for such popularity and have led such a CC platforms to a scenario involving the hosting of other types of systems, such as those that control large volume of data, like database management systems (DBMS)."
1353535,14018,8806,Parametric investigation of a distributed strategy for multiple agents systems applied to cooperative tasks,2014,"Distributed coordination strategy based on modified version of the artificial ant system directs mobile robots to unexplored regions and regions that were not recently explored for accomplishing cooperative tasks as exploration and surveillance. Previously, application of the strategy confirmed that exploration and surveillance general behaviors emerge from the individual agent behavior. The strategy is able to adapt the current system dynamics if the number of robots or the environment structure or both change. In this paper, parametric variation of strategy is executed according to pheromone evaporation and releasing phenomena. Experiment results demonstrate that different configurations of phenomenon affect exploration and surveillance behaviors. Different compiled data sets are considered to assess the strategies, namely: needed time to conclude the task; and time between two consecutive sensory on a specific region. The results show that there is a set of configuration of the phenomena to become the strategy more efficient to execute the exploration and surveillance tasks."
1350324,14018,8806,Optimal stopping methods for finding high quality solutions to satisfiability problems with preferences,2011,"Satisfiability problems with preferences enrich the expressive power of the Boolean Satisfiability problem (SAT) and facilitate the representation of qualitative/quantitative preferences on literals/formulas, defining an optimization problem. In some cases, it is not strictly necessary to compute an optimal solution, but it is enough to compute a sub-optimal solution of high quality and, possibly, provide a lower bound on the probability of finding an optimal solution. The 1/ e  -  rule  is the optimal stopping rule for the secretary problem that guarantees an optimal solution with probability at least 1/ e  can be found. In this paper: we show how to apply the 1/ e - rule  for solving satisfiability problems with preferences; we show that its theoretical success rate of about 37% is greater than 90% on random benchmarks; and, we show that the performance of the 1/ e - rule  on structured benchmarks is sometimes many orders-of-magnitude worse than that of complete search-based algorithms, and we explain the reasons why. We propose an algorithm based on the idea underlying the 1/ e - rule , which needs the generation of just two solutions: the experimental evaluation shows that the average success rate of the proposed algorithm is a good approximation of the theoretical one of the 1/ e - rule , since it is about 50.92% on 1956 structured problems and 48.33% on 2400 randomly generated instances with 200 variables."
2949922,14018,9704,Behavioral Analysis of Registered Web Site Visitors with Help of Mouse Tracking,2012,"We present a method to extract implicit data of registered users of a web site with the help of mouse tracking. This allows us to generate more accurate interest profiles of visitors of a web site and to obtain a solid basis for the calculation of user interests or trend detection in the Web. On the one hand, web site owners have the opportunity to adjust their sites to the interests of their users. On the other hand, they can detect new trends and expand these topics on their web pages accordingly."
855654,14018,9704,Wind speed forecasting using genetic programming,2013,"This contribution presents the application of genetic programming to the problem of time series forecasting. This forecast technique is applied to wind speed time series. The results obtained from the forecasting are used to determine the power generation capacity of a fixed-speed wind turbine, which includes a squirrel cage induction generator. The forecast values obtained with the genetic programming are compared against the original time series data in order to show the precision of this forecast technique."
1621884,14018,8806,"A personal photograph browser for life log analysis based on location, time, and person",2011,"Image browsers are important and useful applications for retrieving images from personal photograph collections. Such browsers can be a life log analysis tool to explore the events of photograph owners. This paper presents a novel photograph browser consisting of two linked views. One of the views displays photographs clustered based on their locations and times, and the other displays people clustered based on their co-occurrences in the events. Specifying a photograph, the corresponding time the picture was shot and people in the photograph are highlighted. Specifying a time, corresponding photographs and people are highlighted. Specifying a person, associated photographs are highlighted, and their corresponding times are shown. The mechanism helps users to discover interested photographs and understand the events of photograph owners. This paper presents a real scenario and user experiment, demonstrating the effectiveness of the presented browser."
2333352,14018,8806,Quantitative analysis of Reo-based service coordination,2014,"Quality of Service analysis of composed software systems is an active research area, with the goal of evaluating and improving performance and resource allocation in service-oriented applications, namely, in the glue code --coordination layer-- of such systems. Stochastic Reo offers constructs for service coordination and allows the specification of stochastic values for channels. But its state-of-the-art semantic models fail in several (important) ways. In this paper, we will see how Interactive Markov chains (IMC), proposed as a stochastic compositional model of concurrency, can be effectively used to serve as a compositional semantic model for Stochastic Reo. Treating IMC as a direct semantic model, gives rise to more faithful models and has obvious efficiency advantages. Moreover, tool support that exists for IMC is made available, without significant effort, to verify and reason about the coordination layer modelled as Reo connectors."
1695695,14018,8806,EENC - energy efficient nested clustering in UASN,2014,"Energy efficiency in Underwater Acoustic Sensor Network (UASN) is a key challenge for extending network lifetime. Base on analysis of energy consumption for LEACH in underwater channel, we propose a novel clustering scheme for UASN based on grouping nodes to ensure that nodes balance energy load by considering residual energy of candidate nodes. We introduce a formation of small clusters (groups) within clusters named as Nested Clustering (NC). Our Energy Efficient Nested Clustering (EENC) scheme divides each cluster into small groups and nodes in each of those small groups switch their operation modes (idle and awake) to achieve energy efficiency. Through simulation results, it is observed that our proposed EENC scheme has better network lifetime and optimized data duplication as compared to the existing clustering schemes."
2375617,14018,8806,Privacy-preserving reputation management,2014,"Reputation systems provide  reputation values  of rated parties to users. These reputation values, typically aggregations of individual user ratings, shall be  reliable , i.e. should enable a realistic assessment of the probability that the rated party behaves as expected in a transaction. In order for the reputation values to stay reliable and, thus, for the reputation system to provide a benefit, the system needs to be resistant against manipulations by users, the rated parties trying to improve their reputation values, and even against competitors trying to worsen a reputation value. At the same time, a reputation system shall provide  privacy protection  for users: rated parties shall not be able to learn who provided a certain rating. Otherwise users might not take part in the system as they fear bad feedback in revenge for bad ratings, or users do not want to be connected to certain transactions based on their provided ratings.   In this paper we come up with a solution that provides both,  reliability  of reputation values on the one hand, and  privacy protection  for users on the other hand. In contrast to related work, our solution only makes use of a single  reputation provider  that needs to be trusted (to a certain extent) and does not require any bulletin boards to be present in the system. We make use of the Paillier cryptosystem to provide an aggregation of individual user ratings in a way that no party can learn which user provided a certain rating."
1772865,14018,8806,SART: dynamic P2P query processing in sensor networks with probabilistic guarantees,2012,"We consider the problem of constructing efficient P2P overlays for sensornets providing Energy-Level Application and Services. In this context, assuming that a sensor is responsible for executing some program task but unfortunately it's energy-level is lower than a pre-defined threshold. Then, this sensor should be able to introduce a query to the whole system in order to discover efficiently another sensor with the desired energy level, in which the task overhead must be eventually forwarded. In this way, the Life-Expectancy of the whole network could be increased. Sensor nodes are mapped to peers based on their energy level. As the energy levels change, the sensor nodes would have to move from one peer to another and this operation is very crucial for the efficient scalability of the proposed system. Similarly, as the energy level of a sensor node becomes extremely low, that node may want to forward it's task to another node with the desired energy level. The method presented in [10] presents a novel P2P overlay for Energy Level discovery in a sensornet. However, this solution is not dynamic, since requires periodical restructuring. In particular, it is not able to support neither join of sensor_nodes with energy level out of the ranges supported by the existing p2p overlay nor leave of empty overlay_peers to which no sensor_nodes are currently associated. On this purpose and based on the efficient P2P method presented in [11], we design a dynamic P2P overlay for Energy Level discovery in a sensornet, the so-called SART (Sensors' Autonomous Range Tree). The adaptation of the P2P index presented in [11] guarantees the best-known dynamic query performance of the above operation. We experimentally verify this performance, via the D-P2P-Sim simulator."
1180881,14018,8806,Reliable supervisory coordination of stochastic communicating processes with data,2013,"We develop a process theory that can model supervisory control loops with data observation for stochastic discrete-event systems. Supervisory controllers safely coordinate distributed components of complex systems by observing their high-level discrete(-event) behavior and making a decision on allowed activities. Models of such controllers can be automatically synthesized based on the formal models of the system components and a formalization of the coordination requirements. We employ generic communication primitives to distinguish between the different flows of information, i.e., observation and supervision, whereas the coordination requirements are compactly expressed in terms of data assignments. The stochastic behavior can be employed for performance or reliability analysis, bringing higher confidence in the control design. We illustrate the framework by remodeling an industrial case study involving safe and reliable coordination of multiple maintenance procedures of a printing process of a high-tech printer."
1638341,14018,8806,Performance analysis of IEEE 802.11 IBSS power save mode using a discrete-time markov model,2012,The power management algorithm in the IEEE 802.11 standard for Independent Basic Service Set (IBSS) mode is an important field of research for power constrained wireless devices. This paper presents an overall analysis of a data frame transmission together with the corresponding ATIM frame transmission using a Markov chain model. The impact of network size on the throughput of the IEEE 802.11 DCF in Power Save Mode (PSM) is analysed and the theoretical results are validated using simulation.
2265566,14018,8806,Indoor localization using SLAM in parallel with a natural marker detector,2013,"Indoor localization poses is a challenge to computer vision research, since one may not make use of GPS-based devices. A classic approach commonly used in museums, research institutes, etc, is the use of fiducial marker to track the users position. However, this approach is intrusive into the ambient and not always possible. A possible solution would be natural marker detection, but algorithms for this, such as SURF, have not yet achieved real-time performance. A promising approach is a Visual Simultaneous Localization and Mapping (VSLAM) algorithm, which, starting from a known position, is capable of generating a map of the surrounding environment in portable systems. The problem of SLAM algorithms is theirs error accumulation that builds up during the movement. This work presents an algorithm to locate 3D positions in non-instrumented indoor environments using a web camera. We define a hybrid approach, using a pattern-recognition method to reinitialize whenever possible a VSLAM algorithm. An implementation of the proposed algorithm use well-known computer vision algorithms, such as SURF and Davison's SLAM. In addition, tests were made on datasets from walks inside a room. Results indicate that our approach is better than a fiducial marker tracking and pure SLAM tracking in our test environment."
1688848,14018,8806,An information extraction system from patient historical documents,2012,"Nowadays, document image retrieval systems are increasingly applicable by various businesses, governmental and academic organizations. ELEPAP (Hellenic Protection and Rehabilitation Centre for Disabled Children) is an organization which needs more efficient ways of managing its huge volume of archived documents. This paper deals with the preprocessing procedures of well-known OCR systems in order to extract specific features from ELEPAP's patients' cards. It is shown that our proposed methodology can provide good IT solutions for ELEPAP in order to extract information from its old archives."
1382431,14018,8806,Goal-driven software product line engineering,2011,"Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the  i * framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application."
1361500,14018,8806,Stack distance based worst-case instruction cache performance analysis,2011,"The worst-case execution time (WCET) analysis is critical to ensure the schedulability and correctness of hard real-time systems. Modern microprocessors, however, make the WCET analysis complicated, mainly because of their performance acceleration features like caches, pipelines, out-of-order execution, etc. This paper focuses on studying an accurate static timing analysis approach for instruction caches with the LRU-based strategy by computing the worst-case stack distance. The experimental results indicate that our approach can accurately predict worst-case instruction cache performance. Also, the stack distance based timing analysis approach can efficiently categorize worst-case instruction cache misses into cold, conflict and capacity misses, which can provide useful insights to improve the worst-case instruction cache performance."
1386408,14018,8806,Root-cause analysis of performance anomalies in web-based applications,2011,"The complexity behind current business-critical applications leads many times to performance problems difficult to anticipate and analyze. In our previous work we described a framework for detection of performance anomalies in web-based and component-based applications. It provides low overhead monitoring, correctly distinguishes performance anomalies from common workload variations and also presents initial information for system or application server changes related with an application performance anomaly.   In this paper we present a framework extension devised to offer root-cause failure analysis for a given performance anomaly. The monitoring module enables application profiling and ANOVA analysis is used to verify if a performance anomaly is due to internal changes within the application (e.g., application updates) or to external changes (e.g., remote services changes, system/application server change). The paper includes some experimental results that show the effectiveness of our approach to pinpoint the root-cause for different types of performance anomalies and remarks its potential to avoid a considerable number of service failures."
1662209,14018,8806,HDOV: an overlay network for wide area spatial data collection,2011,"In this paper, we propose an overlay network called HDOV, a hierarchical extension of Delaunay overlay network for data collection with multiple spatial resolutions. By using HDOV, spatial data with at least specified spatial resolution can be collected reducing the redundant messages for data collection from wide area peer-to-peer network. The proposal in HDOV consists of 1) uniform node selection method for multiple spatial resolution levels and 2) hierarchical overlay network construction methods for the selected nodes. The proposed node selection method in HDOV probabilistically adjusts geographical node densities of the overlay network levels according to the size of the Voronoi cell of each node. We propose two types of hierarchical overlay network construction method: the Selected-Nodes Leading method (SNL) and the Unselected-Nodes Leading method (UNL). Our simulation results show that the proposed method can construct overlay networks that collect data with specified uniform spatial resolutions. The simulation results also show that the UNL requires low network construction cost especially in the skewed node distribution environment and the SNL requires less network reconstruction cost when there are no adjoined node failures."
1534622,14018,8806,Spatial interpolation: an analytical comparison between kriging and RBF networks,2013,"In spatial interpolation domains, a popular method is Kriging. The contribution of this study is to prove mathematically that Kriging and RBF Networks methods produce identical results if properly configured, and that RBF networks are much faster. Complexity was calculated for both methods to show the relative speed of RBF networks. It is shown that both methods share a common structure and, as a consequence, all improvements in one method can be applied to the other. Finally, two experiments were made to show in practice the theoretical results obtained. The RBF networks were 200 times faster than Kriging in one particular experiment, and this difference increases as the data set gets larger."
2193311,14018,8806,Power consumption scheduling for peak load reduction in smart grid homes,2011,"This paper presents a design and evaluates the performance of a power consumption scheduler in smart grid homes, aiming at reducing the peak load in individual homes as well as in the system-wide power transmission network. Following the task model consist of actuation time, operation length, deadline, and a consumption profile, the scheduler copies or maps the profile according to the task type, which can be either preemptive or nonpreemptive. The proposed scheme expands the search space recursively to traverse all the feasible allocations for a task set. A pilot implementation of this scheduling method reduces the peak load by up to 23.1% for the given task set. The execution time greatly depends on the search space of a preemptive task, as its time complexity is estimated to be  O  ( M   N    np   · ( M   M/2  )  N    p  ), where  M, N   np  , and  N   p   are the number of time slots, preemptive tasks, and nonpreemptive tasks, respectively. However, it can not only be reduced almost to 2% but also made stable with a basic constraint processing mechanism which prunes a search branch when the partial peak value already exceeds the current best."
2182191,14018,8806,"Artificial Immune Systems: Models, Applications, and challenges",2012,"The Natural Immune System (NIS) is a distributed, multi-layered, adaptive, dynamic, and life-long learning system. The Artificial Immune System (AIS) is a computational system inspired by the principles and processes of the NIS. The field of AIS has obtained some degree of success as a branch of computational intelligence since it emerged in the 1990s. In this paper, we review the models and applications proposed in the last few years. In addition, we present some challenges that the AIS is facing to really distinguish itself from other established systems, in particular, biology-inspired systems (e.g., artificial neural networks and evolutionary algorithms)."
