ID_Article,communityId,ID_RelatedVenue,title,year,abstract
972882,14133,422,'Beating the news' with EMBERS: forecasting civil unrest using open source indicators,2014,"We describe the design, implementation, and evaluation of EMBERS, an automated, 24x7 continuous system for forecasting civil unrest across 10 countries of Latin America using open source indicators such as tweets, news sources, blogs, economic indicators, and other data sources. Unlike retrospective studies, EMBERS has been making forecasts into the future since Nov 2012 which have been (and continue to be) evaluated by an independent T&E team (MITRE). Of note, EMBERS has successfully forecast the June 2013 protests in Brazil and Feb 2014 violent protests in Venezuela. We outline the system architecture of EMBERS, individual models that leverage specific data sources, and a fusion and suppression engine that supports trading off specific evaluation criteria. EMBERS also provides an audit trail interface that enables the investigation of why specific predictions were made along with the data utilized for forecasting. Through numerous evaluations, we demonstrate the superiority of EMBERS over baserate methods and its capability to forecast significant societal happenings."
1433828,14133,422,Financing lead triggers: empowering sales reps through knowledge discovery and fusion,2013,"Sales representatives must have access to meaningful and actionable intelligence about potential customers to be effective in their roles. Historically, GE Capital Americas sales reps identified leads by manually searching through news reports and financial statements either in print or online. Here we describe a system built to automate the collection and aggregation of information on companies, which is then mined to identify actionable sales leads. The Financing Lead Triggers system is comprised of three core components that perform information fusion, knowledge discovery and information visualization. Together these components extract raw data from disparate sources, fuse that data into information, and then automatically mine that information for actionable sales leads driven by a combination of expert-defined and statistically derived triggers. A web-based interface provides sales reps access to the company information and sales leads in a single location. The use of the Lead Triggers system has significantly improved the performance of the sales reps, providing them with actionable intelligence that has improved their productivity by 30-50%. In 2010, Lead Triggers provided leads on opportunities that represented over $44B in new deal commitments for GE Capital."
2613960,14133,235,Integrating Surface and Abstract Features for Robust Cross-Domain Chinese Word Segmentation,2012,"Current character-based approaches are not robust for cross domain Chin ese word segmentation. In this paper, we alleviate this problem by deriving a novel enhanced ch aracter-based generative model with a new abstract aggregate candidate-feature, which indicates if th e given candidate prefers the corresponding position-tag of the longest dictionary matching wo rd. Since the distribution of the proposed feature is invariant across domains, our model thus possesses better generalization ability. Open tests on CIPS-SIGHAN-2010 show that the enhanced generative model achieves robust cross-domain performance for various OOV coverage rates and obtains the best performance on three out of four domains. The enhanced gen erative model is then further integrated with a discriminative model which also utilizes dictionary information . This integrated model is shown to be either superior or comparable to all other models repo rted in the literatur e on every domain of this task."
2479610,14133,235,Active Learning for Chinese Word Segmentation,2012,"Currently, the best performing models for Chinese word segmentation (CWS) are extremely resource intensive in terms of annotation data quanti ty. One promising solution to minimize the cost of data acquisition is active learning, which aims to actively select the most useful instances to annotate for learning. Active learning on CWS, h owever, remains challenging due to its inherent nature. In this paper, we propose a Word Bounda ry Annotation (WBA) model to make effective active learning on CWS possible. This is achie ved by annotating only those uncertain boundaries. In this way, the manual annotation cost is l argely reduced, compared to annotating the whole character sequence. To further minimize the a nnotation effort, a diversity measurement among the instances is considered to avoid duplicat e annotation. Experimental results show that employing the WBA model and the diversity measurement into active learning on CWS can save much annotation cost with little loss in the perfor mance."
1673225,14133,422,Lung cancer survival prediction using ensemble data mining on SEER data,2012,"We analyze the lung cancer data available from the SEER program with the aim of developing accurate survival prediction models for lung cancer. Carefully designed preprocessing steps resulted in removal/modification/splitting of several attributes, and 2 of the 11 derived attributes were found to have significant predictive power. Several supervised classification methods were used on the preprocessed data along with various data mining optimizations and validations. In our experiments, ensemble voting of five decision tree based classifiers and meta-classifiers was found to result in the best prediction performance in terms of accuracy and area under the ROC curve. We have developed an on-line lung cancer outcome calculator for estimating the risk of mortality after 6 months, 9 months, 1 year, 2 year and 5 years of diagnosis, for which a smaller non-redundant subset of 13 attributes was carefully selected using attribute selection techniques, while trying to retain the predictive power of the original set of attributes. Further, ensemble voting models were also created for predicting conditional survival outcome for lung cancer estimating risk of mortality after 5 years of diagnosis, given that the patient has already survived for a period of time, and included in the calculator. The on-line lung cancer outcome calculator developed as a result of this study is available at http://info.eecs.northwestern.edu:8080/LungCancerOutcomeCalculator/."
1649389,14133,65,Ceiling vision-based topological mapping and exploration in wide-open area,2013,"In this paper, we propose a new ceiling vision-based active mapping and exploration framework for wide-open structural environments such as museum, exhibition center. We adopt the ceiling vision based approach with model-free landmark definition and visual servoing based data association. Additionally, by using a recursive explore and exploit strategy in topological map-building and navigation, we could gradually spread out the mapped region in unknown, wide-open environments without degrading map accuracy. Simulations and real experiment demonstrated that our mapping and exploration algorithm could work well in real environment."
2379863,14133,235,Creating Custom Taggers by Integrating Web Page Annotation and Machine Learning,2014,"We present an on-going work on a software package that integrates discriminative machine learning with the open source WebAnnotator system of Tannier (2012). The WebAnnotator system allows users to annotate web pages within their browser with custom tag sets. Meanwhile, we integrate the WebAnnotator system with a machine learning package which enables automatic tagging of new web pages. We hope the software evolves into a useful information extraction tool for motivated hobbyists who have domain expertise on their task of interest but lack machine learning or programming knowledge. This paper presents the system architecture, including the WebAnnotator-based front-end and the machine learning component. The system is available under an open source license."
1480070,14133,422,A lung cancer outcome calculator using ensemble data mining on SEER data,2011,"We analyze the lung cancer data available from the SEER program with the aim of developing accurate survival prediction models for lung cancer using data mining techniques. Carefully designed preprocessing steps resulted in removal/modification/splitting of several attributes, and 2 of the 11 derived attributes were found to have significant predictive power. Several data mining classification techniques were used on the preprocessed data along with various data mining optimizations and validations. In our experiments, ensemble voting of five decision tree based classifiers and meta-classifiers was found to result in the best prediction performance in terms of accuracy and area under the ROC curve. Further, we have developed an on-line lung cancer outcome calculator for estimating risk of mortality after 6 months, 9 months, 1 year, 2 year, and 5 years of diagnosis, for which a smaller non-redundant subset of 13 attributes was carefully selected using attribute selection techniques, while trying to retain the predictive power of the original set of attributes. The on-line lung cancer outcome calculator developed as a result of this study is available at http://info.eecs.northwestern.edu:8080/LungCancerOutcome-Calculator/"
2000883,14133,422,Nonparametric hierarchal bayesian modeling in non-contractual heterogeneous survival data,2013,"An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the lifetime. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's characteristic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conventional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The extension assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic variables for each customer. Our model creates several customer groups to mirror the structure of the target data set.   The effectiveness of our proposal is confirmed by a comparison involving a real e-commerce transaction dataset and an artificial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance."
262768,14133,256,Discriminative Recurrent Sparse Auto-Encoders,2013,"Abstract: We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. #R##N#From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST."
721906,14133,65,An active audition framework for auditory-driven HRI: Application to interactive robot dancing,2012,"In this paper we propose a general active audition framework for auditory-driven Human-Robot Interaction (HRI). The proposed framework simultaneously processes speech and music on-the-fly, integrates perceptual models for robot audition, and supports verbal and non-verbal interactive communication by means of (pro)active behaviors. To ensure a reliable interaction, on top of the framework a behavior decision mechanism based on active audition policies the robot's actions according to the reliability of the acoustic signals for auditory processing. To validate the framework's application to general auditory-driven HRI, we propose the implementation of an interactive robot dancing system. This system integrates three preprocessing robot audition modules: sound source localization, sound source separation, and ego noise suppression; two modules for auditory perception: live audio beat tracking and automatic speech recognition; and multi-modal behaviors for verbal and non-verbal interaction: music-driven dancing and speech-driven dialoguing. To fully assess the system, we set up experimental and interactive real-world scenarios with highly dynamic acoustic conditions, and defined a set of evaluation criteria. The experimental tests revealed accurate and robust beat tracking and speech recognition, and convincing dance beat-synchrony. The interactive sessions confirmed the fundamental role of the behavior decision mechanism for actively maintaining a robust and natural human-robot interaction."
208409,14133,256,An empirical analysis of dropout in piecewise linear networks,2014,"Abstract: The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient."
1792913,14133,422,Efficient evaluation of large sequence kernels,2012,"Classification of sequences drawn from a finite alphabet using a family of string kernels with inexact matching (e.g., spectrum or mismatch) has shown great success in machine learning. However, selection of optimal mismatch kernels for a particular task is severely limited by inability to compute such kernels for long substrings ( k -mers) with potentially many mismatches ( m ). In this work we introduce a new method that allows us to exactly evaluate kernels for large  k ,  m  and arbitrary alphabet size. The task can be accomplished by first solving the more tractable problem for small alphabets, and then trivially generalizing to any alphabet using a small linear system of equations. This makes it possible to explore a larger set of kernels with a wide range of kernel parameters, opening a possibility to better model selection and improved performance of the string kernels. To investigate the utility of large ( k , m ) string kernels, we consider several sequence classification problems, including protein remote homology detection, fold prediction, and music classification. Our results show that increased  k -mer lengths with larger substitutions can improve classification performance."
1027222,14133,422,Different slopes for different folks: mining for exceptional regression models with cook's distance,2012,"Exceptional Model Mining (EMM) is an exploratory data analysis technique that can be regarded as a generalization of subgroup discovery. In EMM we look for subgroups of the data for which a model fitted to the subgroup differs substantially from the same model fitted to the entire dataset. In this paper we develop methods to mine for exceptional regression models. We propose a measure for the exceptionality of regression models (Cook's distance), and explore the possibilities to avoid having to fit the regression model to each candidate subgroup. The algorithm is evaluated on a number of real life datasets. These datasets are also used to illustrate the results of the algorithm. We find interesting subgroups with deviating models on datasets from several different domains. We also show that under certain circumstances one can forego fitting regression models on up to 40% of the subgroups, and these 40% are the relatively expensive regression models to compute."
2246920,14133,422,Adversarial support vector machine learning,2012,"Many learning tasks such as spam filtering and credit card fraud detection face an active adversary that tries to avoid detection. For learning problems that deal with an active adversary, it is important to model the adversary's attack strategy and develop robust learning models to mitigate the attack. These are the two objectives of this paper. We consider two attack models: a free-range attack model that permits arbitrary data corruption and a restrained attack model that anticipates more realistic attacks that a reasonable adversary would devise under penalties. We then develop optimal SVM learning strategies against the two attack models. The learning algorithms minimize the hinge loss while assuming the adversary is modifying data to maximize the loss. Experiments are performed on both artificial and real data sets. We demonstrate that optimal solutions may be overly pessimistic when the actual attacks are much weaker than expected. More important, we demonstrate that it is possible to develop a much more resilient SVM learning model while making loose assumptions on the data corruption models. When derived under the restrained attack model, our optimal SVM learning strategy provides more robust overall performance under a wide range of attack parameters."
2348231,14133,422,Partially labeled topic models for interpretable text mining,2011,"Abstract Much of the world's electronic text is annotated with human-interpretable labels, such as tags on web pages and subject codes on academic publications. Effective text mining in this setting requires models that can flexibly account for the textual patterns that underlie the observed labels while still discovering unlabeled topics. Neither supervised classification, with its focus on label prediction, nor purely unsupervised learning, which does not model the labels explicitly, is appropriate. In this paper, we present two new partially supervised generative models of labeled text, Partially Labeled Dirichlet Allocation (PLDA) and the Partially Labeled Dirichlet Process (PLDP). These models make use of the unsupervised learning machinery of topic models to discover the hidden topics within each label, as well as unlabeled, corpus-wide latent topics. We explore applications with qualitative case studies of tagged web pages from del.icio.us and PhD dissertation abstracts, demonstrating improved model interpretability over traditional topic models. We use the many tags present in our del.icio.us dataset to quantitatively demonstrate the new models' higher correlation with human relatedness scores over several strong baselines."
1628008,14133,422,"Design principles of massive, robust prediction systems",2012,"Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption."
1574026,14133,422,Large-scale high-precision topic modeling on twitter,2014,"We are interested in organizing a continuous stream of sparse and noisy texts, known as tweets, in real time into an ontology of hundreds of topics with  measurable  and  stringently high  precision. This inference is performed over a full-scale stream of Twitter data, whose statistical distribution evolves rapidly over time. The implementation in an industrial setting with the potential of affecting and being visible to real users made it necessary to overcome a host of practical challenges. We present a spectrum of topic modeling techniques that contribute to a deployed system. These include non-topical tweet detection, automatic labeled data acquisition, evaluation with human computation, diagnostic and corrective learning and, most importantly, high-precision topic inference. The latter represents a novel two-stage training algorithm for tweet text classification and a close-loop inference mechanism for combining texts with additional sources of information. The resulting system achieves 93% precision at substantial overall coverage."
2411825,14133,235,Fast Tweet Retrieval with Compact Binary Codes,2014,"The most widely used similarity measure in the field of natural language processing may be cosine similarity. However, in the context of Twitter, the large scale of massive tweet data inevitably makes it expensive to perform cosine similarity computations among tremendous data samples. In this paper, we exploit binary coding to tackle the scalability issue, which compresses each data sample into a compact binary code and hence enables highly efficient similarity computations via Hamming distances between the generated codes. In order to yield semantics sensitive binary codes for tweet data, we design a binarized matrix factorization model and further improve it in two aspects. First, we force the projection directions employed by the model nearly orthogonal to reduce the redundant information in their resulting binary bits. Second, we leverage the tweets’ neighborhood information to encourage similar tweets to have adjacent binary codes. Evaluated on a tweet dataset using hashtags to create gold labels in an information retrieval scenario, our proposed model shows significant performance gains over competing methods."
2575604,14133,422,Box drawings for learning with imbalanced data,2014,"The vast majority of real world classification problems are imbalanced, meaning there are far fewer data from the class of interest (the positive class) than from other classes. We propose two machine learning algorithms to handle highly imbalanced classification problems. The classifiers are disjunctions of conjunctions, and are created as unions of parallel axis rectangles around the positive examples, and thus have the benefit of being interpretable. The first algorithm uses mixed integer programming to optimize a weighted balance between positive and negative class accuracies. Regularization is introduced to improve generalization performance. The second method uses an approximation in order to assist with scalability. Specifically, it follows a \textit{characterize then discriminate} approach, where the positive class is characterized first by boxes, and then each box boundary becomes a separate discriminative classifier. This method has the computational advantages that it can be easily parallelized, and considers only the relevant regions of feature space."
692135,14133,256,Information Theoretic Learning with Infinitely Divisible Kernels,2013,"Abstract: In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art."
599024,14133,256,k-Sparse Autoencoders,2014,"Abstract: Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied."
2533305,14133,422,Towards anytime active learning: interrupting experts to reduce annotation costs,2013,"Many active learning methods use annotation cost or expert quality as part of their framework to select the best data for annotation. While these methods model expert quality, availability, or expertise, they have no direct influence on any of these elements. We present a novel framework built upon decision-theoretic active learning that allows the learner to directly control label quality by allocating a time budget to each annotation. We show that our method is able to improve performance efficiency of the active learner through an interruption mechanism trading off the induced error with the cost of annotation. Our simulation experiments on three document classification tasks show that some interruption is almost always better than none, but that the optimal interruption time varies by dataset."
2596287,14133,422,Large scale real-life action recognition using conditional random fields with stochastic training,2011,"Action recognition is usually studied with limited lab settings and a small data set. Traditional lab settings assume that the start and the end of each action are known. However, this is not true for the real-life activity recognition, where different actions are present in a continuous temporal sequence, with their boundaries unknown to the recognizer. Also, unlike previous attempts, our study is based on a large-scale data set collected from real world activities. The novelty of this paper is twofold: (1) Large-scale non-boundary action recognition; (2) The first application of the averaged stochastic gradient training with feedback (ASF) to conditional random fields. We find the ASF training method outperforms a variety of traditional training methods in this task."
1997227,14133,422,An improved GLMNET for l1-regularized logistic regression,2011,"GLMNET proposed by Friedman et al. is an algorithm for generalized linear models with elastic net. It has been widely applied to solve L1-regularized logistic regression. However, recent experiments indicated that the existing GLMNET implementation may not be stable for large-scale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient regardless of loosely or strictly solving the optimization problem. Experiments demonstrate that the improved GLMNET is more efficient than a state-of-the-art coordinate descent method."
1802145,14133,422,Multi-label hypothesis reuse,2012,"Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a  hypothesis reuse  mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric."
2590174,14133,422,Clustering in Conjunction with Quantum Genetic Algorithm for Relevant Genes Selection for Cancer Microarray Data,2013,"Quantum Genetic Algorithm, which utilizes the principle of quantum computing and genetic operators, allows efficient exploration and exploitation of large search space simultaneously. It has been used recently to determine a reduced set of features for cancer microarray data to improve the performance of the learning system. However, the length of the chromosome used is the original dimension of the feature vector. Hence, despite the use of the quantum variant of GA, it requires huge memory and computation time for high dimensional data like microarrays. In this paper, we propose a two phase approach, ClusterQGA, that determines a minimal set of relevant and non-redundant genes. Experimental results on publicly available cancer microarray datasets demonstrate the effectiveness of the proposed approach in comparison to existing methods in terms of classification accuracy and number of features. Also, the proposed approach takes less computation time in comparison to Genetic quantum algorithm proposed by Abderrahim et al."
41857,14133,235,A Sentence Vector Based Over-Sampling Method for Imbalanced Emotion Classification,2014,"Imbalanced training data poses a serious problem for supervised learning based text classification. Such a problem becomes more serious in emotion classification task with multiple emotion categories as the training data can be quite skewed. This paper presents a novel over-sampling method to form additional sum sentence vectors for minority classes in order to improve emotion classification for imbalanced data. Firstly, a large corpus is used to train a continuous skip-gram model to form each word vector using word/POS pair as the unit of word vector. The sentence vectors of the training data are then constructed as the sum vector of their word/POS vectors. The new minority class training samples are then generated by randomly add two sentence vectors in the corresponding class until the training samples for each class are the same so that the classifiers can be trained on fully balanced training dataset. Evaluations on NLP&CC2013 Chinese micro blog emotion classification dataset shows that the obtained classifier achieves 48.4% average precision, an 11.9 percent improvement over the state-of-art performance on this dataset at 36.5%. This result shows that the proposed over-sampling method can effectively address the problem of data imbalance and thus achieve much improved performance for emotion classification."
547455,14133,256,Some Improvements on Deep Convolutional Neural Network Based Image Classification,2014,"Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner."
150733,14133,11052,Saliency in Crowd,2014,����� ����� ���� ��� ���� ���� ���� ��� ��� ���� ���� ��� ��� ���� ���� ���� ���� ���� ��� ���� ��� ��� �� ��� �� ��� ��� ��� ��� ��� ��� ��� ��� � ��� ��� � ��� ��� ���� ���� ��� ��� � �� �� � �� ��� ���� ��� ��� ��� ��� ��� �� ��� �� ��� ��� ���� ���� ��� ��� ��� ��� ���� ���� ��� ���� ��� ���� ��� �� ���� ��� ��� �� �� ���� ����� ���� ��� ���� ��� �� ���� ��� ��� ��� ����� ���� ����� ���� ���� ���� ���� ���� ����� ���� ���� ��� ���� ��� ��� ��� �� ��� ��� ��� ���� ���� ���� � ��� ���� ���� ��� ���� ���� ��� ��� ��� ��� ��� ��� � ��� ��� ��� ��� ��� ��� ��� �� � ��� ��� �� ��� ��� � � ��� ��� ��� �� ��� ��� ��� ��� ��� ��� �� ��� �� �� ��� ��� ���� ��� ��� ���� ��� ���� ��� ��� ��� ���� ���� ���� ���� ���� ��� ���� ��� ���� ���� ���� ��� ��� ���� ��� ���� ��� ���� ���� ���� ��� ���� ����� ���� ���� ����� ���� ��� ��� ����� ���� ���� ��� ��� √
1317531,14133,11166,Metric Learning from Relative Comparisons by Minimizing Squared Residual,2012,"Recent studies [1] -- [5] have suggested using constraints in the form of relative distance comparisons to represent domain knowledge: d(a, b)"
480300,14133,20332,"Crowd, the Teaching Assistant: Educational Assessment Crowdsourcing",2013,We propose a novel educational assessment method to enhance teachers' productivity using crowdsourcing and clustering technique.
611431,14133,23922,A PTAS for Agnostically Learning Halfspaces,2014,"We present a PTAS for agnostically learning halfspaces w.r.t. the uniform distribution on the d dimensional sphere. Namely, we show that for every"
1888602,14133,20552,A Logical Characterization of Constraint-Based Causal Discovery,2012,"Scientific publication 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011, 14 juli 2011"
2639091,14133,20332,The value of ignorance about the number of players,2013,We show how giving partial or no information to players about the number of other players may yield higher social welfare in games.
756778,14133,23735,Visual tracking and following of a quadrocopter by another quadrocopter,2012,We present a follow-the-leader scenario with a system of two small low-cost quadrocopters of different types and configurations.
2264942,14133,20552,Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena,2012,Singapore-MIT Alliance for Research and Technology (Subaward Agreement 14 R-252-000-466-592)
2575446,14133,20332,Reinforcement learning on multiple correlated signals,2014,This extended abstract provides a brief overview of my PhD research on multi-objectivization and ensemble techniques in reinforcement learning.
2534256,14133,20332,Dynamic batch mode active learning via L1 regularization,2011,We propose a method for dynamic batch mode active learning where the batch size and selection criteria are integrated into a single formulation.
2827271,14133,20332,Long-Term Declarative Memory for Generally Intelligent Agents.,2011,The goal of my research is to develop and evaluate long-term declarative memory mechanisms that are effective and efficient across a variety of tasks.
155020,14133,10174,"An Investigation of Actions, Change and Space",2013,This work investigates the spatial knowledge and automated  solution of a domain composed of non-trivial objects such as  strings and holed objects.
2772730,14133,20332,Using Autonomous Agent-Based Systems to Counter Asymmetric Threats from Non-State Sponsored Terror Organizations.,2012,"In this paper, we will explore the use of autonomous agent- based systems to counter asymmetric threats from non- state sponsored terror organizations."
208769,14133,22113,A trust and reputation model for supply chain management,2011,My thesis contributes to the field of multi-agent systems by proposing a novel trust-based decision model for supply chain management.
2808988,14133,20332,A Probabilistic Trust and Reputation Model for Supply Chain Management,2011,My thesis contributes to the field of multi-agent systems by proposing a novel trust-based decision model for supply chain management.
2296665,14133,8960,On the Universality of Online Mirror Descent,2011,"We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee."
2237000,14133,20552,Identifiability of Causal Graphs using Functional Models,2012,"Scientific publication 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011; Barcelona; 14 July 2011 through 17 July 2011"
2583462,14133,20332,Approaches to multi-robot exploration and localization,2011,"We present approaches to several fundamental tasks in multi-robot team-based exploration and localization, based on student projects developed in the past year."
1297421,14133,22130,Incremental Light Bundle Adjustment,2012,"Presented at the Ninth Conference on 23rd British Machine Vision Conference (BMVC 2012), 3-7 September 2012, Guildford, Surrey, UK."
255881,14133,20332,Online search algorithm configuration,2014,This paper outlines an online approach for algorithm configuration which uses the power of modern multicore system to evaluate multiple parameters configurations in parallel.
2589910,14133,20332,Scaling Up Game Theory: Achievable Set Methods for Efficiently Solving Stochastic Games of Complete and Incomplete Information.,2011,Proposed Thesis: Achievable set methods can efficiently compute useful game theoretic solutions to general multi-agent reinforcement learning problems.
173482,14133,20332,Computing preferences based on agents' beliefs,2014,"The knowledgebase uncertainty and the argument preferences are considered in this paper. The uncertainty is captured by weighted satisfiability degree, while a preference relation over arguments is derived by the beliefs of an agent."
2634346,14133,22113,An analysis of multiobjective search algorithms and heuristics,2011,"This thesis analyzes the performance of multiobjective heuristic graph search algorithms. The analysis is focused on the influence of heuristic information, correlation between objectives and solution depth."
1147382,14133,20358,Identifying sentiments over N-gram,2012,"Our proposal, identifying sentiment over N-gram (ISN) focuses on both word order and phrases, and the interdependency between specific rating and corresponding sentiment in a text to detect subjective information."
2724069,14133,20332,Collaborative Biomedical Information Retrieval,2012,In the context of two related NIH projects supporting scientific collaboration we seek to implement an environment for collaborative information retrieval and analysis based on utility theory.
2567420,14133,20332,Transfer Learning Framework for Early Detection of Fatigue Using Non-invasive Surface Electromyogram Signals (SEMG),2011,"A multi source domain adaptation based learning for addressing subject based variability in myoelectric signals (SEMG), enabling generalized framework for detecting stages of fatigue."
350717,14133,20552,Multi-objective influence diagrams,2012,Irish Research Council for Science Engineering and Technology (IRCSET Enterprise Partnership Scheme); Science Foundation Ireland (08/PI/I1912); IBM (IRCSET Enterprise Partnership Scheme)
2554456,14133,20358,Political hashtag hijacking in the U.S.,2013,We study the change in polarization of hashtags on Twitter over time and show that certain jumps in polarity are caused by hijackers engaged in a particular type of hashtag war.
1824194,14133,20332,A Commonsense Theory of Mind-Body Interaction,2011,We propose a logical formalization of a commonsense theory of mind-body interaction as a step toward a deep lexical semantics for words and phrases related to this topic.
2703281,14133,20332,Learning names for RFID-tagged objects in activity videos,2012,We describe a method for determining the names of RFID-tagged objects in activity videos using descriptions which have been parsed to provide anaphoric reference resolution and ontological categorization.
2573936,14133,20332,Conflict-driven constraint answer set solving with lazy nogood generation,2011,We present a new approach to enhancing answer set programming (ASP) with constraint programming (CP) techniques based on conflict-driven learning and lazy nogood generation.
2756529,14133,20332,The Formalization of Practical Reasoning: An Opinionated Survey ∗,2011,"I try to frame the general challenge that is presented to logical theory by the problem of formalizing practical reasoning, and to survey the existing resources that might contribute to the development of such a formalizatio"
2629648,14133,20332,Learning for Mobile-Robot Error Recovery (Extended Abstract).,2013,"This paper introduces the novel problem of autonomous mobile-robot error recovery, namely the development of algorithms that enable a robot to extricate itself from a previously unseen trapped configuration."
2586115,14133,20332,Chance-constrained strong controllability of temporal plan networks with uncertainty,2013,This works presents a novel approach for determining chance-constrained strong controllability of Temporal Plan Networks with Uncertainty (TPNU) by framing it as an Optimal Satisfiability Problem (OpSAT).
312350,14133,20552,Similarity Networks for the Construction of Multiple-Faults Belief Networks,2013,"A similarity network is a tool for constructing belief networks for the diagnosis of a single fault. In this paper, we examine modifications to the similarity-network representation that facilitate the construction of belief networks for the diagnosis of multiple coexisting faults."
293496,14133,20332,Designing a Crowdsourcing Tool to Analyze Relationships Among Jazz Musicians: The Case of Linked Jazz 52nd Street,2013,"We discuss the design of Linked Jazz 52nd Street, a crowdsourcing tool using linked data technology to build a social network of jazz musicians through the analysis of their personal and professional relationships."
3038033,14133,20332,Prom Week,2013,"This paper outlines Prom Week, a game which leverages the social AI system Comme il Faut (CiF) to create a novel experience in which social relationships are playable."
1206533,14133,20796,Named entity recognition using a modified Pegasos algorithm,2011,"In this paper, we describe a named entity recognition using a modified Pegasos algorithm for structural SVMs. We show the modified Pegasos algorithm significantly outperformed CRFs and the training time for the modified Pegasos algorithm is reduced 17-26 times compared to CRFs."
2627695,14133,235,Method51 for Mining Insight from Social Media Datasets,2014,"We present Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platform’s application, and motivating our methodological proposals."
1781831,14133,11321,Consistency of Online Random Forests,2013,"As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests."
2544055,14133,20332,A Little Metatheory: Thought on What aTheory of Computational Humor Should Look Like,2012,"This exercise in metatheory presents what any theory consists of and what properties it should have. It, then, adjust the general recipe to a theory of humor and computational humor. In this light, it reviews the state of the art in computational humor and suggests the main lines of development."
173544,14133,20552,Identifying Dynamic Sequential Plans.,2012,"We address the problem of identifying dynamic sequential plans in the framework of causal Bayesian networks, and show that the problem is reduced to identifying causal effects, for which there are complete identi cation algorithms available in the literature."
16201,14133,11187,Experimental comparison of different techniques to generate adaptive sequences,2011,The focus of this paper is to present the results of a set of experiments regarding the construction of an adaptive sequence by a genetic algorithm and other techniques in order to reach a goal state in a non-deterministic finite state machine.
291372,14133,22113,Efficient learning in linearly solvable MDP models.,2012,"University of Minnesota M.S. thesis. June 2012. Major: Computer science. Advisor: Prof. Paul Schrater. 1 computer file (PDF); vi, 38 pages, appendix p. 37."
2559778,14133,20332,A knowledge representation that models memory in narrative comprehension,2014,"We present work toward computationally defining a model of narrative comprehension vis-a-vis memory of narrative events, via an automated planning knowledge representation, capable of being used in a narrative generation context."
673836,14133,22113,Concept generation in language evolution,2013,"This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation."
2482170,14133,8960,Stochastic Optimization of PCA with Capped MSG,2013,"We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as Matrix Stochastic Gradient (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically."
2617201,14133,20332,Progression semantics for disjunctive logic programs,2011,"In this paper, we extend the progression semantics for first-order disjunctive logic programs and show that it coincides with the stable model semantics. Based on it, we further show how disjunctive answer set programming is related to Satisfiability Modulo Theories."
22384,14133,20332,Using partitions and superstrings for lossless compression of pattern databases,2011,We present an algorithm for compressing pattern databases (PDBs) and a method for fast random access of these compressed PDBs. We demonstrate the effectiveness of our technique by compressing two 6-tile sliding-tile PDBs by a factor of 12 and a 7-tile sliding-tile PDB by a factor of 24.
1068080,14133,8235,Automatic generation of question answer pairs from noisy case logs,2014,"In a customer support scenario, a lot of valuable information is recorded in the form of ‘case logs’. Case logs are primarily written for future references or manual inspections and therefore are written in a hasty manner and are very noisy."
2665668,14133,22113,Attack semantics for abstract argumentation,2011,"In this paper we conceptualize abstract argumentation in terms of successful and unsuccessful attacks, such that arguments are accepted when there are no successful attacks on them. We characterize the relation between attack semantics and Dung's approach, and we define an SCC recursive algorithm for attack semantics using attack labelings."
2107010,14133,20332,Modular answer set solving,2013,Modularity is essential for modeling large-scale practical applications. We propose modular logic programs as a modular version of answer set programming and study the relationship of our formalism to an earlier concept of lp-modules.
2635678,14133,20332,Communication-restricted exploration for small teams,2014,"We propose a novel algorithm that allows a small team of robots to fully explore an unknown environment, even in the face of extreme communication restrictions. We verify, through proofs and experiments, that the algorithm will achieve full exploration."
2578633,14133,23922,A simple multi-armed bandit algorithm with optimal variation-bounded regret,2011,"We pose the question of whether it is possible to design a simple, linear-time algorithm for the basic multi-armed bandit problem in the adversarial setting which has a regret bound of O( p Q logT ), where Q is the total quadratic variation of all the arms."
2762612,14133,22113,High-level program execution in multi-agent settings,2013,"In this paper, we state the challenges of high-level program execution in multi-agent settings. We first introduce high-level program execution and the related work. Then we describe the completed work, the future work and its approaches. We conclude with the expected contributions of our research."
4825,14133,11187,Fast independent component analysis using a new property,2011,"In this paper we present a new theoretical characterization of the solutions to the Independent Component Analysis (ICA) problem. As an application, we also propose an algorithm that is directly based on that theoretical characterization. The algorithm has a very low computational complexity, and the experiments show that it is fast and reliable."
650901,14133,22113,RCC8 is polynomial on networks of bounded treewidth,2011,"We construct an homogeneous (and ω-categorical) representation of the relation algebra RCC8, which is one of the fundamental formalisms for spatial reasoning. As a consequence we obtain that the network consistency problem for RCC8 can be solved in polynomial time for networks of bounded treewidth."
2591941,14133,20332,"Imputation, social choice, and partial preferences",2014,"A novel technique for deciding the outcome of an election when only partial preference ballots are submitted, using machine learning. An explicit connection between machine learning and social choice is discovered, which suggests many possible avenues of future work."
2565731,14133,20332,Playable experiences at AIIDE 2014,2014,"AIIDE 2014 is the second AIIDE event that has featured a playable experience track. This paper describes the seven entries that were accepted in the 2014 track, as well as the motivation behind the track and the criteria used to evaluate and accept entries."
115003,14133,20332,Inferring causal directions in errors-in-variables models,2014,A method for inferring causal directions based on errors-in-variables models where both the cause variable and the effect variable are observed with measurement errors is concerned in this paper. The inference technique and estimation algorithms are given. Some experiments are included to illustrate our method.
1875980,14133,8960,The Power of Asymmetry in Binary Hashing,2013,"When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x′ as the hamming distance between f (x) and g(x′), for two distinct binary codes f, g, rather than as the hamming distance between f (x) and f (x′)."
2610162,14133,20332,Analysis of Temporal Propagation Property to Evaluate Gene Regulatory Networks,2013,"Harmonic Pulse Analysis (HPA) is a method to analyze gene regulatory networks,  which measures the plausibility of gene regulatory networks.  HPA is used to  predict gene regulatory networks. Generated networks were analyzed by the harmonic pulse analysis.  7.3% of systematically generated networks were within 5% margin of the harmonic pulse density of the benchmark networks."
53962,14133,11187,Exponential Synchronization of a Class of RNNs with Discrete and Distributed Delays,2013,"This paper studies the exponential synchronization of RNNs. The investigations are carried out by means of Lyapunov stability method and the Halanay inequality lemma. Finally, a numerical example with graphical illustrations is given to illuminate the presented synchronization scheme."
2583968,14133,20332,Solution diversity in planning,2012,"Diverse planning consists of generating multiple different solutions for the same planning problem. I explore solution diversity, based on quantitative (domain independent) and qualitative (domain dependent) distance metrics, in determi nistic and nondeterministic planning domains."
1197970,14133,20552,A Method for Using Belief Networks as Influence Diagrams,2013,"This paper demonstrates a method for using belief-network algorithms to solve influence diagram problems. In particular, both exact and approximation belief-network algorithms may be applied to solve influence-diagram problems. More generally, knowing the relationship between belief-network and influence-diagram problems may be useful in the design and development of more efficient influence diagram algorithms."
545292,14133,22113,Three generalizations of the FOCUS constraint,2013,"The FOCUS constraint expresses the notion that solutions are concentrated. In practice, this constraint suffers from the rigidity of its semantics. To tackle this issue, we propose three generalizations of the FOCUS constraint. We provide for each one a complete filtering algorithm as well as discussing decompositions."
2799468,14133,20332,Playing chess with a human-scale mobile manipulator,2011,This paper describes our efforts preparing a mobile manipulator for the 2011 AAAI Small Scale Manipulation Challenge. We describe our approach to building a low-cost mobile manipulator for human-scale environments using ROS and off-the-shelf sensory and servos.
2613962,14133,20332,Reasoning in the description logic BEL using Bayesian networks,2014,"We study the problem of reasoning in the probabilistic Description Logic BEL. Using a novel structure, we show that probabilistic reasoning in this logic can be reduced in polynomial time to standard inferences over a Bayesian network. This reduction provides tight complexity bounds for probabilistic reasoning in BEL."
2662766,14133,22113,Incorporating expert judgement into Bayesian network machine learning,2013,"We review the challenges of Bayesian network learning, especially parameter learning, and specify the problem of learning with sparse data. We explain how it is possible to incorporate both qualitative knowledge and data with a multinomial parameter learning method to achieve more accurate predictions with sparse data."
2552555,14133,20332,Logics of Contingency,2011,"We introduce the logic of positive and negative contingency. Together with modal operators of necessity and impossibility they allow to dispense of negation. We study classes of Kripke models where the number of points is restricted, and show that the modalities reduce in the corresponding logics."
27822,14133,20332,Take or wait? learning turn-taking from multiparty data,2013,"We build turn-taking models for autonomous characters in language-based interactions with small groups of children. Two models explore the use of support vector machines given the same multimodal features, but different methods for collecting turn-taking labels."
185446,14133,20332,Iterative Voting under Uncertainty for Group Recommender Systems (Research Abstract),2012,Group Recommendation Systems (GRS's) assist groups when trying to reach a joint decision. I use probabilistic data and apply voting theory to GRS’s in order to minimize user interaction and output an approximate or definite “winner item
2613732,14133,23922,New Bounds for Learning Intervals with Implications for Semi-Supervised Learning,2012,"We study learning of initial intervals in the prediction model. We show that for each distribution D over the domain, there is an algorithmAD, whose probability of a mistake in round m is at most 1 +o(1) 1 m . We also show that the best possible bound that can be achieved in the case in which the same algorithmA must be applied for all distributions D is at least 1"
153416,14133,20332,Task Redundancy Strategy Based on Volunteers’ Credibility for Volunteer Thinking Projects,2013,"In this paper, we propose a task redundancy strategy based on measures of accuracy of volunteers. Simulation results show that our strategy reduces the number of generated task replicas compared to the pessimistic and moderate strategies. It also generates similar or less task replicas compared to the optimistic strategy."
149150,14133,20332,Do Jokes Have to Be Funny: Analysis of 50 “Theoretically Jokes”,2012,"This talk will analyze responses to funniness of five versions of 10 different jokes. The responses of one of them will then be compared to theoretical analysis and representation of the same joke based on Script-based Semantics Theory of Humor, General Theory of Verbal Humor, and Ontological Semantic Theory of Humor."
613065,14133,20332,Baby gym for robots: a new platform for testing developmental learning algorithms,2011,This extended abstract describes a new platform for robotic manipulation research that was inspired by some of the first toys that human infants learn to manipulate. It summarizes the results of our existing research on pressing buttons and formulates some ideas for future work.
2556083,14133,22113,Agent-based negotiation teams,2011,Agent-based negotiation teams are negotiation parties formed by more than a single individual. Individuals unite as a single negotiation party because they share a common goal that is related to a negotiation with one or several opponents. My research goal is providing agent-based computational models for negotiation teams in multi-agent systems.
239323,14133,20332,Simplified lattice models for protein structure prediction: how good are they?,2013,"In this paper, we present a local search framework for lattice fit problem of proteins. Our algorithm significantly improves state-of-the-art results and justifies the significance of the lattice models. In addition to these, our analysis reveals the weakness of several energy functions used."
481711,14133,20332,Monte Carlo simulation adjusting,2014,"In this paper, we propose a new learning method simulation adjusting that adjusts simulation policy to improve the move decisions of the Monte Carlo method. We demonstrated simulation adjusting for 4 × 4 board Go problems. We observed that the rate of correct answers moderately increased."
1989436,14133,8960,Optimal rates for k-NN density and mode estimation,2014,"We present two related contributions of independent interest: (1) high-probability finite sample rates for k-NN density estimation, and (2) practical mode estimators - based on k-NN - which attain minimax-optimal rates under surprisingly general distributional conditions."
1899094,14133,11321,The Kernelized Stochastic Batch Perceptron,2012,"We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives."
2552945,14133,20332,Integrated Symbolic Planning in the Tidyup-Robot Project,2013,We present the integration of our symbolic planner as the high-level executive in the Tidyup-Robot project. Tidyup-Robot deals with mobile manipulation scenarios in a household setting. We introduce our system architecture and report on issues and advantages observed during development and deployment.
416676,14133,20552,On Some Equivalence Relations between Incidence Calculus and Dempster-Shafer Theory of Evidence,2013,"Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories to describe agents' degrees of belief in propositions, thus being appropriate to represent uncertainty in reasoning systems. This paper presents a straightforward equivalence proof between some special cases of these theories."
1331661,14133,22035,Overview of Eye tracking Datasets,2013,"Datasets of images or videos annotated with eye tracking data constitute important ground truth for studies on saliency models, which have applications in quality assessment and other areas. Over two dozen such databases are now available in the public domain; they are presented in this paper."
2578412,14133,20332,Fast algorithm for non-stationary gaussian process prediction,2014,The FNSGP algorithm for Gaussian process model is proposed in this paper. It reduces the time cost to accelerate the task of non-stationary time series prediction without loss of accuracy. Some experiments are verified on the real world power load data.
1950567,14133,11321,Agglomerative Bregman Clustering,2012,"This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondierentiable convex functions."
2583624,14133,11321,BCDNPKL: Scalable Non-Parametric Kernel Learning Using Block Coordinate Descent,2011,"Most existing approaches for non-parametric kernel learning (NPKL) suer from expensive computation, which would limit their applications to large-scale problems. To address the scalability problem of NPKL, we propose a novel algorithm called BCDNPKL, which is very ecient and scalable. Superior to most"
2482781,14133,8960,Adaptivity to Local Smoothness and Dimension in Kernel Regression,2013,We present the first result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown Holder-continuity of the regression function at x. The result holds with high probability simultaneously at all points x in a general metric space X of unknown structure.
575578,14133,20332,Lottery-Based Payment Mechanism for Microtasks,2013,We propose using lotteries as an alternative payment mechanism through which to incentivize and recruit workers to microtasking. We present initial findings gained via experiments on Amazon Mechanical Turk and focus on discussing the benefits and potential pitfalls in employing a lottery-based payment mechanism for microtasking.
67198,14133,11187,Towards a bio-computational model of natural language learning,2011,"This paper tries to bring together the theory of Grammatical Inference and the studies of natural language acquisition.We discuss how the studies of natural language acquisition can improve results in the field of Grammatical Inference, and how a computational model inspired by such studies can help to answer several key questions in natural language learning."
114791,14133,22113,Fixpoints in temporal description logics,2011,We study a decidable fixpoint extension of temporal description logics. To this end we employ and extend decidability results obtained for various temporally first-order monodic extensions of (firstorder) description logics. Using these techniques we obtain decidability and tight complexity results for various fixpoint extensions of temporal description logics.
96264,14133,22113,Social abstract argumentation,2011,"In this paper we take a step towards using Argumentation in Social Networks and introduce Social Abstract Argumentation Frameworks, an extension of Dung's Abstract Argumentation Frameworks that incorporates social voting. We propose a class of semantics for these new Social Abstract Argumentation Frameworks and prove some important non-trivial properties which are crucial for their applicability in Social Networks."
2340157,14133,8960,Abstraction in decision-makers with limited information processing capabilities,2013,A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems
2597020,14133,20332,Additive counterexample-guided Cartesian abstraction refinement,2013,We recently showed how counterexample-guided abstraction refinement can be used to derive informative heuristics for optimal classical planning. In this work we introduce an algorithm for building additive abstractions and demonstrate that they outperform other state-of-the-art abstraction heuristics on many benchmark domains.
1458368,14133,20358,Conversations reconstruction in the social web,2012,"We propose a socio-semantic approach for building conversations from social interactions following three steps: (i) content linkage, (ii) participants (users) linkage, and (iii) temporal linkage. Preliminary evaluations on a Twitter dataset show promising and interesting results."
441238,14133,20552,A Criterion for Parameter Identification in Structural Equation Models,2012,This paper deals with the problem of identifying direct causal effects in recursive linear structural equation models. The paper establishes a sufficient criterion for identifying individual causal effects and provides a procedure computing identified causal effects in terms of observed covariance matrix.
1944820,14133,8960,Distributed Parameter Estimation in Probabilistic Graphical Models,2014,"This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent."
2617522,14133,20332,"Lego plays chess: a low-cost, low-complexity approach to intelligent robotics",2011,"The design and implementation of a robotic chess agent is described. Shallow Blue, a competitor in the AAAI 2011 Small Scale Manipulation Challenge, is constructed with low-cost components including Lego NXT bricks and is programmed using Java and Lejos."
2439730,14133,11187,Bounds for approximate solutions of Fredholm integral equations using kernel networks,2011,Approximation of solutions of integral equations by networks with kernel units is investigated theoretically. There are derived upper bounds on speed of decrease of errors in approximation of solutions of Fredholm integral equations by kernel networks with increasing numbers of units. The estimates are obtained for Gaussian and degenerate kernels.
2617837,14133,20332,IAAI-13 Preface.,2013,"Welcome to the Twenty-Fifth Annual Conference on the Innovative Applications of Artificial Intelligence (IAAI-13), the premier conference focusing on applied AI research ranging from exciting new potential ap- plications to innovative deployments of AI technology."
33068,14133,11187,Two-Layer Vector Perceptron,2013,"A new model  two-layer vector perceptron  is offered. Though, comparing with a single-layer perceptron, its operation needs slightly more (by 5%) calculations and more effective computer memory, it excels in a much lower error rate (four orders of magnitude as lower)."
3006099,14133,20332,"Counting, Ranking, and Randomly Generating CP-Nets",2014,"We introduce a method for generating CP-nets uniformly at random. As CP-nets encode a subset of partial orders, ensuring that we generate samples uniformly at random is not a trivial task. We present algorithms for counting CP-nets, ranking and computing the rank of an arbitrary CP-net for a given number of nodes, and generating a CP-net given its rank.  We also show how to generate all CP-nets with a given number of nodes."
2103169,14133,20332,Representing Biological Processes in Modular Action Language ALM,2011,"This paper presents the formalization of a biological process, cell division, in modular action languageALM. We show how the features ofALM—modularity, separation between an uninterpreted theory and its interpretation—lead to a simple and elegant solution that can be used in answering questions from biology textbooks."
2593771,14133,21089,Predicting Power Relations between Participants in Written Dialog from a Single Thread,2014,We introduce the problem of predicting who has power over whom in pairs of people based on a single written dialog. We propose a new set of structural features. We build a supervised learning system to predict the direction of power; our new features significantly improve the results over using previously proposed features.
598243,14133,23922,"Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees",2013,We study the complexity of approximate representation and learning of submodular functions over the uniform distribution on the Boolean hypercubef0; 1g n . Our main result is the following structural theorem: any submodular function is -close in‘2 to a real-valued decision tree (DT) of depth O(1= 2 ). This immediately implies that any submodular function is -close to a function of at most
2616318,14133,22113,Modeling social causality and responsibility judgment in multi-agent interactions: extended abstract,2013,"Based on psychological attribution theory, this paper presents a domain-independent computational model to automate social causality and responsibility judgment according to an agent's causal knowledge and observations of interaction. The proposed model is also empirically validated via experimental study."
1928754,14133,20332,Value function approximation in reinforcement learning using the fourier basis,2011,"We describe the Fourier basis, a linear value function approximation scheme based on the Fourier series. We empirically demonstrate that it performs well compared to radial basis functions and the polynomial basis, the two most popular fixed bases for linear value function approximation, and is competitive with learned proto-value functions."
162394,14133,20332,Abstract preference frameworks: a unifying perspective on separability and strong equivalence,2013,"We introduce abstract preference frameworks to study general properties common across a variety of preference formalisms. In particular, we study strong equivalence in preference formalisms and their separability. We identify abstract postulates on preference frameworks, satisfied by most of the currently studied preference formalisms, that lead to characterizations of both properties of interest."
2735737,14133,20332,Activity Recognition with Time-Delay Emobeddings,2011,"We outline an approach that uses time-delay embedding models and machine learning in order to recognize physical activities, based on data coming from cell phone accelerometers.  The approach is very robust, cheap in terms of the amount of data and computation required, and easy to deploy."
178734,14133,22113,Capturing an evader in a polygonal environment with obstacles,2011,We study a pursuit-evasion game in which one or more cops try to capture a robber by moving onto the robber's current location. All players have equal maximum velocities. They can observe each other at all times. We show that three cops can capture the robber in any polygonal environment (which can contain any finite number of holes).
2584742,14133,9677,Written Dialog and Social Power: Manifestations of Different Types of Power in Dialog Behavior,2013,"Dialog behavior is affected by power relations among the discourse participants. We show that four different types of power relations (hierarchical power, situational power, influence, and power over communication) affect written dialog behavior in different ways. We also present a system that can identify power relations given a written dialog."
2560169,14133,20332,An optimal task assignment policy and performance diagnosis strategy for heterogeneous hadoop cluster,2013,"The goal of the proposed research is to improve the performance of Hadoop-based software running on a heterogeneous cluster. My approach lies in the intersection of machine learning, scheduling and diagnosis. We mainly focus on heterogeneous Hadoop clusters and try to improve the performance by implementing a more efficient scheduler for this class of cluster."
47287,14133,11187,Optimal control using functional type SIRMs fuzzy reasoning method,2011,A mathematical framework for studying a fuzzy optimal control using functional type SIRMs reasoning method is discussed in this paper. The existence of SIRMs which minimize the cost function of fuzzy control system is proved with continuity of approximate reasoning and topological property of the set of membership functions in SIRMs.
39542,14133,20332,Wordnet based multi-way concept hierarchy construction from text corpus,2013,"In this paper, we propose an approach to build a multiway concept hierarchy from a text corpus, which is based on WordNet and multi-way hierarchical clustering. In addition, a new evaluation metric is presented, and our approach is compared with 4 kinds of existing methods on the Amazon Customer Review data set."
1645403,14133,507,Automatic web-scale information extraction,2012,"In this demonstration, we showcase the technologies that we are building at Yahoo! for Web-scale Information Extraction. Given any new Website, containing semi-structured information about a pre-specified set of schemas, we show how to populate objects in the corresponding schema by automatically extracting information from the Website."
2088547,14133,20332,The Ubuntu Chat Corpus for Multiparticipant Chat Analysis,2013,"We present the Ubuntu Chat Corpus as a data source for multiparticipant chat analysis. This addresses the problem of the lack of a large, publicly suitable corpora for research in this medium. The advantages of using this corpus for research is its large number of chat messages, its multiple languages, its technical nature, and all of the original chat messages are in the public domain."
664043,14133,20552,IDEAL: A Software Package for Analysis of Influence Diagrams,2013,IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software environment for creation and evaluation of belief networks and influence diagrams. IDEAL is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework. This paper describes IDEAL and some lessons learned during its development.
379181,14133,20332,Product Concept Evaluation System Applying Preference Market.,2014,"A product concept evaluation system combining conjoint analysis with prediction markets is developed. It is also proposed how to determine the payoff for each prediction security corresponding to a product concept, so as to have participants to behave truthfully in the market. Further, how the proposed system works is investigated by evolutionary game simulation."
2430013,14133,20552,From ordinary differential equations to structural causal models: the deterministic case,2013,"We show how, and under which conditions, the equilibrium states of a first-order Ordinary Differential Equation (ODE) system can be described with a deterministic Structural Causal Model (SCM). Our exposition sheds more light on the concept of causality as expressed within the framework of Structural Causal Models, especially for cyclic models."
303412,14133,20552,Identifying the Relevant Nodes Without Learning the Model,2012,"We propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes. Our method is simple, effcient, consistent, and does not require learning a Bayesian network first. Therefore, our method can be applied to high-dimensional databases, e.g. gene expression databases."
2711014,14133,20332,On a noun-driven syntactic paradigm,2013,In addressing a verb bias in syntactic analysis we present the beginnings of a noun-driven analysis paradigm. Such a paradigm may complement and compensate for some weaknesses in the existing verb-driven paradigm (in which the noun is subordinated) as applied to information sources or contexts in which the data is structured in objects and more than in events.
613268,14133,20552,On Implementing Usual Values,2013,In many cases commonsense knowledge consists of knowledge of what is usual. In this paper we develop a system for reasoning with usual information. This system is based upon the fact that these pieces of commonsense information involve both a probabilistic aspect and a granular aspect. We implement this system with the aid of possibility-probability granules.
874321,14133,23735,Automatic extraction of goal-scoring behaviors from soccer matches,2012,"In a soccer match, a cooperative behavior emerges from the combined execution of simple actions by players. A cooperative behavior can be planned if players are previously committed to its execution prior to its start or unplanned otherwise. The ability to reproduce some of these behaviors can be useful to help a team achieve better performances."
144204,14133,11187,Extraction of prototype-based threshold rules using neural training procedure,2012,"Complex neural and machine learning algorithms usually lack comprehensibility. Combination of sequential covering with prototypes based on threshold neurons leads to a prototype-threshold based rule system. This kind of knowledge representation can be quite efficient, providing solutions to many classification problems with a single rule."
2109345,14133,22288,A Web service composition method based on Sub Web Service,2011,"With the proliferation of Web services and the evolution towards the Semantic Web comes the opportunity to automate various Web services tasks. In this paper, we propose the conception of Sub Web Service and present an algorithm of automatic web service composition which based on Sub Web Service. The most contribution of this paper is: (1) propose the conception of Sub Web Service; (2) present a automatic composition algorithm. The simulation experiment indicated that the algorithm we proposed is effective and efficiency when applied in web service composition engine."
106084,14133,20332,Adding default attributes to EL,2011,"The research on low-complexity nonmonotonic description logics recently identified a fragment of EL⊥, supporting defeasible inheritance with overriding, where reasoning can be carried out in polynomial time. We contribute to that framework by supporting more axiom schemata and all the concepts of EL++ without increasing asymptotic complexity."
2563880,14133,20332,On Microeconomic Errors and Ordinal Group Decision Making,2012,"The notion of ‘mathematical impossibility’ of group decision making is founded on errors in microeconomics, game theory, and related disciplines. We highlight mathematical errors that have been committed by von Neumann and Morgenstern, Pareto, Hicks, Samuelson, and Debreu which have been propagated throughout the social sciences."
74638,14133,22113,Assumption-based argumentation dialogues,2011,"We propose a formal model for argumentation-based dialogues between agents, using assumption-based argumentation (ABA). The model is given in terms of ABA-specific utterances, trees drawn from dialogues and legal-move and outcome functions. We prove a formal connection between these dialogues and argumentation semantics. We illustrate persuasion as an application of the dialogue model."
847899,14133,20358,Impact of ad impressions on dynamic commercial actions: value attribution in marketing campaigns,2012,"We develop a descriptive method to estimate the impact of ad impressions on commercial actions dynamically without tracking cookies. We analyze 2,885 campaigns for 1,251 products from the Advertising.com ad network. We compare our method with A/B testing for 2 campaigns, and with a public synthetic dataset."
2607246,14133,23922,Consistency of Nearest Neighbor Classification under Selective Sampling,2012,"This paper studies nearest neighbor classification in a model where unlabeled data points arrive in a stream, and the learner decides, for each one, whether to ask for its label. Are there generic ways to augment or modify any selective sampling strategy so as to ensure the consistency of the resulting nearest neighbor classifier?"
150017,14133,11187,Multi-valued neurons: hebbian and error-correction learning,2011,"In this paper, we observe some important aspects of Hebbian and errorcorrection learning rules for the multi-valued neuron with complex-valued weights. It is shown that Hebbian weights are the best starting weights for the errorcorrection learning. Both learning rules are also generalized for a complex-valued neuron whose inputs and output are arbitrary complex numbers."
480236,14133,20332,A GWAP Approach for Collecting Qualitative Product Attributes and Perceptual Mapping,2014,An ESP-like game is proposed for collecting qualitative product attributes relevant to the preferences of consumers efficiently. It is also discussed how to analyze the qualita-tive attributes and the supplemental data such as matching counts and input times acquired through the game and to yield a perceptual map.
3028224,14133,20332,Integrating rules and ontologies in the first-order stable model semantics (preliminary report),2011,"We present an approach to integrating rules and ontologies on the basis of the first-order stable model semantics proposed by Ferraris, Lee and Lifschitz. We show that some existing integration proposals can be uniformly reformulated in terms of the first-order stable model semantics. The reformulations are simpler than the original proposals in the sense that they do not refer to grounding."
556640,14133,11187,Visual mining of epidemic networks,2011,We show how an interactive graph visualization method based on maximal modularity clustering can be used to explore a large epidemic network. The visual representation is used to display statistical tests results that expose the relations between the propagation of HIV in a sexual contact network and the sexual orientation of the patients.
483931,14133,22113,Learning from polyhedral sets,2013,"Parameterized linear systems allow for modelling and reasoning over classes of polyhedra. Collections of squares, rectangles, polytopes, and so on, can readily be defined by means of linear systems with parameters. In this paper, we investigate the problem of learning a parameterized linear system whose class of polyhedra includes a given set of example polyhedral sets and it is minimal."
496548,14133,20332,Post It or Not: Viewership Based Posting of Crowdsourced Tasks.,2014,"We propose an online scheduling algorithm for posting crowdsourcing tasks which maximizes a novel metric called task viewership. This metric is computed using stochastic model based on coverage process and it measures the likelihood that a task is viewed by multiple crowd workers, which is correlated to the likelihood that it will be selected and completed."
104140,14133,20332,Matching state-based sequences with rich temporal aspects,2012,"A General Similarity Measurement (GSM), which takes into account of both non temporal and rich temporal aspects in cluding temporal order, temporal duration and temporal gap, is proposed for state sequence matching. It is believed to be versatile enough to subsume representative existing measurements as its special cases."
2788531,14133,22113,Approximation algorithms for max-sum-product problems,2013,"Many tasks in probabilistic reasoning can be cast as max-sum-product problems, a hard class of combinatorial problems. We describe our results in obtaining a new approximation scheme for the problem, that can be turned into an anytime procedure. For many tasks, this scheme can be shown to be asymptotically the best possible heuristic."
437452,14133,20332,Symmetry breaking constraints: recent results,2012,"Symmetry is an important problem in many combinatorial problems. One way of dealing with symmetry is to add constraints that eliminate symmetric solutions. We survey recent results in this area, focusing especially on two common and useful cases: symmetry breaking constraints for row and column symmetry, and symmetry breaking constraints for eliminating value symmetry."
375356,14133,20552,AND/OR importance sampling,2012,"The paper introduces AND/OR importance sampling for probabilistic graphical models. In contrast to importance sampling, AND/OR importance sampling caches samples in the AND/OR space and then extracts a new sample mean from the stored samples. We prove that AND/OR importance sampling may have lower variance than importance sampling; thereby providing a theoretical justification for preferring it over importance sampling. Our empirical evaluation demonstrates that AND/OR importance sampling is far more accurate than importance sampling in many cases."
2605435,14133,20332,Automating Environmental Impact Assessment during the Conceptual Phase of Product Design,2011,Product environmental impact reduction efforts largely focus on incremental changes during detailed design. Application of automated concept generation using a design repository and integral life cycle assessment approach is explored to evaluate and reduce environmental impacts in the conceptual phase of product design.
443530,14133,22113,Translation-based constraint answer set solving,2011,"We solve constraint satisfaction problems through translation to answer set programming (ASP). Our reformulations have the property that unit-propagation in the ASP solver achieves well defined local consistency properties like arc, bound and range consistency. Experiments demonstrate the computational value of this approach."
19592,14133,9463,Predicting Overt Display of Power in Written Dialogs,2012,We analyze overt displays of power (ODPs) in written dialogs. We present an email corpus with utterances annotated for ODP and present a supervised learning system to predict it. We obtain a best cross validation F-measure of 65.8 using gold dialog act features and 55.6 without using them.
107088,14133,20332,Volatile multi-armed bandits for guaranteed targeted social crawling,2013,"We introduce a new variant of the multi-armed bandit problem, called Volatile Multi-Arm Bandit (VMAB). A general policy for VMAB is given with proven regret bounds. The problem of collecting intelligence on profiles in social networks is then modeled as a VMAB and experimental results show the superiority of our proposed policy."
1305411,14133,21106,Sum-product networks: A new deep architecture,2011,"The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs) and will present in this abstract."
2793193,14133,20332,The common origins of language and action,2011,"The motor system organization shows some interesting parallels with the language organization. Here we draw the possible communalities between Action and Language, basing our claims on neurophysiological, neuroanatomical and neuroimaging data. Furthermore, we speculate that the motor system may have furnished the basic computational capabilities for the emergence of both semantics and syntax."
2713057,14133,20552,Maximum likelihood fitting of acyclic directed mixed graphs to binary data,2012,"Acyclic directed mixed graphs, also known as semi-Markov models represent the conditional independence structure induced on an observed margin by a DAG model with latent variables. In this paper we present the first method for fitting these models to binary data using maximum likelihood estimation."
166472,14133,20332,Selecting the appropriate consistency algorithm for CSPs using machine learning classifiers,2013,"Computing the minimal network of a Constraint Satisfaction Problem (CSP) is a useful and difficult task. Two algorithms, PerTuple and AllSol, were proposed to this end. The performances of these algorithms vary with the problem instance. We use Machine Learning techniques to build a classifier that predicts which of the two algorithms is likely to be more effective."
2638561,14133,20332,PATSy and VL-PATSy: Online Case-Based Training for Healthcare Professionals,2011,"This paper describes PATSy, an online repository of virtual patient cases for training and research for >students and practitioners in the clinical sciences. A typical student session with PATSy is illustrated. An extension to PATSy that adds vicarious learning resources (VL-PATSy) is also described. The concept of vicarious learning is outlined and results from a study of learning outcomes from VL-PATSy are presented. PATSy and VL-PATSy will be demonstrated at the symposium."
2669841,14133,20332,Towards Creative Humanoid Conceptualization Learning from Metaphor-Guided Pretense Play,2013,This paper describes a newly proposed approach towards investigating how social humanoid robots can learn creative conceptualizations through interaction with children in metaphor-guided pretense play using a componential creativity framework.  We describe this metaphor-guided pretense play approach and then illustrate it by describing a social robot interaction pretense play scenario between a child and a humanoid robot.
162099,14133,11187,Implication triples versus adjoint triples,2011,"Implication triples and adjoint triples are two of the more general residuated operators which have been applied independently in manifold important fields. This paper presents diverse properties of adjoint triples in order to relate them to implication triples. As a consequence of this relation, we obtain, for example, that a multi-adjoint lattice in multi-adjoint logic programming is a particular case of a complete adjointness lattice."
948838,14133,9896,I'd have to vote against you: issue campaigning via twitter,2013,"Using tweets posted with #SOPA and #PIPA hashtags and directed at members of Congress, we identify six strategies constituents employ when using Twitter to lobby their elected officials. In contrast to earlier research, we found that constituents do use Twitter to try to engage their officials and not just as a soapbox to express their opinions."
1880973,14133,8960,An Empirical Evaluation of Thompson Sampling,2011,"Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against."
2821865,14133,20332,Improving Predictions with Hybrid Markets.,2012,"Statistical models almost always yield predictions that are more accurate than those of human experts. However, humans are better at data acquisition and at recognizing atypical circumstances. We use prediction markets to combine predictions from  groups of  humans and artificial-intelligence agents and show that they are more robust than those from groups of humans or agents alone."
2582659,14133,20332,A Distributed Spanning Tree Method for Extracting Systems and Environmental Information from a Network of Mobile Robots,2011,"A multi-robot system, like a robot formation, contains information that is distributed throughout the system. As the collective increases in numbers or explores distant or difficult areas, obtaining collective situational awareness becomes critical. We propose a method for extracting system and environmental information distributed over a collective of robots."
82226,14133,20332,Invited Speaker and Special Presentation Abstracts,2012,"Invited speakers for this symposium will include Atul J. Butte, Rong Chen, Chirag Patel, Sudheendra Hangal, Rollin McCraty, Ernesto R. Ramirez, Frank X Chen, Eric B Hekler, Shauna Shapiro, and Abby King. This document contains abstracts of their invited talks."
2644312,14133,22113,Dynamic of argumentation frameworks,2013,"My thesis work aims to study change operations for argumentation systems, especially for abstract argumentation systems a la Dung. This paper presents a first study of the AGM revision adapted to the case of argumentation. We also sketch future research works planned to complete the one already achieved."
550952,14133,20552,Normalized online learning,2013,"We introduce online learning algorithms which are independent of feature scales, proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale. This has several useful effects: there is no need to prenormalize data, the test-time and test-space complexity are reduced, and the algorithms are more robust."
73570,14133,11187,A novel heuristic for building reduced-set SVMs using the self-organizing map,2011,"We introduce a novel heuristic based on the Kohonen's SOM, called Opposite Maps, for building reduced-set SVM classifiers. When applied to the standard SVM (trained with the SMO algorithm) and to the LS-SVM method, the corresponding reduced-set classifiers achieve equivalent (or superior) performances than standard full-set SVMs."
1201725,14133,422,On exploiting hierarchical label structure with pairwise classifiers,2011,"The goal of this work was to test whether the performance of a regular pairwise classifier can be improved when additional information about the hierarchical class structure is added to the training sets. Somewhat surprisingly, the additional information seems to hurt the performance. We explain this with the fact that the structure of the class hierarchy is not reflected in the distribution of the instances."
87336,14133,20332,Autonomous skill acquisition on a mobile manipulator,2011,"We describe a robot system that autonomously acquires skills through interaction with its environment. The robot learns to sequence the execution of a set of innate controllers to solve a task, extracts and retains components of that solution as portable skills, and then transfers those skills to reduce the time required to learn to solve a second task."
2631341,14133,22113,On the complexity of EL with defeasible inclusions,2011,"We analyze the complexity of reasoning in EL with defeasible inclusions and extensions thereof. The results by Bonatti et al., 2009a are extended by proving tight lower complexity bounds and by relaxing the syntactic restrictions adopted there. We further extend the old framework by supporting arbitrary priority relations."
2625165,14133,20332,Mining of Agile Business Processes.,2011,"Organizational agility is a key challenge in today's business world. The Knowledge-Intensive Service Support approach tackles agility by combining process modeling and business rules. In the paper at hand, we present five approaches of process mining that could further increase the agility of processes by improving an existing process model."
137887,14133,22113,Distributed constraint optimization problems related with soft arc consistency,2011,"Distributed Constraint Optimization Problems (DCOPs) can be optimally solved by distributed search algorithms, such as ADOPT and BnB-ADOPT. In centralized solving, maintaining soft arc consistency during search has proved to be beneficial for performance. In this thesis we aim to explore the maintenance of different levels of soft arc consistency in distributed search when solving DCOPs."
2595308,14133,20332,Partial domain search tree for constraint-satisfaction problems,2013,"CSP solvers usually search a partial assignment search tree. We present a new formalization for CSP solvers, which spans a conceptually different search tree, where each node represents subsets of the original domains for the variables. We experiment with a simple backtracking algorithm for this search tree and show that it outperforms a simple backtracking algorithm on the traditional search tree in many cases."
133638,14133,20332,A hybrid algorithm for coalition structure generation,2012,"The current state-of-the-art algorithm for optimal coalition structure generation is IDP-IP--an algorithm that combines IDP (a dynamic programming algorithm due to Rahwan and Jennings, 2008b) with IP (a tree-search algorithm due to Rahwan et al., 2009). In this paper we analyse IDP-IP, highlight its limitations, and then develop a new approach for combining IDP with IP that overcomes these limitations."
2563605,14133,20332,The Clock Ticking Changes Our Performance,2013,"We examined influence of a clock ticking on task performance using a laboratory experiment. We determined performance at various clock speeds using a trick clock that allowed us to control the speed of the ticking. We found that which the clock ticked, and people performed more slowly with a slow clock. We demonstrated that common environmental stimuli we encounter in life, such as the ticking of a clock, have a significant effect on human behavior."
2392948,14133,11187,An efficient way of combining SVMs for handwritten digit recognition,2012,This paper presents a method of combining SVMs (support vector machines) for multiclass problems that ensures a high recognition rate and a short processing time when compared to other classifiers. This hierarchical SVM combination considers the high recognition rate and short processing time as evaluation criteria. The used case study was the handwritten digit recognition problem with promising results.
2525732,14133,21089,Automatic Keyphrase Extraction: A Survey of the State of the Art,2014,"While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead."
623870,14133,11187,Some comparisons of networks with radial and kernel units,2012,"Two types of computational models, radial-basis function networks with units having varying widths and kernel networks where all units have a fixed width, are investigated in the framework of scaled kernels. The impact of widths of kernels on approximation of multivariable functions, generalization modelled by regularization with kernel stabilizers, and minimization of error functionals is analyzed."
2704393,14133,20332,Social capital in network organizations,2014,"In a dynamic network organization, member agents usually interact to coordinate their actions and to cooperate towards a common goal with which they have no previous experience. These relations allow them to produce a cohesive group to build and maintain their network. This paper will outline the effect of social capital on a network structure inside a network organization."
545132,14133,22113,Fully proportional representation as resource allocation: approximability results,2013,"We study the complexity of (approximate) winner determination under Monroe's and Chamberlin-Courant's multiwinner voting rules, where we focus on the total (dis)satisfaction of the voters (the utilitarian case) or the (dis)satisfaction of the worst-off voter (the egalitarian case). We show good approximation algorithms for the satisfaction-based utilitarian cases, and inapproximability results for the remaining settings."
412429,14133,20332,A dichotomy for 2-constraint forbidden CSP patterns,2012,Novel tractable classes of the binary CSP (constraint satisfaction problem) have recently been discovered by studying classes of instances defined by excluding subproblems described by patterns. The complete characterisation of all tractable classes defined by forbidden patterns is a challenging problem. We demonstrate a dichotomy in the case of forbidden patterns consisting of two constraints.
2491746,14133,20552,The Inductive Logic of Information Systems,2013,"An inductive logic can be formulated in which the elements are not propositions or probability distributions, but information systems. The logic is complete for information systems with binary hypotheses, i.e., it applies to all such systems. It is not complete for information systems with more than two hypotheses, but applies to a subset of such systems. The logic is inductive in that conclusions are more informative than premises. Inferences using the formalism have a strong justification in terms of the expected value of the derived information system."
1919722,14133,11104,Hurricane eye extraction from SAR image using saliency-based visual attention algorithm,2013,"Automatic hurricane information extraction in synthetic aperture radar (SAR) images has been a research topic in development. In this study, using saliency-based visual attention model, we developed an image processing procedure to extract hurricane eyes from SAR images. Experiment results show that hurricane eyes can be well extracted even when it is not visually obvious in images."
77503,14133,21106,Eye blink based detection of liveness in biometric authentication systems using conditional random fields,2012,The goal of this paper was to verify whether the conditional random fields are suitable and enough efficient for eye blink detection in user authentication systems based on face recognition with a standard web camera. To evaluate this approach several experiments were carried on using a specially developed test application and video database.
2213884,14133,11321,Bounded Planning in Passive POMDPs,2012,"In Passive POMDPs actions do not aect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an ecient and simple algorithm for nding an optimum."
2595843,14133,20332,Generating explanations for complex biomedical queries,2011,"We present a computational method to generate explanations to answers of complex queries over biomedical ontologies and databases, using the high-level representation and efficient automated reasoners of Answer Set Programming. We show the applicability of our approach with some queries related to drug discovery over PHARMGKB, DRUGBANK, BIOGRID, CTD and SIDER."
2271128,14133,21102,A normalized soft window-based similarity measure to extend the Rand index,2011,"This article addresses the problem of the construction of concordance measures between two crisp, fuzzy and possibilistic partitions from their coincidence matrices. Two existing approaches are reviewed and their advantages and drawbacks are exhibited so that a new Rand index taking profits of both is proposed. Numerous experimental results show that it outperforms other extensions of the Rand index."
2572530,14133,20332,On the discovery and utility of precedence constraints in temporal planning,2011,"We extend the precedence constraints contexts heuristic (hpcc) to a temporal and numeric setting, and propose rules to account precedence constraints among comparison variables and logical variables. Experimental results on benchmark domains show that our extension has the potential to lead to better plan quality than that with the heuristic proposed by Eyerich and others."
376041,14133,20332,GameLab: A Tool Suit to Support Designers of Systems with Homo Ludens in the Loop,2013,Digital games are an interesting method to motivate contributors to part take in a human computation process. Howeverthis approach poses its own challenges. Especially quality management or immediate and diverse feedback top layers are recurrent challenges. This paper introduces a tool suit to support designers with these challenges.
61245,14133,11187,Fuzzy property-oriented concept lattices in morphological image and signal processing,2013,"Fuzzy property-oriented concept lattices are a formal tool for modeling and processing incomplete information in information systems. This paper relates this theory to fuzzy mathematical morphology, which scope, for instance, is to process and analyze images and signals. Consequently, the theory developed in the concept lattice framework can be used in these particular settings."
1050550,14133,21106,Towards a theory of compositional learning and encoding of objects,2011,This paper develops a theory for learning compositional models of objects. It gives a theoretical basis for explaining the effectiveness of recent learning algorithms which exploit compositionality in order to perform structure induction of graphical models. It describes how compositional learning can be considered as learning either probability models or efficient codes for objects.
2900162,14133,11166,Visual-VM: A Social Network Visualization Tool for Viral Marketing,2014,"The paper presents Visual-VM, a social network visualization tool, where the main focus is to provide utilities for viral marketing (e.g., Influence maximization). Besides, Visual-VM utilizes the location information of each user (which could be estimated from the user's profile) for social network visualization, which is not used in existing social network visualization tools. Visual-VM also supports common utilities for social network exploration."
567741,14133,422,Evasion attack of multi-class linear classifiers,2012,"Machine learning has yield significant advances in decision-making for complex systems, but are they robust against adversarial attacks? We generalize the evasion attack problem to the multi-class linear classifiers, and present an efficient algorithm for approximating the optimal disguised instance. Experiments on real-world data demonstrate the effectiveness of our method."
2686573,14133,20332,On Moving Objects in Dynamic Domains,2011,"In the physical world, an object is a moving object if its position changes over time. In a symbolic dynamic system such as a computer program or a blocks world, what are the moving objects? In this paper, we propose a definition, consider ways to generate moving objects and their “positions”. We also introduce the flow graph of a moving object. It depicts the “trajectory” of the object, thus should be useful when planning to achieve a goal that involves moving some objects around."
2431931,14133,20358,TCRec: product recommendation via exploiting social-trust network and product category information,2013,"In this paper, we develop a novel product recommendation method called TCRec, which takes advantage of consumer rating history record, social-trust network and product category information simultaneously. Compared experiments are conducted on two real-world datasets and outstanding performance is achieved, which demonstrates the effectiveness of TCRec."
2545958,14133,20332,A Commonsense Theory of Microsociology: Interpersonal Relationships,2011,"We are developing an ontology of microsocial concepts for use in an   instructional system for teaching cross-cultural communication.  We   report here on that part of the ontology relating to interpersonal   relationships.  We first explicate the key concepts of commitment,   shared plans, and good will.  Then in terms of these we present   a formal account of the host-guest relationship."
2432443,14133,8960,Active Learning with a Drifting Distribution,2011,"We study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time. We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under Tsybakov noise. We further prove minimax lower bounds for this problem."
300689,14133,11321,Rethinking Collapsed Variational Bayes Inference for LDA,2012,"We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the alpha-divergence. We show that the CVB0 inference is composed of two different divergence projections: alpha=1 and -1. This interpretation will help shed light on CVB0 works."
481209,14133,20552,A Constraint Propagation Approach to Probabilistic Reasoning,2013,"The paper demonstrates that strict adherence to probability theory does not preclude the use of concurrent, self-activated constraint-propagation mechanisms for managing uncertainty. Maintaining local records of sources-of-belief allows both predictive and diagnostic inferences to be activated simultaneously and propagate harmoniously towards a stable equilibrium."
2805814,14133,20332,"Robotic Swarms as Solids, Liquids and Gasses",2012,"This paper presents an intuitive physics-inspired framework for controlling robot swarms that is based on the three major physical states of matter: solid, liquid and gas. An analogy is drawn between the states of matter and the basic swarming behaviors of clustering, translating and wandering. Mobility and localization requirements to achieve each of the states are specified."
2623860,14133,9677,Analyzing the Dynamics of Research by Extracting Key Aspects of Scientific Papers,2011,"We present a method for characterizing a research work in terms of its focus, domain of application, and techniques used. We show how tracing these aspects over time provides a novel measure of the influence of research communities on each other. We extract these characteristics by matching semantic extraction patterns, learned using bootstrapping, to the dependency trees of sentences in an article’s"
2287974,14133,21102,Formalizing object membership in fuzzy ontology with property importance and property priority,2011,"In this paper, we formalize the object membership in fuzzy ontology with property importance and property priority, while previous models lack building blocks to handle the importance and priority of properties. A formal mechanism used to measure object memberships in concepts is proposed. Such a mechanism can measure object memberships in concepts defined by properties with importance or priority well. We show that our model is more reasonable in measuring object memberships in concepts than previous models by examples and experiments."
2029481,14133,11321,Support Vector Machines as Probabilistic Models,2011,"We show how the SVM can be viewed as a maximum likelihood estimate of a class of probabilistic models. This model class can be viewed as a reparametrization of the SVM in a similar vein to the -SVM reparametrizing the classical (C-)SVM. It is not discriminative, but has a non-uniform marginal. We illustrate the benets of this new view by rederiving and re-investigating two established SVM-related algorithms."
260576,14133,20552,Description Logics with Fuzzy Concrete Domains,2012,"We present a fuzzy version of description logics with concrete domains. Main features are: (i) concept constructors are based on t-norm, t-conorm, negation and implication; (ii) concrete domains are fuzzy sets; (iii) fuzzy modifiers are allowed; and (iv) the reasoning algorithm is based on a mixture of completion rules and bounded mixed integer programming."
1929067,14133,20332,Manipulating Social Roles in a Tagging Environment,2013,"Social roles structure daily life because people adjust their behavior according to the role that they have in a specific situation. Online social roles are not necessarily the same as those in daily life and, because they are not so strictly assigned, the question arises whether they can be manipulated. We conducted a structured experiment to investigate whether the manipulation of online social roles can affect user behavior in a tagging task."
2596265,14133,20332,Uncovering Response Biases in Recommendation,2014,"An   user-specific tendency of biased movie rating is investigated, leading six identified types of rating pattern in a massive movie rating dataset. Based on the observed bias assumption, we propose a rescaling method of preferential scores by considering the rating types.  Experimental results show significant enhancement for movie recommendation systems."
2578388,14133,20332,Jim: a platform for affective AI in an interdisciplinary setting,2014,"We report on Jim, an inexpensive student designed platform for embodied affective AI. The project brings together students from backgrounds in computer science, physics, engineering, and Digital Media Arts (DMA) in an informal educational setting. The platform will be used in AI courses and autism treatment studies."
100122,14133,20332,Domain-Independent Optimistic Initialization for Reinforcement Learning,2014,"In Reinforcement Learning (RL), it is common to use optimistic initialization of value functions to encourage exploration. However, such an approach generally depends on the domain, viz., the scale of the rewards must be known, and the feature representation must have a constant norm. We present a simple approach that performs optimistic initialization with less dependence on the domain."
462722,14133,22113,Reasoning about preferences in intelligent agent systems,2011,"Agent systems based on the BDI paradigm need to make decisions about which plans are used to achieve their goals. Usually the choice of which plan to use to achieve a particular goal is left up to the system to determine. In this paper we show how preferences, which can be set by the user of the system, can be incorporated into the BDI execution process and used to guide the choices made."
1796252,14133,8960,Time--Data Tradeoffs by Aggressive Smoothing,2014,"This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems."
423860,14133,22113,A generalized arc-consistency algorithm for a class of counting constraints,2011,"This paper introduces the SEQ BIN meta-constraint with a polytime algorithm achieving generalized arc-consistency. SEQ BIN can be used for encoding counting constraints such as CHANGE, SMOOTH, or INCREASING NVALUE. For all of them the time and space complexity is linear in the sum of domain sizes, which improves or equals the best known results of the literature."
2552659,14133,23922,Open problem: Learning dynamic network models from a static snapshot,2012,"In this paper we consider the problem of learning a graph generating process given the evolving graph at a single point in time. Given a graph of sufficient size, can we learn the (repeatable) process that generated it? We formalize the generic problem and then consider two simple instances which are variations on the well-know graph generation models by Erd˝ )"
118560,14133,20332,"A formal framework for the specification, verification and synthesis of diagnosers",2013,"In this work we present a formal approach to the design of Fault Detection and Identification (FDI) components. We define a comprehensive language for the specification of FDI, and discuss how to check whether a given FDI component fulfills its specification. Then, we propose an automatic procedure to synthesize an FDI component that satisfies a given specification. The approach has been implemented and tested in realistic case studies from the aerospace domain."
2586563,14133,20332,Reliability of computational experiments on virtualised hardware,2011,"We present preliminary results of an investigation into the suitability of virtualised hardware - in particular clouds - for running computational experiments. Our main concern was that the reported CPU time would not be reliable and reproducible. The results demonstrate that while this is true in cases where many virtual machines are running on the same physical hardware, there is no inherent variation introduced by using virtualised hardware compared to non-virtualised hardware."
1748743,14133,22130,Label-embedding for text recognition,2013,"A system and method for comparing a text image and a character string are provided. The method includes embedding a character string into a vectorial space by extracting a set of features from the character string and generating a character string representation based on the extracted features, such as a spatial pyramid bag of characters (SPBOC) representation. A text image is embedded into a vectorial space by extracting a set of features from the text image and generating a text image representation based on the text image extracted features. A compatibility between the text image representation and the character string representation is computed, which includes computing a function of the text image representation and character string representation."
581579,14133,20552,Iterated risk measures for risk-sensitive Markov decision processes with discounted cost,2011,"We demonstrate a limitation of discounted expected utility, a standard approach for representing the preference to risk when future cost is discounted. Specifically, we provide an example of the preference of a decision maker that appears to be rational but cannot be represented with any discounted expected utility. A straightforward modification to discounted expected utility leads to inconsistent decision making over time. We will show that an iterated risk measure can represent the preference that cannot be represented by any discounted expected utility and that the decisions based on the iterated risk measure are consistent over time."
27299,14133,11187,Infinite sparse threshold unit networks,2012,"In this paper we define a kernel function which is the dual space equivalent of infinitely large sparse threshold unit networks. We first explain how to couple a kernel function to an infinite recurrent neural network, and next we use this definition to apply the theory to sparse threshold unit networks. We validate this kernel function with a theoretical analysis and an illustrative signal processing task."
2635711,14133,22113,Audience-based uncertainty in abstract argument games,2013,"The paper generalizes abstract argument games to cope with cases where proponent and opponent argue in front of an audience whose type is known only with uncertainty. The generalization, which makes use of basic tools from probability theory, is motivated by several examples and delivers a class of abstract argument games whose adequacy is proven robust against uncertainty."
216821,14133,20332,An Intelligent Conversational Agent for Promoting Long-Term Health Behavior Change Using Motivational Interviewing,2011,We present a conversational agent designed as a virtual counselor for health behavior change. The agent incorporates techniques drawn from Motivational Interviewing to enhance client motivation and confidence to change; these techniques are modeled and implemented based on a domain-specific taxonomy of dialogue acts. We discuss the design and preliminary evaluation of the agent.
2212469,14133,11321,Learning Optimally Sparse Support Vector Machines,2013,"We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice."
474698,14133,22113,A practical automata-based technique for reasoning in expressive description logics,2011,"In this work we describe the theoretical foundations and the implementation of a new automata-based technique for reasoning over expressive Description Logics that is worst-case optimal and lends itself to an efficient implementation. In order to show the feasibility of the approach, we have realized a working prototype of a reasoner based upon these techniques. An experimental evaluation of this prototype shows encouraging results."
228127,14133,20332,Configuration Planning with Multiple Dynamic Goals,2013,"We propose an approach to configuration planning for robotic systems in which plans are represented as constraint networks and planning is defined as search in the space of such networks. The approach supports reasoning about time, resources, and information dependencies between actions. In addition, the system can leverage the flexibility of such networks at execution time to support dynamic goal posting and re-planning."
2617098,14133,20332,Improving hierarchical planning performance by the use of landmarks,2012,"In hierarchical planning, landmarks are tasks that occur on every search path leading from the initial plan to a solution. In this work, we present novel domain-independent planning strategies based on such hierarchical landmarks. Our empirical evaluation on four benchmark domains shows that these landmark-aware strategies outperform established search strategies in many cases."
201317,14133,8960,Generalised Entropy MDPs and Minimax Regret,2014,"Bayesian methods suffer from the problem of how to specify prior beliefs.#R##N#One interesting idea is to consider worst-case priors. This requires solving#R##N#a stochastic zero-sum game. In this paper, we extend well-known results#R##N#from bandit theory in order to discover minimax-Bayes policies and discuss#R##N#when they are practical."
2434222,14133,8502,Saliency detection based on proto-objects and topic model,2011,"This paper proposes a novel computational framework for saliency detection, which integrates the saliency map computation and proto-objects detection. The proto-objects are detected based on the saliency map using latent topic model. The detected proto-objects are then utilized to improve the saliency map computation. Extensive experiments are performed on two publicly available datasets. The experimental results show that the proposed framework outperforms the state-of-art methods."
2435951,14133,11321,Generalized Exponential Concentration Inequality for Renyi Divergence Estimation,2014,"Estimating divergences in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of R´ enyi- divergence for a smooth"
1792805,14133,23735,Learning impedance controller parameters for lower-limb prostheses,2013,"Impedance control is a common framework for control of lower-limb prosthetic devices. This approach requires choosing many impedance controller parameters. In this paper, we show how to learn these parameters for lower-limb prostheses by observation of unimpaired human walkers. We validate our approach in simulation of a transfemoral amputee, and we demonstrate the performance of the learned parameters in a preliminary experiment with a lower-limb prosthetic device."
665003,14133,22113,Modeling attempt and action failure in probabilistic stit logic,2011,We define an extension of stit logic that encompasses subjective probabilities representing beliefs about simultaneous choice exertion of other agents. The formalism enables us to express the notion of 'attempt' as a choice exertion that maximizes the chance of success with respect to an action effect. The notion of attempt (or effort) is central in philosophical and legal discussions on responsibility and liability.
100814,14133,20332,A hybrid grammar-based approach for learning and recognizing natural hand gestures,2014,"In this paper, we present a hybrid grammar formalism designed to learn structured models of natural iconic gesture performances that allow for compressed representation and robust recognition. We analyze a dataset of iconic gestures and show how the proposed Featurebased Stochastic Context-Free Grammar (FSCFG) can generalize over both structural and feature-based variations among different gesture performances."
623063,14133,20332,Plan-based character diversity,2012,"Non-player character diversity enriches game environments increasing their replay value. We propose a method for obtaining character behavior diversity based on the diversity of plans enacted by characters, and demonstrate this method in a scenario in which characters have multiple choices. Using case-based planning techniques, we reuse plans for varied character behavior, which simulate different personality traits."
180799,14133,20332,Movie recommender system for profit maximization (short LBP),2013,"In this paper we provide an algorithm for utility maximization of a movie supplier service, in two different settings, one with prices and the other without. This algorithm is provided along with an extensive experiment demonstrating its performance.#R##N##R##N#We also uncover a phenomenon where movie consumers prefer watching and even paying for movies that they have already seen in the past than movies that are new to them."
2644387,14133,20332,Information sharing for care coordination,2014,"Teamwork and care coordination are of increasing importance to health care delivery and patient safety and health. My research aims at developing agents that are able to make intelligent information sharing decisions to support a diverse, evolving team of care providers in constructing and maintaining a shared plan that operates in uncertain environments."
164673,14133,11187,Variable-Sized kohonen feature map probabilistic associative memory,2012,"In this paper, we propose a Variable-sized Kohonen Feature Map Probabilistic Associative Memory (VKFMPAM). The proposed model can realize the probabilistic association for the training set including one-to-many relations, and neurons can be added in the Map Layer if necessary. We carried out a series of computer experiments and confirmed the effectiveness of the proposed model."
2626245,14133,20332,Can a Robot Learn Language as a Child Does,2012,"This paper gives a brief retrospective of a research project begun in 1987 and continuing to the present on the topic of language acquisition by an autonomous humanoid robot.  We recount the motivations for, theoretical bases of and experimental results on this subject.  Important results include novel models and algorithms resulting in interesting linguistic function of our robots."
2040879,14133,8960,Sparse Prediction with the k-Support Norm,2012,"We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an l1 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and can thus be advantageous in in sparse prediction problems. We also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use."
2716331,14133,22113,Symbolic merge-and-shrink for cost-optimal planning,2013,"Symbolic PDBs and Merge-and-Shrink (M&S) are two approaches to derive admissible heuristics for optimal planning. We present a combination of these techniques, Symbolic Merge-and-Shrink (SM&S), which uses M&S abstractions as a relaxation criterion for a symbolic backward search. Empirical evaluation shows that SM&S has the strengths of both techniques deriving heuristics at least as good as the best of them for most domains."
2797727,14133,20332,Incorporating Variable Vascular Heat Exchangers into Models Of Human Thermoregulation,2011,"Models of human thermoregulatory function generally assume that heat transfer across the skin surface is uniform. However, only glabrous skin regions contain unique vascular structures that enable a large volume of blood to flow immediately below the skin surface. These are the body’s radiators. We are constructing a novel model of thermoregulatory function that incorporates heat transfer across the glabrous skin regions as separate from heat transfer across the general skin surface."
2665688,14133,20332,A computational cognitive model of mirroring processes: a position statement,2013,In order to fully utilize robots for our benefit and design better agents that can collaborate smoothly and naturally with humans we need to understand how humans think. My goal is to understand the mirroring process and use that knowledge to build a computational cognitive model to enable a robot/agent to infer intentions and therefore collaborate more naturally in a human environment.
25706,14133,11187,Dengue model described by differential inclusions,2011,"In this paper we formulate a differential inclusion to model an epidemic outbreak of Dengue fever in the Cuban conditions. The model takes into account interaction of human and mosquito populations as well as vertical transmission in the mosquito population. Finally, we propose a mathematical framework allowing us to make suitable predictions about the populations of humans, mosquitoes and eggs infected during the epidemic time."
2636850,14133,20332,A speech-driven second screen application for TV program discovery,2014,"In this paper, we present a speech-driven second screen application for TV program discovery. We give an overview of the application and its architecture. We also present a user study along with a failure analysis. The results from the study are encouraging, and demonstrate our application's effectiveness in the target domain. We conclude with a discussion of follow-on efforts to further enhance our application."
309408,14133,20552,Can Evidence Be Combined in the Dempster-Shafer Theory,2013,"Dempster's rule of combination has been the most controversial part of the Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on the noncombinability of evidence from a relational model of the D-S theory. In this paper, we will describe another relational model where D-S masses are represented as conditional granular distributions. By comparing it with Zadeh's relational model, we will show how Zadeh's conjecture on combinability does not affect the applicability of Dempster's rule in our model."
29143,14133,22113,Contributions to personalizable knowledge integration,2011,"Inconsistency and partial information is the norm in knowledge bases used in many real world applications that support, among other things, human decision making processes. In this work we argue that the management of this kind of data needs to be context-sensitive, creating a synergy with the user to build useful, flexible data management systems."
347281,14133,20332,Using Agent-Based Simulation to Determine an Optimal Lane-Changing Strategy on a Multi-Lane Highway,2011,"Lane changing can increase or impede the flow of vehicular traffic, depending on traffic density and the lane-changing strategies used by individual drivers. We implement and extend the Nagel-Schreckenberg (N-S) traffic model as an agent-based model to investigate lane-changing behavior on a multi-lane roadway, with the goal of determining which lane changing strategies result in the greatest overall traffic flow. We show that in heavier traffic, an aggressive lane changing policy may be beneficial for overall traffic flow."
2457050,14133,20552,On Measurement Bias in Causal Inference,2012,"This paper addresses the problem of measurement errors in causal inference and highlights several algebraic and graphical methods for eliminating systematic bias induced by such errors. In particulars, the paper discusses the control of partially observable confounders in parametric and non parametric models and the computational problem of obtaining biasfree effect estimates in such models."
84135,14133,11187,Learning scheme for complex neural networks using simultaneous perturbation,2011,"Usually, the back-propagation learning rule is widely used for complex-valued neural networks as well. On the other hand, in this paper, learning rule for complex-valued neural networks using the simultaneous perturbation optimization method is proposed. Comparison between the back-propagation method and the proposed. simultaneous perturbation learning rule is made for some test problems. Simplicity of the proposed method results in faster learning speed."
2670103,14133,10174,Optimal Control as a Graphical Model Inference Problem,2013,In this paper we show the identification between stochastic optimal control computation and probabilistic inference on a graphical model for certain class of control problems. We refer to these problems as Kullback-Leibler (KL) control problems. We illustrate how KL control can be used to model a multi-agent cooperative game for which optimal control can be approximated using belief propagation when exact inference is unfeasible.
1807582,14133,8960,Compete to Compute,2013,"Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time."
577776,14133,22113,Case adaptation with qualitative algebras,2013,"This paper proposes an approach for the adaptation of spatial or temporal cases in a case-based reasoning system. Qualitative algebras are used as spatial and temporal knowledge representation languages. The intuition behind this adaptation approach is to apply a substitution and then repair potential inconsistencies, thanks to belief revision on qualitative algebras. A temporal example from the cooking domain is given."
2516903,14133,20332,Bounding the cost of stability in games over interaction networks,2013,"We study the stability of cooperative games played over an interaction network, in a model that was introduced by My-erson (1977). We show that the cost of stability of such games (i.e., the subsidy required to stabilize the game) can be bounded in terms of natural parameters of their underlying interaction networks. Specifically, we prove that if the treewidth of the interaction network H is k, then the relative cost of stability of any game played over H is at most k + 1, and if the pathwidth of H is k′, then the relative cost of stability is at most k′. We show that these bounds are tight for all k ≥ 2 and all k′ ≥ 1, respectively."
2224666,14133,20332,Untangling topic threads in chat-based communication: a case study,2011,Analyzing chat traffic has important applications for both the military and the civilian world. This paper presents a case study of a real-world application of chat analysis in support of team training exercise in the military. It compares the results of an unsupervised learning approach with those of a supervised classification approach. The paper also discusses some of the specific challenges presented by this domain.
2379860,14133,8960,Multiresolution analysis on the symmetric group,2012,"There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group, find the corresponding wavelet functions, and describe a fast wavelet transform for sparse signals. We discuss potential applications in ranking, sparse approximation, and multi-object tracking."
582890,14133,20552,Discounting and Combination Operations in Evidential Reasoning,2013,"Evidential reasoning is now a leading topic in Artificial Intelligence. Evidence is represented by a variety of evidential functions. Evidential reasoning is carried out by certain kinds of fundamental operation on these functions. This paper discusses two of the basic operations on evidential functions, the discount operation and the well-known orthogonal sum operation. We show that the discount operation is not commutative with the orthogonal sum operation, and derive expressions for the two operations applied to the various evidential functions."
1446688,14133,23735,Finding and Navigating to Household Objects with UHF RFID Tags by Optimizing RF Signal Strength,2014,"©2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works."
1299175,14133,23735,Learning to Reach into the Unknown: Selecting Initial Conditions When Reaching in Clutter,2014,"©2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works."
418653,14133,11187,Classification of distorted patterns by feed-forward spiking neural networks,2012,"In this paper, a feed forward spiking neural network is tested with spike train patterns with additional and missing spikes. The network is trained with noisy and distorted patterns with an extension of the ReSuMe learning rule to networks with hidden layers. The results show that the multilayer ReSuMe can reliably learn to discriminate highly distorted patterns spanning over 500 ms."
246847,14133,11187,Improving the Associative Rule Chaining Architecture,2013,"This paper describes improvements to the rule chaining architecture presented in [1]. The architecture uses distributed associative memories to allow the system to utilise memory efficiently, and superimposed distributed representations in order to reduce the time complexity of a tree search to O(d), where d is the depth of the tree. This new work reduces the memory required by the architecture, and can also further reduce the time complexity."
2373829,14133,21102,Are fuzzy description logics with general concept inclusion axioms decidable,2011,"This paper concentrates on a fuzzy Description Logic with product t-norm and involutive negation. It does not answer the question posed in its title for this logic, but it gives strong indications that the answer might in fact be “no.” On the one hand, it shows that an algorithm that was claimed to answer the question affirmatively for this logic is actually incorrect. On the other hand, it proves undecidability of a variant of this logic."
2629042,14133,20332,The RhetFig project: computational rhetorics and models of persuasion,2011,"In this talk, we will describe the work of the RhetFig Project, which has so far included: (1) development of a comprehensive theorized ontology of figures, the first ever based on cognitive principles; (2) computational applications of the ontology (e.g., automated detection and classification of figures in social media, rhetoric-based metrics for text summarization); and (3) investigation of epistemic topoi for linguistic annotation)."
66933,14133,20332,Repeated sequential auctions with dynamic task clusters,2012,Sequential auctions can be used to provide solutions to the multi-robot task-allocation problem. In this paper we extend previous work on sequential auctions and propose an algorithm that clusters and auctions uninitiated task clusters repeatedly upon the completion of individual tasks. We demonstrate empirically that our algorithm results in lower overall team costs than other sequential auction algorithms that only assign tasks once.
2362377,14133,11321,The Convexity and Design of Composite Multiclass Losses,2012,"We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisher-consistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk."
1932994,14133,23922,Open Problem: Efficient Online Sparse Regression,2014,"In practical scenarios, it is often necessary to be able to make predictions with very limited access to the features of any example. We provide one natural formulation as an online sparse regression problem with squared loss, and ask whether it is possible to achieve sublinear regret with ecient algorithms (i.e. polynomial running time in the natural parameters of the problem)."
209467,14133,20332,Phylo and Open-Phylo: A Human-Computing Platform for Comparative Genomics.,2014,"Comparative genomics is a field of research that aims to provide us accurate mappings between the genetic material of multiple species. These techniques are useful for biomedical research and evolutionary studies. We present Phylo and Open-Phylo, an open citizen science platform and human computing-game for comparative genomic studies that reached more than 300,000 users."
1704981,14133,9078,Fast image scanning with deep max-pooling convolutional neural networks,2013,"Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present."
2648853,14133,11187,A Geometrical Approach for Parameter Selection of Radial Basis Functions Networks,2014,The RBF network is commonly used for classification and function approximation. The center and radius of the activation function of neurons is an important parameter to be found before the network training. This paper presents a method based on computational geometry to find these coefficients without any parameters provided by the user. The method is compared with a SVM and experimental results showed that our approach is promising.
2632868,14133,20332,A Web-Based Environment for Explanatory Biological Modeling,2012,"In this paper, we describe an interactive environment for the representation, interpretation, and revision of explanatory biological models. We illustrate our approach on the systems biology of aging, a complex topic that involves many interacting components, and discuss our experiences using this environment to codify an informal model of aging. We close by discussing related efforts and directions for future research."
2348350,14133,21089,Word Epoch Disambiguation: Finding How Words Change Over Time,2012,"In this paper we introduce the novel task of word epoch disambiguation, defined as the problem of identifying changes in word usage over time. Through experiments run using word usage examples collected from three major periods of time (1800, 1900, 2000), we show that the task is feasible, and significant differences can be observed between occurrences of words in different periods of time."
2101409,14133,9616,NEATER: Filtering of Over-sampled Data Using Non-cooperative Game Theory,2014,"We present a method for the filtering of over-sampled data using non-cooperative game theory (NEATER) to address the imbalanced data problem using game theory. Specifically, the problem is formulated as a non-cooperative game where all the data are players and the goal is to uniformly and consistently label all of the synthetic data created by any over-sampling technique. We present extensive experimental results which demonstrate the advantages of our method."
555441,14133,11187,State prediction: a constructive method to program recurrent neural networks,2011,"We introduce a novel technique to program desired state sequences into recurrent neural networks in one shot. The basic methodology and its scalability to large and input-driven networks is demonstrated by shaping attractor landscapes, transient dynamics and programming limit cycles. The approach unifies programming of transient and attractor dynamics in a generic framework."
250279,14133,20332,A propagator design framework for constraints over sequences,2014,"Constraints over variable sequences are ubiquitous and many of their propagators have been inspired by dynamic programming (DP). We propose a conceptual framework for designing such propagators: pruning rules, in a functional notation, are refined upon the application of transformation operators to a DP-style formulation of a constraint; a representation of the (tuple) variable domains is picked; and a control of the pruning rules is picked."
3015024,14133,8960,Privacy Aware Learning,2012,"We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator."
2468943,14133,22021,Convergence of generalized linear coordinate-descent message-passing for quadratic optimization,2012,"We study the generalized linear coordinate-descent (GLiCD) algorithm for the quadratic optimization problem. As an extension of the linear coordinate-descent (LiCD) algorithm, the GLiCD algorithm incorporates feedback from last iteration in generating new messages. We show that if the amount of feedback signal from last iteration is above a threshold and the GLiCD algorithm converges, it computes the optimal solution. Based on the result, we further show that if the feedback signal is large enough, the GLiCD algorithm is guaranteed to converge."
2574404,14133,20332,Cognitive Assistants for Evidence-Based Reasoning Tasks,2011,"Evidence-based reasoning is at the core of many problem solving and decision making tasks in a wide variety of domains. This paper introduces a computational theory of evidence-based reasoning, the architecture of a learning agent shell which incorporates general knowledge for evidence-based reasoning, a methodology that uses the shell to rapidly develop cognitive assistants in a specific domain, and a sample cognitive assistant for intelligence analysis."
387462,14133,20332,Automating Crowdsourcing Tasks in an Industrial Environment,2013,"Crowdsourcing based applications are starting to gain traction in industrial environments. Crowdsourcing research has proven that it is possible to get good quality labels at the fraction of the cost and time. However, implementing such applications at large scale requires new infrastructure. In this demo we present a system that allows the automation of crowdsourcing tasks for information retrieval experiments."
2441529,14133,20332,A demonstration of ScriptEase II,2011,"This demonstration describes ScriptEase II, a tool that allows game story authors to generate scripts that control objects in video games by manipulating high level story patterns and game objects. ScriptEase II can generate scripting code for any game engine for which a translator is written. Currently there are translators for Neverwinter Nights and real Pinball games."
562208,14133,20332,An Introduction to the Zooniverse,2013,"The Zooniverse (zooniverse.org) began in 2007 with the launch of Galaxy Zoo, a project in which more than 175,000 people provided shape analyses of more than 1 million galaxy images sourced from the Sloan Digital Sky Survey. In this session we will outline the current architecture of the Zooniverse platform and introduce new functionality being developed that should be of interest to the HCOMP community."
642057,14133,23922,Most Correlated Arms Identification,2014,We study the problem of finding the most mutually correlated arms among many arms. We show that adaptive arms sampling strategies can have significant advantages over the non-adaptive uniform sampling strategy. Our proposed algorithms rely on a novel correlation estimator. The use of this accurate estimator allows us to get improved results for a wide range of problem instances.
2821962,14133,20332,Joint Inference for Extracting Text Descriptors from Triage Images of Mass Disaster Victims,2011,"The major contributions of this work include a set of biographical feature extractors brought together by a probabilistic graphical model. The graphical model acts as a communication medium between different feature extractors, together resulting in a text descriptor to describe triage images of disaster victims. The model is built using domain information gathered from data and literature."
2605528,14133,20332,Analogy tutor: a tutoring system for promoting conceptual learning via comparison,2014,"A major challenge in artificial intelligence is building intelligent, interactive learning environments that can support students in human-like ways. Analogical reasoning can be a catalyst for conceptual learning, yet very few systems support analogical reasoning as an instructional activity. In my thesis, I plan to demonstrate that an analogy tutor can assist conceptual learning by guiding students through instructional comparisons."
324288,14133,11321,Tighter Variational Representations of f-Divergences via Restriction to Probability Measures,2012,We show that the variational representations for f-divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f-divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.
2716153,14133,10174,Property Directed Reachability for Automated Planning.,2014,"Property Directed Reachability (PDR), also known as IC3, is a very promising recent method for deciding reachability in symbolically represented transition systems. While originally conceived as a model checking algorithm for hardware circuits, it has already been successfully applied in several other areas. This paper summarizes the first investigation of PDR from the perspective of automated planning."
342161,14133,20552,Conditioning Methods for Exact and Approximate Inference in Causal Networks,2013,"We present two algorithms for exact and approximate inference in causal networks. The first algorithm, dynamic conditioning, is a refinement of cutset conditioning that has linear complexity on some networks for which cutset conditioning is exponential. The second algorithm, B-conditioning, is an algorithm for approximate inference that allows one to trade-off the quality of approximations with the computation time. We also present some experimental results illustrating the properties of the proposed algorithms."
2788802,14133,20332,Automated generation of diverse NPC-controlling FSMs using nondeterministic planning techniques,2013,"We study the problem of generating a set of Finite State Machines (FSMs) modeling the behavior of multiple, distinct NPCs. We observe that nondeterministic planning techniques can be used to generate FSMs by following conventions typically used when manually creating FSMs modeling NPC behavior. We implement our ideas in DivNDP, the first algorithm for automated diverse FSM generation."
151714,14133,20552,EDML: A Method for Learning Parameters in Bayesian Networks,2012,"We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically."
2531520,14133,8960,Recursive Inversion Models for Permutations,2014,We develop a new exponential family probabilistic model for permutations that can capture hierarchical structure and that has the Mallows and generalized Mallows models as subclasses. We describe how to do parameter estimation and propose an approach to structure search for this class of models. We provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.
111927,14133,22113,A case-based solution to the cold-start problem in group recommenders,2013,"In this paper we offer a potential solution to the cold-start problem in group recommender systems. To do so, we use information about previous group recommendation events and copy ratings from a user who played a similar role in some previous group event. We show that copying in this way, i.e. conditioned on groups, is superior to copying nothing and also superior to copying ratings from the most similar user known to the system."
53267,14133,20332,AAAI-13 Preface,2013,"Welcome to the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI-13! As can be seen in these proceedings, AI’s scope and influence continue to grow. This year, we received 827 submissions across a variety of tracks, allowing us to put together a diverse and exciting technical program featuring the field’s top research."
2596167,14133,20332,Preference Trees: A Language for Representing and Reasoning about Qualitative Preferences,2014,"We introduce a novel qualitative preference representation language,   preference trees  , or   P-trees  . We show that the lan- guage is intuitive to specify preferences over combinatorial domains and it extends existing preference formalisms such as   LP-trees  ,   ASO-rules   and   possibilistic logic  . We study rea- soning problems with P-trees and obtain computational com- plexity results."
1296347,14133,11166,Finding Discriminatory Genes: A Methodology for Validating Microarray Studies,2013,"This paper explores the challenge of efficiently collecting data to find which genes (from a given set of candidates) are differentially expressed. We consider several algorithms for this task, including some that assume there are only two types of genes: those that are not differentially expressed, and those that are differentially expressed to the same level. We provide a framework for evaluating such algorithms and also present an algorithm that has nice theoretical properties and performs very well on both real and simulated data."
2713633,14133,22113,Object recognition based on visual grammars and Bayesian networks,2013,"A novel proposal for object recognition based on relational grammars and Bayesian Networks is presented. Based on a Symbol-Relation grammar an object is represented as a hierarchy of features and spatial relations. This representation is transformed to a Bayesian network structure which parameters are learned from examples. Thus, recognition is based on probabilistic inference in the Bayesian network representation. Preliminary results in modeling natural objects are presented."
1365206,14133,11166,Sparse Group Lasso for Regression on Land Climate Variables,2011,"The large amount of reliable climate data available today has promoted the development of statistical predictive models for climate variables. In this paper we have applied Sparse Group Lasso to build a predictive model for land climate variables using ocean climate variables as covariates. We demonstrate that the sparse model provides better predictive performance than the state-of-the-art, is climatologically interpretable and robust in variable selection."
2602673,14133,20332,Apoptotic Stigmergic Agents for Real-Time Swarming Simulation,2012,"One common use for swarming agents is in social simulation. This paper reports on such a model developed to track protest activities at the May 2012 NATO summit in Chicago. The use of apoptotic stigmergic agents allows the model to run on-line, consuming two kinds of external data and reporting its results in real time."
2779267,14133,20332,Robust decision making under strategic uncertainty in multiagent environments,2011,"We introduce the notion of strategic uncertainty for boundedly rational, non-myopic agents as an analog to the equilibrium selection problem in classical game theory. We then motivate the need for and feasibility of addressing strategic uncertainty and present an algorithm that produces decisions that are robust to it. Finally, we show how agents' rationality levels and planning horizons alter the robustness of their decisions."
2012742,14133,8960,Truncation-free Online Variational Inference for Bayesian Nonparametric Models,2012,"We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms."
560696,14133,20552,Predicting Conditional Quantiles via Reduction to Classification,2012,We show how to reduce the process of predicting general order statistics (and the median in particular) to solving classification. The accompanying theoretical statement shows that the regret of the classifier bounds the regret of the quantile regression under a quantile loss. We also test this reduction empirically against existing quantile regression methods on large real-world datasets and discover that it provides state-of-the-art performance.
1472395,14133,9475,Bandits with budgets,2013,"Motivated by online advertising applications, we consider a version of the classical multi-armed bandit problem where there is a cost associated with pulling each arm, and a corresponding budget which limits the number of times that an arm can be pulled. We derive regret bounds on the expected reward in such a bandit problem using a modification of the well-known upper confidence bound algorithm UCB1."
2584981,14133,10174,A Complete Algorithm for Generating Landmarks.,2011,A collection of landmarks is complete if the cost of a minimum-cost hitting set equals  h  +  and there is a minimum-cost hitting set that is an optimal relaxed plan. We present an algorithm for generating a complete collection of landmarks and we show that this algorithm can be extended into effective polytime heuristics for optimal and satisficing planning. The new admissible heuristics are compared with current state-of-the-art heuristics for optimal planning on benchmark problems from the IPC.
1406872,14133,422,Practical collapsed variational bayes inference for hierarchical dirichlet process,2012,"We propose a novel collapsed variational Bayes (CVB) inference for the hierarchical Dirichlet process (HDP). While the existing CVB inference for the HDP variant of latent Dirichlet allocation (LDA) is more complicated and harder to implement than that for LDA, the proposed algorithm is simple to implement, does not require variance counts to be maintained, does not need to set hyper-parameters, and has good predictive performance."
118622,14133,22113,Hustling in repeated zero-sum games with imperfect execution,2011,"We study repeated games in which players have imperfect execution skill and one player's true skill is not common knowledge. In these settings the possibility arises of a player hustling, or pretending to have lower execution skill than they actually have. Focusing on repeated zero-sum games, we provide a hustle-proof strategy; this strategy maximizes a player's payoff, regardless of the true skill level of the other player."
19776,14133,11187,Learning curves for Gaussian processes via numerical cubature integration,2011,This paper is concerned with estimation of learning curves for Gaussian process regression with multidimensional numerical integration. We propose an approach where the recursion equations for the generalization error are approximately solved using numerical cubature integration methods. The advantage of the approach is that the eigenfunction expansion of the covariance function does not need to be known. The accuracy of the proposed method is compared to eigenfunction expansion based approximations to the learning curve.
499510,14133,20552,Causal reasoning in graphical time series models,2012,"We propose a definition of causality for time series in terms of the effect of an intervention in one component of a multivariate time series on another component at some later point in time. Conditions for identifiability, comparable to the back-door and front-door criteria, are presented and can also be verified graphically. Computation of the causal effect is derived and illustrated for the linear case."
2596479,14133,20332,Towards the integration of multi-attribute optimization and game theory for border security patrolling strategies,2011,"The problem of border security is extremely complex, due to both the adversarial nature of the problem and the many different objectives of the border patrol, illegal entrants, and smugglers. We present a simplified model of the border patrolling problem, and describe initial progress towards combining game theory and multi-attribute optimization to find effective patrolling strategies in this domain."
558586,14133,20552,Beyond Log-Supermodularity: Lower Bounds and the Bethe Partition Function,2013,"A recent result has demonstrated that the Bethe partition function always lower bounds the true partition function of binary, log-supermodular graphical models. We demonstrate that these results can be extended to other interesting classes of graphical models that are not necessarily binary or log-supermodular: the ferromagnetic Potts model with a uniform external field and its generalizations and special classes of weighted graph homomorphism problems."
2692187,14133,22113,Refutation in dummett logic using a sign to express the truth at the next possible world,2011,"In this paper we use the Kripke semantics characterization of Dummett logic to introduce a new way of handling non-forced formulas in tableau proof systems. We pursue the aim of reducing the search space by strictly increasing the number of forced propositional variables after the application of noninvertible rules. The focus of the paper is on a new tableau system for Dummett logic, for which we have an implementation."
180287,14133,22113,Generative structure learning for Markov logic networks based on graph of predicates,2011,"In this paper we present a new algorithm for generatively learning the structure of Markov Logic Networks. This algorithm relies on a graph of predicates, which summarizes the links existing between predicates and on relational information between ground atoms in the training database. Candidate clauses are produced by means of a heuristical variabilization technique. According to our first experiments, this approach appears to be promising."
220434,14133,20332,Opportunities and challenges for constraint programming,2012,"Constraint programming has become an important technology for solving hard combinatorial problems in a diverse range of application domains. It has its roots in artificial intelligence, mathematical programming, operations research, and programming languages. This paper gives a perspective on where constraint programming is today, and discusses a number of opportunities and challenges that could provide focus for the research community into the future."
2684708,14133,20332,GenEth: a general ethical dilemma analyzer,2014,"We contend that ethically significant behavior of autonomous systems should be guided by explicit ethical principles determined through a consensus of ethicists. To provide assistance in developing these ethical principles, we have developed GENETH, a general ethical dilemma analyzer that, through a dialog with ethicists, codifies ethical principles in any given domain. GENETH has been used to codify principles in a number of domains pertinent to the behavior of autonomous systems and these principles have been verified using an Ethical Turing Test."
2646245,14133,20332,Top-down abstraction learning using prediction as a supervisory signal,2013,We present a top-down approach for learning abstractions whereby a robot begins with a coarse representation of the world and incrementally finds new distinctions as they enable the robot to better predict its environment. The approach has been implemented on a simulated robot that learns new distinctions in the form of variable discretizations through autonomous exploration. This paper discusses how to generalize this approach to learning broader abstractions.
2653614,14133,20332,A deeper empirical analysis of CBP algorithm: grounding is the bottleneck,2014,"In this work-in-progress, we consider a lifted inference algorithm and analyze its scaling properties. We compare two versions of this algorithm - the original implementation and a newer implementation built on a database. Our preliminary results show that constructing the factor graph from the relational model rather than the construction of the compressed model is the key bottleneck for the application of lifted inference in large domains."
2538132,14133,11321,Consistency of Causal Inference under the Additive Noise Model,2014,"We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting."
2538865,14133,11321,Sparse stochastic inference for latent Dirichlet allocation,2012,We present a hybrid algorithm for Bayesian topic models that combines the eciency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.
2575355,14133,11187,Robust sensor and actuator fault diagnosis with GMDH neural networks,2013,The uncertainty of neural model influences the effectiveness of the neural model-based FDI and FTC systems. The application of the GMDH approach to the state-space neural model structure selection allows reducing the model uncertainty. The state-space representation of the neural model enables to develop a new technique of estimation of the neural model inputs based on the RUIF. This result enables performing robust fault detection and isolation of the actuators.
137785,14133,11187,Computational intelligence in multimedia processing,2011,"Computational intelligence (CI) is awell-established paradigm that incorporates characteristics of biological computers (brains) to perform a variety of tasks that are difficult or impossible to do with conventional computers. This paper reviews some of the applications of CI in multimedia processing, including shot detection in video, logotype detection, video copy detection and retrieval, and faces coding in video sequences."
594294,14133,11187,"Hybrid artificial neural networks: models, algorithms and data",2011,"Artificial neural networks (ANNs) constitute a class of flexible nonlinear models designed to mimic biological neural systems. ANNs are one of the three main components of computational intelligence and, as such, they have been often hybridized from different perspectives. In this paper, a review of some of the main contributions for hybrid ANNs is given, considering three points of views: models, algorithms and data."
146712,14133,22113,From an agent logic to an agent programming language for partially observable stochastic domains,2011,"Broadly speaking, my research concerns combining logic of action and POMDP theory in a coherent, theoretically sound language for agent programming. We have already developed a logic for specifying partially observable stochastic domains. A logic for reasoning with the models specified must still be developed. An agent programming language will then be developed and used to design controllers for robots."
2535616,14133,11321,Efficient Semi-supervised and Active Learning of Disjunctions,2013,"We provide ecient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan & Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classication noise setting."
2114908,14133,22021,On the difficulty of learning power law graphical models,2013,"A power-law graph is any graph G = (V, E), whose degree distribution follows a power law i.e. the number of vertices in the graph with degree i, y i , is proportional to i -β  : y i  ∝ i -β . In this paper, we provide information-theoretic lower bounds on the sample complexity of learning such power-law graphical models i.e. graphical models whose Markov graph obeys the power law. In addition, we briefly revisit some existing state of the art estimators, and explicitly derive their sample complexity for power-law graphs."
2612849,14133,22113,On temporal regulations and commitment protocols,2011,"The proposal of Elisa Marengo's thesis is to extend commitment protocols in order to (i) allow for expressing commitments to temporal regulations, and (ii) to supply a tool for expressing laws, conventions and the like, in order to specify legal interactions. These two aspects will be deeply investigated in the proposal of a unified framework. This proposal is part of ongoing work that will be included in the thesis."
323322,14133,20332,Learning CP-net preferences online from user queries,2013,"We present an online, heuristic algorithm for learning Conditional Preference networks CP-nets from user queries. This is the first efficient and resolute CP-net learning algorithm: if a preference order can be represented as a CP-net, our algorithm learns a CP-net in time n p , where p is a bound on the number of parents a node may have. The learned CP-net is guaranteed to be consistent with the original CP-net on all queries from the learning process. We tested the algorithm on randomly generated CP-nets; the learned CP-nets agree with the originals on a high percent of non-training preference comparisons."
2690857,14133,20332,Statistical Quality Estimation for General Crowdsourcing Tasks.,2013,"One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control,which is to expect high-quality results from crowd workers.We propose an unsupervised statistical quality estimation method for general crowdsourcing tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces."
2623859,14133,21089,Improved Iterative Correction for Distant Spelling Errors,2014,"Noisy channel models, widely used in modern spellers, cope with typical misspellings, but do not work well with infrequent and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima problem and improve the F1 measure by 6.6% on distant spelling errors."
2659686,14133,22113,Using strategic logics to reason about agent programs,2013,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the agents' operational know-how, as defined by their libraries of abstract plans. In our logic, it is possible to refer to rational strategies for agents developed under the Belief-Desire-Intention agent paradigm. This allows us to express and verify properties of BDI systems using ATL-type logical frameworks."
2241281,14133,11321,Exploiting Ontology Structures and Unlabeled Data for Learning,2013,"We present and analyze a theoretical model designed to understand and explain the effectiveness of ontologies for learning multiple related tasks from primarily unlabeled data. We present both information-theoretic results as well as efficient algorithms.#R##N##R##N#We show in this model that an ontology, which specifies the relationships between multiple outputs, in some cases is sufficient to completely learn a classification using a large unlabeled data source."
2249557,14133,21089,A Comprehensive Gold Standard for the Enron Organizational Hierarchy,2012,"Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems."
2879102,14133,20332,Biased games,2014,"We present a novel extension of normal form games that we call biased games. In these games, a player's utility is influenced by the distance between his mixed strategy and a given base strategy. We argue that biased games capture important aspects of the interaction between software agents. Our main result is that biased games satisfying certain mild conditions always admit an equilibrium. We also tackle the computation of equilibria in biased games."
352578,14133,20332,Coordination of multiple teams of robots for an optimal global plan,2014,"We consider multiple teams of heterogeneous robots, where each team is given a feasible task to complete in its workspace on its own, and where teams are allowed to transfer robots between each other. We study the problem of finding a coordination of robot transfers between teams to ensure an optimal global plan (with minimum makespan) so that all tasks can be completed as soon as possible by helping each other. We propose to solve this problem using answer set programming."
154360,14133,11187,Hybrid approach for 2d strip packing problem using genetic algorithm,2013,"In this paper we have studied the two-dimensional cutting stock problem, in which large number of small rectangles are to be placed in the big container such that the trim loss and height of the layout is minimized. We have proposed a placement approach along with a relevant fitness function to evaluate the overall goodness of the design layout. The computation results validate the solution and the effectiveness of the approach."
858878,14133,11470,The Image Matting Method with Regularized Matte,2012,"Image matting refers to the problem of accurately extracting foreground objects in images and video. The most recent works in natural image matting relies on the local and manifold smoothness assumptions on foreground and background colors on which a cost function is established. In this paper, we present a framework of formulating new regularization for robust solutions and illustrate new algorithms using the standard benchmark images."
1782721,14133,20332,"Lifelong forgetting: a critical ingredient of lifelong learning, and its implementation in the opencog integrative AI framework",2011,"Of all the aspects differentiating lifelong learning from shorter-term, more specialized learning, perhaps none is more central than forgetting — or, to frame the issue more generally and technically, memory access speed deprioritization.  This extended abstract reviews some of the ideas involved in forgetting for lifelong learning systems, and briefly discusses the forgetting mechanisms used in the OpenCog integrative cognitive architecture."
2647173,14133,22113,FQHT: the logic of stable models for logic programs with intensional functions,2013,"We study a logical system FQHT that is appropriate for reasoning about nonmonotonic theories with intensional functions as treated in the approach of [Bartholomew and Lee, 2012]. We provide a logical semantics, a Gentzen style proof theory and establish completeness results. The adequacy of the approach is demonstrated by showing that it captures the Bartholemew/Lee semantics and satisfies a strong equivalence property."
2420803,14133,20332,Added Value of Sociofact Analysis for Business Agility,2011,"The increasing agility of business requires an accelerated adaptation of organizations to continuously changing conditions. Individual and organizational learning are prominent means to achieve this. Hereby learning is always accompanied by the development of knowledge artifacts. For the entire of learning and artifact development the term knowledge maturing has been introduced recently, which focuses on these three manifestations of knowledge: cognifacts, sociofacts, and artifacts. In this paper we will focus on sociofacts as the subject-bound knowledge manifestation of social actions. Sociofacts are rooted in respective cognifacts play an independent role due to their binding to collective actions and subjects. These are particularly difficult to grasp but play a decisive role for the performance of organizations and the collaboration in there. The presented paper approaches the notion of sociofacts, discusses them on a theoretical level and establishes a first formal notation for sociofacts. We use the case of a merger between two companies to describe the advantages of sociofact analysis for such process. Some sociofact related problems during a merger are described and possible solutions are presented. We identify technical approaches for seizing sociofacts from tool-mediated social interaction and discuss open question for future research."
677540,14133,8494,Live demonstration: A scaled-down version of the BrainScaleS wafer-scale neuromorphic system,2012,"This demonstration is based on the wafer-scale neuromophic system presented in [1], [2] and [3]. One wafer of this system contains 384 analog network chips (ANC) for a total of 40 M synapses and 200k neurons. 12 FPGA boards hosting 48 digital network chips (DNC) [4] connect the wafer to outside stimuli by 768 2 Gb/s serial links. Since it is not feasible to bring a wafer module to the conference, a scaled-down demonstrator system was prepared, containing one FPGA board, one DNC and eight ANC chips (Fig. 1). The FPGA connects to a host PC via Gbit Ethernet and an oscilloscope will be used to visualize the analog signals within the network chips. The total setup will contain nearly one million individually programmable synapses and 4k neurons. Each neuron can be separately configured by 24 analog parameters. The whole system can emulate biological neural networks up to 10 5  times faster than biological real time."
1740903,14133,422,Stability of influence maximization,2014,"The present article serves as an erratum to our paper of the same title, which was presented and published in the KDD 2014 conference. In that article, we claimed falsely that the objective function defined in Section 1.4 is non-monotone submodular. We are deeply indebted to Debmalya Mandal, Jean Pouget-Abadie and Yaron Singer for bringing to our attention a counter-example to that claim.      Subsequent to becoming aware of the counter-example, we have shown that the objective function is in fact NP-hard to approximate to within a factor of O(n 1-e ) for any e > 0.      In an attempt to fix the record, the present article combines the problem motivation, models, and experimental results sections from the original incorrect article with the new hardness result. We would like readers to only cite and use this version (which will remain an unpublished note) instead of the incorrect conference version."
2549227,14133,235,Automatic identification of cause-effect relations in tamil using CRFs,2011,"We present our work on automatic identification of cause-effect relations in a given Tamil text. Based on the analysis of causal constructions in Tamil, we identified a set of causal markers for Tamil and arrived at certain features used to develop our language model. We manually annotated a Tamil corpus of 8648 sentences for cause-effect relations. With this corpus, we developed the model for identifying causal relations using the machine learning technique, Conditional Random Fields (CRFs). We performed experiments and the results are encouraging. We performed an error analysis of the results and found that the errors can be attributed to some very interesting structural interdependencies between closely occurring causal relations. After comparing these structures in Tamil and English, we claim that at discourse level, the complexity of structural interdependencies between causal relations is more complex in Tamil than in English due to the free word order nature of Tamil."
2347641,14133,422,Improving quality control by early prediction of manufacturing outcomes,2013,"We describe methods for continual prediction of manufactured product quality prior to final testing. In our most expansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microprocessors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collection of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective supply chain management actions. Developing statistical learning models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action opportunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than average. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal operation."
2388684,14133,422,Introduction to the special section on educational data mining,2012,"Educational Data Mining (EDM) is an emerging multidisciplinary research area, in which methods and techniques for exploring data originating from various educational information systems have been developed. EDM is both a learning science, as well as a rich application area for data mining, due to the growing availability of educational data. EDM contributes to the study of how students learn, and the settings in which they learn. It enables data-driven decision making for improving the current educational practice and learning material. We present a brief overview of EDM and introduce four selected EDM papers representing a crosscut of different application areas for data mining in education."
381390,14133,256,Neuronal Synchrony in Complex-Valued Deep Networks,2014,"Abstract: Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations. #R##N#We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks."
2589088,14133,235,Tibetan Base Noun Phrase Identification Framework Based on Chinese-Tibetan Sentence Aligned Corpus,2012,"This paper presents an identification framework for extracting Tibetan base noun phrase (NP). The framework includes two phases. In the first phase, Chinese base NPs are extracted from all Chinese sentences in the sentence aligned Chinese-Tibetan corpus using Stanford Chinese parser. In the second phase, the Tibetan translations of those Chinese NPs are identified using four different methods, that is, word alignment, iterative re-evaluation, dictionary and word alignment, and sequence intersection method. We implemented and tested these methods on Chinese-Tibetan sentence aligned unlabelled corpus without Tibetan POS tagger and Treebank. The experimental results demonstrate these methods can get satisfactory results, and the best performance with 0.5283 precision is got using sequence intersection identification method. The identification framework can also be extended to extract Tibetan verb phrase."
2002622,14133,256,On Fast Dropout and its Applicability to Recurrent Networks,2014,"Abstract: Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overfitting has seen considerably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. The derivatives of that regularizer are exclusively based on the training error signal. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets."
2420835,14133,422,Time-varying learning and content analytics via sparse factor analysis,2014,"We propose SPARFA-Trace, a new machine learning-based framework for time-varying learning and content analytics for educational applications. We develop a novel message passing-based, blind, approximate Kalman filter for sparse factor analysis (SPARFA) that jointly traces learner concept knowledge over time, analyzes learner concept knowledge state transitions (induced by interacting with learning resources, such as textbook sections, lecture videos, etc., or the forgetting effect), and estimates the content organization and difficulty of the questions in assessments. These quantities are estimated solely from binary-valued (correct/incorrect) graded learner response data and the specific actions each learner performs (e.g., answering a question or studying a learning resource) at each time instant. Experimental results on two online course datasets demonstrate that SPARFA-Trace is capable of tracing each learner's concept knowledge evolution over time, analyzing the quality and content organization of learning resources, and estimating the question--concept associations and the question difficulties. Moreover, we show that SPARFA-Trace achieves comparable or better performance in predicting unobserved learner responses compared to existing collaborative filtering and knowledge tracing methods."
82136,14133,374,Who wrote this code? identifying the authors of program binaries,2011,"Program authorship attribution--identifying a programmer based on stylistic characteristics of code--has practical implications for detecting software theft, digital forensics, and malware analysis. Authorship attribution is challenging in these domains where usually only binary code is available; existing source code-based approaches to attribution have left unclear whether and to what extent programmer style survives the compilation process. Casting authorship attribution as a machine learning problem, we present a novel program representation and techniques that automatically detect the stylistic features of binary code. We apply these techniques to two attribution problems: identifying the precise author of a program, and finding stylistic similarities between programs by unknown authors. Our experiments provide strong evidence that programmer style is preserved in program binaries."
1509483,14133,422,Predicting student risks through longitudinal analysis,2014,"Poor academic performance in K-12 is often a precursor to unsatisfactory educational outcomes such as dropout, which are associated with significant personal and social costs. Hence, it is important to be able to predict students at risk of poor performance, so that the right personalized intervention plans can be initiated. In this paper, we report on a large-scale study to identify students at risk of not meeting acceptable levels of performance in one state-level and one national standardized assessment in Grade 8 of a major US school district. An important highlight of our study is its scale - both in terms of the number of students included, the number of years and the number of features, which provide a very solid grounding to the research. We report on our experience with handling the scale and complexity of data, and on the relative performance of various machine learning techniques we used for building predictive models. Our results demonstrate that it is possible to predict students at-risk of poor assessment performance with a high degree of accuracy, and to do so well in advance. These insights can be used to pro-actively initiate personalized intervention programs and improve the chances of student success."
2067146,14133,235,Prior-informed Distant Supervision for Temporal Evidence Classification,2014,"Temporal evidence classification, i.e., finding associations between temporal expressions and relations expressed in text, is an important part of temporal relation extraction. To capture the variations found in this setting, we employ a distant supervision approach, modeling the task as multi-class text classification. There are two main challenges with distant supervision: (1) noise generated by incorrect heuristic labeling, and (2) distribution mismatch between the target and distant supervision examples. We are particularly interested in addressing the second problem and propose a sampling approach to handle the distribution mismatch. Our prior-informed distant supervision approach improves over basic distant supervision and outperforms a purely supervised approach when evaluated on TAC-KBP data, both on classification and end-to-end metrics."
662501,14133,256,A Generative Product-of-Filters Model of Audio,2014,"Abstract: We propose the product-of-filters (PoF) model, a generative model that decomposes audio spectra as sparse linear combinations of filters in the log-spectral domain. PoF makes similar assumptions to those used in the classic homomorphic filtering approach to signal processing, but replaces hand-designed decompositions built of basic signal processing operations with a learned decomposition based on statistical inference. This paper formulates the PoF model and derives a mean-field method for posterior inference and a variational EM algorithm to estimate the model's free parameters. We demonstrate PoF's potential for audio processing on a bandwidth expansion task, and show that PoF can serve as an effective unsupervised feature extractor for a speaker identification task."
1622802,14133,65,Towards evaluating recovery strategies for situated grounding problems in human-robot dialogue,2013,"Robots can use information from their surroundings to improve spoken language communication with people. Even when speech recognition is correct, robots face challenges when interpreting human instructions. These situated grounding problems include referential ambiguities and impossible-to-execute instructions. We present an approach to resolving situated grounding problems through spoken dialogue recovery strategies that robots can invoke to repair these problems. We describe a method for evaluating these strategies in human-robot navigation scenarios."
290946,14133,256,Barnes-Hut-SNE,2013,"Abstract: The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects."
2325991,14133,65,Following route graphs in urban environments,2011,"In this paper, an approach is presented that allows a robot to navigate in an urban environment by following natural language route instructions. In this situation, neither maps nor GPS information are available to the robot thus it has to rely solely on the human-given route description and the observations from its sensors. An architecture for solving problems such as navigation on the sidewalk, street direction inference, and environment labeling that arise in this situation is presented. Our initial experiments indicate that the proposed methods enable a robot to safely navigate in urban environments by following abstract route descriptions and reach previously unknown points in a city."
1188950,14133,422,Data-driven multi-touch attribution models,2011,"In digital advertising, attribution is the problem of assigning credit to one or more advertisements for driving the user to the desirable actions such as making a purchase. Rather than giving all the credit to the last ad a user sees, multi-touch attribution allows more than one ads to get the credit based on their corresponding contributions. Multi-touch attribution is one of the most important problems in digital advertising, especially when multiple media channels, such as search, display, social, mobile and video are involved. Due to the lack of statistical framework and a viable modeling approach, true data-driven methodology does not exist today in the industry. While predictive modeling has been thoroughly researched in recent years in the digital advertising domain, the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification. Traditional classification models fail to achieve those goals.   In this paper, we first propose a bivariate metric, one measures the variability of the estimate, and the other measures the accuracy of classifying the positive and negative users. We then develop a bagged logistic regression model, which we show achieves a comparable classification accuracy as a usual logistic regression, but a much more stable estimate of individual advertising channel contributions. We also propose an intuitive and simple probabilistic model to directly quantify the attribution of different advertising channels. We then apply both the bagged logistic model and the probabilistic model to a real-world data set from a multi-channel advertising campaign for a well-known consumer software and services brand. The two models produce consistent general conclusions and thus offer useful cross-validation. The results of our attribution models also shed several important insights that have been validated by the advertising team.   We have implemented the probabilistic model in the production advertising platform of the first author's company, and plan to implement the bagged logistic regression in the next product release. We believe availability of such data-driven multi-touch attribution metric and models is a break-through in the digital advertising industry."
1427575,14133,422,eTrust: understanding trust evolution in an online world,2012,"Most existing research about online trust assumes static trust relations between users. As we are informed by social sciences, trust evolves as humans interact. Little work exists studying trust evolution in an online world. Researching online trust evolution faces unique challenges because more often than not, available data is from passive observation. In this paper, we leverage social science theories to develop a methodology that enables the study of online trust evolution. In particular, we propose a framework of evolution trust, eTrust, which exploits the dynamics of user preferences in the context of online product review. We present technical details about modeling trust evolution, and perform experiments to show how the exploitation of trust evolution can help improve the performance of online applications such as rating and trust prediction."
407854,14133,235,Ontology-Driven construction of domain corpus with frame semantics annotations,2012,"Semantic Role Labeling plays a key role in many text mining applications. The development of SRL systems for the biomedical domain is frustrated by the lack of large domain specific corpora that are labeled with semantic roles. In this paper we proposed a method for building corpus that are labeled with semantic roles for the domain of biomedicine. The method is based on the theory of frame semantics, and uses domain knowledge provided by ontologies. By using the method, we have built a corpus for transport events strictly following the domain knowledge provided by GO biological process ontology. We compared one of our frames to a BioFrameNet frame. We also examined the gaps between the semantic classification of the target words in this domain-specific corpus and in FrameNet and PropBank/VerbNet data. The successful corpus construction demonstrates that ontologies, as a formal representation of domain knowledge, can instruct us and ease all the tasks in building this kind of corpus. Furthermore, ontological domain knowledge leads to well-defined semantics exposed on the corpus, which will be very valuable in text mining applications."
1685568,14133,422,Next challenges for adaptive learning systems,2012,"Learning from evolving streaming data has become a 'hot' research topic in the last decade and many adaptive learning algorithms have been developed. This research was stimulated by rapidly growing amounts of industrial, transactional, sensor and other business data that arrives in real time and needs to be mined in real time. Under such circumstances, constant manual adjustment of models is in-efficient and with increasing amounts of data is becoming infeasible. Nevertheless, adaptive learning models are still rarely employed in business applications in practice. In the light of rapidly growing structurally rich 'big data', new generation of parallel computing solutions and cloud computing services as well as recent advances in portable computing devices, this article aims to identify the current key research directions to be taken to bring the adaptive learning closer to application needs. We identify six forthcoming challenges in designing and building adaptive learning (pre-diction) systems: making adaptive systems scalable, dealing with realistic data, improving usability and trust, integrat-ing expert knowledge, taking into account various application needs, and moving from adaptive algorithms towards adaptive tools. Those challenges are critical for the evolving stream settings, as the process of model building needs to be fully automated and continuous."
727939,14133,507,So who won?: dynamic max discovery with the crowd,2012,"We consider a crowdsourcing database system that may cleanse, populate, or filter its data by using human workers. Just like a conventional DB system, such a crowdsourcing DB system requires data manipulation functions such as select, aggregate, maximum, average, and so on, except that now it must rely on human operators (that for example compare two objects) with very different latency, cost and accuracy characteristics. In this paper, we focus on one such function,  maximum , that finds the highest ranked object or tuple in a set. In particularm we study two problems: given a set of votes (pairwise comparisons among objects), how do we select the maximum? And how do we improve our estimate by requesting additional votes? We show that in a crowdsourcing DB system, the optimal solution to both problems is NP-Hard. We then provide heuristic functions to select the maximum given evidence, and to select additional votes. We experimentally evaluate our functions to highlight their strengths and weaknesses."
2229041,14133,422,Incorporating SAT solvers into hierarchical clustering algorithms: an efficient and flexible approach,2011,"The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints."
1285618,14133,422,Improving predictions using aggregate information,2011,"In domains such as consumer products or manufacturing amongst others, we have problems that warrant the prediction of a continuous target. Besides the usual set of explanatory attributes we may also have exact (or approximate) estimates of  aggregated targets , which are the sums of disjoint sets of individual targets that we are trying to predict. Hence, the question now becomes can we use these aggregated targets, which are a coarser piece of information, to improve the quality of predictions of the individual targets? In this paper, we provide a simple yet provable way of accomplishing this. In particular, given predictions from any regression model of the target on the test data, we elucidate a provable method for improving these predictions in terms of mean squared error, given exact (or accurate enough) information of the aggregated targets. These estimates of the aggregated targets may be readily available or obtained -- through multilevel regression -- at different levels of granularity. Based on the proof of our method we suggest a criterion for choosing the appropriate level. Moreover, in addition to estimates of the aggregated targets, if we have exact (or approximate) estimates of the mean and variance of the target distribution, then based on our general strategy we provide an optimal way of incorporating this information so as to further improve the quality of predictions of the individual targets. We then validate the results and our claims by conducting experiments on synthetic and real industrial data obtained from diverse domains."
1230104,14133,422,Real-time bidding algorithms for performance-based display ad allocation,2011,"We describe a real-time bidding algorithm for performance-based display ad allocation. A central issue in performance display advertising is matching campaigns to ad impressions, which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability. The current practice is to solve the optimization problem offline at a tractable level of impression granularity (e.g., the page level), and to serve ads online based on the precomputed static delivery scheme. Although this offline approach takes a global view to achieve optimality, it fails to scale to ad allocation at the individual impression level. Therefore, we propose a real-time bidding algorithm that enables fine-grained impression valuation (e.g., targeting users with real-time conversion data), and adjusts value-based bids according to real-time constraint snapshots (e.g., budget consumption levels). Theoretically, we show that under a linear programming (LP) primal-dual formulation, the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input. In other words, the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have. Empirically, we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace: one adjusts bids against real-time constraint satisfaction levels using control-theoretic methods, and the other adjusts bids also based on the statistically modeled historical bidding landscape. Finally, we show experimental results with real-world ad delivery data that support our theoretical conclusions."
1403605,14133,422,TM-LDA: efficient online modeling of latent topic transitions in social media,2012,"Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints."
1242958,14133,422,Two approaches to understanding when constraints help clustering,2012,"Most algorithm work in data mining focuses on designing algorithms to address a learning problem. Here we focus our attention on designing algorithms to determine the ease or difficulty of a problem instance. The area of clustering under constraints has recently received much attention in the data mining community. We can view the constraints as restricting (either directly or indirectly) the search space of a clustering algorithm to just feasible clusterings. However, to our knowledge no work explores methods to count the feasible clusterings or other measures of difficulty nor the importance of these measures. We present two approaches to efficiently characterize the difficulty of satisfying must-link (ML) and cannot-link (CL) constraints: calculating the fractional chromatic polynomial of the constraint graph using LP and approximately counting the number of feasible clusterings using MCMC samplers. We show that these measures are correlated to the classical performance measures of constrained clustering algorithms. From these insights and our algorithms we construct new methods of generating and pruning constraints and empirically demonstrate their usefulness."
1554899,14133,422,Towards scalable critical alert mining,2014,"Performance monitor software for data centers typically generates a great number of alert sequences. These alert sequences indicate abnormal network events. Given a set of observed alert sequences, it is important to identify the most critical alerts that are potentially the causes of others. While the need for mining critical alerts over large scale alert sequences is evident, most alert analysis techniques stop at modeling and mining the causal relations among the alerts.   This paper studies the critical alert mining problem: Given a set of alert sequences, we aim to find a set of k critical alerts such that the number of alerts potentially triggered by them is maximized. We show that the problem is intractable; therefore, we resort to approximation and heuristic algorithms. First, we develop an approximation algorithm that obtains a near-optimal alert set in quadratic time, and propose pruning techniques to improve its runtime performance. Moreover, we show a faster approximation exists, when the alerts follow certain causal structure. Second, we propose two fast heuristic algorithms based on tree sampling techniques. On real-life data, these algorithms identify a critical alert from up to 270,000 mined causal relations in 5 seconds; meanwhile, they preserve more than 80% of solution quality, and are up to 5,000 times faster than their approximation counterparts."
2513034,14133,422,Display advertising impact: search lift and social influence,2011,"We study the impact of display advertising on user search behavior using a field experiment. In such an experiment, the treatment group users are exposed to some display advertising campaign, while the control group users are not. During the campaign and the post-campaign period we monitor the user search queries and we label them as relevant or irrelevant to the campaign using techniques that leverage the bipartite query-URL click graph. Our results indicate that users who are exposed to the advertising campaign submit 5% to 25% more queries that are relevant to it compared to the unexposed users.   Using the social graph of the experiment users, we also explore how users are affected by their friends who are exposed to ads. Our results indicate that a user with exposed friends is more likely to submit queries relevant to the campaign, as compared to a user without exposed friends. The result is surprising given that the display advertising campaign that we study does not include any incentive for social action, e.g., discount for recommending friends."
656254,14133,256,Discrete Restricted Boltzmann Machines,2013,"Abstract: We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naive Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension."
1372376,14133,422,RecMax: exploiting recommender systems for fun and profit,2012,"In recent times, collaborative filtering based Recommender Systems (RS) have become extremely popular. While research in recommender systems has mostly focused on improving the accuracy of recommendations, in this paper, we look at the flip side of a RS. That is, instead of improving existing recommender algorithms, we ask whether we can use an existing operational RS to launch a targeted marketing campaign. To this end, we propose a novel problem called  RecMax  that aims to select a set of seed users for a marketing campaign for a new product, such that if they endorse the product by providing relatively high ratings, the number of other users to whom the product is recommended by the underlying RS algorithm is maximum. We motivate  RecMax  with real world applications. We show that seeding can make a substantial difference, if done carefully. We prove that  RecMax  is not only NP-hard to solve optimally, it is NP-hard to even approximate within any reasonable factor. Given this hardness, we explore several natural heuristics on 3 real world datasets - Movielens, Yahoo! Music and Jester Joke and report our findings. We show that even though  RecMax  is hard to approximate, simple natural heuristics may provide impressive gains, for targeted marketing using RS."
1601911,14133,422,2D-interval predictions for time series,2011,"Research on time series forecasting is mostly focused on point predictions - models are obtained to estimate the expected value of the target variable for a certain point in future. However, for several relevant applications this type of forecasts has limited utility (e.g. costumer wallet value estimation, wind and electricity power production, control of water quality, etc.). For these domains it is frequently more important to be able to forecast a range of plausible future values of the target variable. A typical example is wind power production, where it is of high relevance to predict the future wind variability in order to ensure that supply and demand are balanced. This type of predictions will allow timely actions to be taken in order to cope with the expected values of the target variable on a certain future time horizon. In this paper we study this type of predictions - the prediction of a range of expected values for a future time interval. We describe some possible approaches to this task and propose an alternative procedure that our extensive experiments on both artificial and real world domains show to have clear advantages."
976039,14133,422,Multimedia features for click prediction of new ads in display advertising,2012,"Non-guaranteed display advertising (NGD) is a multi-billion dollar business that has been growing rapidly in recent years. Advertisers in NGD sell a large portion of their ad campaigns using performance dependent pricing models such as cost-per-click (CPC) and cost-per-action (CPA). An accurate prediction of the probability that users click on ads is a crucial task in NGD advertising because this value is required to compute the expected revenue. State-of-the-art prediction algorithms rely heavily on historical information collected for advertisers, users and publishers. Click prediction of new ads in the system is a challenging task due to the lack of such historical data. The objective of this paper is to mitigate this problem by integrating multimedia features extracted from display ads into the click prediction models. Multimedia features can help us capture the attractiveness of the ads with similar contents or aesthetics. In this paper we evaluate the use of numerous multimedia features (in addition to commonly used user, advertiser and publisher features) for the purposes of improving click prediction in ads with no history. We provide analytical results generated over billions of samples and demonstrate that adding multimedia features can significantly improve the accuracy of click prediction for new ads, compared to a state-of-the-art baseline model."
141537,14133,422,Hybrid- ε -greedy for mobile context-aware recommender system,2012,"The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid- e  -greedy algorithm. It also presents preliminary results by comparing the hybrid- e  -greedy and the standard  e  -greedy algorithm."
2447535,14133,422,Multi-label relational neighbor classification using social context features,2013,"Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers.   In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances' social features, which are in turn extracted from the network topology. This class-propagation probability captures the node's intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors' class labels in the collective inference procedure. Experiments on several real-world datasets demonstrate that our proposed classifier boosts classification performance over common benchmarks on networked multi-label data."
1358537,14133,422,Large-scale distributed non-negative sparse coding and sparse dictionary learning,2012,"We consider the problem of building compact, unsupervised representations of large, high-dimensional, non-negative data using sparse coding and dictionary learning schemes, with an emphasis on executing the algorithm in a Map-Reduce environment. The proposed algorithms may be seen as parallel optimization procedures for constructing sparse non-negative factorizations of large, sparse matrices. Our approach alternates between a parallel sparse coding phase implemented using greedy or convex ( l  1 ) regularized risk minimization procedures, and a sequential dictionary learning phase where we solve a set of  l  0  optimization problems exactly. These two-fold sparsity constraints lead to better statistical performance on text analysis tasks and at the same time make it possible to implement each iteration in a single Map-Reduce job. We detail our implementations and optimizations that lead to the ability to factor matrices with more than 100 million rows and billions of non-zero entries in just a few hours on a small commodity cluster."
1990992,14133,422,Locally-scaled spectral clustering using empty region graphs,2012,"This paper introduces a new method for estimating the local neighborhood and scale of data points to improve the robustness of spectral clustering algorithms. We employ a subset of empty region graphs - the β-skeleton - and non-linear diffusion to define a locally-adapted affinity matrix, which, as we demonstrate, provides higher quality clustering than conventional approaches based on κ nearest neighbors or global scale parameters. Moreover, we show that the clustering quality is far less sensitive to the choice of β and other algorithm parameters, and to transformations such as geometric distortion and random perturbation. We summarize the results of an empirical study that applies our method to a number of 2D synthetic data sets, consisting of clusters of arbitrary shape and scale, and to real multi-dimensional classification examples from benchmarks, including image segmentation."
2172497,14133,422,Active learning for sparse bayesian multilabel classification,2014,"We study the problem of active learning for multilabel classification. We focus on the real-world scenario where the average number of positive (relevant) labels per data point is small leading to positive label sparsity. Carrying out mutual information based near-optimal active learning in this setting is a challenging task since the computational complexity involved is exponential in the total number of labels. We propose a novel inference algorithm for the sparse Bayesian multilabel model of [17]. The benefit of this alternate inference scheme is that it enables a natural approximation of the mutual information objective. We prove that the approximation leads to an identical solution to the exact optimization problem but at a fraction of the optimization cost. This allows us to carry out efficient, non-myopic, and near-optimal active learning for sparse multilabel classification. Extensive experiments reveal the effectiveness of the method."
45232,14133,422,Incremental Constrained Clustering: A Decision Theoretic Approach,2013,Typical constrained clustering algorithms incorporate a set of must-link and cannot-link constraints into the clustering process. These instance level constraints specify relationships between pairs of data items and are generally derived by a domain expert. Generating these constraints is considered as a cumbersome and expensive task.#R##N##R##N#In this paper we describe an incremental constrained clustering framework to discover clusters using a decision theoretic approach. Our framework is novel since we provide an overall evaluation of the clustering in terms of quality in decision making and use this evaluation to generate instance level constraints. We do not assume any domain knowledge to start with. We show empirical validation of this approach on several test domains and show that we achieve better performance than a feature selection based approach.
677253,14133,256,Auto-Encoding Variational Bayes,2014,"Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
601316,14133,22051,Generalized Optimization Framework for Graph-based Semi-supervised Learning,2011,"Theme: NetworksandTelecommunicationsNetworks,SystemsandServices,DistributedComputingEquipes-ProjetsMaestroRapportderecherche n° 7774—October2011—20pagesAbstract: Wedevelopageneralizedoptimizationframeworkforgraph-basedsemi-supervisedlearning. TheframeworkgivesasparticularcasestheStandardLaplacian,NormalizedLaplacianandPageRankbasedmethods. Wehavealsoprovided new probabilistic interpretation based on random walks and charac-terized the limiting behaviour of the methods. The random walk based inter-pretationallowsustoexplaindiﬀerencesbetweentheperformancesofmethodswithdiﬀerentsmoothingkernels. ItappearsthatthePageRankbasedmethodis robust with respect to the choice of the regularization parameter and thelabelled data. We illustrate our theoretical results with two realistic datasets,characterizingdiﬀerentchallenges: LesMiserablescharacterssocialnetworkandWikipedia hyper-link graph. The graph-based semi-supervised learning classi-ﬁestheWikipediaarticleswithverygoodprecisionandperfectrecallemployingonlytheinformationaboutthehyper-textlinks.Key-words: Semi-supervisedLearning,PageRank,RandomWalkonGraphs,WikipediaAutomaticArticleClassiﬁcation"
678929,14133,344,Complexity of Word Collocation Networks: A Preliminary Structural Analysis,2014,"In this paper, we explore complex network properties of word collocation networks (Ferret, 2002) from four different genres. Each document of a particular genre was converted into a network of words with word collocations as edges. We analyzed graphically and statistically how the global properties of these networks varied across different genres, and among different network types within the same genre. Our results indicate that the distributions of network properties are visually similar but statistically apart across different genres, and interesting variations emerge when we consider different network types within a single genre. We further investigate how the global properties change as we add more and more collocation edges to the graph of one particular genre, and observe that except for the number of vertices and the size of the largest connected component, network properties change in phases, via jumps and drops."
1467111,14133,65,Designing robot behavior in conversations based on contemporary colloquial theatre theory,2014,"Due to human sensitivity for humanlike objects, designing human-like behavior for a robot that interacts with humans remains a central issue in the HRI field. This issue is formidable because human-likeness is an important factor for better interaction since it can easily convey negative impressions if the design is not perfect, as in the uncanny valley phenomena. This paper addresses this issue with a novel approach that utilizes implicit know-how for performing on stage and is dedicated to represent human beings. Contemporary colloquial theatre theory (CCTT), which is a method of directing plays, is appropriate for this purpose since its reality-oriented instructions are directly applicable for improving robot behavior. In this paper, we report case studies on performing a play involving both humans and a robot. The play in our study was evaluated in public performances in Japan. We report detailed analysis of comparable short plays on human-robot interaction or human-human-robot interaction. Our analysis implies that utterance and motion timings should be tuned depending on the situation. Future work will use a motion capture system to get more precise data and more useful knowledge."
2627544,14133,235,A PAC-Bayesian Approach to Minimum Perplexity Language Modeling,2014,"Despite the overwhelming use of statistical language models in speech recognition, machine translation, and several other domains, few high probability guarantees exist on their generalization error. In this paper, we bound the test set perplexity of two popular language models ‐ the n-gram model and class-basedn-grams ‐ using PAC-Bayesian theorems for unsupervised learning. We extend the bound to sequence clustering, wherein classes represent longer context such as phrases. The new bound is dominated by the maximum number of sequences represented by each cluster, which is polynomial in the vocabulary size. We show that we can still encourage small sample generalization by sparsifying the cluster assignment probabilities. We incorporate our bound into an efficient HMM-based sequence clustering algorithm and validate the theory with empirical results on the resource management corpus."
1146299,14133,65,Motion learning from observation using Affinity Propagation clustering,2013,"During robot imitation learning, a key problem when observing the motions of a demonstrator is the modeling and recognition of movement prototypes. This paper proposes using Affinity Propagation (AP) to cluster motions modeled using either Dynamic Movement Primitives (DMPs) or Hidden Markov Models (HMMs). The proposed AP clustering algorithm is simple and efficient, provides robust results and automatically identifies representative exemplars for each motion group, leading to a minimal representation of the observations that can also be used to generate motions. In experiments using videos and motion capture data of human demonstrations, it is shown that the weight parameters of the DMP model can be used as features for motion recognition and the proposed method can distinguish between different (coarse distinction) or similar (fine distinction) motion groups."
1880998,14133,422,Unbiased online active learning in data streams,2011,"Unlabeled samples can be intelligently selected for labeling to minimize classification error. In many real-world applications, a large number of unlabeled samples arrive in a streaming manner, making it impossible to maintain all the data in a candidate pool. In this work, we focus on binary classification problems and study selective labeling in data streams where a decision is required on each sample sequentially. We consider the unbiasedness property in the sampling process, and design optimal instrumental distributions to minimize the variance in the stochastic process. Meanwhile, Bayesian linear classifiers with weighted maximum likelihood are optimized online to estimate parameters. In empirical evaluation, we collect a data stream of user-generated comments on a commercial news portal in 30 consecutive days, and carry out offline evaluation to compare various sampling strategies, including unbiased active learning, biased variants, and random sampling. Experimental results verify the usefulness of online active learning, especially in the non-stationary situation with concept drift."
2128326,14133,235,Discriminative Boosting from Dictionary and Raw Text -- A Novel Approach to Build A Chinese Word Segmenter,2012,"Chinese word segmentation (CWS) is a basic and important task for Chinese information processing. Standard approaches to CWS treat it as a sequence labelling task. Without manually annotated corpora, these approaches are ineffective. When a dictionary is available, dictionary maximum matching (DMM) is a good alternative. However, its performance is far from perfect due to the poor ability on out-of-vocabulary (OOV) words recognition. In this paper, we propose a novel approach that integrates the advantages of discriminative training and DMM, to build a high quality word segmenter with only a dictionary and a raw text. Experiments in CWS on different domains show that, compared with DMM, our approach brings significant improvements in both the news domain and the Chinese medicine patent domain, with error reductions of 21.50% and 13.66%, respectively. Furthermore, our approach achieves recall rate increments of OOV words by 42.54% and 23.72%, respectively in both domains."
106000,14133,256,Zero-Shot Learning by Convex Combination of Semantic Embeddings,2014,"Abstract: Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional \nway{} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing \nway{} image classifier and a semantic word embedding model, which contains the $\n$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task."
2046023,14133,65,Human-like action segmentation for option learning,2011,"Robots learning interactively with a human partner has several open questions, one of which is increasing the efficiency of learning. One approach to this problem in the Reinforcement Learning domain is to use options, temporally extended actions, instead of primitive actions. In this paper, we aim to develop a robot system that can discriminate meaningful options from observations of human use of low-level primitive actions. Our approach is inspired by psychological findings about human action parsing, which posits that we attend to low-level statistical regularities to determine action boundary choices. We implement a human-like action segmentation system for automatic option discovery and evaluate our approach and show that option-based learning converges to the optimal solutions faster compared with primitive-action-based learning."
2645318,14133,20552,Distribution over Beliefs for Memory Bounded Dec-POMDP Planning,2012,We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuris- tic estimation of the prior probability of be- liefs to choose a bounded number of policy trees: this choice is formulated as a combina- torial optimisation problem minimising the error induced by pruning.
2567729,14133,22113,Adapting surface sketch recognition techniques for surfaceless sketches,2013,"Researchers have made significant strides in developing recognition techniques for surface sketches, with realized and potential applications to motivate extending these techniques towards analogous surfaceless sketches. Yet surface sketch recognition techniques remain largely untested in surfaceless environments and are still highly constrained for related surfaceless gesture recognition techniques. The focus of the research is to investigate the performance of surface sketch recognition techniques in more challenging surfaceless environments, with the aim of addressing existing limitations through improved surfaceless sketch recognition techniques."
2617240,14133,20332,A multi-agent control architecture for a rescue robot,2012,"Cognitive architectures investigate the components and interactions neccessary for construction of an intelligent system. Despite much progress and theory, implementations of architectures are rare. This research presents a novel cognitive architecture grounded in the design of a control system for an autonomous rescue robot. Experiments are conducted in high-fidelity 3D simulation of a rescue environment based on NISTs RoboCup Rescue."
2689329,14133,20332,Augmenting Weight Constraints with Complex Preferences,2011,"Preference-based reasoning is a form of commonsense reasoning that makes many problems easier to express and sometimes more likely to have a solution. We present an approach to introduce preferences in the weight constraint construct, which is a very useful programming construct widely adopted in Answer Set Programming (ASP). We show the usefulness of the proposed extension, and we outline how to accordingly extend the ASP semantics."
692384,14133,20332,"An experimentally efficient method for (MSS,CoMSS) partitioning",2014,"The concepts of MSS (Maximal Satisfiable Subset) and CoMSS (also called Minimal Correction Subset) play a key role in many A.I. approaches and techniques. In this paper, a novel algorithm for partitioning a Boolean CNF formula into one MSS and the corresponding CoMSS is introduced. Extensive empirical evaluation shows that it is more robust and more efficient on most instances than currently available techniques."
20178,14133,20358,Quality assurance in crowdsourcing via matrix factorization based task routing,2014,"We investigate a method of crowdsourced task routing based on matrix factorization. From a preliminary analysis of a real crowdsourced data, we begin an exploration of how to route crowdsourcing task via Matrix factorization (MF) which efficiently estimate missing values in a worker-task matrix. Our preliminary results show the benefits of task routing over random assignment, the strength of probabilistic MF over baseline methods."
1905539,14133,11321,A Hybrid Algorithm for Convex Semidefinite Optimization,2012,"We present a hybrid algorithm for optimizing a convex, smooth function over the cone of positive semidenite matrices. Our algorithm converges to the global optimal solution and can be used to solve general largescale semidenite programs and hence can be readily applied to a variety of machine learning problems. We show experimental results on three machine learning problems. Our approach outperforms state-of-the-art algorithms."
300348,14133,20552,Strategies for Generating Micro Explanations for Bayesian Belief Networks,2013,"Bayesian Belief Networks have been largely overlooked by Expert Systems practitioners on the grounds that they do not correspond to the human inference mechanism. In this paper, we introduce an explanation mechanism designed to generate intuitive yet probabilistically sound explanations of inferences drawn by a Bayesian Belief Network. In particular, our mechanism accounts for the results obtained due to changes in the causal and the evidential support of a node."
2448709,14133,8960,Scalable Non-linear Learning with Adaptive Polynomial Expansions,2014,"Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines."
2650852,14133,20358,On the evolution of social groups during coffee breaks,2014,"This paper focuses on the analysis of group evolution events in networks of face-to-face proximity. First, we analyze statistical properties of group evolution, e.g., individual activity and typical group sizes. Furthermore, we define a set of specific group evolution events. We analyze these using real-world data collected at the LWA 2010 conference using the Conferator system, and discuss patterns according to different phases of the conference."
2169020,14133,8960,Online Regret Bounds for Undiscounted Continuous Reinforcement Learning,2012,"We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities."
2614501,14133,20332,Towards joint inference for complex ontology matching,2013,"In this paper, we show how to model the matching problem as a problem of joint inference. In opposite to existing approaches, we distinguish between the layer of labels and the layer of concepts and properties. Entities from both layers appear as first class citizens in our model. We present an example and explain the benefits of our approach. Moreover, we argue that our approach can be extended to generate correspondences involving complex concept descriptions."
440943,14133,20552,A Heuristic Bayesian Approach to Knowledge Acquisition: Application to Analysis of Tissue-Type Plasminogen Activator,2013,"This paper describes a heuristic Bayesian method for computing probability distributions from experimental data, based upon the multivariate normal form of the influence diagram. An example illustrates its use in medical technology assessment. This approach facilitates the integration of results from different studies, and permits a medical expert to make proper assessments without considerable statistical training."
145896,14133,20332,Grounding natural language references to unvisited and hypothetical locations,2013,"While much research exists on resolving spatial natural language. to known locations, little work deals with handling references to unknown locations. In this paper we introduce and evaluate algorithms integrated into a cognitive architecture which allow an agent to learn about its environment while resolving references to both known and unknown locations. We also describe how multiple components in the architecture jointly facilitate these capabilities."
392581,14133,20552,Super-Samples from Kernel Herding,2012,We extend the herding algorithm to continuous spaces by using the kernel trick. The resulting kernel herding algorithm is an infinite memory deterministic process that learns to approximate a PDF with a collection of samples. We show that kernel herding decreases the error of expectations of functions in the Hilbert space at a rate O(1/T) which is much faster than the usual O(1/pT) for iid random samples. We illustrate kernel herding by approximating Bayesian predictive distributions.
31603,14133,11187,Basic analysis of digital spike maps,2012,"This paper studies digital spike maps that can generate various periodic spike-trains. In order to analyze the maps, we present a simple analysis algorithm to calculate basic feature quantities. We then analyze a typical example of the map given by discretizing the bifurcating neuron. Applying the algorithm to the example, we demonstrate complex dynamics and give basic classification of the dynamics."
2120919,14133,8960,t-divergence Based Approximate Inference,2011,"Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by defining a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student's t-prior."
137624,14133,11187,A new distance for probability measures based on the estimation of level sets,2012,"In this paper we propose to consider Probability Measures (PM) as generalized functions belonging to some functional space endowed with an inner product. This approach allows to introduce a new family of distances for PMs. We propose a particular (non parametric) metric for PMs belonging to this class, based on the estimation of density level sets. Some real and simulated data sets are used for a first exploration of its performance."
2563002,14133,20332,An object-oriented approach to reinforcement learning in an action game,2011,"In this work, we look at the challenge of learning in an action game, Infinite Mario. Learning to play an action game can be divided into two distinct but related problems, learning an object-related behavior and selecting a primitive action. We propose a framework that allows for the use of reinforcement learning for both of these problems. We present promising results in some instances of the game and identify some problems that might affect learning."
2576435,14133,8960,Beating SGD: Learning SVMs in Sublinear Time,2011,"We present an optimization approach for linear SVMs based on a stochastic primal-dual approach, where the primal step is akin to an importance-weighted SGD, and the dual step is a stochastic update on the importance weights. This yields an optimization method with a sublinear dependence on the training set size, and the first method for learning linear SVMs with runtime less then the size of the training set required for learning!"
2663793,14133,20332,A Spectrum of Linguistic Humor: Humor as Linguistic Design Space Construction Based on Meta-Linguistic Constraints,2012,"Nearly all humor derives from some element of surprise, discrepancy, unexpectedness, pattern-breaking, or anomalous inference. This speculative paper will briefly discuss aspects of linguistic humor, from simple wordplay including shm-reduplication, punning, simple language games, simple humorous linguistic and textual genres (limericks, Pig Latin, “Name Game”), to more complex genres that go beyond humor into linguistic and textual artistic innovation such as modernism (Joyce’s Ulysses, Finnegan’s Wake), post-modernism (Theater of the Absurd, Beckett, John Barth’s Giles Goat Boy, Chimera), OuLiPo (Ouvroir de Litterature Potentielle, “workshop for potential literature”) (constraint-based postmodernism), and science fiction (world creation).  In many cases, both humor and linguistic and  textual innovation can be considered to have notions of friction or pressure within a constrained communicative channel, and more generally as breaking a common linguistic pattern based on implicit or explicit meta-linguistic constraints. My speculative approach includes developing a linguistic spectrum (from phono-morphological to discourse components and beyond) to describe the range of techniques used for humor, but also a very early foray into a theoretical account of humor and creativity that focuses on creating an object-level design space (structure and model) that is guided by meta-linguistic constraints."
2768484,14133,22113,A method for evaluating and standardizing ontologies,2011,"The Open Biomedical Ontology (OBO) Foundry initiative is a collaborative effort for developing interoperable, science-based ontologies. The Basic Formal Ontology (BFO) serves as the upper ontology for the domain-level ontologies of OBO. BFO is an upper ontology of types as conceived by defenders of realism. Among the ontologies developed for OBO use, there are those which have been ratified, and those currently holding the status of candidate. To maintain consistency between ontologies, it is important to establish formal principled criteria that a candidate ontology must meet for ratification. Members of the OBO Foundry have expressed interest in using OntoClean to help construct BFO compliant domain ontologies. OntoClean is a system that decomposes the notion of sortal into criteria for deciding when subsumption can hold between classes. OntoClean primarily includes three components, based on the notions of Rigidity, Identity, and Unity. The methodology for integrating the OntoClean and BFO approaches to constructing consistent ontologies has been clarified by this dissertation. #R##N#A formal integration between BFO, the Relation Ontology of BFO (RO), and OntoClean is given. The informal aspects and differing formalisms within and between the theories are analyzed and integrated within the axioms and theorems of one first-order system put forth in the dissertation. To set the foundation for this work, the categorical units of type and property of BFO and OntoClean, respectively, are unified under class. The modal logic axioms that OntoClean's theory of Rigidity is expressed within are interpreted and reformulated in our system, where axioms connecting it with BFO's categorical unit type is given. Central to this work is the axiom that a type is a class that is Rigid, i.e., a class whose definition is fundamental to the existence of the members of the class. #R##N#A unity criterion for a class is a way in which certain parts of a member of the class are related such that they form the whole member. Our reformulation of this work focuses on the notion of the underlying unifying relation. As opposed to the informal approach taken by OntoClean, we express the notion that a class is unified under a relation as a meta-predicate defined by a definition schema. An identity criterion for a class is a test by which a member of the class can be re-identified. However as given in OntoClean, the notion of identity criteria is ontologically ambiguous. A formalization is given that provides a mapping to processes during which identity is confirmed. The reformulations of Unity and Identity are discussed in terms of Material Entity subtypes of BFO's theory. #R##N#The integration work and resulting formal system affords a theoretically sound ontological foundation. A method for evaluating and standardizing candidate OBO Foundry ontologies is developed atop this foundation, where the method focuses on BFO's integration with OntoClean's notion of Rigidity. The method is given as a decision tree algorithm that evaluates one class at a time as they are introduced into an ontology, asking a prospective modeler questions that map to specific integration axioms. #R##N#Finally, an implementation of the decision tree is provided in the form of an interactive Wizard plugin for the ontology editor Protege. In an iterative approach informal user testing was applied to improve the questions and error messages. The plugin serves to facilitate ontologists and subject matter experts in making explicit what is implicit in, or unclearly specified for, the classes they choose to introduce into an ontology. Ultimately though, the integration axioms serves as a software platform-independent foundation for future software designed to evaluate and standardize candidate domain ontologies for the OBO Foundry."
411286,14133,256,Factorized Topic Models,2013,"In this paper we present a modification to a latent topic model, which makes themodel exploit supervision to produce a factorized representation of the observeddata. The structured parameterization separately encodes variance that is sharedbetween classes from variance that is private to each class by the introduction of anew prior over the topic space. The approach allows for a more efficient inferenceand provides an intuitive interpretation of the data in terms of an informative signaltogether with structured noise. The factorized representation is shown to enhanceinference performance for image, text, and video classification."
2612006,14133,235,UIMA Ruta Workbench: Rule-based Text Annotation,2014,"UIMA Ruta is a rule-based system designed for information extraction tasks, but it is also applicable for many natural language processing use cases. This demonstration gives an overview of the UIMA Ruta Workbench, which provides a development environment and tooling for the rule language. It was developed to ease every step in engineering rule-based applications. In addition to the full-featured rule editor, the user is supported by explanation of the rule execution, introspection in results, automatic validation and rule induction. Furthermore, the demonstration covers the usage and combination of arbitrary components for natural language processing."
606691,14133,256,Intriguing properties of neural networks,2014,"Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. #R##N#First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. #R##N#Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input."
648420,14133,256,How to Construct Deep Recurrent Neural Networks,2014,"Abstract: In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs."
2581979,14133,344,Iterative Constrained Clustering for Subjectivity Word Sense Disambiguation,2014,"Subjectivity word sense disambiguation (SWSD) is a supervised and applicationspecific word sense disambiguation task disambiguating between subjective and objective senses of a word. Not surprisingly, SWSD suffers from the knowledge acquisition bottleneck. In this work, we use a “cluster and label” strategy to generate labeled data for SWSD semiautomatically. We define a new algorithm called Iterative Constrained Clustering (ICC) to improve the clustering purity and, as a result, the quality of the generated data. Our experiments show that the SWSD classifiers trained on the ICC generated data by requiring only 59% of the labels can achieve the same performance as the classifiers trained on the full dataset."
602437,14133,256,Network In Network,2014,"Abstract: We propose a novel deep network structure called Network In Network (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
2512188,14133,11187,A Markov random field approach to neural encoding and decoding,2011,"We introduce a new approach to neural encoding and decoding which makes use of sparse regression and Markov random fields. We show that interesting response functions were estimated from neuroimaging data acquired while a subject was watching checkerboard patterns and geometrical figures. Furthermore, we demonstrate that reconstructions of the original stimuli can be generated by loopy belief propagation in a Markov random field."
1249377,14133,23620,On coinductive equivalences for higher-order probabilistic functional programs,2014,"We study bisimulation and context equivalence in a probabilistic lambda-calculus. The contributions of this paper are threefold. Firstly we show a technique for proving congruence of probabilistic applicative bisimilarity. While the technique follows Howe's method, some of the technicalities are quite different, relying on non-trivial disentangling properties for sets of real numbers. Secondly we show that, while bisimilarity is in general strictly finer than context equivalence, coincidence between the two relations is attained on pure lambda-terms. The resulting equality is that induced by Levy-Longo trees, generally accepted as the finest extensional equivalence on pure lambda-terms under a lazy regime. Finally, we derive a coinductive characterisation of context equivalence on the whole probabilistic language, via an extension in which terms akin to distributions may appear in redex position. Another motivation for the extension is that its operational semantics allows us to experiment with a different congruence technique, namely that of logical bisimilarity."
600896,14133,235,What is being measured in an information graphic,2013,"Information graphics (such as bar charts and line graphs) are widely used in popular media. The majority of such non-pictorial graphics have the purpose of communicating a high-level message which is often not repeated in the text of the article. Thus, information graphics together with the textual segments contribute to the overall purpose of an article and cannot be ignored. Unfortunately, information graphics often do not label the dependent axis with a full descriptor of what is being measured. In order to realize the high-level message of an information graphic in natural language, a referring expression for the dependent axis must be generated. This task is complex in that the required referring expression often must be constructed by extracting and melding pieces of information from the textual content of the graphic. Our heuristic-based solution to this problem has been shown to produce reasonable text for simple bar charts. This paper presents the extensibility of that approach to other kinds of graphics, in particular to grouped bar charts and line graphs. We discuss the set of component texts contained in these two kinds of graphics, how the methodology for simple bar charts can be extended to these kinds, and the evaluation of the enhanced approach."
880702,14133,422,VOXSUP: a social engagement framework,2012,"Social media websites are currently central hubs on the Internet. Major online social media platforms are not only places for individual users to socialize but are increasingly more important as channels for companies to advertise, public figures to engage, etc. In order to optimize such advertising and engaging efforts, there is an emerging challenge for knowledge discovery on today's Internet. The goal of knowledge discovery is to understand the entire online social landscape instead of merely summarizing the statistics. To answer this challenge, we have created VOXSUP as a unified social engagement framework. Unlike most existing tools, VOXSUP not only aggregates and filters social data from the Internet, but also provides what we call Voxsupian Knowledge Discovery (VKD). VKD consists of an almost human-level understanding of social conversations at any level of granularity from a single comment sentiment to multi-lingual inter-platform user demographics. Here we describe the technologies that are crucial to VKD, and subsequently go beyond experimental verification and present case studies from our live VOXSUP system."
1189360,14133,422,Bid landscape forecasting in online ad exchange marketplace,2011,"Display advertising has been a significant source of revenue for publishers and ad networks in online advertising ecosystem. One important business model in online display advertising is Ad Exchange marketplace, also called non-guaranteed delivery (NGD), in which advertisers buy targeted page views and audiences on a spot market through real-time auction. In this paper, we describe a bid landscape forecasting system in NGD marketplace for any advertiser campaign specified by a variety of targeting attributes. In the system, the impressions that satisfy the campaign targeting attributes are partitioned into multiple mutually exclusive samples. Each sample is one unique combination of quantified attribute values. We develop a divide-and-conquer approach that breaks down the campaign-level forecasting problem. First, utilizing a novel star-tree data structure, we forecast the bid for each sample using non-linear regression by gradient boosting decision trees. Then we employ a mixture-of-log-normal model to generate campaign-level bid distribution based on the sample-level forecasted distributions. The experiment results of a system developed with our approach show that it can accurately forecast the bid distributions for various campaigns running on the world's largest NGD advertising exchange system, outperforming two baseline methods in term of forecasting errors."
1485241,14133,422,Automated hypothesis generation based on mining scientific literature,2014,"Keeping up with the ever-expanding flow of data and publications is untenable and poses a fundamental bottleneck to scientific progress. Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. We present an initial case study on KnIT, a prototype system that mines the information contained in the scientific literature, represents it explicitly in a queriable network, and then further reasons upon these data to generate novel and experimentally testable hypotheses. KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to identify potential new properties of entities that are strongly implied by existing relationships. We discuss a successful application of our approach that mines the published literature to identify new protein kinases that phosphorylate the protein tumor suppressor p53. Retrospective analysis demonstrates the accuracy of this approach and ongoing laboratory experiments suggest that kinases identified by our system may indeed phosphorylate p53. These results establish proof of principle for automated hypothesis generation and discovery based on text mining of the scientific literature."
2214065,14133,422,Applying data mining techniques to address disaster information management challenges on mobile devices,2011,"The improvement of Crisis Management and Disaster Recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade. Our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event. With the proliferation of smart phones and wireless tablets, professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication. Further, with the rise of social media, technology savvy consumers are also using these devices extensively for situational updates. In this paper, we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management. We design and implement an All-Hazard Disaster Situation Browser (ADSB) system that runs on Apple's mobile operating system (iOS) and iPhone and iPad mobile devices. Our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering. Specifically, hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities; probabilistic models are proposed to dynamically generate query forms based on user's feedback; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization. Furthermore, the developed techniques are designed to be all-hazard capable so that they can be used in earthquake, terrorism, or other unanticipated disaster situations."
808909,14133,507,SkylineSearch: semantic ranking and result visualization for pubmed,2011,"Life sciences researchers perform scientific literature search as part of their daily activities. Many such searches are executed against PubMed, a central repository of life sciences articles, and often return hundreds, or even thousands, of results, pointing to the need for data exploration tools. In this demonstration we present SkylineSearch, a semantic ranking and result visualization system designed specifically for PubMed, and available to the scientific community at skyline.cs.columbia.edu. Our system leverages semantic annotations of articles with terms from the MeSH controlled vocabulary, and presents results as a two-dimensional skyline, plotting relevance against publication date. We demonstrate that SkylineSearch supports a richer data exploration experience than does the search functionality of PubMed, allowing users to find relevant references more easily. We also show that SkylineSearch executes queries and presents results in interactive time."
1471954,14133,422,AssocExplorer: an association rule visualization system for exploratory data analysis,2012,"We present a system called AssocExplorer to support exploratory data analysis via association rule visualization and exploration. AssocExplorer is designed by following the visual information-seeking mantra: overview first, zoom and filter, then details on demand. It effectively uses coloring to deliver information so that users can easily detect things that are interesting to them. If users find a rule interesting, they can explore related rules for further analysis, which allows users to find interesting phenomenon that are difficult to detect when rules are examined separately. Our system also allows users to compare rules and inspect rules with similar item composition but different statistics so that the key factors that contribute to the difference can be isolated."
2174422,14133,422,"Good-enough brain model: challenges, algorithms and discoveries in multi-subject experiments",2014,"Given a simple noun such as {\em apple}, and a question such as is it edible?, what processes take place in the human brain? More specifically, given the stimulus, what are the interactions between (groups of) neurons (also known as functional connectivity) and how can we automatically infer those interactions, given measurements of the brain activity? Furthermore, how does this connectivity differ across different human subjects?   In this work we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId, which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. Moreover, GeBM is able to simulate basic psychological phenomena such as habituation and priming (whose definition we provide in the main text).   We evaluate GeBM by using both synthetic and real brain data. Using the real data, GeBM produces brain activity patterns that are strikingly similar to the real ones, and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multi-subject brain activity measurements."
1399016,14133,339,pTwitterRec: a privacy-preserving personalized tweet recommendation framework,2014,"Twitter is one of the most popular Online Social Networks (OSNs) nowadays. Twitter users retrieve information from other users by subscribing to their tweets. Twitter users, especially those who have many followees, may receive hundreds or even thousands of tweets daily. Currently, all tweets are shown to users in chronological order. Consequently, a Twitter user may accidentally overlook useful and interesting tweets because the user is overwhelmed by the huge volume of uninteresting tweets. Researchers in the recommendation system community have proposed using recommendation techniques such as collaborative filtering to predict users' preference of tweets and highlight those tweets in which users are most likely to be interested. At the same time, while OSNs such as Twitter have enabled people to conveniently share information and interact with each other online, OSN users are getting increasingly concerned about their online privacy. Researchers in the security community have proposed using techniques such as encrypted tweets to protect users' privacy. In this paper, we propose a privacy-preserving personalized tweet recommendation framework, pTwitterRec, in a Twitter-like social network where users' tweets are hidden from the OSN provider. pTwitterRec provides users with personalized tweet recommendations while keeping users' tweets and interests hidden from the OSN provider as well as other unauthorized entities. pTwitterRec splits the tweet recommendation task between the provider and a semi-trusted third party, so that neither can derive users' sensitive information alone while working together to provide users with personalized tweet recommendations. We implement a prototype and demonstrate through evaluation that pTwitterRec incurs tolerable overhead on today's smartphones."
1358889,14133,422,Bid optimizing and inventory scoring in targeted online advertising,2012,"Billions of online display advertising spots are purchased on a daily basis through real time bidding exchanges (RTBs). Advertising companies bid for these spots on behalf of a company or brand in order to purchase these spots to display banner advertisements. These bidding decisions must be made in fractions of a second after the potential purchaser is informed of what location (Internet site) has a spot available and who would see the advertisement. The entire transaction must be completed in near real-time to avoid delays loading the page and maintain a good users experience. This paper presents a bid-optimization approach that is implemented in production at Media6Degrees for bidding on these advertising opportunities at an appropriate price. The approach combines several supervised learning algorithms, as well as second price auction theory, to determine the correct price to ensure that the right message is delivered to the right person, at the right time."
1434248,14133,422,A transfer learning based framework of crowd-selection on twitter,2013,"Crowd selection is essential to crowd sourcing applications, since choosing the right workers with particular expertise to carry out crowdsourced tasks is extremely important. The central problem is simple but tricky: given a crowdsourced task, who are the most knowledgable users to ask? In this demo, we show our framework that tackles the problem of crowdsourced task assignment on Twitter according to the social activities of its users. Since user profiles on Twitter do not reveal user interests and skills, we transfer the knowledge from categorized  Yahoo! Answers  datasets for learning user expertise. Then, we select the right crowd for certain tasks based on user expertise. We study the effectiveness of our system using extensive user evaluation. We further engage the attendees to participate a game called--Whom to Ask on Twitter?. This helps understand our ideas in an interactive manner. Our crowd selection can be accessed by the following url http://webproject2.cse.ust.hk:8034/tcrowd/."
1926846,14133,422,Combining proper name-coreference with conditional random fields for semi-supervised named entity recognition in Vietnamese text,2011,"Named entity recognition (NER) is the process of seeking to locate atomic elements in text into predefined categories such as the names of persons, organizations and locations.Most existingNERsystems are based on supervised learning. This method often requires a large amount of labelled training data, which is very time-consuming to build. To solve this problem, we introduce a semi-supervised learning method for recognizing named entities in Vietnamese text by combining proper name coreference, named-ambiguityheuristicswithapowerful sequential learningmodel,Conditional RandomFields. Our approach inherits the idea of Liao and Veeramachaneni [6] and expands it by using proper name coreference. Starting by training the model using a small data set that is annotated manually, the learning model extracts high confident named entities and finds low confident ones by using proper name coreference rules. The low confident named entities are put in the training set to learn new context features. The F-scores of the systemfor extracting Person, Location and Organization entities are 83.36%, 69.53% and 65.71%when applying heuristics proposed by Liao andVeeramachaneni.Those valueswhen using our proposed heuristics are 93.13%, 88.15% and 79.35%, respectively. It shows that our method is good in increasing the system accuracy."
1315749,14133,422,Mining for geographically disperse communities in social networks by leveraging distance modularity,2013,"Social networks where the actors occupy geospatial locations are prevalent in military, intelligence, and policing operations such as counter-terrorism, counter-insurgency, and combating organized crime. These networks are often derived from a variety of intelligence sources. The discovery of communities that are geographically disperse stems from the requirement to identify higher-level organizational structures, such as a logistics group that provides support to various geographically disperse terrorist cells. We apply a variant of Newman-Girvan modularity to this problem known as distance modularity. To address the problem of finding geographically disperse communities, we modify the well-known Louvain algorithm to find partitions of networks that provide near-optimal solutions to this quantity. We apply this algorithm to numerous samples from two real-world social networks and a terrorism network data set whose nodes have associated geospatial locations. Our experiments show this to be an effective approach and highlight various practical considerations when applying the algorithm to distance modularity maximization. Several military, intelligence, and law-enforcement organizations are working with us to further test and field software for this emerging application."
971599,14133,422,Apolo: interactive large graph sensemaking by combining machine learning and visualization,2011,"We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO's usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4."
937911,14133,65,Evaluating a social multi-user interaction model using a Nao robot,2014,"This paper presents results from a user evaluation of a robot bartender system, which supports social engagement and interaction with multiple customers. The system is a Nao-based alternative version of an existing robot bartender developed in the JAMES project [1]. The Nao-based version has given us a local experimentation platform, allowing us to focus on social multi-user interaction rather than the robot technology of object manipulation. We will describe the design of the Nao-based system and discuss the differences with the original JAMES system. In a recent evaluation of the JAMES system with real users, a trained and a hand-coded version of the action selection policy were compared [2]. Here we present results from a similar comparative user evaluation on the Nao-based system, which confirm the conclusions of the previous experiment and provide further evidence in favour of the trained action selection mechanism. Task success was found to be almost 20% higher with the trained policy, with interaction times being about 10% shorter. Participants also rated the trained system as significantly more natural, more understanding, and better at providing appropriate attention."
870432,14133,422,Latent graphical models for quantifying and predicting patent quality,2011,"The number of patents filed each year has increased dramatically in recent years, raising concerns that patents of questionable validity are restricting the issuance of truly innovative patents. For this reason, there is a strong demand to develop an objective model to quantify patent quality and characterize the attributes that lead to higher-quality patents. In this paper, we develop a latent graphical model to infer patent quality from related measurements. In addition, we extract advanced lexical features via natural language processing techniques to capture the quality measures such as clarity of claims, originality, and importance of cited prior art. We demonstrate the effectiveness of our approach by validating its predictions with previous court decisions of litigated patents."
1344260,14133,422,Transparent user models for personalization,2012,"Personalization is a ubiquitous phenomenon in our daily online experience. While such technology is critical for helping us combat the overload of information we face, in many cases, we may not even realize that our results are being tailored to our personal tastes and preferences. Worse yet, when such a system makes a mistake, we have little recourse to correct it.   In this work, we propose a framework for addressing this problem by developing a new user-interpretable feature set upon which to base personalized recommendations. These features, which we call badges, represent fundamental traits of users (e.g., vegetarian or Apple fanboy) inferred by modeling the interplay between a user's behavior and self-reported identity. Specifically, we consider the microblogging site Twitter, where users provide short descriptions of themselves in their profiles, as well as perform actions such as tweeting and retweeting. Our approach is based on the insight that we can define badges using high precision, low recall rules (e.g., Twitter profile contains the phrase 'Apple fanboy'), and with enough data, generalize to other users by observing shared behavior. We develop a fully Bayesian, generative model that describes this interaction, while allowing us to avoid the pitfalls associated with having positive-only data.   Experiments on real Twitter data demonstrate the effectiveness of our model at capturing rich and interpretable user traits that can be used to provide transparency for personalization."
1407603,14133,65,Handling uncertain input in multi-user human-robot interaction,2014,"In this paper we present results from a user evaluation of a robot bartender system which handles state uncertainty derived from speech input by using belief tracking and generating appropriate clarification questions. We present a combination of state estimation and action selection components in which state uncertainty is tracked and exploited, and compare it to a baseline version that uses standard speech recognition confidence score thresholds instead of belief tracking. The results suggest that users are served fewer incorrect drinks when the uncertainty is retained in the state."
1210917,14133,422,Unfolding physiological state: mortality modelling in intensive care units,2014,"Accurate knowledge of a patient's disease state and trajectory is critical in a clinical setting. Modern electronic healthcare records contain an increasingly large amount of data, and the ability to automatically identify the factors that influence patient outcomes stand to greatly improve the efficiency and quality of care.   We examined the use of latent variable models (viz. Latent Dirichlet Allocation) to decompose free-text hospital notes into meaningful features, and the predictive power of these features for patient mortality. We considered three prediction regimes: (1) baseline prediction, (2) dynamic (time-varying) outcome prediction, and (3) retrospective outcome prediction. In each, our prediction task differs from the familiar time-varying situation whereby data accumulates; since fewer patients have long ICU stays, as we move forward in time fewer patients are available and the prediction task becomes increasingly difficult.   We found that latent topic-derived features were effective in determining patient mortality under three timelines: in-hospital, 30 day post-discharge, and 1 year post-discharge mortality. Our results demonstrated that the latent topic features important in predicting hospital mortality are very different from those that are important in post-discharge mortality. In general, latent topic features were more predictive than structured features, and a combination of the two performed best.   The time-varying models that combined latent topic features and baseline features had AUCs that reached 0.85, 0.80, and 0.77 for in-hospital, 30 day post-discharge and 1 year post-discharge mortality respectively. Our results agreed with other work suggesting that the first 24 hours of patient information are often the most predictive of hospital mortality. Retrospective models that used a combination of latent topic features and structured features achieved AUCs of 0.96, 0.82, and 0.81 for in-hospital, 30 day, and 1-year mortality prediction.   Our work focuses on the dynamic (time-varying) setting because models from this regime could facilitate an on-going severity stratification system that helps direct care-staff resources and inform treatment strategies."
1645742,14133,422,Practical Lessons from Predicting Clicks on Ads at Facebook,2014,"Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with."
1184400,14133,422,Comprehensible classification models: a position paper,2014,"The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users."
106640,14133,235,A fast subspace text categorization method using parallel classifiers,2012,"In today's world, the number of electronic documents made available to us is increasing day by day. It is therefore important to look at methods which speed up document search and reduce classifier training times. The data available to us is frequently divided into several broad domains with many sub-category levels. Each of these domains of data constitutes a subspace which can be processed separately. In this paper, separate classifiers of the same type are trained on different subspaces and a test vector is assigned to a subspace using a fast novel method of subspace detection. This parallel classifier architecture was tested with a wide variety of basic classifiers and the performance compared with that of a single basic classifier on the full data space. It was observed that the improvement in subspace learning was accompanied by a very significant reduction in training times for all types of classifiers used."
1100618,14133,422,Grouping students in educational settings,2014,"Given a class of large number of students, each exhibiting a different ability level, how can we group them into sections so that the overall gain for students is maximized? This question has been a topic of central concern and debate amongst social scientists and policy makers for a long time. We propose a framework for rigorously studying this question, taking a computational perspective. We present a formal definition of the grouping problem and investigate some of its variants. Such variants are determined by the desired number of groups as well as the definition of the gain for each student in the group. We focus on two natural instantiations of the gain function and we show that for both of them the problem of identifying a single group of students that maximizes the gain among its members can be solved in polynomial time. The corresponding partitioning problem, where the goal is to partition the students into non-overlapping groups appear to be much harder. However, the algorithms for the single-group version can be leveraged for solving the more complex partitioning problem. Our experiments with generated data coming from different distributions demonstrate that our algorithm is significantly better than the current strategies in vogue for dividing students in a class into sections."
1758534,14133,422,Inactive learning?: difficulties employing active learning in practice,2011,"Despite the tremendous level of adoption of machine learning techniques in real-world settings, and the large volume of research on active learning, active learning techniques have been slow to gain substantial traction in practical applications. This reluctance of adoption is contrary to active learning's promise of reduced model-development costs and increased performance on a model-development budget. This essay presents several important and under-discussed challenges to using active learning well in practice. We hope this paper can serve as a call to arms for researchers in active learning--an encouragement to focus even more attention on how practitioners might actually use active learning."
1150505,14133,422,Modeling delayed feedback in display advertising,2014,"In performance display advertising a key metric of a campaign effectiveness is its conversion rate -- the proportion of users who take a predefined action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One difficulty however is that the conversions can take place long after the impression -- up to a month -- and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample -- when the elapsed time is larger than the predicted delay -- or should be discarded from the training set -- when it is too early to tell. We provide experimental results on real traffic logs that demonstrate the effectiveness of the proposed model."
182831,14133,374,Efficient privacy-enhanced familiarity-based recommender system,2013,"Recommender systems can help users to find interesting content, often based on similarity with other users. However, studies have shown that in some cases familiarity gives comparable results to similarity. Using familiarity has the added bonus of increasing privacy between users and utilizing a smaller dataset. In this paper, we propose an efficient privacy-enhanced recommender system that is based on familiarity. It is built on top of any given social network (without changing its behaviour) that already has information about the social relations between users. Using secure multi-party computation techniques and somewhat homomorphic encryption the privacy of the users can be ensured, assuming honest-but-curious participants. Two different solutions are given, one where all users are online, and another where most users are offline. Initial results on a prototype and a dataset of 50 familiar users and 1000 items show a recommendation time of four minutes for the solution with online users and of five minutes for the solution with offline users."
2094526,14133,30,Feature Selection and Classification in Supporting Report-Based Self-Management for People with Chronic Pain,2011,"Chronic pain is a common long-term condition that affects a person's physical and emotional functioning. Currently, the integrated biopsychosocial approach is the mainstay treatment for people with chronic pain. Self-reporting (the use of questionnaires) is one of the most common methods to evaluate treatment outcome. The questionnaires can consist of more than 300 questions, which is tedious for people to complete at home. This paper presents a machine learning approach to analyze self-reporting data collected from the integrated biopsychosocial treatment, in order to identify an optimal set of features for supporting self-management. In addition, a classification model is proposed to differentiate the treatment stages. Four different feature selection methods were applied to rank the questions. In addition, four supervised learning classifiers were used to investigate the relationships between the numbers of questions and classification performance. There were no significant differences between the feature ranking methods for each classifier in overall classification accuracy or AUC ( p  >; 0.05); however, there were significant differences between the classifiers for each ranking method ( p  <; 0.001). The results showed the multilayer perceptron classifier had the best classification performance on an optimized subset of questions, which consisted of ten questions. Its overall classification accuracy and AUC were 100% and 1, respectively."
1411216,14133,422,Community discovery and profiling with social messages,2012,"Discovering communities from social media and collaboration systems has been of great interest in recent years. Existing work show prospects of modeling contents and social links, aiming at discovering social communities, whose definition varies by application. We believe that a community depends not only on the group of people who actively participate, but also the topics they communicate about or collaborate on. This is especially true for workplace email communications. Within an organization, it is not uncommon that employees multifunction, and groups of employees collaborate on multiple projects at the same time. In this paper, we aim to automatically discovering and profiling users' communities by taking into account both the contacts and the topics. More specifically, we propose a community profiling model called COCOMP, where the communities labels are latent, and each social document corresponds to an information sharing activity among the most probable community members regarding the most relevant community issues. Experiment results on several social communication datasets, including emails and Twitter messages, demonstrate that the model can discover users' communities effectively, and provide concrete semantics."
1437949,14133,422,On the semantic annotation of places in location-based social networks,2011,"In this paper, we develop a semantic annotation technique for location-based social networks to automatically annotate all places with category tags which are a crucial prerequisite for location search, recommendation services, or data cleaning. Our annotation algorithm learns a binary support vector machine (SVM) classifier for each tag in the tag space to support multi-label classification. Based on the check-in behavior of users, we extract features of places from i) explicit patterns (EP) of individual places and ii) implicit relatedness (IR) among similar places. The features extracted from EP are summarized from all check-ins at a specific place. The features from IR are derived by building a novel network of related places (NRP) where similar places are linked by virtual edges. Upon NRP, we determine the probability of a category tag for each place by exploring the relatedness of places. Finally, we conduct a comprehensive experimental study based on a real dataset collected from a location-based social network, Whrrl. The results demonstrate the suitability of our approach and show the strength of taking both EP and IR into account in feature extraction."
1646118,14133,422,Article clipper: a system for web article extraction,2011,"Many people use the Web as the main source of information in their daily lives. However, most web pages contain non-informative components such as side bars, footers, headers, and advertisements, which are undesirable for certain applications like printing. We demonstrate a system that automatically extracts the informative contents from news- and blog-like web pages. In contrast to many existing methods that are limited to identifying only the text or the bounding rectangular region, our system not only identifies the content but also the structural roles of various content components such as title, paragraphs, images and captions. The structural information enables re-layout of the content in a pleasing way. Besides the article text extraction, our system includes the following components: 1) print-link detection to identify the URL link for printing, and to use it for more reliable analysis and recognition; 2) title detection incorporating both visual cues and HTML tags; 3) image and caption detection utilizing extensive visual cues; 4) multiple-page and next page URL detection. The performance of our system has been thoroughly evaluated using a human labeled ground truth dataset consisting of 2000 web pages from 100 major web sites. We show accurate results using such a dataset."
1464147,14133,422,Incremental and decremental training for linear classification,2014,"In classification, if a small number of instances is added or removed, incremental and decremental techniques can be applied to quickly update the model. However, the design of incremental and decremental algorithms involves many considerations. In this paper, we focus on linear classifiers including logistic regression and linear SVM because of their simplicity over kernel or other methods. By applying a warm start strategy, we investigate issues such as using primal or dual formulation, choosing optimization methods, and creating practical implementations. Through theoretical analysis and practical experiments, we conclude that a warm start setting on a high-order optimization method for primal formulations is more suitable than others for incremental and decremental learning of linear classification."
775492,14133,339,Decide Now or Decide Later?: Quantifying the Tradeoff between Prospective and Retrospective Access Decisions,2014,"One of the greatest challenges an organization faces is determining when an employee is permitted to utilize a certain resource in a system. This insider threat can be addressed through two strategies: i) prospective methods, such as access control, that make a decision at the time of a request, and ii) retrospective methods, such as post hoc auditing, that make a decision in the light of the knowledge gathered afterwards. While it is recognized that each strategy has a distinct set of benefits and drawbacks, there has been little investigation into how to provide system administrators with practical guidance on when one or the other should be applied. To address this problem, we introduce a framework to compare these strategies on a common quantitative scale. In doing so, we translate these strategies into classification problems using a context-based feature space that assesses the likelihood that an access request is legitimate. We then introduce a technique called bispective analysis to compare the performance of the classification models under the situation of non-equivalent costs for false positive and negative instances, a significant extension on traditional cost analysis techniques, such as analysis of the receiver operator characteristic (ROC) curve. Using domain-specific cost estimates and access logs of several months from a large Electronic Medical Record (EMR) system, we demonstrate how bispective analysis can support meaningful decisions about the relative merits of prospective and retrospective decision making for specific types of hospital personnel."
1108536,14133,422,Estimating conversion rate in display advertising from past erformance data,2012,"In targeted display advertising, the goal is to identify the best opportunities to display a banner ad to an online user who is most likely to take a desired action such as purchasing a product or signing up for a newsletter. Finding the best ad impression, i.e., the opportunity to show an ad to a user, requires the ability to estimate the probability that the user who sees the ad on his or her browser will take an action, i.e., the user will convert. However, conversion probability estimation is a challenging task since there is extreme data sparsity across different data dimensions and the conversion event occurs rarely. In this paper, we present our approach to conversion rate estimation which relies on utilizing past performance observations along user, publisher and advertiser data hierarchies. More specifically, we model the conversion event at different select hierarchical levels with separate binomial distributions and estimate the distribution parameters individually. Then we demonstrate how we can combine these individual estimators using logistic regression to accurately identify conversion events. In our presentation, we also discuss main practical considerations such as data imbalance, missing data, and output probability calibration, which render this estimation problem more difficult but yet need solving for a real-world implementation of the approach. We provide results from real advertising campaigns to demonstrate the effectiveness of our proposed approach."
605054,14133,256,Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks,2014,"Abstract: Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over $96\%$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over $90\%$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators."
716701,14133,256,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,2013,"Abstract: We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation."
1135076,14133,422,Trial and error in influential social networks,2013,"In this paper, we introduce a trial-and-error model to study information diffusion in a social network. Specifically, in every discrete period, all individuals in the network concurrently try a new technology or product with certain respective probabilities. If it turns out that an individual observes a better utility, he will then adopt the trial; otherwise, the individual continues to choose his prior selection.   We first demonstrate that the trial and error behavior of individuals characterizes certain global community structures of a social network, from which we are able to detect macro-communities through the observation of micro-behavior of individuals. We run simulations on classic benchmark testing graphs, and quite surprisingly, the results show that the trial and error dynamics even outperforms the Louvain method (a popular modularity maximization approach) if individuals have dense connections within communities. This gives a solid justification of the model.   We then study the influence maximization problem in the trial-and-error dynamics. We give a heuristic algorithm based on community detection and provide experiments on both testing and large scale collaboration networks. Simulation results show that our algorithm significantly outperforms several well-studied heuristics including degree centrality and distance centrality in almost all of the scenarios. Our results reveal the relation between the budget that an advertiser invests and marketing strategies, and indicate that the mixing parameter, a benchmark evaluating network community structures, plays a critical role for information diffusion."
31062,14133,422,Info-cluster based regional influence analysis in social networks,2011,"Influence analysis and expert finding have received a great deal of attention in social networks. Most of existing works, however, aim to maximize influence based on communities structure in social networks. They ignored the location information, which often imply abundant information about individuals or communities. In this paper, we propose Info-Cluster, an innovative concept to describe how the information originated from a location cluster propagates in or between communities. According to this concept, we propose a framework for identifying the Info-Cluster in social networks, which uses both location information and communities structure. Taking the location information into consideration, we first adopt the K-Means algorithm to find location clusters. Next, we identify the communities for the whole network data set. Given the location clusters and communities, we present the information propagation based Info-Cluster detection algorithm. Experiments on Renren networks show that our method can reveal many meaningful results about regional influence analysis."
1780572,14133,507,Information diffusion in online social networks,2013,"Online social networks play a major role in the spread of information at very large scale and it becomes essential to provide means to analyze this phenomenon. Analyzing information diffusion proves to be a challenging task since the raw data produced by users of these networks are a flood of ideas, recommendations, opinions, etc. The aim of this PhD work is to help in the understanding of this phenomenon. So far, our contributions are the following: (i) a survey of developments in the field; (ii) T-BaSIC, a graph-based model for information diffusion prediction; (iii) SONDY, an open source platform that helps understanding social network users' interests and activity by providing emerging topics and events detection as well as network analysis functionalities."
1616190,14133,422,Fast algorithms for comprehensive n-point correlation estimates,2012,"The  n -point correlation functions (npcf) are powerful spatial statistics capable of fully characterizing any set of multidimensional points. These functions are critical in key data analyses in astronomy and materials science, among other fields, for example to test whether two point sets come from the same distribution and to validate physical models and theories. For example, the npcf has been used to study the phenomenon of dark energy, considered one of the major breakthroughs in recent scientific discoveries. Unfortunately, directly estimating the continuous npcf at a single value requires  O ( N n  ) time for $N$ points, and  n  may be 2, 3, 4 or even higher, depending on the sensitivity required. In order to draw useful conclusions about real scientific problems, we must repeat this expensive computation both for many different scales in order to derive a smooth estimate and over many different subsamples of our data in order to bound the variance.   We present the first comprehensive approach to the entire  n -point correlation function estimation problem, including fast algorithms for the computation at multiple scales and for many subsamples. We extend the current state-of-the-art tree-based approach with these two algorithms. We show an order-of-magnitude speedup over the current best approach with each of our new algorithms and show that they can be used together to obtain over 500x speedups over the state-of-the-art in order to enable much larger datasets and more accurate scientific analyses than were possible previously."
2098546,14133,256,Exact solutions to the nonlinear dynamics of learning in deep linear neural networks,2014,"Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos."
1243246,14133,507,CrowdScreen: algorithms for filtering data with humans,2012,"Given a large set of data items, we consider the problem of  filtering  them based on a set of properties that can be verified by humans. This problem is commonplace in crowdsourcing applications, and yet, to our knowledge, no one has considered the formal optimization of this problem. (Typical solutions use heuristics to solve the problem.) We formally state a few different variants of this problem. We develop deterministic and probabilistic algorithms to optimize the expected cost (i.e., number of questions) and expected error. We experimentally show that our algorithms provide definite gains with respect to other strategies. Our algorithms can be applied in a variety of crowdsourcing scenarios and can form an integral part of any query processor that uses human computation."
1311048,14133,256,On the number of inference regions of deep feed forward networks with piece-wise linear activations,2014,"Abstract: This paper explores the complexity of deep feed forward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has $kn$ hidden units and $n_0$ inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$ layer model with $n$ hidden units on each layer it is $\Omega(\left( {n}/{n_0}\right)^{k-1}n^{n_0})$. $\left({n}/{n_0}\right)^{k-1}$ grows faster then $k^{n_0}$ when either $n$ goes to infinity or $k$ goes to infinity and $n > 2n_0$. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis."
1162459,14133,507,A formal approach to finding explanations for database queries,2014,"As a consequence of the popularity of big data, many users with a variety of backgrounds seek to extract high level information from datasets collected from various sources and combined using data integration techniques. A major challenge for research in data management is to develop tools to assist users in explaining observed query outputs. In this paper we introduce a principled approach to provide explanations for answers to SQL queries based on intervention: removal of tuples from the database that significantly affect the query answers. We provide a formal definition of intervention in the presence of multiple relations which can interact with each other through foreign keys. First we give a set of recursive rules to compute the intervention for any given explanation in polynomial time (data complexity). Then we give simple and efficient algorithms based on SQL queries that can compute the top-K explanations by using standard database management systems under certain conditions. We evaluate the quality and performance of our approach by experiments on real datasets."
2059996,14133,507,In search of influential event organizers in online social networks,2014,"Recently, with the emergence of event-based online social services(e.g. Meetup), there have been increasing online activities to create, distribute, and organize social events. In this paper, we take the first systematic step to discover influential event organizers from online social networks who are essential to the overall success of social events. Informally, such event organizers comprise a small group of people who not only have the relevant skills or expertise that are required for an event (e.g. conference) but they are also able to influence largest number of people to actively contribute to it. We formulate it as the problem of mining influential cover set (ICS) where we wish to find k users in a social network G that together have the required skills or expertise (modeled as attributes of nodes in G) to organize an event such that they can influence the greatest number of individuals to participate in the event. The problem is, however, NP-hard. Hence, we propose three algorithms to find approximate solutions to the problem. The first two algorithms are greedy; they run faster, but have no guarantees. The third algorithm is 2-approximate and guarantees to find a feasible solution if any. Our empirical study over several real-world networks demonstrates the superiority of our proposed solutions."
2631181,14133,235,Employing Event Inference to Improve Semi-Supervised Chinese Event Extraction,2014,"Although semi-supervised model can extract the event mentions matching frequent event patterns, it suffers much from those event mentions, which match infrequent patterns or have no matching pattern. To solve this issue, this paper introduces various kinds of linguistic knowledge-driven event inference mechanisms to semi-supervised Chinese event extraction. These event inference mechanisms can capture linguistic knowledge from four aspects, i.e. semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events, to further recover missing event mentions from unlabeled texts. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms significantly outperform the refined state-of-the-art semi-supervised Chinese event extraction system in F1-score by 8.5%."
1835630,14133,422,Building blocks for exploratory data analysis tools,2013,"Data exploration is largely manual and labor intensive. Although there are various tools and statistical techniques that can be applied to data sets, there is little help to identify what questions to ask of a data set, let alone what domain knowledge is useful in answering the questions. In this paper, we study user queries against production data sets in Splunk. Specifically, we characterize the interplay between data sets and the operations used to analyze them using latent semantic analysis, and discuss how this characterization serves as a building block for a data analysis recommendation system. This is a work-in-progress paper."
1736098,14133,422,Distance metric learning using dropout: a structured regularization approach,2014,"Distance metric learning (DML) aims to learn a distance metric better than Euclidean distance. It has been successfully applied to various tasks, e.g., classification, clustering and information retrieval. Many DML algorithms suffer from the over-fitting problem because of a large number of parameters to be determined in DML. In this paper, we exploit the dropout technique, which has been successfully applied in deep learning to alleviate the over-fitting problem, for DML. Different from the previous studies that only apply dropout to training data, we apply dropout to both the learned metrics and the training data. We illustrate that application of dropout to DML is essentially equivalent to matrix norm based regularization. Compared with the standard regularization scheme in DML, dropout is advantageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers. We verify, both empirically and theoretically, that dropout is effective in regulating the learned metric to avoid the over-fitting problem. Last, we examine the idea of wrapping the dropout technique in the state-of-art DML methods and observe that the dropout technique can significantly improve the performance of the original DML methods."
442402,14133,344,A Comparative Study of Reinforcement Learning Techniques on Dialogue Management,2012,"Adaptive Dialogue Systems are rapidly becoming part of our everyday lives. As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment. Research in this field is currently focused on how to achieve adaptation, and particularly on applying Reinforcement Learning (RL) techniques, so a comparative study of the related methods, such as this, is necessary. In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers/developers choose the appropriate RL algorithm for their system. This is the first work, to the best of our knowledge, to evaluate online RL algorithms on the dialogue problem and in a dynamic environment."
2627861,14133,235,Constructing Chinese Abbreviation Dictionary: A Stacked Approach,2012,"Abbreviation is a common linguistic phenomenon with wide popularity and high rate of growth. Correctly linking full forms to their abbreviations will be helpful in many applications. For example, it can improve the recall of information retrieval systems. An intuition to solve this is to build an abbreviation dictionary in advance. This paper investigates an automatic abbreviation generation method, which uses a stacked approach for Chinese abbreviation generation. We tackle this problem in two stages. First we use a sequence labeling method to generate a list of candidate abbreviations. Then, we try to use search engine to incorporate web data to re-rank the candidates, and finally get the best candidate. We use a Chinese abbreviation corpus which contains 8015 abbreviation pairs to evaluate the performance. Experiments revealed that our method gave better performance than the baseline methods."
1425748,14133,507,Information diffusion in online social networks: a survey,2013,"Online social networks play a major role in the spread of information at very large scale. A lot of effort have been made in order to understand this phenomenon, ranging from popular topic detection to information diffusion modeling, including influential spreaders identification. In this article, we present a survey of representative methods dealing with these issues and propose a taxonomy that summarizes the state-of-the-art. The objective is to provide a comprehensive analysis and guide of existing efforts around information diffusion in social networks. This survey is intended to help researchers in quickly understanding existing works and possible improvements to bring."
1378861,14133,422,Confluence: conformity influence in large social networks,2013,"Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behavior. We formally define several major types of conformity in individual, peer, and group levels. We propose Confluence model to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near-linear speedup. Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that Confluence significantly improves the prediction accuracy by up to 5-10% compared with several alternative methods."
1604657,14133,422,Optimal exact least squares rank minimization,2012,"In multivariate analysis, rank minimization emerges when a low-rank structure of matrices is desired as well as a small estimation error. Rank minimization is nonconvex and generally NP-hard, imposing one major challenge. In this paper, we consider a nonconvex least squares formulation, which seeks to minimize the least squares loss function with the rank constraint. Computationally, we develop efficient algorithms to compute a global solution as well as an entire regularization solution path. Theoretically, we show that our method reconstructs the oracle estimator exactly from noisy data. As a result, it recovers the true rank optimally against any method and leads to sharper parameter estimation over its counterpart. Finally, the utility of the proposed method is demonstrated by simulations and image reconstruction from noisy background."
2279969,14133,422,Density-based logistic regression,2013,"This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods."
1708916,14133,65,Incorporating global and local observation models for human pose tracking,2013,"Tracking human pose is attractive to many applications such as Human Robot Interface (HRI), motion capture system, video surveillance, action recognition, etc. Though various methods were introduced during last decades, including both color and depth camera based, it is still considered that feature sets for them are not discriminative enough. In this paper, we propose a human pose tracking method based on a graphical model which incorporates global and local feature sets including Histogram of Oriented Gradients (HOG) and color distribution. HumanEva-I dataset is used for testing effectiveness of the proposed method."
336468,14133,256,Deep Convolutional Ranking for Multilabel Image Annotation,2014,"Abstract: Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-$k$ ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10%, obtaining the best reported performance in the literature."
1257829,14133,22130,Exploring SVM for Image Annotation in Presence of Confusing Labels.,2013,MBRM[1] 0.24/0.25/0.245/122 0.18/0.19/0.185/209 0.24/0.23/0.235/233 JEC[3] 0.27/0.32/0.293/139 0.22/0.25/0.234/224 0.28/0.29/0.285/250 TagProp-ML[2] 0.31/0.37/0.337/146 0.49/0.20/0.284/213 0.48/0.25/0.329/227 TagProp-s ML[2] 0.33/0.42/0.370/160 0.39/0.27/0.319/239 0.46/0.35/0.398/266 KSVM 0.29/0.43/0.346/174 0.30/0.28/0.290/256 0.43/0.27/0.332/266 KSVM-VT (Ours) 0.32/0.42/0.363/179 0.33/0.32/0.325/259 0.47/0.29/0.359/268 Table 1: Performance comparison among different methods. The prefix ‘K’ corresponds to kernelization using chi-squared kernel.
1197982,14133,422,On cross-validation and stacking: building seemingly predictive models on random data,2011,"A number of times when using cross-validation (CV) while trying to do classification/probability estimation we have observed surprisingly low AUC's on real data with very few positive examples. AUC is the area under the ROC and measures the ranking ability and corresponds to the probability that a positive example receives a higher model score than a negative example. Intuition seems to suggest that no reasonable methodology should ever result in a model with an AUC significantly below 0.5. The focus of this paper is not on the estimator properties of CV (bias/variance/significance), but rather on the properties of the 'holdout' predictions based on which the CV performance of a model is calculated. We show that CV creates predictions that have an 'inverse' ranking with AUC well below 0.25 using features that were initially entirely unpredictive and models that can only perform monotonic transformations. In the extreme, combining CV with bagging (repeated averaging of out-of-sample predictions) generates 'holdout' predictions with perfectly opposite rankings on random data. While this would raise immediate suspicion upon inspection, we would like to caution the data mining community against using CV for stacking or in currently popular ensemble methods. They can reverse the predictions by assigning negative weights and produce in the end a model that appears to have close to perfect predictability while in reality the data was random."
2308702,14133,23593,Bridging the GPGPU-FPGA efficiency gap,2011,"This paper compares an implementation of a Bayesian inference algorithm across several FPGAs and GPGPUs, while embracing both the execution model and high-level architecture of a GPGPU. Our study is motivated by recent work in template-based programming and architectural models for FPGA computing. The comparison we present is meant to demonstrate the FPGA's potential, while constraining the design to follow the microarchitectural template of more programmable devices such as GPGPUs.   The FPGA implementation proves capable of matching the performance of a high-end Nvidia Fermi-based GPU - the most advanced GPGPU available to us at the time of this study. Further investigation shows that each FPGA core outperforms workstation GPGPU cores by a factor of ~ 3.14x, and mobile GPGPU cores by ~ 4.25x despite a ~ 4x reduction in core clock frequency. Using these observations, we discuss the efficiency gap between these two platforms, and the challenges associated with template-based programming models."
56517,14133,235,Comment spam classification in blogs through comment analysis and comment-blog post relationships,2012,"Spamming refers to the process of providing unwanted and irrelevant information to the users. It is a widespread phenomenon that is often noticed in e-mails, instant messages, blogs and forums. In our paper, we consider the problem of spamming in blogs. In blogs, spammers usually target commenting systems which are provided by the authors to facilitate interaction with the readers. Unfortunately, spammers abuse these commenting systems by posting irrelevant and unsolicited content in the form of spam comments. Thus, we propose a novel methodology to classify comments into spam and non-spam using previously-undescribed features including certain blog post-comment relationships. Experiments conducted using our methodology produced a spam detection accuracy of 94.82% with a precision of 96.50% and a recall of 95.80%."
342204,14133,235,Identification of reduplicated multiword expressions using CRF,2011,"This paper deals with the identification of Reduplicated Multiword Expressions (RMWEs) which is important for any natural language applications like Machine Translation, Information Retrieval etc. In the present task, reduplicated MWEs have been identified in Manipuri language texts using CRF tool. Manipuri is highly agglutinative in nature and reduplication is quite high in this language. The important features selected for running the CRF tool include stem words, number of suffixes, number of prefixes, prefixes in the word, suffixes in the word, Part Of Speech (POS) of the surrounding words, surrounding stem words, length of the word, word frequency and digit feature. Experimental results show the effectiveness of the proposed approach with the overall average Recall, Precision and F-Score values of 92.91%, 91.90% and 92.40% respectively."
1541967,14133,422,Corporate residence fraud detection,2014,"With the globalisation of the world's economies and ever-evolving financial structures, fraud has become one of the main dissipaters of government wealth and perhaps even a major contributor in the slowing down of economies in general. Although corporate residence fraud is known to be a major factor, data availability and high sensitivity have caused this domain to be largely untouched by academia. The current Belgian government has pledged to tackle this issue at large by using a variety of in-house approaches and cooperations with institutions such as academia, the ultimate goal being a fair and efficient taxation system. This is the first data mining application specifically aimed at finding corporate residence fraud, where we show the predictive value of using both structured and fine-grained invoicing data. We further describe the problems involved in building such a fraud detection system, which are mainly data-related (e.g. data asymmetry, quality, volume, variety and velocity) and deployment-related (e.g. the need for explanations of the predictions made)."
620815,14133,235,Axiomatizing Complex Concepts from Fundamentals,2014,"We have been engaged in the project of encoding commonsense theories of cognition, or how we think we think, in a logical representation. In this paper we use the concept of a serious threat as our prime example, and examine the infrastructure required for capturing the meaning of this complex concept. It is one of many examples we could have used, but it is particularly interesting because building up to this concept from fundamentals, such as causality and scalar notions, highlights a number of representational issues that have to be faced along the way, where the complexity of the target concepts strongly influences how we resolve those issues.#R##N##R##N#We first describe our approach to definition, defeasibility, and reification, where hard decisions have to bemade to get the enterprise off the ground.We then sketch our approach to causality, scalar notions, goals, and importance. Finally we use all this to characterize what it is to be a serious threat. All of this is necessarily sketchy, but the key ideas essential to the target concept should be clear."
1046507,14133,422,Drug-target interaction prediction for drug repurposing with probabilistic similarity logic,2013,"The high development cost and low success rate of drug discovery from new compounds highlight the need for methods to discover alternate therapeutic effects for currently approved drugs. Computational methods can be effective in focusing efforts for such drug repurposing. In this paper, we propose a novel drug-target interaction prediction framework based on probabilistic similarity logic (PSL) [5]. Interaction prediction corresponds to link prediction in a bipartite network of drug-target interactions extended with a set of similarities between drugs and between targets. Using probabilistic first-order logic rules in PSL, we show how rules describing link predictions based on triads and tetrads can effectively make use of a variety of similarity measures. We learn weights for the rules based on training data, and report relative importance of each similarity for interaction prediction. We show that the learned rule weights significantly improve prediction precision. We evaluate our results on a dataset of drug-target interactions obtained from Drugbank [27] augmented with five drug-based and three target-based similarities. We integrate domain knowledge in drug-target interaction prediction and match the performance of the state-of-the-art drug-target interaction prediction systems [22] with our model using simple triad-based rules. Furthermore, we apply techniques that make link prediction in PSL more efficient for drug-target interaction prediction."
2287418,14133,344,Aspectual Type and Temporal Relation Classification,2012,"In this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. the problems of the recent TempEval challenges.#R##N##R##N#For a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types.#R##N##R##N#We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. The improved performance of the resulting models shows that (i) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that (ii) temporal information processing can profit from information about aspectual type."
1303972,14133,104,Database principles in information extraction,2014,"Information Extraction commonly refers to the task of populating a relational schema, having predefined underlying semantics, from textual content. This task is pervasive in contemporary computational challenges associated with Big Data. This tutorial gives an overview of the algorithmic concepts and techniques used for performing Information Extraction tasks, and describes some of the declarative frameworks that provide abstractions and infrastructure for programming extractors. In addition, the tutorial highlights opportunities for research impact through principles of data management, illustrates these opportunities through recent work, and proposes directions for future research."
1793459,14133,422,A system for extracting top-K lists from the web,2012,"List data is an important source of structured data on the web. This paper is concerned with top-k pages, which are web pages that describe a list of k instances of a particular topic or concept. Examples include the 10 tallest persons in the world and the 50 hits of 2010 you don't want to miss. Compared to normal web list data, top-k lists contain richer information and are easier to understand. Therefore the extraction of such lists can help enrich existing knowledge bases about general concepts, or act as a preprocessing step to produce facts for a fact answering engine. We present an efficient system that extracts the target lists from web pages with high accuracy. We have used the system to process up to 160 million, or 1/10 of a high-frequency web snapshot from Bing, and obtained over 140,000 lists with 90.4% precision."
1931515,14133,8235,Automatic extraction of top-k lists from the web,2013,"This paper is concerned with information extraction from top-k web pages, which are web pages that describe top k instances of a topic which is of general interest. Examples include “the 10 tallest buildings in the world”, “the 50 hits of 2010 you don't want to miss”, etc. Compared to other structured information on the web (including web tables), information in top-k lists is larger and richer, of higher quality, and generally more interesting. Therefore top-k lists are highly valuable. For example, it can help enrich open-domain knowledge bases (to support applications such as search or fact answering). In this paper, we present an efficient method that extracts top-k lists from web pages with high performance. Specifically, we extract more than 1.7 million top-k lists from a web corpus of 1.6 billion pages with 92.0% precision and 72.3% recall."
2506538,14133,390,SCALABLE MAMMOGRAM RETRIEVAL USING ANCHOR GRAPH HASHING,2014,"Mammogram analysis is known to provide early-stage diagnosis of breast cancer in reducing its morbidity and mortality. In this paper, we propose a scalable content-based image retrieval (CBIR) framework for digital mammograms. CBIR is of great significance for breast cancer diagnosis as it can provide doctors image-guided avenues to access relevant cases. Clinical decisions based on such cases offer a reliable and consistent supplement for doctors. In our framework, we employ an unsupervised algorithm, Anchor Graph Hashing (AGH), to compress the mammogram features into compact binary codes, and then perform searching in the Hamming space. In addition, we also propose to fuse different features in AGH to improve its search accuracy. Experiments on the Digital Database for Screening Mammography (DDSM) demonstrate that our system is capable of providing content-based accesses to proven diagnosis, and aiding doctors to make reliable clinical decisions. What’s more, our system is applicable to large-scale mammogram database, such that high number analogical cases would be retrieved as clinical references."
451878,14133,422,Peer matrix alignment: a new algorithm,2012,"Web data extraction has been one of the keys for web content mining that tries to understand Web pages and discover valuable information from them. Most of the developed Web data extraction systems have used data (string/tree) alignment techniques. In this paper, we suggest a new algorithm for multiple string (peer matrix) alignment. Each row in the matrix represents one string of characters, where every character (symbol) corresponds to a subtree in the DOM tree of a web page. Two subtrees take the same symbol in the peer matrix if they are similar, where similarity can be measured using either structural, content, or visual information. Our algorithm is not a generalization of 2-strings alignment; it looks at multiple strings at the same time. Also, our algorithm considers the common problems in the field of Web data extraction: missing, multi-valued, multi-ordering, and disjunctive attributes. The experiments show a perfect alignment result with the matrices constructed from the nodes closed to the top (root) and an encourage result for the nodes closed to the leaves of the DOM trees of the test web pages."
1327775,14133,422,Matching unstructured product offers to structured product specifications,2011,"An e-commerce catalog typically comprises of specifications for millions of products. The search engine receives millions of sales offers from thousands of independent merchants that must be matched to the right products. We describe the challenges that a system for matching unstructured offers to structured product descriptions must address, drawing upon our experience from building such a system for Bing Shopping. The heart of our system is a data-driven component that learns the matching function off-line, which is then applied at run-time for matching offers to products. We provide the design of this and other critical components of the system as well as the details of the extensive experiments we performed to assess the readiness of the system. This system is currently deployed in an experimental Commerce Search Engine and is used to match all the offers received by Bing Shopping to the Bing product catalog."
2171731,14133,422,Interactive learning for efficiently detecting errors in insurance claims,2011,"Many practical data mining systems such as those for fraud detection and surveillance deal with building classifiers that are not autonomous but part of a larger interactive system with an expert in the loop. The goal of these systems is not just to maximize the performance of the classifier but to make the experts more efficient at performing their task, thus maximizing the overall Return on Investment of the system. This paper describes an interactive system for detecting payment errors in insurance claims with claim auditors in the loop. We describe an interactive claims prioritization component that uses an online cost-sensitive learning approach (more-like-this) to make the system efficient. Our interactive prioritization component is built on top of a batch classifier that has been trained to detect payment errors in health insurance claims and optimizes the interaction between the classifier and the domain experts who are consuming the results of this system. The goal is to make these auditors more efficient and effective as well as improving the classification performance of the system. The result is both a reduction in time it takes for the auditors to review and label claims as well as improving the precision of the system in finding payment errors. We show results obtained from applying this system at two major US health insurance companies indicating significant reduction in claim audit costs and potential savings of $20-$26 million/year making the insurance providers more efficient and lowering their operating costs. Our system reduces the money being wasted by providers and insurers dealing with incorrectly processed claims and makes the healthcare system more efficient."
1627052,14133,422,An empirical study of reserve price optimisation in real-time bidding,2014,"In this paper, we report the first empirical study and live test of the reserve price optimisation problem in the context of Real-Time Bidding (RTB) display advertising from an operational environment. A reserve price is the minimum that the auctioneer would accept from bidders in auctions, and in a second price auction it could potentially uplift the auctioneer's revenue by charging winners the reserve price instead of the second highest bids. As such it has been used for sponsored search and been well studied in that context. However, comparing with sponsored search and contextual advertising, this problem in the RTB context is less understood yet more critical for publishers because 1) bidders have to submit a bid for each individual impression, which mostly is associated with user data that is subject to change over time. This, coupled with practical constraints such as the budget, campaigns' life time, etc. makes the theoretical result from optimal auction theory not necessarily applicable and a further empirical study is required to confirm its optimality from the real-world system; 2) in RTB an advertiser is facing nearly unlimited supply and the auction is almost done in last second, which encourages spending less on the high cost ad placements. This could imply the loss of bid volume over time if a correct reserve price is not in place. In this paper we empirically examine several commonly adopted algorithms for setting up a reserve price. We report our results of a large scale online experiment in a production platform. The results suggest the our proposed game theory based OneShot algorithm performed the best and the superiority is significant in most cases."
96477,14133,235,Information extraction from webpages based on DOM distances,2012,"Retrieving information from Internet is a difficult task as it is demonstrated by the lack of real-time tools able to extract information from webpages. The main cause is that most webpages in Internet are implemented using plain (X)HTML which is a language that lacks structured semantic information. For this reason much of the efforts in this area have been directed to the development of techniques for URLs extraction. This field has produced good results implemented by modern search engines. But, contrarily, extracting information from a single webpage has produced poor results or very limited tools. In this work we define a novel technique for information extraction from single webpages or collections of interconnected webpages. This technique is based on DOM distances to retrieve information. This allows the technique to work with any webpage and, thus, to retrieve information online. Our implementation and experiments demonstrate the usefulness of the technique."
2538707,14133,390,A new segmentation framework for infrared spectroscopic imaging using frequent pattern mining,2011,"Histologic analysis of a stained tissue sample by a trained pathologist forms the definitive diagnosis of prostate cancer. Rapid and objective second opinions are highly desirable to make more accurate diagnostic decisions. One alternate method is to use Fourier transform infrared (FT-IR) spectroscopic imaging, which is an emerging technique that combines the molecular selectivity of spectroscopy with the spatial specificity of optical microscopy. While instrumentation is well-developed for FT-IR imaging, information extraction from the data could benefit greatly from improved approaches. Here we propose a new approach to segment histologic classes in a tissue for FT-IR imaging using frequent pattern mining. Prior to applying frequent pattern mining, FT-IR images are discretized, and subsequent pruning method and feature selection method result in a classifier for the segmentation. The method is evaluated using two different datasets. Results indicate that accurate histologic segmentation is achievable by this approach."
1592398,14133,422,Unexpected results in automatic list extraction on the web,2011,"The discovery and extraction of general lists on the Web continues to be an important problem facing theWeb mining community. There have been numerous studies that claim to automatically extract structured data (i.e. lists, record sets, tables, etc.) from the Web for various purposes. Our own recent experiences have shown that the list-finding methods used as part of these larger frameworks do not generalize well and therefore ought to be reevaluated. This paper briefly describes some of the current approaches, and tests them on various list-pages. Based on our findings, we conclude that analyzing aWeb page's DOM-structure is not sufficient for the general list finding task."
326611,14133,256,Sequentially Generated Instance-Dependent Image Representations for Classification,2014,"Abstract: In this paper, we investigate a new framework for image classification that adaptively generates spatial representations. Our strategy is based on a sequential process that learns to explore the different regions of any image in order to infer its category. In particular, the choice of regions is specific to each image, directed by the actual content of previously selected regions.The capacity of the system to handle incomplete image information as well as its adaptive region selection allow the system to perform well in budgeted classification tasks by exploiting a dynamicly generated representation of each image. We demonstrate the system's abilities in a series of image-based exploration and classification tasks that highlight its learned exploration and inference abilities."
1165816,14133,369,A Generalization of Residual Belief Propagation for Flexible Reduced Complexity LDPC Decoding,2011,"Beside the well known iterative Belief Propagation algorithm several alternative decoding schemes for Low-Density Parity-Check (LDPC) codes providing better performance in terms of residual error rate, convergence speed or computational complexity have been developed in the last years. Recently, Informed Dynamic Scheduling has been proposed in [1] providing different decoding strategies that dynamically decide which messages are passed throughout the decoding process. It was shown that the overall convergence can be sped up considerably and also more errors can be corrected compared to other (non-dynamic) decoding strategies. However, these improvements are somehow overshadowed by a significant amount of additional computational complexity that is needed for the selection of the messages to be updated in each decoding step. We propose two novel dynamic decoding strategies that allow for a flexible adaptation of the decoder's dynamics and reduce the additional complexity remarkably while maintaining, and in some cases even exceeding, the convergence speed and error rate performance of currently known dynamic schedules."
2368687,14133,369,Automatic Modulation Classification Using Information Theoretic Similarity Measures,2012,"Modern wireless systems employ adaptive techniques to provide high throughput while observing desired coverage, Quality of Service (QoS) and capacity. An alternative to further enhance data rate is to apply cognitive radio concepts, where a system is able to exploit unused spectrum on existing licensed bands by sensing the spectrum and opportunistically access unused portions. Techniques like Automatic Modulation Classification (AMC) could help or be vital for such scenarios. Usually, AMC implementations rely on some form of signal pre-processing, which may introduce a high computational cost or make assumptions about the received signal which may not hold (e.g. Gaussianity of noise). This work proposes a new method to perform AMC which uses a similarity measure from the Information Theoretic Learning (ITL) framework, known as correntropy coefficient. It is capable of extracting similarity measurements over a pair of random processes using higher order statistics, yielding in better similarity estimations than by using e.g. correlation coefficient. Experiments with binary modulations show that in the presence of Additive White Gaussian Noise (AWGN), a 97% success rate in classification is achieved at a Signal-to-Noise Rate (SNR) of 5dB without requiring any pre-processing at all."
812577,14133,65,Incremental learning using partial feedback for gesture-based human-swarm interaction,2012,"In this paper we consider a human-swarm interaction scenario based on hand gestures. We study how the swarm can incrementally learn hand gestures through the interaction with a human instructor providing training gestures and correction feedback. The main contribution of the paper is a novel incremental machine learning approach that makes the robot swarm learn and recognize the gestures in a distributed and decentralized fashion using binary (i.e., yes/no) feedback. It exploits cooperative information exchange and swarm's intrinsic parallelism and redundancy. We perform extensive tests using real gesture images, showing that good classification accuracies are obtained even with rather few training samples and relatively small swarms. We also show the good scalability of the approach and its relatively low requirements in terms of communication overhead."
708223,14133,422,Stackelberg games for adversarial prediction problems,2011,"The standard assumption of identically distributed training and test data is violated when test data are generated in response to a predictive model. This becomes apparent, for example, in the context of email spam filtering, where an email service provider employs a spam filter and the spam sender can take this filter into account when generating new emails. We model the interaction between learner and data generator as a Stackelberg competition in which the learner plays the role of the leader and the data generator may react on the leader's move. We derive an optimization problem to determine the solution of this game and present several instances of the Stackelberg prediction game. We show that the Stackelberg prediction game generalizes existing prediction models. Finally, we explore properties of the discussed models empirically in the context of email spam filtering."
2523048,14133,422,Robust sparse estimation of multiresponse regression and inverse covariance matrix via the L2 distance,2013,"We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data."
35877,14133,422,OMC-IDS: at the cross-roads of OLAP mining and intrusion detection,2012,"Due to the growing threat of network attacks, the efficient detection as well as the network abuse assessment are of paramount importance. In this respect, the Intrusion Detection Systems (IDS) are intended to protect information systems against intrusions. However, IDS are plugged with several problems that slow down their development, such as low detection accuracy and high false alarm rate. In this paper, we introduce a new IDS, called OMC-IDS, which integrates data mining techniques and On Line Analytical Processing (OLAP) tools. The association of the two fields can be a powerful solution to deal with the defects of IDS. Our experiment results show the effectiveness of our approach in comparison with those fitting in the same trend."
2623583,14133,235,A Computational Cognitive Model for Semantic Sub-Network Extraction from Natural Language Queries,2012,"Semantic query sub-network is the representation of a natural language query as a graph of semantically connected words. Such sub-networks can be identified as sub-graphs in larger ontologies like DBpedia or Google knowledge graph, which allows for domain and concepts identification, especially in noisy queries. In this paper, we present a novel standalone NLP technique that leverages the cognitive psychology notion of semantic forms for semantic subnetwork extraction from natural language queries. Semantic forms, borrowed from cognitive psychology models, are one of the fundamental structures employed by human cognition to construct semantic information in the brain. We propose a computational cognitive model by means of conditional random fields and explore the interaction patterns among such forms. Our results suggest that the cognitive abstraction provided by semantic forms during labelling can significantly improve parsing and sub-network extraction compared to pure lexical approaches like parts of speech tagging. We conduct experiments on approximately 5000 queries from three diverse datasets to demonstrate the robustness and efficiency of the proposed approach."
1795871,14133,422,A semi-supervised incremental clustering algorithm for streaming data,2012,"Nowadays many applications need to deal with  evolving data streams  . In this work, we propose an incremental clustering approach for the exploitation of user constraints on data streams. Conventional constraints do not make sense on streaming data, so we extend the classic notion of constraint set into a  constraint stream  . We propose methods for using the constraint stream as data items are forgotten or new items arrive. Also we present an on-line clustering approach for the cost-based enforcement of the constraints during cluster adaptation on evolving data streams. Our method introduces the concept of multi-clusters (m-clusters) to capture arbitrarily shaped clusters. An m-cluster consists of multiple dense overlapping regions, named s-clusters, each of which can be efficiently represented by a single point. Also it proposes the definition of outliers clusters in order to handle outliers while it provides methods to observe changes in structure of clusters as data evolves."
1631599,14133,422,The sum is greater than the parts: ensembling models of student knowledge in educational software,2012,"Many competing models have been proposed in the past decade for predicting student knowledge within educational software. Recent research attempted to combine these models in an effort to improve performance but have yielded inconsistent results. While work in the 2010 KDD Cup data set showed the benefits of ensemble methods, work in the Genetics Tutor failed to show similar benefits. We hypothesize that the key factor has been data set size. We explore the potential for improving student performance prediction with ensemble methods in a data set drawn from a different tutoring system, the ASSISTments Platform, which contains 15 times the number of responses of the Genetics Tutor data set. We evaluated the predictive performance of eight student models and eight methods of ensembling predictions. Within this data set, ensemble approaches were more effective than any single method with the best ensemble approach producing predictions of student performance 10% better than the best individual student knowledge model."
1995504,14133,422,A case study in a recommender system based on purchase data,2011,"Collaborative filtering has been extensively studied in the context of ratings prediction. However, industrial recommender systems often aim at predicting a few items of immediate interest to the user, typically products that (s)he is likely to buy in the near future. In a collaborative filtering setting, the prediction may be based on the user's purchase history rather than rating information, which may be unreliable or unavailable. In this paper, we present an experimental evaluation of various collaborative filtering algorithms on a real-world dataset of purchase history from customers in a store of a French home improvement and building supplies chain. These experiments are part of the development of a prototype recommender system for salespeople in the store. We show how different settings for training and applying the models, as well as the introduction of domain knowledge may dramatically influence both the absolute and the relative performances of the different algorithms. To the best of our knowledge, the influence of these parameters on the quality of the predictions of recommender systems has rarely been reported in the literature."
591502,14133,256,Feature grouping from spatially constrained multiplicative interaction,2013,"Abstract: We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other's connections. We show how frequency/orientation columns as well as topographic filter maps follow naturally from training the model on image pairs. The model also helps explain why square-pooling models yield feature groups with similar grouping properties. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model."
1680435,14133,535,Expert-based reward shaping and exploration scheme for boosting policy learning of dialogue management,2013,"This paper investigates the conditions under which expert knowledge can be used to accelerate the policy optimization of a learning agent. Recent works on reinforcement learning for dialogue management allowed to devise sophisticated methods for value estimation in order to deal all together with exploration/exploitation dilemma, sample-efficiency and non-stationary environments. In this paper, a reward shaping method and an exploration scheme, both based on some intuitive hand-coded expert advices, are combined with an efficient temporal difference-based learning procedure. The key objective is to boost the initial training stage, when the system is not sufficiently reliable to interact with real users (e.g. clients). Our claims are illustrated by experiments based on simulation and carried out using a state-of-the-art goal-oriented dialogue management framework, the Hidden Information State (HIS)."
1634710,14133,422,Prototype-based learning on concept-drifting data streams,2014,"Data stream mining has gained growing attentions due to its wide emerging applications such as target marketing, email filtering and network intrusion detection. In this paper, we propose a prototype-based classification model for evolving data streams, called SyncStream, which dynamically models time-changing concepts and makes predictions in a local fashion. Instead of learning a single model on a sliding window or ensemble learning, SyncStream captures evolving concepts by dynamically maintaining a set of prototypes in a new data structure called the P-tree. The prototypes are obtained by error-driven representativeness learning and synchronization-inspired constrained clustering. To identify abrupt concept drift in data streams, PCA and statistics based heuristic approaches are employed. SyncStream has several attractive benefits: (a) It is capable of dynamically modeling evolving concepts from even a small set of prototypes and is robust against noisy examples. (b) Owing to synchronization-based constrained clustering and the P-Tree, it supports an efficient and effective data representation and maintenance. (c) Gradual and abrupt concept drift can be effectively detected. Empirical results shows that our method achieves good predictive performance compared to state-of-the-art algorithms and that it requires much less time than another instance-based stream mining algorithm."
2411626,14133,422,DeepWalk: online learning of social representations,2014,"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or  deep learning ) from sequences of words to graphs.   DeepWalk uses local information obtained from truncated random walks to  learn  latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.   DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection."
2255879,14133,422,Scalable distributed inference of dynamic user interests for behavioral targeting,2011,"Historical user activity is key for building user profiles to predict the user behavior and affinities in many web applications such as targeting of online advertising, content personalization and social recommendations. User profiles are temporal, and changes in a user's activity patterns are particularly useful for improved prediction and recommendation. For instance, an increased interest in car-related web pages may well suggest that the user might be shopping for a new vehicle.In this paper we present a comprehensive statistical framework for user profiling based on topic models which is able to capture such effects in a fully \emph{unsupervised} fashion. Our method models topical interests of a user dynamically where both the user association with the topics and the topics themselves are allowed to vary over time, thus ensuring that the profiles remain current.   We describe a streaming, distributed inference algorithm which is able to handle tens of millions of users. Our results show that our model contributes towards improved behavioral targeting of display advertising relative to baseline models that do not incorporate topical and/or temporal dependencies. As a side-effect our model yields human-understandable results which can be used in an intuitive fashion by advertisers."
1198191,14133,422,Collaborative matrix factorization with multiple similarities for predicting drug-target interactions,2013,"We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many methods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matrices over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to automatically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization(MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are estimated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank matrices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over elements, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive performance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions."
544340,14133,256,The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization,2013,"Abstract: Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware."
2593793,14133,344,Accelerated Estimation of Conditional Random Fields using a Pseudo-Likelihood-inspired Perceptron Variant,2014,"We discuss a simple estimation approach for conditional random fields (CRFs). The approach is derived heuristically by defining a variant of the classic perceptron algorithm in spirit of pseudo-likelihood for maximum likelihood estimation. The resulting approximative algorithm has a linear time complexity in the size of the label set and contains a minimal amount of tunable hyper-parameters. Consequently, the algorithm is suitable for learning CRFbased part-of-speech (POS) taggers in presence of large POS label sets. We present experiments on five languages. Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods."
2566897,14133,422,Active learning for node classification in assortative and disassortative networks,2011,"In many real-world networks, nodes have class labels or variables that affect the network's topology. If the topology of the network is known but the labels of the nodes are hidden, we would like to select a small subset of nodes such that, if we knew their labels, we could accurately predict the labels of all the other nodes. We develop an active learning algorithm for this problem which uses information-theoretic techniques to choose which nodes to explore. We test our algorithm on networks from three different domains: a social network, a network of English words that appear adjacently in a novel, and a marine food web. Our algorithm makes no initial assumptions about how the groups connect, and performs well even when faced with quite general types of network structure. In particular, we do not assume that nodes of the same class are more likely to be connected to each other - only that they connect to the rest of the network in similar ways."
1895858,14133,344,Tree Representations in Probabilistic Models for Extended Named Entities Detection,2012,"In this paper we deal with Named Entity Recognition (NER) on transcriptions of French broadcast data. Two aspects make the task more difficult with respect to previous NER tasks: i) named entities annotated used in this work have a tree structure, thus the task cannot be tackled as a sequence labelling task; ii) the data used are more noisy than data used for previous NER tasks. We approach the task in two steps, involving Conditional Random Fields and Probabilistic Context-Free Grammars, integrated in a single parsing algorithm. We analyse the effect of using several tree representations. Our system outperforms the best system of the evaluation campaign by a significant margin."
2007545,14133,390,Hippocampus segmentation using a stable maximum likelihood classifier ensemble algorithm,2011,"We develop a new algorithm to segment the hippocampus from MR images. Our method uses a new classifier ensemble algorithm to correct segmentation errors produced by a multi-atlas based segmentation method. Our classifier ensemble algorithm searches for the maximum likelihood solution via gradient ascent optimization. Compared to the additive regression based algorithm, LogitBoost, our algorithm avoids the numerical instability problem. Experiments on a hippocampus segmentation problem using the ADNI data show that our algorithm consistently converges faster and generalizes better than AdaBoost."
46330,14133,422,Random ensemble decision trees for learning concept-drifting data streams,2011,"Few online classification algorithms based on traditional inductive ensembling focus on handling concept drifting data streams while performing well on noisy data. Motivated by this, an incremental algorithm based on random Ensemble Decision Trees for Concept-drifting data streams (EDTC) is proposed in this paper. Three variants of random feature selection are developed to implement split-tests. To better track concept drifts in data streams with noisy data, an improved twothreshold-based drifting detection mechanism is introduced. Extensive studies demonstrate that our algorithm performs very well compared to several known online algorithms based on single models and ensemble models. A conclusion is hence drawn that multiple solutions are provided for learning from concept drifting data streams with noise."
2607379,14133,422,Collaborative filtering ensemble for ranking,2011,"This paper provides the solution of the team commendo on the Track2 dataset of the KDD Cup 2011 Dror et al.. Yahoo Labs provides a snapshot of their music-rating database as dataset for the competition, consisting of approximately 62 million ratings from 250k users on 300k items. The dataset includes hierachical information about the items. The goal of the competition is to distinguish beteen High rated and Not rated items of a user. The rating scale is discrete and ranges from 0 to 100, while a High rating is a rating≥80. The error measure is the percent of false rated tracks over all users, known as the fractions of misclassifications. The task is to minimize this error rate, hence the ranking should be optimized. Our final submission is a blend of different collaborative filtering algorithms enhanced, with basic statistics. The algorithms are trained consecutively and they are blended together with a neural network. Each of the algorithms optimizes a rank error measure."
2434390,14133,422,Large scale predictive modeling for micro-simulation of 3G air interface load,2014,"This paper outlines the approach developed together with the Radio Network Strategy & Design Department of a large European telecom operator in order to forecast the Air-Interface load in their 3G network, which is used for planning network upgrades and budgeting purposes. It is based on large scale intelligent data analysis and modeling at the level of thousands of individual radio cells resulting in 30,000 models per day. It has been embedded into a scenario simulation framework that is used by end users not experienced in data mining for studying and simulating the behavior of this complex networked system, as an example of a systematic approach to the deployment step in the KDD process. This system is already in use for two years in the country where it was developed and it is a part of a standard business process. In the last six months this national operator became a competence center for predictive modeling for micro-simulation of 3G air interface load for four other operators of the same parent company."
413769,14133,235,Comparing manual text patterns and machine learning for classification of e-mails for automatic answering by a government agency,2011,"E-mails to government institutions as well as to large companies may contain a large proportion of queries that can be answered in a uniform way. We analysed and manually annotated 4,404 e-mails from citizens to the Swedish Social Insurance Agency, and compared two methods for detecting answerable e-mails: manually-created text patterns (rule-based) and machine learning-based methods. We found that the text pattern-based method gave much higher precision at 89 percent than the machine learning-based method that gave only 63 percent precision. The recall was slightly higher (66 percent) for the machine learning-based methods than for the text patterns (47 percent). We also found that 23 percent of the total e-mail flow was processed by the automatic e-mail answering system."
2621103,14133,344,A Probabilistic Approach to Persian Ezafe Recognition,2014,"In this paper, we investigate the problem of Ezafe recognition in Persian language. Ezafe is an unstressed vowel that is usually not written, but is intelligently recognized and pronounced by human. Ezafe marker can be placed into noun phrases, adjective phrases and some prepositional phrases linking the head and modifiers. Ezafe recognition in Persian is indeed a homograph disambiguation problem, which is a useful task for some language applications in Persian like TTS. In this paper, Part of Speech tags augmented by Ezafe marker (POSE) have been used to train a probabilistic model for Ezafe recognition. In order to build this model, a ten million word tagged corpus was used for training the system. For building the probabilistic model, three different approaches were used; Maximum Entropy POSE tagger, Conditional Random Fields (CRF) POSE tagger and also a statistical machine translation approach based on parallel corpus. It is shown that comparing to previous works, the use of CRF POSE tagger can achieve outstanding results."
2330231,14133,422,Understanding atrophy trajectories in alzheimer's disease using association rules on MRI images,2011,"Alzheimer's disease (AD) is associated with progressive cognitive decline leading to dementia. The atrophy/loss of brain structure as seen on Magnetic Resonance Imaging (MRI) is strongly correlated with the severity of the cognitive impairment in AD. In this paper, we set out to find associations between predefined regions of the brain (regions of interest; ROIs) and the severity of the disease. Specifically, we use these associations to address two important issues in AD: (i) typical versus atypical atrophy patterns and (ii) the origin and direction of progression of atrophy, which is currently under debate.   We observed that each AD-related ROI is associated with a wide range of severity and that the difference between ROIs is merely a difference in severity distribution. To model differences between the severity distribution of a subpopulation (with significant atrophy in certain ROIs) and the severity distribution of the entire population, we developed the concept of Distributional Association Rules. Using the Distributional Association Rules, we clustered ROIs into disease subsystems. We define a disease subsystem as a contiguous set of ROIs that are collectively implicated in AD. AD is known to be heterogeneous in the sense that multiple sets of ROIs may be related to the disease in a population. We proposed an enhancement to the association rule mining where the algorithm only discovers association rules with ROIs that form an approximately contiguous volume. Next, we applied these association rules to infer the direction of disease progression based on the support measures of the association rules. We also developed a novel statistical test to determine the statistical significance of the discovered direction.   We evaluated the proposed method on the Mayo Clinic Alzheimer's Disease Research Center (ADRC) prospective patient cohorts. The key achievements of the methodology is that it accurately identified larger disease subsystems implicated in typical and atypical AD and it successfully mapped the directions of disease progression.   The wealth of data available in Radiology gives rise to opportunities for applying this methodology to map out the trajectory of several other diseases, e.g. other neuro-degenerative diseases and cancers, most notably, breast cancer. The applicability of this method is not limited to image data, as associating predictors with severity provides valuable information in most areas of medicine as well as other industries."
278468,14133,235,"Learning relation extraction grammars with minimal human intervention: strategy, results, insights and plans",2011,"The paper describes the operation and evolution of a linguistically oriented framework for the minimally supervised learning of relation extraction grammars from textual data. Cornerstones of the approach are the acquisition of extraction rules from parsing results, the utilization of closed-world semantic seeds and a filtering of rules and instances by confidence estimation. By a systematic walk through the major challenges for this approach the obtained results and insights are summarized. Open problems are addressed and strategies for solving these are outlined."
1257798,14133,65,Monte Carlo preference elicitation for learning additive reward functions,2012,"AI agents including robots often use reward functions to evaluate tradeoffs between different states and actions and to determine optimal policies. We are particularly interested in reward functions that can be decomposed into an additive sum of subrewards that are computed on independent subproblems or features of the state space. If these subrewards capture different reward metrics, such as user satisfaction and task completion time, it is unclear how to scale the subrewards in the reward function to produce an appropriate policy. In this work, we propose and evaluate a novel Monte Carlo method for learning the scaling factors of subrewards, in which the training elicits humans' preferences between two state-action scenarios. Because the algorithm elicits preferences over explicit scenarios, it is less susceptible to human error than previous elicitation approaches. The preferences are used to generate a set of inequalities over the scaling factors that we solve efficiently using a linear program. We show that our algorithm asks for a number of preferences proportional to log of the number of scaling factor hypotheses used in the Monte Carlo method."
10501,14133,235,How Preprocessing Affects Unsupervised Keyphrase Extraction,2014,"Unsupervised keyphrase extraction techniques generally consist of candidate phrase selection and ranking techniques. Previous studies treat the candidate phrase selection and ranking as a whole, while the effectiveness of identifying candidate phrases and the impact on ranking algorithms have remained undiscovered. This paper surveys common candidate selection techniques and analyses the effect on the performance of ranking algorithms from different candidate selection approaches. Our evaluation shows that candidate selection approaches with better coverage and accuracy can boost the performance of the ranking algorithms."
84793,14133,235,Temporal classifiers for predicting the expansion of medical subject headings,2013,"Ontologies such as the Medical Subject Headings (MeSH) and the Gene Ontology (GO) play a major role in biology and medicine since they facilitate data integration and the consistent exchange of information between different entities. They can also be used to index and annotate data and literature, thus enabling efficient search and analysis. Unfortunately, maintaining the ontologies manually is a complex, error-prone, and time and personnel-consuming effort. One major problem is the continuous growth of the biomedical literature, which expands by almost 1 million new scientific papers per year, indexed by Medline. The enormous annual increase of scientific publications constitutes the task of monitoring and following the changes and trends in the biomedical domain extremely difficult. For this purpose, approaches that try to learn and maintain ontologies automatically from text and data have been developed in the past. The goal of this paper is to develop temporal classifiers in order to create, for the first time to the best of our knowledge, an automated method that may predict which regions of the MeSH ontology will expand in the near future."
2458189,14133,390,Planar deformable motion estimation incorporating mass conservation and image gradient constancy,2011,"The accuracy of optical flow estimation algorithms has been improving steadily by refining the objective function which should be optimized. A novel energy function for computing 2-D optical flow from X-ray CT images is presented. One advantage of the optical flow framework is the possibility to enforce physical constraints on the numerical solutions. The physical constraints which have been included here are: brightness constancy, gradient constancy, continuity equation based on mass conservation, and discontinuity-preserving spatio-temporal smoothness. Both qualitative and quantitative evaluation of the proposed method demonstrates that the method results in significantly better angular errors than previous optical flow techniques for estimation of deformable lung motion. Future research will include extension of the proposed techniques to 3-D."
561745,14133,256,The Neural Representation Benchmark and its Evaluation on Brain and Machine,2013,"Abstract: A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4. In our analysis of representational learning algorithms, we find that three-layer models approach the representational performance of V4 and the algorithm in [Le et al., 2012] surpasses the performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance comparable to that of IT for an intermediate level of image variation difficulty, and surpasses IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that exceeds our current estimate of IT representation performance. We hope that this benchmark will assist the community in matching the representational performance of visual cortex and will serve as an initial rallying point for further correspondence between representations derived in brains and machines."
1269112,14133,422,Activity-edge centric multi-label classification for mining heterogeneous information networks,2014,"Multi-label classification of heterogeneous information networks has received renewed attention in social network analysis. In this paper, we present an activity-edge centric multi-label classification framework for analyzing heterogeneous information networks with three unique features. First, we model a heterogeneous information network in terms of a collaboration graph and multiple associated activity graphs. We introduce a novel concept of vertex-edge homophily in terms of both vertex labels and edge labels and transform a general collaboration graph into an activity-based collaboration multigraph by augmenting its edges with class labels from each activity graph through activity-based edge classification. Second, we utilize the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity-based collaboration multigraph. We incorporate both the structure affinity and the label vicinity into a unified classifier to speed up the classification convergence. Third, we design an iterative learning algorithm, AEClass, to dynamically refine the classification result by continuously adjusting the weights on different activity-based edge classification schemes from multiple activity graphs, while constantly learning the contribution of the structure affinity and the label vicinity in the unified classifier. Extensive evaluation on real datasets demonstrates that AEClass outperforms existing representative methods in terms of both effectiveness and efficiency."
1455440,14133,422,Sparse methods for biomedical data,2012,"Following recent technological revolutions, the investigation of massive biomedical data with growing scale, diversity, and complexity has taken a center stage in modern data analysis. Although complex, the underlying representations of many biomedical data are often sparse. For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a few genes are relevant to the disease; a gene network is sparse since a regulatory pathway involves only a small number of genes; many biomedical signals are sparse or compressible in the sense that they have concise representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. Sparse methods based on the '1 norm have attracted a great amount of research efforts in the past decade due to its sparsity-inducing property, convenient convexity, and strong theoretical guarantees. They have achieved great success in various applications such as biomarker selection, biological network construction, and magnetic resonance imaging. In this paper, we review state-of-the-art sparse methods and their applications to biomedical data."
2016929,14133,422,Toward personalized care management of patients at risk: the diabetes case study,2011,"Chronic diseases constitute the leading cause of mortality in the western world, have a major impact on the patients' quality of life, and comprise the bulk of healthcare costs. Nowadays, healthcare data management systems integrate large amounts of medical information on patients, including diagnoses, medical procedures, lab test results, and more. Sophisticated analysis methods are needed for utilizing these data to assist in patient management and to enhance treatment quality at reduced costs. In this study, we take a first step towards better disease management of diabetic patients by applying state-of-the art methods to anticipate the patient's future health condition and to identify patients at high risk. Two relevant outcome measures are explored: the need for emergency care services and the probability of the treatment producing a sub-optimal result, as defined by domain experts. By identifying the high-risk patients our prediction system can be used by healthcare providers to prepare both financially and logistically for the patient needs. To demonstrate a potential downstream application for the identified high-risk patients, we explore the association between the physician treating these patients and the treatment outcome, and propose a system that can assist healthcare providers in optimizing the match between a patient and a physician.   Our work formulates the problem and examines the performance of several learning models on data from several thousands of patients. We further describe a pilot system built on the results of this analysis. We show that the risk for the two considered outcomes can be evaluated from patients' characteristics and that features of the patient-physician match improve the prediction accuracy for the treatment's success. These results suggest that personalized medicine can be valuable for high risk patients and raise interesting questions for future improvements."
2141597,14133,30,Reliable Confidence Measures for Medical Diagnosis With Evolutionary Algorithms,2011,"Conformal Predictors (CPs) are machine learning algorithms that can provide predictions complemented with valid confidence measures. In medical diagnosis, such measures are highly desirable, as medical experts can gain additional information for each machine diagnosis. A risk assessment in each prediction can play an important role for medical decision making, in which the outcome can be critical for the patients. Several classical machine learning methods can be incorporated into the CP framework. In this paper, we propose a CP that makes use of evolved rule sets generated by a genetic algorithm (GA). The rule-based GA has the advantage of being human readable. We apply our method on two real-world datasets for medical diagnosis, one dataset for breast cancer diagnosis, which contains data gathered from fine needle aspirate of breast mass; and one dataset for ovarian cancer diagnosis, which contains proteomic patterns identified in serum. Our results on both datasets show that the proposed method is as accurate as the classical techniques, while it provides reliable and useful confidence measures."
1059635,14133,390,Anatomical landmark detection using multiple instance boosting with spatial regularization,2013,"We propose a novel multiple instance boosting approach with spatial regularization for detecting anatomical landmark to alleviate the manual annotation burden and to address imprecise annotations. It features three contributions. The first is the introduction of soft max cost function for better handling the practical situation in object detection that most positive bags only contain very few true positives while including the ISR rule and AdaBoost as special examples. The second is to exploit for better detection the spatial context embedded in a medical image, specifically the grid arrangement of the training instances with strong correlation. This is in contrast with conventional methods that treat instances in a bag independently. The third is to encourage a concentrated detection response map so that the final detection result can be derived with more confidence. The latter two contributions are realized using total variation regularization. Experimentally the proposed approach achieves significantly better detection performance than state-of-the-art detection methods in detecting anatomical landmarks with few or even no annotations."
1367793,14133,422,Scalable diffusion-aware optimization of network topology,2014,"How can we optimize the topology of a networked system to bring a flu under control, propel a video to popularity, or stifle a network malware in its infancy? Previous work on information diffusion has focused on modeling the diffusion dynamics and selecting nodes to maximize/minimize influence. Only a paucity of recent studies have attempted to address the network modification problems, where the goal is to either facilitate desirable spreads or curtail undesirable ones by adding or deleting a small subset of network nodes or edges. In this paper, we focus on the widely studied linear threshold diffusion model, and prove, for the first time, that the network modification problems under this model have supermodular objective functions. This surprising property allows us to design efficient data structures and scalable algorithms with provable approximation guarantees, despite the hardness of the problems in question. Both the time and space complexities of our algorithms are linear in the size of the network, which allows us to experiment with millions of nodes and edges. We show that our algorithms outperform an array of heuristics in terms of their effectiveness in controlling diffusion processes, often beating the next best by a significant margin."
328840,14133,256,The return of AdaBoost.MH: multi-class Hamming trees,2014,"Abstract: Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH."
1179384,14133,422,Detecting anomalies in dynamic rating data: a robust probabilistic model for rating evolution,2014,"Rating data is ubiquitous on websites such as Amazon, TripAdvisor, or Yelp. Since ratings are not static but given at various points in time, a temporal analysis of rating data provides deeper insights into the evolution of a product's quality. In this work, we tackle the following question: Given the time stamped rating data for a product or service, how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous? We propose a Bayesian model that represents the rating data as sequence of categorical mixture models. In contrast to existing methods, our method does not require any aggregation of the input but it operates on the original time stamped data. To capture the dynamic effects of the ratings, the categorical mixtures are temporally constrained: Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time. Our method automatically determines the intervals where anomalies occur, and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions. For learning our model, we propose an efficient algorithm combining principles from variational inference and dynamic programming. In our experimental study we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets."
1691587,14133,422,Mining evolutionary multi-branch trees from text streams,2013,"Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this paper, we propose an evolutionary multi-branch tree clustering method for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likelihood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm."
555827,14133,256,Learning Transformations for Classification Forests,2014,"Abstract: This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework."
1540441,14133,422,"FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning",2014,"The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking problems with certain advantages over existing formulations. Our objective, in this paper, is to develop an extreme multi-label classifier that is faster to train and more accurate at prediction than the state-of-the-art Multi-label Random Forest (MLRF) algorithm [2] and the Label Partitioning for Sub-linear Ranking (LPSR) algorithm [35]. MLRF and LPSR learn a hierarchy to deal with the large number of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves significantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores."
2611962,14133,344,Measuring the Similarity between Automatically Generated Topics,2014,"Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison of the topics’ word probability distributions. This paper presents alternative approaches, including ones based on distributional semantics and knowledgebased measures, evaluated by comparison with human judgements. The best performing methods provide reliable estimates of topic similarity comparable with human performance and should be used in preference to the word probability distribution measures used previously."
2117098,14133,422,FBLG: a simple and effective approach for temporal dependence discovery from time series data,2014,"Discovering temporal dependence structure from multivariate time series has established its importance in many applications. We observe that when we look in reversed order of time, the temporal dependence structure of the time series is usually preserved after switching the roles of cause and effect. Inspired by this observation, we create a new time series by reversing the time stamps of original time series and combine both time series to improve the performance of temporal dependence recovery. We also provide theoretical justification for the proposed algorithm for several existing time series models. We test our approach on both synthetic and real world datasets. The experimental results confirm that this surprisingly simple approach is indeed effective under various circumstances."
443006,14133,235,Labelwise margin maximization for sequence labeling,2011,"In sequence labeling problems, the objective functions of most learning algorithms are usually inconsistent with evaluation measures, such as Hamming loss. In this paper, we propose an online learning algorithm that addresses the problem of labelwise margin maximization for sequence labeling. We decompose the sequence margin to per-label margins and maximize these per-label margins individually, which can result to minimize the Hamming loss of sequence. We compare our algorithm with three state-of-art methods on three tasks, and the experimental results show our algorithm outperforms the others."
2372440,14133,20552,The DLR Hierarchy of Approximate Inference,2012,"The DLR Hierarchy of Approximate Inference Michal Rosen-Zvi Michael I. Jordan Computer Sciences and Engineering Computer Science and Statistics Hebrew University of Jerusalem University of California Jerusalem, Israel 91904 Berkeley, CA 94720 Alan L. Yuille Statistics and Psychology University of California Los Angeles, CA 90095 Abstract We propose a hierarchy for approximate in- ference based on the Dobrushin, Lanford, Ruelle (DLR) equations. This hierarchy in- cludes existing algorithms, such as belief propagation, and also motivates novel algo- rithms such as factorized neighbors (FN) al- gorithms and variants of mean ﬁeld (MF) al- gorithms. In particular, we show that ex- trema of the Bethe free energy correspond to approximate solutions of the DLR equations. In addition, we demonstrate a close connec- tion between these approximate algorithms and Gibbs sampling. Finally, we compare and contrast various of the algorithms in the DLR hierarchy on spin-glass problems. The experiments show that algorithms higher up in the hierarchy give more accurate results when they converge but tend to be less sta- ble. variety of new algorithms. There are, however, some limitations to the variational point of view. Consider ﬁrst the (loopy) BP algorithm, one of the most successful approximate inference al- gorithms. Although BP can be viewed variationally as the minimization of the Bethe free energy (Yedidia et al., 2001), it is not the case that the BP iteration is a descent step in Bethe free energy, and thus its motivation from the variational framework is not en- tirely straightforward. Moreover, although algorithms that are descent algorithms can be developed, they do not necessarily outperform BP (Yuille, 2002, Welling and Teh, 2001, Kappen and Wiegerinck, 2002, Heskes et al., 2003, Ikeda et al., 2004), a fact which suggests that the behavior of BP may not be entirely under- standable from its characterization as a variational al- gorithm. Second, the characterization of algorithms as variational has thus far not proved very helpful in suggesting links between those algorithms and Markov chain Monte Carlo (MCMC), the other main source of approximate inference algorithms. Such links would be helpful in the design of hybrid algorithms. Finally, although the variational framework naturally suggests certain kinds of approximations, there may be other approximations that are also worth exploring. This paper presents a framework for the design and analysis of approximate inference algorithms that is complementary to the variational framework. The framework is based on a linear system of equations known as the Dobrushin, Lanford and Ruelle (DLR) equations (Georgii, 1988, Parisi, 1988). Solving these equations exactly is tantamount to performing exact inference, a task that is deemed impossible for the purposes of this paper. Instead, we design inference algorithms by choosing subsets of the DLR equations. Special cases of this general approach already exist in the literature (Leisink and Kappen, 2001, Pretti and Pelizzola, 2003), but the framework has not yet been exploited systematically as a source of inference al- gorithms. In the current paper we show that many INTRODUCTION The design and analysis of approximate inference al- gorithms for large-scale models remains one of the cen- tral problems in the graphical models ﬁeld. Much progress has been made in recent years by taking a variational point of view—the exact inference problem (e.g., marginalization or maximization) is expressed as an optimization problem, an approximation is made to the optimization functional or the constraint set (or both), and approximate inference algorithms are expressed in terms of the minimization of the per- turbed problem. Several algorithms that originally en- tered the graphical model ﬁeld as heuristics—including mean ﬁeld (MF) algorithms and belief propagation (BP)—have been usefully recast within a variational framework (Amit, 1992, Yedidia et al., 2001). The variational framework has also been used to derive a"
1282080,14133,20411,Descriptive modelling of text classification and its integration with other IR tasks,2011,"Nowadays, Information Retrieval (IR) systems have to deal with multiple sources of data available in different formats. Datasets can consist of complex and heterogeneous objects with relationships between them. In addition, information needs can vary wildly and they can include different tasks. As a result, the importance of flexibility in IR systems is rapidly growing. This fact is specially important in environments where the information required at different moments is very different and its utility may be contingent on timely implementation. In these cases, how quickly a new problem is solved is as important as how well you solve it.   Current systems are usually developed for specific cases. It implies that too much engineering effort is needed to adapt them when new knowledge appears or there are changes in the requirements. Furthermore, heterogeneous and linked data present greater challenges, as well as the simultaneous application of different tasks.   This research proposes the usage of descriptive approaches for three different purposes: the modelling of the specific task of Text Classification (TC), focusing on knowledge and complex data exploitation; the flexible application of models to different tasks; and the simultaneously application of different IR-tasks. This investigation will contribute to the long-term goal of achieving a descriptive and composable IR technology that provides a modular framework that knowledge engineers can compose into a task-specific solution. The ultimate goal is to develop a flexible framework that offers classifiers, retrieval models, information extractors, and other functions. In addition, those functional blocks could be customised to satisfy user needs.   Descriptive approaches allow a high-level definition of algorithms which are, in some cases, as compact as mathematical formulations. One of the expected benefits is to make the implementation clearer and the knowledge transfer easier. They allow models from different tasks to be defined as modules that can be concatenated, processing the information as a pipeline where some of the outputs of one module are the input of the following one. This combination involves minimum engineering effort due to the paradigm's Plug & Play capabilities offered by its functional syntax. This solution provides the flexibility needed to customise and quickly combine different IR-tasks and/or models.   Classification is a desired candidate for being part of a flexible IR framework because it can be required in several situations for different purposes. In particular, descriptive approaches will improve its modelling with complex and heterogeneous objects. Furthermore, we aim to show how this approach allows to apply TC models for ad-hoc retrieval (and vice versa) and their simultaneous application for complex information needs.   The main hypothesis of this research is that a seamless approach for modelling TC and its integration with other IR-tasks will provide a general framework for rapid prototyping and modelling of solutions for specific users. In addition, it will allow new complex models that take into account relationships and inference from large ontologies. The importance of flexibility for information systems and the exploitation of complex information and knowledge from heterogeneous sources are the main points for discussion. The main challenges are expressiveness and scalability. Abstraction improves flexibility and maintainability. However, it limits the modelling power. A balance between abstraction and expressiveness has to be reached. On the other hand, scalability has been traditionally a challenge for descriptive modelling. Our goal is to prove the feasibility of our approach for real-scale environments."
1905346,14133,20332,Recognizing Deception: A Model of Dynamic Belief Attribution,2011,"Social cognition is a key feature of human-level intelligence. However, social reasoning faculties are rarely included in cognitive systems. To encourage research in this direction, we introduce a practical, computational framework that enables socially aware inference .W e demonstrate the framework’s ability to model a common, complex, and under-investigated aspect of human social behavior: deception. Moreover, we show how a system implementing this framework could dynamically respond once it has detected a lie. We then discuss some of the challenges associated with deception, ending with an outline of future research directions. 1 Introducing. . . Deception A bitter reality faces the intelligent system released into the world: agents lie. Incapable of judging a statement’s veracity, the naif program believes everything it hears. The machine’s guiding principle is that every interaction provides a clear and accurate signal of the state of the world. This assumption, perhaps unfortunately, lacks credibility. People routinely mask their beliefs and goals to impress others, to avoid humiliation, to manipulate their circumstances, and generally to control social situations. Intelligent systems incapable of detecting deception may respond inappropriately to common situations and leave themselves open to potentially disastrous consequences. Consider a realistic situation initiated when a mother, K, brings her daughter, S, to the doctor’s office. K explains that S has a history of chronic sinus problems and gastrointestinal pain. This is S’s fifth visit to the doctor this year, and she has seen three physicians before this one. Moreover, S has undergone multiple tests and surgical procedures in the past four years, but K insists that her illnesses continue to reoccur. The doctor carries out a physical examination and finds no apparent medical problems, but K maintains that S routinely complains of headaches, sinus drainage, abdominal pain, and nausea. K pleads with the doctor to order new tests to help diagnose her daughter’s condition. In reality, S is quite healthy, and K has fabricated her daughter’s illnesses for years. This scenario illustrates a particularly heinous form of deception directed at medical practitioners. The underlying condition is called “fabricated or induced illness” or “M¨ unchausen by proxy” and is characterized by a caretaker who presents their ward for examination, insisting on nonexistent or induced symptoms and requesting unnecessary and potentially invasive medical procedures. The medical literature contains multiple attempts at explaining the behavior (e.g., seeking attention, asserting power or control), but acknowledges that detection alone can be difficult (Squires & Squires 2010). The physician is biased against detection both by professional ethics that encourage the most suitable treatment possible and by experience that suggests that parents typically do not manufacture illnesses for their children. Furthermore, the consequences of false accusations are high. These and other factors can lead medical professionals to overlook substantial evidence for fabricated symptoms."
1698833,14133,8494,Mapping arbitrary mathematical functions and dynamical systems to neuromorphic VLSI circuits for spike-based neural computation,2014,"Brain-inspired, spike-based computation in elec- tronic systems is being investigated for developing alternative, non-conventional computing technologies. The Neural Engineer- ing Framework provides a method for programming these devices to implement computation. In this paper we apply this approach to perform arbitrary mathematical computation using a mixed signal analog/digital neuromorphic multi-neuron VLSI chip. This is achieved by means of a network of spiking neurons with multiple weighted connections. The synaptic weights are stored in a 4-bit on-chip programmable SRAM block. We propose a parallel event-based method for calibrating appropriately the synaptic weights and demonstrate the method by encoding and decoding arbitrary mathematical functions, and by implementing dynamical systems via recurrent connections. I. INTRODUCTION Threatened by the approaching limits of Moore's law (1), semiconductor industries and research labs started investigat- ing alternative signal processing and computational approaches for developing new generations of computing technologies that can go beyond standard Complementary Metal-Oxide- Semiconductor (CMOS) solutions (2). One promising ap- proach is that of implementing brain-inspired models of neural computation, based on massively parallel networks of low- power silicon neuron circuits (3). Within this context, several promising devices have recently proposed, using both digital and analog design techniques (4)-(9). However, in order to implement full-fledged computing systems, starting from these types of devices, it is necessary to adopt a formalism that can best exploit the properties of such computing elements. The Neural Engineering Framework (NEF) (10) represents a syn- thesis of multiple approaches in computational neuroscience, computer science, communications and control theory, that can provide such formalism. This computational framework has been previously introduced in (10) and it has been used to sim- ulate in Software (SW) large spiking neural networks capable of reproducing many aspects of neural systems ranging from the neurophysiological level all the way up to the behavioral one (11). In this paper we validate the NEF by applying its princi- ples to a population of compact low-power silicon neurons, designed using neuromorphic analog circuits and fabricated using a standard 0.18 µm CMOS process. We provide ex- perimental results showing the outcome of the calibration procedures required to implement NEF, and of a successful real-time computation of mathematical functions. In addition, using this framework, we construct a dynamical system with two distributed memory states that represent the neural corre- late of working memory, and demonstrate its correct real-time performance in Hardware (HW)."
128118,14133,22113,Towards spatial methods for socially assistive robotics: validation with children with autism spectrum disorders,2011,"Socially Assistive Robotics (SAR) defines the research regarding robots which provide assistance to users through social interaction [Feil-Seifer and Matari´ 2005]. Socially assistive robots are being studied for therapeutic use with children with autism spectrum disorders (ASD). It has been observed that children with ASD interact with robots differently than with people or toys. This may indicate an intrinsic interest in such machines, which could be applied as a robot augmentation for an intervention for children with ASD. Preliminary studies suggest that robots may act as intrinsicallyrewarding social partners for children with autism. However, enabling a robot to understand social behavior, and do so while interacting with the child, is a challenging problem. Children are highly individual and thus technology used for social interaction requires recognition of a wide-range of social behavior. This work addresses the challenge of designing behaviors for socially assistive robots in order to enable them to recognize and appropriately respond to a childs free-form behavior in unstructured play contexts. The focus on free-form behavior is inspired by and grounded in existing approaches to therapeutic intervention with children with ASD. This model emphasizes creating circles of communication and fostering engagement through play. A key aspect of this approach is to recognize social behavior and use engagements to bolster social interaction behavior, and to study the ethical implications of therapeutic robotics applications. This research will present a methodology and a validated experimental framework for enabling fully autonomous robots to interact with both typically developing children and children with autism spectrum disorders (ASD) in undirected scenarios using socially appropriate behavior especially where spatial interaction is concerned. This work holds autonomous operation as a critical aspect of the development and implementation of a robot system. Save for safety interventions by a human operator, the robot system presented in this work acts of its own accord. The methodology of this work holds that free-form interaction is best served by allowing a child to move about a space as they choose, and we wish to enable a robot that can allow for such freedom and function effectively for its interaction goals. As such, the robot and child interact, in part, though distance-oriented behavior, and the robot must be able to recognize those behaviors and appropriately respond to them. An overarching goal of this work is to develop a methodology which did not preclude human-human interaction, and in fact encourages human-human interaction. We wish to use this system was to be used as an augmentation, rather than a replacement for a human therapist. There is no substitute for human-human interaction in social interaction. However, the compelling interaction between children with ASD and robots is encouraging for their use as a therapeutic aid. This work aims for the following with an eye toward therapeutic potential: • Detection and mitigation of a childs distress: we define a methodology for learning and applying a datadriven spatio-temporal model of social behavior based on proxemic features to automatically differentiate between typical child-robot interactive behavior and behavior that would suggest an aversive response. Using a Gaussian Mixture Model learned over proxemic feature data the developed system is able to detect and interpret social behavior of the child with sufficient accuracy to recognize distress on the part of the child. The robot uses this model to change its own behavior to encourage positive social interaction [Feil-Seifer and Matari´ c,"
1914924,14133,20552,Some Extensions of Probabilistic Logic,2013,"In [12], Nilsson proposed the probabilistic logic in which the truth values of logical propositions are probability values between 0 and 1. It is applicable to any logical system for which the consistency of a finite set of propositions can be established. The probabilistic inference scheme reduces to the ordinary logical inference when the probabilities of all propositions are either 0 or 1. This logic has the same limitations of other probabilistic reasoning systems of the Bayesian approach. For common sense reasoning, consistency is not a very natural assumption. We have some well known examples: {Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick is a Republican}and {Tweety is a bird, birds can fly, Tweety is a penguin}. In this paper, we shall propose some extensions of the probabilistic logic. In the second section, we shall consider the space of all interpretations, consistent or not. In terms of frames of discernment, the basic probability assignment (bpa) and belief function can be defined. Dempster's combination rule is applicable. This extension of probabilistic logic is called the evidential logic in [ 1]. For each proposition s, its belief function is represented by an interval [Spt(s), Pls(s)]. When all such intervals collapse to single points, the evidential logic reduces to probabilistic logic (in the generalized version of not necessarily consistent interpretations). Certainly, we get Nilsson's probabilistic logic by further restricting to consistent interpretations. In the third section, we shall give a probabilistic interpretation of probabilistic logic in terms of multi-dimensional random variables. This interpretation brings the probabilistic logic into the framework of probability theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical propositions. Each proposition may have true or false values; and may be considered as a random variable. We have a probability distribution for each proposition. The e-dimensional random variable (sl,..., Sn) may take values in the space of all interpretations of 2n binary vectors. We may compute absolute (marginal), conditional and joint probability distributions. It turns out that the permissible probabilistic interpretation vector of Nilsson [12] consists of the joint probabilities of S. Inconsistent interpretations will not appear, by setting their joint probabilities to be zeros. By summing appropriate joint probabilities, we get probabilities of individual propositions or subsets of propositions. Since the Bayes formula and other techniques are valid for e-dimensional random variables, the probabilistic logic is actually very close to the Bayesian inference schemes. In the last section, we shall consider a relaxation scheme for probabilistic logic. In this system, not only new evidences will update the belief measures of a collection of propositions, but also constraint satisfaction among these propositions in the relational network will revise these measures. This mechanism is similar to human reasoning which is an evaluative process converging to the most satisfactory result. The main idea arises from the consistent labeling problem in computer vision. This method is originally applied to scene analysis of line drawings. Later, it is applied to matching, constraint satisfaction and multi sensor fusion by several authors [8], [16] (and see references cited there). Recently, this method is used in knowledge aggregation by Landy and Hummel [9]."
242420,14133,11187,Online Learning of Invariant Object Recognition in a Hierarchical Neural Network,2014,"We propose the Temporal Correlation Net (TCN) as an object recognition system implementing three basic principles: forming temporal groups of features, learning in a hierarchical structure, and using feedback to predict future input. It is a further development of the Temporal Cor- relation Graph (1) and shows improved performance on standard datasets like ETH80, COIL100, and ALOI. In contrast to its predecessor it can be trained online on all levels rather than in a level per level batch mode. Training images are presented in temporal order showing objects under- going specific transformations under viewing conditions the system is sup- posed to learn invariance under. Computation time and memory demands are low because of sparse learned connectivity and efficient handling of neural activities. Visual processing is one of the brain functions most mimicked in artificial sys- tems. The aims of different modeling approaches range from theories about the brain's computational processes during cognition to practical applications like surveillance, driver assistance, or control of industrial production processes. These purposes used to be pursued using very different techniques, but the last 10 years have brought several systems that respect important properties of biologi- cal neurons and the neural architecture in the brain and properties of biological neurons, and, thanks to computational power, become applicable to large stan- dard test databases and hence interesting for practical applications. Examples include HMAX (2), HTM (3), and VisNetL (4). Three basic are fruitful in this respect: 1. Learning of temporal sequences to create invariance to transformations con- tained in the training data. 2. Learning in a hierarchical structure, such that invariance increases gradually from level to level. Additionally lower level knowledge can be reused in higher level contexts and thereby makes memory usage efficient. 3. Prediction of future signals by feedback to disambiguate noisy input."
1072150,14133,422,Batch mode active sampling based on marginal probability distribution matching,2012,"Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on certain criteria. In this article we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimize the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and two biomedical image databases demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. In addition, we present a joint optimization framework for performing both transfer and active learning simultaneously unlike the existing approaches of learning in two separate stages, that is, typically, transfer learning followed by active learning. We specifically minimize a common objective of reducing distribution difference between the domain adapted source, the queried and labeled samples and the rest of the unlabeled target domain data. Our empirical studies on two biomedical image databases and on a publicly available 20 Newsgroups dataset show that incorporation of uncertainty information and transfer learning further improves the performance of the proposed active learning based classifier. Our empirical studies also show that the proposed transfer-active method based on the joint optimization framework performs significantly better than a framework which implements transfer and active learning in two separate stages."
803860,14133,9099,Iterative Multi-View Hashing for Cross Media Indexing,2014,"Cross media retrieval engines have gained massive popularity with rapid development of the Internet. Users may perform queries in a corpus consisting of audio, video, and textual information. To make such systems practically possible for large mount of multimedia data, two critical issues must be carefully considered: (a) reduce the storage as much as possible; (b) model the relationship of the heterogeneous media data. Recently academic community have proved that encoding the data into compact binary codes can drastically reduce the storage and computational cost. However, it is still unclear how to integrate multiple information sources properly into the binary code encoding scheme.    In this paper, we study the cross media indexing problem by learning the discriminative hashing functions to map the multi-view datum into a shared hamming space. Not only meaningful within-view similarity is required to be preserved, we also incorporate the between-view correlations into the encoding scheme, where we map the similar points close together and push apart the dissimilar ones. To this end, we propose a novel hashing algorithm called  Iterative Multi-View Hashing  (IMVH) by taking these information into account simultaneously. To solve this joint optimization problem efficiently, we further develop an iterative scheme to deal with it by using a more flexible quantization model. In particular, an optimal alignment is learned to maintain the between-view similarity in the encoding scheme. And the binary codes are obtained by directly solving a series of binary label assignment problems without continuous relaxation to avoid the unnecessary quantization loss. In this way, the proposed algorithm not only greatly improves the retrieval accuracy but also performs strong robustness. An extensive set of experiments clearly demonstrates the superior performance of the proposed method against the state-of-the-art techniques on both multimodal and unimodal retrieval tasks."
204176,14133,11187,Variational EM Learning of DSBNs with Conditional Deep Boltzmann Machines,2014,"Variational EM (VEM) is an efficient parameter learning scheme for sigmoid belief networks with many layers of latent variables. The choice of the inference model that forms the variational lower bound of the log likelihood is critical in VEM learning. The mean field approx- imations and wake-sleep algorithm use simple models that are computa- tionally efficient, but may be poor approximations to the true posterior densities when the latent variables have strong mutual dependencies. In this paper, we describe a variational EM learning method of DSBNs with a new inference model known as the conditional deep Boltzmann machine (cDBM), which is an undirected graphical model capable of representing complex dependencies among latent variables. We show that this al- gorithm does not require the computation of the intractable partition function in the undirected cDBM model, and can be accelerated with contrastive learning. Performances of the proposed method are evalu- ated and compar ed on handwritten digit data. A deep sigmoid belief network (DSBN) is a generative probabilistic model that has many latent variables organized in a hierarchy, with lower layers correspond- ing to basic features and upper layers representing more abstract concepts (7), Fig.1(a). DSBNs provide great flexibility in modeling complex dependencies of the hidden causes that generate data (1). However, both maximum likelihood and expectation-maximization (EM) (4) learning become difficult for a densely connected DSBNs with many layers of latent variables, mainly because the pos- teriors, i.e., the conditional distribution of the latent variables given data, are usually complicated and prohibit efficient computation (13). In variational EM learning (VEM) (12), the intractable posterior distribution of DSBN is approximated with a computationally efficient inference model ,a nd learning becomes an iterative optimization with regards to a variational lower bound of the (log) likelihood formed using the inference model. For any legit- imate inference model, the VEM algorithm has a guaranteed convergence to a local maximum of the variational lower bound. Good choices of the inference model can lead to learning results close to that using maximum likelihood, and thus critical for the effectiveness of VEM learning. In this paper, we describe a VEM learning method for DSBNs using conditional deep Boltzmann machine (cDBM), which is an undirected graphical model capable of representing complex dependencies among latent variables. We first briefly review the gradient ascent"
331496,14133,20332,Using Visibility to Control Collective Attention in Crowdsourcing,2013,"Online crowdsourcing provides new opportunities for ordinary people to create original content. This has led to a rapidly growing volume of user-generated content, and consequently a challenge to readily identify high quality items. Due to people’s limited attention, the presentation of content strongly affects how people allocate effort to the available content. We evaluate this effect experimentally using Amazon Mechanical Turk and show that it is possible to manipulate attention to accomplish desired goals. Peer production systems allow ordinary people to contribute original content in the form of photos (e.g., Flickr, Instagram), videos (e.g., YouTube, Vimeo), news and text (e.g., Twitter, blogs), reviews (e.g., Amazon, Yelp), and much more. While the quantity of user-generated content has skyrocketed, its quality varies dramatically: on YouTube, for example, one can find home videos of a four year old’s first violin recital, as well as virtuoso performances by accomplished violinists. Content providers have innovative methods to identify high quality content based on crowdsourcing or peer recommendation. Social news aggregator Digg, Flickr and Yelp for example, ask their users to recommend interesting news stories, photos and restaurants respectively, and prominently feature most recommended items. Content providers face a dual goal: identify quality content while keeping users engaged by showing them quality content. These goals often conflict. To understand why, consider a simple content evaluation strategy in which the provider shows users a random selection of items and asks them to evaluate them, for example, by recommending interesting items. After enough people examine an item, its quality, or how interesting it is to people, should be reflected in the number of recommendations it receives (its popularity) (Salganik, Dodds, and Watts 2006). However, if there are only a few quality items, the selections seen by most people may not contain a single interesting item. When users continue to see low quality content, they may not return to the site. The provider could instead present the highest quality items that it knows about, but if users do not inspect all items, some high quality items may be missed."
2805328,14133,20332,Logistic methods for resource selection functions and presence-only species distribution models,2011,"In order to better protect and conserve biodiversity, ecologists use machine learning and statistics to understand how species respond to their environment and to predict how they will respond to future climate change, habitat loss and other threats. A fundamental modeling task is to estimate the probability that a given species is present in (or uses) a site, conditional on environmental variables such as precipitation and temperature. For a limited number of species, survey data consisting of both presence and absence records are available, and can be used to fit a variety of conventional classification and regression models. For most species, however, the available data consist only of occurrence records — locations where the species has been observed. In two closely-related but separate bodies of ecological literature, diverse special-purpose models have been developed that contrast occurrence data with a random sample of available environmental conditions. The most widespread statistical approaches involve either fitting an exponential model of species' conditional probability of presence, or fitting a naive logistic model in which the random sample of available conditions is treated as absence data; both approaches have well-known drawbacks, and do not necessarily produce valid probabilities. After summarizing existing methods, we overcome their drawbacks by introducing a new scaled binomial loss function for estimating an underlying logistic model of species presence/absence. Like the Expectation-Maximization approach of Ward et al. and the method of Steinberg and Cardell, our approach requires an estimate of population prevalence, Pr(y = 1), since prevalence is not identifiable from occurrence data alone. In contrast to the latter two methods, our loss function is straightforward to integrate into a variety of existing modeling frameworks such as generalized linear and additive models and boosted regression trees. We also demonstrate that approaches by Lele and Keim and by Lancaster and Imbens that surmount the identifiability issue by making parametric data assumptions do not typically produce valid probability estimates."
1771239,14133,8927,LASER: a scalable response prediction platform for online advertising,2014,"We describe LASER, a scalable response prediction platform currently used as part of a social network advertising system. LASER enables the familiar logistic regression model to be applied to very large scale response prediction problems, including ones beyond advertising. Though the underlying model is well understood, we apply a whole-system approach to address model accuracy, scalability, explore-exploit, and real-time inference. To facilitate training with both large numbers of training examples and high dimensional features on commodity clustered hardware, we employ the Alternating Direction Method of Multipliers (ADMM). Because online advertising applications are much less static than classical presentations of response prediction, LASER employs a number of techniques that allows it to adapt in real time. LASER models can be divided into components with different re-training frequencies, allowing us to learn from changes in ad campaign performance frequently without incurring the cost of retraining larger, more stable sections of the model. Thompson sampling during online inference further helps by efficiently balancing exploration of new ads with exploitation of long running ones. To enable predictions made with the most recent feature data, we employ a range of techniques, including extensive caching and lazy evaluation, to permit real time, low latency scoring. LASER models are defined using a configuration language that ties together the training, validation, and inference pieces and permits even non-programming analysts to experiment with different model structures without modifications to code or interruptions to running servers. Finally, we show via extensive offline experiments and online A/B tests that this system provides significant benefits to prediction accuracy, gains in revenue and CTR, and reductions in system latency."
2636040,14133,20332,An Automated Machine Learning Approach Applied to Robotic Stroke Rehabilitation,2012,"While machine learning methods have proven to be a highly valuable tool in solving numerous problems in assistive technology, state-of-the-art machine learning algorithms and corresponding results are not always accessible to assistive technology researchers due to required domain knowledge and complicated model parameters. This short paper highlights the use of recent work in machine learning to entirely automate the machine learning pipeline, from feature extraction to classification. A nonparametrically guided autoencoder is used to extract features and perform classification while Bayesian optimization is used to automatically tune the parameters of the model for best performance. Empirical analysis is performed on a real-world rehabilitation research problem. The entirely automated approach significantly outperforms previously published results using carefully tuned machine learning algorithms on the same data. As better healthcare worldwide is improving longevity and the baby boomer generation is aging, the proportion of elderly adults within the population is rapidly growing. Healthcare systems and governments are seeking new ways to alleviate the burden on society of caring for this aging population. Artificial intelligence has been shown to be a promising solution, as many of the simpler tasks that burden caregivers can be automated. This also suggests solutions for promoting independence and aging in place, because it alleviates the need for the constant presence of a caregiver in the home. The benefits of the application of machine learning to problems in assistive technology are becoming ever more clear. However, the application of machine learning to problems in assistive technology remains challenging. In particular, it is often unclear what machine learning model or approach is most appropriate for a given task. A common paradigm is to apply multiple standard machine learning tools in a black box manner and compare the results. This proceeds according to the following steps: 1. Collect data representative of the problem of interest. 2. Extract a set of features from these data. Copyright c 2012, Association for the Advancement of Artificial"
14233,14133,8231,Constrained-hLDA for Topic Discovery in Chinese Microblogs,2014,"Since microblog service became information provider on web scale, research on microblog has begun to focus more on its content mining. Most research on microblog context is often based on topic models, such as: Latent Dirichlet Allocation(LDA) and its variations. However,there are some challenges in previous research. On one hand, the number of topics is fixed as a priori, but in real world, it is input by the users. On the other hand, it ignores the hierar- chical information of topics and cannot grow structurally as more data are ob- served. In this paper, we propose a semi-supervised hierarchical topic model, which aims to explore more reasonable topics in the data space by incorporating some constraints into the modeling process that are extracted automatically. The new method is denoted as constrained hierarchical Latent Dirichlet Allocation (constrained-hLDA). We conduct experiments on Sina microblog, and evaluate the performance in terms of clustering and empirical likelihood. The experimen- tal results show that constrained-hLDA has a significant improvement on the in- terpretability, and its predictive ability is also better than that of hLDA. In the information explosion era, social network not only contains relationships, but also much unstructured information such as context. Furthermore, how to effectively dig out latent topics and internal semantic structures from social network is an im- portant research issue. Early work on microblogs mainly focused on user relationship and community structure. (1) studied the topological and geographical properties of Twitter. Others work such as (2) studied user behaviors and geographic growth pat- terns of Twitter. Only little research on content analysis of microblog was proposed recently. (3) was mainly based on traditional text mining algorithms. (4) proposed MB- LDA by overall considering contactor relevance relation and document relevance re- lation of microblogs. In this paper, we propose a novel probabilistic generative model based on hLDA, called constrained-hLDA, which focuses on both text content and topic hierarchy."
2040172,14133,8960,Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding,2012,"Modelling natural images with sparse coding (SC) has faced two main challenges: flexibly representing varying pixel intensities and realistically representing low-level image components. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA) instead of a linear combination. The major challenge is parameter optimization because a model with either (1) or (2) results in strongly multimodal posteriors. We show for the first time that a model combining both improvements can be trained efficiently while retaining the rich structure of the posteriors. We design an exact piece-wise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model's predictions with in vivo neural recordings. In contrast to standard SC, we find that the optimal prior favors asymmetric and bimodal activity of simple cells. Testing our model for consistency we find that the average posterior is approximately equal to the prior. Furthermore, we find that the model predicts a high percentage of globular receptive fields alongside Gabor-like fields. Similarly high percentages are observed in vivo. Our results thus argue in favor of improvements of the standard sparse coding model for simple cells by using flexible priors and nonlinear combinations."
2044961,14133,20358,Crowdsourced judgement elicitation with endogenous proficiency,2013,"Crowdsourcing is now widely used to replace judgement or evaluation by an expert authority with an aggregate evaluation from a number of non-experts, in applications ranging from rating and categorizing online content all the way to evaluation of student assignments in massively open online courses (MOOCs) via peer grading. A key issue in these settings, where direct monitoring of both effort and accuracy is infeasible, is incentivizing agents in the 'crowd' to put in  effort  to make good evaluations, as well as to truthfully report their evaluations. We study the design of mechanisms for crowdsourced judgement elicitation when workers strategically choose both their reports and the effort they put into their evaluations. This leads to a new family of information elicitation problems with unobservable ground truth, where an agent's proficiency--- the probability with which she correctly evaluates the underlying ground truth--- is  endogenously  determined by her strategic choice of how much effort to put into the task.   Our main contribution is a simple, new, mechanism for binary information elicitation for multiple tasks when agents have  endogenous proficiencies , with the following properties: (i) Exerting maximum effort followed by truthful reporting of observations is a Nash equilibrium. (ii) This is the equilibrium with  maximum payoff  to all agents, even when agents have different maximum proficiencies, can use mixed strategies, and can choose a different strategy for each of their tasks. Our information elicitation mechanism requires only minimal bounds on the priors, asks agents to only report their own evaluations, and does not require any conditions on a diverging number of agent reports per task to achieve its incentive properties. The main idea behind our mechanism is to use the presence of multiple tasks and ratings to estimate a reporting statistic to identify and penalize low-effort agreement--- the mechanism rewards agents for agreeing with another 'reference' report on the same task, but also penalizes for  blind agreement  by subtracting out this statistic term, designed so that agents obtain rewards only when they put in effort into their observations."
2620473,14133,20332,Spatiotemporal Knowledge Representation and Reasoning under Uncertainty for Action Recognition in Smart Homes,2011,"We apply artificial intelligence techniques to perform data analysis and activity recognition in smart homes. Sensors embedded in smart home provide primary data for reasoning about observations. The final goal is to provide appropriate assistance for residents to complete their Daily living Activities. Here, we introduce a qualitative approach that considers spatiotemporal specifications of activities in the Activity Recognition Agent to do knowledge representation and reasoning about the observations. We consider different existing uncertainties within sensors observations and Observed Agent’s activities. In the introduced approach, the more details about environment context would cause the less activity recognition process complexity and more precise functionality. To represent the knowledge, we apply the fuzzy logic to represent the world state by the fuzzified received values from sensors. The knowledge would be represented in the fuzzy context frame. To reduce the amount of collected data, meaningful changes in sensors generated values are considered to do Activity Recognition. Applying possibility distributions for event occurrence orders and sequences within different scenarios of activities realization, we are able to generate hypotheses about future possible occur-able events. The possible occur-able events and fuzzy digit parameters of their possible happening moments are represented in matrix format. The hypotheses about possible future observable contexts are generated considering spatial, temporal and other environmental parameters and then they would be ranked. Our final goal is to better explain the observations. If no possible explanation about observation be found, it would be recognized as abnormal behavior. In the case that no expected event be observed, we can reason that maybe event has occurred but not triggered and so next available events in previously learned scenarios would be expected. The system patience for number of possible missed events depends to trade-off between the degrees of resident's forgetfulness and probability of events trigger by applied sensors."
1750598,14133,422,Multi-source learning with block-wise missing data for Alzheimer's disease prediction,2013,"With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both the complete and incomplete models. We have performed comprehensive evaluations of the proposed models on the application of AD diagnosis. Our proposed models compare favorably against existing approaches."
2520501,14133,422,FeaFiner: biomarker identification from medical data through feature generalization and selection,2013,"Traditionally, feature construction and feature selection are two important but separate processes in data mining. However, many real world applications require an integrated approach for creating, refining and selecting features. To address this problem, we propose  FeaFiner  (short for Feature Refiner), an efficient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Specifically, we formulate a double sparsity optimization problem that identifies groups in the low-level features, generalizes higher level features using the groups and performs feature selection. Since in many clinical researches non- overlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed formulation is challenging to solve due to the orthogonality constraints, non-convexity objective and non-smoothness penal- ties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non-monotone spectral projected gradient method. Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality. We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation.   Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cognitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy."
1743997,14133,20796,Learning to Propagate Rare Labels,2014,"Label propagation is a well-explored family of methods for training a semi-supervised classifier where input data points (both labeled and unlabeled) are connected in the form of a weighted graph. For binary classification, the performance of these methods starts degrading considerably whenever input dataset exhibits following characteristics - (i)  one of the class label is rare label or equivalently, class imbalance (CI) is very high , and (ii)  degree of supervision (DoS) is very low -- defined as fraction of labeled points . These characteristics are common in many real-world datasets relating to network fraud detection. Moreover, in such applications, the amount of class imbalance is not known a priori. In this paper, we have proposed and justified the use of an alternative formulation for graph label propagation under such extreme behavior of the datasets. In our formulation, objective function is the  difference of two convex quadratic functions  and the constraints are box constraints. We solve this program using  Concave-Convex Procedure (CCCP) . Whenever the problem size becomes too large, we suggest to work with a  k -NN subgraph of the given graph which can be sampled by using  Locality Sensitive Hashing (LSH)  technique. We have also discussed various issues that one typically faces while sampling such a  k -NN subgraph in practice. Further, we have proposed a novel  label flipping  method on top of the CCCP solution, which improves the result of CCCP further whenever class imbalance information is made available a priori. Our method can be easily adopted for a MapReduce platform, such as Hadoop. We have conducted experiments on 11 datasets comprising a graph size of up to 20K nodes, CI as high as 99:6%, and DoS as low as 0:5%. Our method has resulted up to 19:5-times improvement in  F -measure and up to 17:5-times improvement in AUC-PR measure against baseline methods."
1245379,14133,422,Supervised deep learning with auxiliary networks,2014,"Deep learning well demonstrates its potential in learning latent feature representations. Recent years have witnessed an increasing enthusiasm for regularizing deep neural networks by incorporating various side information, such as user-provided labels or pairwise constraints. However, the effectiveness and parameter sensitivity of such algorithms have been major obstacles for putting them into practice. The major contribution of our work is the exposition of a novel supervised deep learning algorithm, which distinguishes from two unique traits. First, it regularizes the network construction by utilizing similarity or dissimilarity constraints between data pairs, rather than sample-specific annotations. Such kind of side information is more flexible and greatly mitigates the workload of annotators. Secondly, unlike prior works, our proposed algorithm decouples the supervision information and intrinsic data structure. We design two heterogeneous networks, each of which encodes either supervision or unsupervised data structure respectively. Specifically, we term the supervision-oriented network as auxiliary network since it is principally used for facilitating the parameter learning of the other one and will be removed when handling out-of-sample data. The two networks are complementary to each other and bridged by enforcing the correlation of their parameters. We name the proposed algorithm SUpervision-Guided AutoencodeR (SUGAR). Comparing prior works on unsupervised deep networks and supervised learning, SUGAR better balances numerical tractability and the flexible utilization of supervision information. The classification performance on MNIST digits and eight benchmark datasets demonstrates that SUGAR can effectively improve the performance by using the auxiliary networks, on both shallow and deep architectures. Particularly, when multiple SUGARs are stacked, the performance is significantly boosted. On the selected benchmarks, ours achieve up to 11.35% relative accuracy improvement compared to the state-of-the-art models."
2239635,14133,8960,Fast Sampling-Based Inference in Balanced Neuronal Networks,2014,"Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong - but transient - selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains."
2090448,14133,8960,Strategic Impatience in Go/NoGo versus Forced-Choice Decision-Making,2012,"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by eliminating the need for response selection as in 2AFC, a consistent tendency for subjects to make more Go responses (both higher hits and false alarm rates) in the GNG task raises the concern that there may be fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1, 2]) and the related leaky competing accumulator models [3, 4], capture various aspects of behavioral performance, but do not clarify the provenance of the Go bias in GNG. We postulate that this impatience to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of the 2AFC and GNG tasks: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes not only error rate but also average decision delay naturally exhibits the experimentally observed Go bias. The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again just before the response deadline. The initial rise in the threshold is due to the diminishing temporal advantage of choosing the fast Go response compared to the fixed-delay NoGo response. We also show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such fixed-threshold approximations cannot reproduce the Go bias. Our results suggest that observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and thus need not imply any other difference in the underlying sensory and cognitive processes."
2203008,14133,11166,Mining Summaries of Propagations,2013,"Analyzing the traces left by a meme of information propagating through a social network or by a user browsing a website can help to unveil the structure and dynamics of such complex networks. This may in turn open the door to concrete applications, such as finding influential users for a topic in a social network, or detecting the typical structure of a web browsing session that leads to a product purchase. In this paper we define the problem of mining summaries of propagations as a constrained pattern-mining problem. A propagation is a DAG where an entity (e.g., information exchanged in a social network, or a user browsing a website) flows following the underlying hierarchical structure of the nodes. A summary is a set of propagations that (i) involve a similar population of nodes, and (ii) exhibit a coherent hierarchical structure when merged altogether to form a single graph. The first constraint is defined based on the Jaccard coefficient, while the definition of the second one relies on the graph-theoretic concept of agony of a graph. It turns out that both constraints satisfy the downward closure property, thus enabling Apriori-like algorithms. However, motivated by the fact that computing agony is much more expensive than computing Jaccard, we devise two algorithms that explore the search space differently. The first algorithm is an Apriori-like, bottom-up method that checks both the constraints level-by-level. The second algorithm consists of a first phase where the search space is pruned as much as possible by exploiting the Jaccard constraint only, while involving the second constraint only afterwards, in a subsequent phase. We test our algorithms on four real-world datasets. Quantitative results reveal that the choice of the most efficient algorithm depends on the selectivity of the two constraints. Qualitative results show the relevance of the extracted summaries in a number of real-world scenarios."
1150709,14133,20411,Effective sentiment stream analysis with self-augmenting training and demand-driven projection,2011,"How do we analyze sentiments over a set of opinionated Twitter messages? This issue has been widely studied in recent years, with a prominent approach being based on the application of classification techniques. Basically, messages are classified according to the implicit attitude of the writer with respect to a query term. A major concern, however, is that Twitter (and other media channels) follows the data stream model, and thus the classifier must operate with limited resources, including labeled data for training classification models. This imposes serious challenges for current classification techniques, since they need to be constantly fed with fresh training messages, in order to track sentiment drift and to provide up-to-date sentiment analysis.   We propose solutions to this problem. The heart of our approach is a training augmentation procedure which takes as input a small training seed, and then it automatically incorporates new relevant messages to the training data. Classification models are produced on-the-fly using association rules, which are kept up-to-date in an incremental fashion, so that at any given time the model properly reflects the sentiments in the event being analyzed. In order to track sentiment drift, training messages are projected on a demand driven basis, according to the content of the message being classified. Projecting the training data offers a series of advantages, including the ability to quickly detect trending information emerging in the stream. We performed the analysis of major events in 2010, and we show that the prediction performance remains about the same, or even increases, as the stream passes and new training messages are acquired. This result holds for different languages, even in cases where sentiment distribution changes over time, or in cases where the initial training seed is rather small. We derive lower-bounds for prediction performance, and we show that our approach is extremely effective under diverse learning scenarios, providing gains that range from 7% to 58%."
744069,14133,422,Trading representability for scalability: adaptive multi-hyperplane machine for nonlinear classification,2011,"Support Vector Machines (SVMs) are among the most popular and successful classification algorithms. Kernel SVMs often reach state-of-the-art accuracies, but suffer from the curse of kernelization due to linear model growth with data size on noisy data. Linear SVMs have the ability to efficiently learn from truly large data, but they are applicable to a limited number of domains due to low representational power. To fill the representability and scalability gap between linear and nonlinear SVMs, we propose the Adaptive Multi-hyperplane Machine (AMM) algorithm that accomplishes fast training and prediction and has capability to solve nonlinear classification problems. AMM model consists of a set of hyperplanes (weights), each assigned to one of the multiple classes, and predicts based on the associated class of the weight that provides the largest prediction. The number of weights is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum. Since the generalization bound decreases with the number of weights, a weight pruning mechanism is proposed and analyzed. The experiments on several large data sets show that AMM is nearly as fast during training and prediction as the state-of-the-art linear SVM solver and that it can be orders of magnitude faster than kernel SVM. In accuracy, AMM is somewhere between linear and kernel SVMs. For example, on an OCR task with 8 million highly dimensional training examples, AMM trained in 300 seconds on a single-core processor had 0.54% error rate, which was significantly lower than 2.03% error rate of a linear SVM trained in the same time and comparable to 0.43% error rate of a kernel SVM trained in 2 days on 512 processors. The results indicate that AMM could be an attractive option when solving large-scale classification problems. The software is available at www.dabi.temple.edu/~vucetic/AMM.html."
595961,14133,20552,Propagation of Belief Functions: A Distributed Approach,2013,"In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence. We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework. Bayesian schemes, on the other hand, often make unrealistic demands for structure. The propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done. We would like to emphasize that the basic idea of local computation for propagating probabilities is due to Judea Pearl. It is a very innovative idea; we do not believe that it can be found in the Bayesian literature prior to Pearl's work. We see our contribution as extending the usefulness of Pearl's idea by generalizing it from Bayesian probabilities to belief functions. In the next section, we give a brief introduction to belief functions. The notions of qualitative independence for partitions and a qualitative Markov tree are introduced in Section III. Finally, in Section IV, we describe a scheme for propagating belief functions in qualitative Markov trees."
1666733,14133,23620,Synthesis of biological models from mutation experiments,2013,"Executable biology presents new challenges to formal methods. This paper addresses two problems that cell biologists face when developing formally analyzable models.   First, we show how to automatically synthesize a concurrent in-silico model for cell development given in-vivo experiments of how particular mutations influence the experiment outcome. The problem of synthesis under mutations is unique because mutations may produce non-deterministic outcomes (presumably by introducing races between competing signaling pathways in the cells) and the synthesized model must be able to replay all these outcomes in order to faithfully describe the modeled cellular processes. In contrast, a regular concurrent program is correct if it picks any outcome allowed by the non-deterministic specification. We developed synthesis algorithms and synthesized a model of cell fate determination of the earthworm C. elegans. A version of this model previously took systems biologists months to develop.   Second, we address the problem of under-constrained specifications that arise due to incomplete sets of mutation experiments. Under-constrained specifications give rise to distinct models, each explaining the same phenomenon differently. Addressing the ambiguity of specifications corresponds to analyzing the space of plausible models. We develop algorithms for detecting ambiguity in specifications, i.e., whether there exist alternative models that would produce different fates on some unperformed experiment, and for removing redundancy from specifications, i.e., computing minimal non-ambiguous specifications.   Additionally, we develop a modeling language and embed it into Scala. We describe how this language design and embedding allows us to build an efficient synthesizer. For our C. elegans case study, we infer two observationally equivalent models expressing different biological hypotheses through different protein interactions. One of these hypotheses was previously unknown to biologists."
38310,14133,11321,Ensemble Methods for Structured Prediction,2014,"We present a series of learning algorithms and theoretical guarantees for designing accurate en- sembles of structured prediction tasks. This in- cludes several randomized and deterministic al- gorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style al- gorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conver- sions and learning guarantees. We also report the results of extensive experiments with these algo- rithms in several structured prediction tasks. pronunciation models or experts are available for transcrib- ing words into sequences of phonemes. These models may have been derived using other machine learning algorithms or they may be based on carefully hand-crafted rules. In general, none of these pronunciation experts is fully ac- curate and each expert may be making mistakes at differ- ent positions in the output sequence. One can hope that a model that patches together the pronunciation of different experts could achieve a superior performance. Similar ensemble structured prediction problems arise in other tasks, including machine translation, part-of-speech tagging, optical character recognition and computer vision, with structures or substructures varying with each task. We seek to tackle all of these problems simultaneously and consider the general setting where the label or output as- sociated to an input x2X is a structure y2Y that can be decomposed and represented by l substructures y 1 ,...,y l . For the pronunciation example just discussed, x is a spe- cific word or word sequence and y its phonemic transcrip- tion. A natural choice for the substructures y k is then the individual phonemes forming y. Other possible choices in- clude n-grams of consecutive phonemes or more general subsequences."
884934,14133,422,Tracking trends: incorporating term volume into temporal topic models,2011,"Text corpora with documents from a range of time epochs are natural and ubiquitous in many fields, such as research papers, newspaper articles and a variety of types of recently emerged social media. People not only would like to know what kind of topics can be found from these data sources but also wish to understand the temporal dynamics of these topics and predict certain properties of terms or documents in the future. Topic models are usually utilized to find latent topics from text collections, and recently have been applied to temporal text corpora. However, most proposed models are general purpose models to which no real tasks are explicitly associated. Therefore, current models may be difficult to apply in real-world applications, such as the problems of tracking trends and predicting popularity of keywords. In this paper, we introduce a real-world task, tracking trends of terms, to which temporal topic models can be applied. Rather than building a general-purpose model, we propose a new type of topic model that incorporates the volume of terms into the temporal dynamics of topics and optimizes estimates of term volumes. In existing models, trends are either latent variables or not considered at all which limits the potential for practical use of trend information. In contrast, we combine state-space models with term volumes with a supervised learning model, enabling us to effectively predict the volume in the future, even without new documents. In addition, it is straightforward to obtain the volume of latent topics as a by-product of our model, demonstrating the superiority of utilizing temporal topic models over traditional time-series tools (e.g., autoregressive models) to tackle this kind of problem. The proposed model can be further extended with arbitrary word-level features which are evolving over time. We present the results of applying the model to two datasets with long time periods and show its effectiveness over non-trivial baselines."
981351,14133,11491,Learning reconfigurable hashing for diverse semantics,2011,"In recent years, locality-sensitive hashing (LSH) has gained plenty of attention from both the multimedia and computer vision communities due to its empirical success and theoretic guarantee in large-scale visual indexing and retrieval. Conventional LSH algorithms are designated either for generic metrics such as Cosine similarity, e 2 -norm and Jaccard index, or for the metrics learned from user-supplied supervision information. The common drawbacks of existing algorithms are their incapability to be adapted to metric changes, along with the inefficacy when handling diverse semantics ( e. g ., more than 1K different categories in the well-known  ImageNet  database). For the metrics underlying the hashing structure, even tiny changes tend to nullify previous indexing efforts, which motivates our proposed framework towards reconfigurable hashing. The basic idea is to maintain a large pool of over-complete hashing functions embedded in the ambient feature space, which serves as the common infrastructure of high-level diverse semantics. At the runtime, the algorithm dynamically selects relevant hashing bits by maximizing the consistency to specific semantics-induced metric, thereby achieving reusability of the pre-computed hashing bits. Such a reusable scheme especially benefits the indexing and retrieval of large-scale dataset, since it facilitates one-off indexing rather than continuous computation-intensive maintenance towards metric adaptation. We propose a sequential bit-selection algorithm based on local consistency and global regularization. Extensive studies are conducted on large-scale image benchmarks to comparatively investigate the performance of different strategies on reconfigurable hashing. Despite the vast literature on hashing, to our best knowledge rare endeavors have been spent toward the reusability of hashing structures in large-scale datasets."
741456,14133,23757,Multi-label collective classification in multi-attribute multi-relational network data,2014,"Classical machine learning techniques assume the data to be i.i.d., but the real world data is inherently relational and can generally be represented using graphs or some variants of a graph representation. The importance of modeling relational data is evident from its increasing presence in many domains: Telecom networks, WWW, social networks, organizational networks, images, protein sequences, etc. This field has recently been receiving a lot of attention in various communities under different themes depending on the problem addressed and the nature of solution proposed. Collective classification is one such popular approach which involves the use of a local classifier that embeds the node's own attributes and neighbors' information in a feature vector, and classifies the nodes in an iterative procedure. Despite the increasing popularity, there is not much attention paid towards datasets with multiple attributes and multi-relational (MAMR) networks under multi-label scenarios. In MAMR data, nodes can be represented using multiple types of attributes (attribute views) and there are multiple link types between the nodes. For example, in Twitter, users can be represented using their tweets, urls shared, hashtags and list memberships. And different Twitter users can be connected using follower, followed by and re-tweet links. Secondly, in many networks, nodes are associated with more than one label. For instance, Twitter users can be tagged with one or more labels from a set L, where L contains various movie genres that a user might like. Motivated by this, we propose a learning technique for multi-label collective classification using multiple attribute views on multi-relational network data which captures complex label correlations within and across attribute/relationship types. We empirically evaluate our proposed approach on Twitter and MovieLens datasets, and we show that it performs better than the state-of-art approaches."
1700613,14133,9616,Children Gender Recognition Under Unconstrained Conditions Based on Contextual Information,2014,"One of the biggest challenges faced by law enforcement entities in the present digital era, is fighting against online Child Sexual Abuse (CSA), due in particular to the massive amount of data that they receive for analysis. Pattern recognition system can provide an aid, e.g., to ease the identification of both the perpetrator and the victim of the crime. In particular, ancillary cues related the identity of the involved persons, like age, race or gender, can represent a significant aid for identification. These cues can be estimated using statistical classifiers on face features. In this work, we explore one of these ancillary cues, namely the gender. The research community has provided methods for gender recognition able to achieve good performance with adults. However, in the case of CSA, victims are minors (typically, very young children). Children gender recognition may be difficult even for humans, due to the lack of many gender-specific face traits usually present in adult faces. Totally uncontrolled poses and illumination conditions, that might be found in CSA material, represent an additional issue. We propose to tackle this problem by the use of contextual information to complement face features used by traditional algorithms. In particular, we exploit the image context of the face, that is, the portion of the image surrounding the face. This is motivated by the usage that humans themselves make of face external information, such as the hair or earrings, to take decisions on this task. The proposed approach is tested on a novel data base of faces of children, collected from royalty-free stock-photography web sites, which show totally unconstrained conditions. The reported results are promising and set the way for a deeper study of the use of the face context for estimating ancillary identification cues."
2408660,14133,8960,Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models,2012,"Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be naturally implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb's rule and describe an extension of this work which allows us to deal with time varying and correlated latent causes."
1292472,14133,422,A multi-task learning formulation for predicting disease progression,2011,"Alzheimer's Disease (AD), the most common type of dementia, is a severe neurodegenerative disorder. Identifying markers that can track the progress of the disease has recently received increasing attentions in AD research. A definitive diagnosis of AD requires autopsy confirmation, thus many clinical/cognitive measures including Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) have been designed to evaluate the cognitive status of the patients and used as important criteria for clinical diagnosis of probable AD. In this paper, we propose a multi-task learning formulation for predicting the disease progression measured by the cognitive scores and selecting markers predictive of the progression. Specifically, we formulate the prediction problem as a multi-task regression problem by considering the prediction at each time point as a task. We capture the intrinsic relatedness among different tasks by a temporal group Lasso regularizer. The regularizer consists of two components including an L2,1-norm penalty on the regression weight vectors, which ensures that a small subset of features will be selected for the regression models at all time points, and a temporal smoothness term which ensures a small deviation between two regression models at successive time points. We have performed extensive evaluations using various types of data at the baseline from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database for predicting the future MMSE and ADAS-Cog scores. Our experimental studies demonstrate the effectiveness of the proposed algorithm for capturing the progression trend and the cross-sectional group differences of AD severity. Results also show that most markers selected by the proposed algorithm are consistent with findings from existing cross-sectional studies."
1547094,14133,20358,How to influence people with partial incentives,2014,"We study the power of fractional allocations of resources to maximize our influence in a network. This work extends in a natural way the well-studied model by Kleinberg, Kempe, and Tardos (2003), where a designer selects a (small) seed set of nodes in a social network to influence directly, this influence cascades when other nodes reach certain thresholds of neighbor influence, and the goal is to maximize the final number of influenced nodes. Despite extensive study from both practical and theoretical viewpoints, this model limits the designer to a binary choice for each node, with no chance to apply intermediate levels of influence. This model captures some settings precisely, such as exposure to an idea or pathogen, but it fails to capture very relevant concerns in others, for example, a manufacturer promoting a new product by distributing five 20% off coupons instead of giving away a single free product.   While fractional versions of problems tend to be easier to solve than integral versions, for influence maximization, we show that the two versions have essentially the same computational complexity. On the other hand, the two versions can have vastly different solutions: the added flexibility of fractional allocation can lead to significantly improved influence. Our main theoretical contribution is to show how to adapt the major positive results from the integral case to the fractional case. Specifically, Mossel and Roch (2006) used the submodularity of influence to obtain their integral results; we introduce a new notion of  continuous submodularity , and use this to obtain matching fractional results. We conclude that we can achieve the same greedy (1-1/e-e)-approximation for the fractional case as the integral case, and that other heuristics are likely to carry over as well. In practice, we find that the fractional model performs substantially better than the integral model, according to simulations on real-world social network data."
1812987,14133,20796,Coarse-to-fine classification via parametric and nonparametric models for computer-aided diagnosis,2011,"Classification is one of the core problems in Computer-Aided Diagnosis (CAD), targeting for early cancer detection using 3D medical imaging interpretation. High detection sensitivity with desirably low false positive (FP) rate is critical for a CAD system to be accepted as a valuable or even indispensable tool in radiologists' workflow. Given various spurious imagery noises which cause observation uncertainties, this remains a very challenging task. In this paper, we propose a novel, two-tiered coarse-to-fine (CTF) classification cascade framework to tackle this problem. We first obtain classification-critical data samples (e.g., implicit samples on the decision boundary) extracted from the holistic data distributions using a robust parametric model (e.g., [13]); then we build a graph-embedding based nonparametric classifier on sampled data, which can more accurately preserve or formulate the complex classification boundary. These two steps can also be considered as effective sample pruning and feature pursuing + kNN/template matching, respectively. Our approach is validated comprehensively in colorectal polyp detection and lung nodule detection CAD systems, as the top two deadly cancers, using hospital scale, multi-site clinical datasets. The results show that our method achieves overall better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers, such as the support vector machine variants [17], boosting [15], logistic regression [11], relevance vector machine [13], k-nearest neighbor [9] or spectral projections on graph [2]."
2381677,14133,8960,Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study,2012,"The ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a matching-pennies game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the go signal appeared on a computer screen. They won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects' decisions can be detected in intracranial local field potentials (LFP) prior to the onset of the action.#R##N##R##N#We found that combined low-frequency (0.1-5 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68±3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less confident. Our system demonstrates— for the first time—the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs."
289940,14133,20358,Traffic quality based pricing in paid search using two-stage regression,2013,"While the cost-per-click (CPC) pricing model is main stream in sponsored search, the quality of clicks with respect to conversion rates and hence their values to advertisers may vary considerably from publisher to publisher in a large syndication network. Traffic quality shall be used to establish price discounts for clicks from different publishers. These discounts are intended to maintain incentives for high-quality online traffic and to make it easier for advertisers to maintain long-term bid stability. Conversion signal is noisy as each advertiser defines conversion in their own way. It is also very sparse. Traditional way of overcoming signal sparseness is to allow for longer time in accumulating modeling data. However, due to fast-changing conversion trends, such longer time leads to deterioration of the precision in measuring quality. To allow models to adjust to fast-changing trends with sufficient speed, we had to limit time-window for conversion data collection and make it much shorter than the several weeks window commonly used. Such shorter time makes conversions in the training set extremely sparse. To overcome resulting obstacles, we used two-stage regression similar to hurdle regression. First we employed logistic regression to predict zero conversion outcomes. Next, conditioned on non-zero outcomes, we used random forest regression to predict the value of the quotient of two conversion rates. Two-stage model accounts for the zero inflation due to the sparseness of the conversion signal. The combined model maintains good precision and allows faster reaction to the temporal changes in traffic quality including changes due to certain actions by publishers that may lead to click-price inflation."
2137755,14133,8960,"Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain",2014,"Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization [1,2]. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. [3] proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost [4], together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI) [5]. We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers."
1432077,14133,422,Simultaneous feature and feature group selection through hard thresholding,2014,"Selecting an informative subset of features has important applications in many data mining tasks especially for high-dimensional data. Recently, simultaneous selection of features and feature groups (a.k.a bi-level selection) becomes increasingly popular since it not only reduces the number of features but also unveils the underlying grouping effect in the data, which is a valuable functionality in many applications such as bioinformatics and web data mining. One major challenge of bi-level selection (or even feature selection only) is that computing a globally optimal solution requires a prohibitive computational cost. To overcome such a challenge, current research mainly falls into two categories. The first one focuses on finding suitable continuous computational surrogates for the discrete functions and this leads to various convex and nonconvex optimization models. Although efficient, convex models usually deliver sub-optimal performance while nonconvex models on the other hand require significantly more computational effort. Another direction is to use greedy algorithms to solve the discrete optimization directly. However, existing algorithms are proposed to handle single-level selection only and it remains challenging to extend these methods to handle bi-level selection. In this paper, we fulfill this gap by introducing an efficient sparse group hard thresholding algorithm. Our main contributions are: (1) we propose a novel bi-level selection model and show that the key combinatorial problem admits a globally optimal solution using dynamic programming; (2) we provide an error bound between our solution and the globally optimal under the RIP (Restricted Isometry Property) theoretical framework. Our experiments on synthetic and real data demonstrate that the proposed algorithm produces encouraging performance while keeping comparable computational efficiency to convex relaxation models."
1345631,14133,422,Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data,2012,"Incomplete data present serious problems when integrating large-scale brain imaging data sets from different imaging modalities. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), for example, over half of the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing measures are discarded, resulting in a severe loss of available information. We address this problem by proposing two novel learning methods where all the samples (with at least one available data source) can be used. In the first method, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. Our second method learns a base classifier for each data source independently, based on which we represent each source using a single column of prediction scores; we then estimate the missing prediction scores, which, combined with the existing prediction scores, are used to build a multi-source fusion model. To illustrate the proposed approaches, we classify patients from the ADNI study into groups with Alzheimer's disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI's 780 participants (172 AD, 397 MCI, 211 Normal), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithms. Comprehensive experiments show that our proposed methods yield stable and promising results."
2114538,14133,8960,Inferring synaptic conductances from spike trains with a biophysically inspired point process model,2014,"A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite push-pull fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyper-polarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. The stimulus-dependence of both excitatory and inhibitory conductances can be well described by a linear-nonlinear cascade, with the filter driving inhibition exhibiting opposite sign and a slight delay relative to the filter driving excitation. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances."
1204741,14133,11166,Diagnosis of Coronary Artery Disease Using Cost-Sensitive Algorithms,2012,"One of the main causes of death the world over are cardiovascular diseases, of which coronary artery disease (CAD) is a major type. This disease occurs when the diameter narrowing of one of the left anterior descending, left circumflex, or right coronary arteries is equal to or greater than 50 percent. Angiography is the principal diagnostic modality for the stenos is of heart vessels, however, because of its complications and costs, researchers are looking for alternative methods such as data mining. This study conducts data mining algorithms on the Z-Alizadeh Sani dataset which has been collected from 303 random visitors to Tehran's Shaheed Rajaei Cardiovascular, Medical and Research Center. In this paper, the reason of effectiveness of a preprocessing algorithm on the dataset is investigated. This algorithm which has been merely introduced in our previous works, extracts three new features from the dataset. These features are then used to enrich the primary dataset in order to achieve more accurate results. Moreover, despite the fact that misclassification of diseased patients has more side effects than that of healthy ones, to the best of our knowledge cost-sensitive algorithms have yet to be used in this field. Therefore, in this paper 10-fold cross validation on cost-sensitive algorithms along with base classifiers of Naive Bayes, Sequential Minimal Optimization (SMO), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and C4.5 were employed. As a result, the SMO algorithm has yield to very high sensitivity (97.22%) and accuracy (92.09%) rates, the likes of which have not been reported simultaneously in the existing literature."
857783,14133,422,Shallow semantic parsing of product offering titles (for better automatic hyperlink insertion),2014,"With billions of database-generated pages on the Web where consumers can readily add priced product offerings to their virtual shopping cart, several opportunities will become possible once we can automatically recognize what exactly is being offered for sale on each page. We present a case study of a deployed data-driven system that first chunks individual titles into semantically classified sub-segments, and then uses this information to improve a hyperlink insertion service.   To accomplish this process, we propose an annotation structure that is general enough to apply to offering titles from most e-commerce industries while also being specific enough to identify useful semantics about each offer. To automate the parsing task we apply the best-practices approach of training a supervised conditional random fields model and discover that creating separate prediction models for some of the industries along with the use of model-ensembles achieves the best performance to date.   We further report on a real-world application of the trained parser to the task of growing a lexical dictionary of product-related terms which critically provides background knowledge to an affiliate-marketing hyperlink insertion service. On a regular basis we apply the parser to offering titles to produce a large set of labeled terms. From these candidates we select the most confidently predicted novel terms for review by crowd-sourced annotators. The agreed on terms are then added into a dictionary which significantly improves the performance of the link-insertion service. Finally, to continually improve system performance, we retrain the model in an online fashion by performing additional annotations on titles with incorrect predictions on each batch."
1888729,14133,20358,SCAD: collective discovery of attribute values,2011,"Search engines today offer a rich user experience, no longer restricted to ten blue links. For example, the query Canon EOS Digital Camera returns a photo of the digital camera, and a list of suitable merchants and prices. Similar results are offered in other domains like food, entertainment, travel, etc. All these experiences are fueled by the availability of structured data about the entities of interest.   To obtain this structured data, it is necessary to solve the following problem: given a category of entities with its schema, and a set of Web pages that mention and describe entities belonging to the category, build a structured representation for the entity under the given schema. Specifically, collect structured numerical or discrete attributes of the entities.   Most previous approaches regarded this as an information extraction problem on individual documents, and made no special use of numerical attributes. In contrast, we present an end-to-end framework which leverages signals not only from the Web page context, but also from a collective analysis of all the pages corresponding to an entity, and from constraints related to the actual values within the domain.   Our current implementation uses a general and flexible Integer Linear Program (ILP) to integrate all these signals into holistic decisions over all attributes. There is one ILP per entity and it is small enough to be solved in under 38 milliseconds in our experiments.   We apply the new framework to a setting of significant practical importance: catalog expansion for Commerce search engines, using data from Bing Shopping. Finally, we present experiments that validate the effectiveness of the framework and its superiority to local extraction."
1916885,14133,8960,Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA,2012,"Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules emulating a monosynaptic spinal loop. Emulated activities are qualitatively similar to real human data. Also discussed is the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows emulating pathological abnormalities such that motor symptoms will emerge and can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions."
1947953,14133,11321,Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,2013,"Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.#R##N##R##N#In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures."
2325426,14133,422,SPF-GMKL: generalized multiple kernel learning with a million kernels,2012,"Multiple Kernel Learning (MKL) aims to learn the kernel in an SVM from training data. Many MKL formulations have been proposed and some have proved effective in certain applications. Nevertheless, as MKL is a nascent field, many more formulations need to be developed to generalize across domains and meet the challenges of real world applications. However, each MKL formulation typically necessitates the development of a specialized optimization algorithm. The lack of an efficient, general purpose optimizer capable of handling a wide range of formulations presents a significant challenge to those looking to take MKL out of the lab and into the real world.   This problem was somewhat alleviated by the development of the Generalized Multiple Kernel Learning (GMKL) formulation which admits fairly general kernel parameterizations and regularizers subject to mild constraints. However, the projected gradient descent GMKL optimizer is inefficient as the computation of the step size and a reasonably accurate objective function value or gradient direction are all expensive. We overcome these limitations by developing a Spectral Projected Gradient (SPG) descent optimizer which: a) takes into account second order information in selecting step sizes; b) employs a non-monotone step size selection criterion requiring fewer function evaluations; c) is robust to gradient noise, and d) can take quick steps when far away from the optimum.   We show that our proposed SPG-GMKL optimizer can be an order of magnitude faster than projected gradient descent on even small and medium sized datasets. In some cases, SPG-GMKL can even outperform state-of-the-art specialized optimization algorithms developed for a single MKL formulation. Furthermore, we demonstrate that SPG-GMKL can scale well beyond gradient descent to large problems involving a million kernels or half a million data points. Our code and implementation are available publically."
2752901,14133,20332,Backdoors to normality for disjunctive logic programs,2013,"Over the last two decades, propositional satisfiability (SAT) has become one of the most successful and widely applied techniques for the solution of NP-complete problems. The aim of this paper is to investigate theoretically how SAT can be utilized for the efficient solution of problems that are harder than NP or co-NP. In particular, we consider the fundamental reasoning problems in propositional disjunctive answer set programming (ASP), BRAVE REASONING and SKEPTICAL REASONING, which ask whether a given atom is contained in at least one or in all answer sets, respectively. Both problems are located at the second level of the Polynomial Hierarchy and thus assumed to be harder than NP or co-NP. One cannot transform these two reasoning problems into SAT in polynomial time, unless the Polynomial Hierarchy collapses.#R##N##R##N#We show that certain structural aspects of disjunctive logic programs can be utilized to break through this complexity barrier, using new techniques from Parameterized Complexity. In particular, we exhibit transformations from BRAVE and SKEPTICAL REASONING to SAT that run in time O(2kn2) where k is a structural parameter of the instance and n the input size. In other words, the reduction is fixed-parameter tractable for parameter k. As the parameter k we take the size of a smallest backdoor with respect to the class of normal (i.e., disjunction-free) programs. Such a backdoor is a set of atoms that when deleted makes the program normal. In consequence, the combinatorial explosion, which is expected when transforming a problem from the second level of the Polynomial Hierarchy to the first level, can now be confined to the parameter k, while the running time of the reduction is polynomial in the input size n, where the order of the polynomial is independent of k. We show that such a transformation is not possible if we consider backdoors with respect to tightness instead of normality.#R##N##R##N#We think that our approach is applicable to many other hard combinatorial problems that lie beyond NP or co-NP, and thus significantly enlarge the applicability of SAT."
2608029,14133,20332,Accessing Structured Health Information through English Queries and Automatic Deduction,2011,"While much health data is available online, patients who are not technically astute may be unable to access it because they may not know the relevant resources, they may be reluctant to confront an unfamiliar interface, and they may not know how to compose an answer from information provided by multiple heterogeneous resources. We describe ongoing research in using natural English text queries and automated deduction to obtain answers based on multiple structured data sources in a specific subject domain. Each English query is transformed using natural language technology into an unambiguous logical form; this is submitted to a theorem prover that operates over an axiomatic theory of the subject domain. Symbols in the theory are linked to relations in external databases known to the system. An answer is obtained from the proof, along with an English language explanation of how the answer was obtained. Answers need not be present explicitly in any of the databases, but rather may be deduced or computed from the information they provide. Although English is highly ambiguous, the natural language technology is informed by subject domain knowledge, so that readings of the query that are syntactically plausible but semantically impossible are discarded. When a question is still ambiguous, the system can interrogate the patient to determine what meaning was intended. Additional queries can clarify earlier ones or ask questions referring to previously computed answers. We describe a prototype system, Quadri, which answers questions about HIV treatment using the Stanford HIV Drug Resistance Database and other resources. Natural language processing is provided by PARC’s Bridge, and the deductive mechanism is SRI’s SNARK theorem prover. We discuss some of the problems that must be faced to make this approach work, and some of our solutions."
2018610,14133,8960,Select and Sample - A Model of Efficient Neural Inference and Learning,2011,"An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions."
1050319,14133,11166,Medical Data Mining for Early Deterioration Warning in General Hospital Wards,2011,"Data mining on medical data has great potential to improve the treatment quality of hospitals and increase the survival rate of patients. Every year, $4$ -- $17\%$ of patients undergo cardiopulmonary or respiratory arrest while in hospitals. Early prediction techniques have become an apparent need in many clinical area. Clinical study has found early detection and intervention to be essential for preventing clinical deterioration in patients at general hospital units. In this paper, based on data mining technology, we propose an early warning system (EWS) designed to identify the signs of clinical deterioration and provide early warning for serious clinical events. Our EWS is designed to provide reliable early alarms for patients at the general hospital wards (GHWs). EWS automatically identifies patients at risk of clinical deterioration based on their existing electronic medical record. The main task of EWS is a challenging classification problem on high-dimensional stream data with irregular, multi-scale data gaps, measurement errors, outliers, and class imbalance. In this paper, we propose a novel data mining framework for analyzing such medical data streams. The framework addresses the above challenges and represents a practical approach for early prediction and prevention based on data that would realistically be available at GHWs. We assess the feasibility of the proposed EWS approach through retrospective study that includes data from 28,927 visits at a major hospital. Finally, we apply our system in a real-time clinical trial and obtain promising results. This project is an example of multidisciplinary cyber-physical systems involving researchers in clinical science, data mining, and nursing staff in the hospital. Our early warning algorithm shows promising result: the transfer of patients to ICU was predicted with sensitivity of 0.4127 and specificity of 0.950 in the real time system."
781287,14133,20796,A pairwise ranking based approach to learning with positive and unlabeled examples,2011,"A large fraction of binary classification problems arising in web applications are of the type where the positive class is well defined and compact while the negative class comprises everything else in the distribution for which the classifier is developed; it is hard to represent and sample from such a broad negative class. Classifiers based only on positive and unlabeled examples reduce human annotation effort significantly by removing the burden of choosing a representative set of negative examples. Various methods have been proposed in the literature for building such classifiers. Of these, the state of the art methods are Biased SVM and Elkan & Noto's methods. While these methods often work well in practice, they are computationally expensive since hyperparameter tuning is very important, particularly when the size of labeled positive examples set is small and class imbalance is high. In this paper we propose a pairwise ranking based approach to learn from positive and unlabeled examples (LPU) and we give a theoretical justification for it. We present a pairwise RankSVM (RSVM) based method for our approach. The method is simple, efficient, and its hyperparameters are easy to tune. A detailed experimental study using several benchmark datasets shows that the proposed method gives competitive classification performance compared to the mentioned state of the art methods, while training 3-10 times faster. We also propose an efficient AUC based feature selection technique in the LPU setting and demonstrate its usefulness on the datasets. To get an idea of the goodness of the LPU methods we compare them against supervised learning (SL) methods that also make use of negative examples in training. SL methods give a slightly better performance than LPU methods when there is a rich set of negative examples; however, they are inferior when the number of negative training examples is not large enough."
1973312,14133,8960,Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis,2011,"Diagnosis of Alzheimer's disease (AD) at the early stage of the disease development is of great clinical importance. Current clinical assessment that relies primarily on cognitive measures proves low sensitivity and specificity. The fast growing neuroimaging techniques hold great promise. Research so far has focused on single neuroimaging modality. However, as different modalities provide complementary measures for the same disease pathology, fusion of multi-modality data may increase the statistical power in identification of disease-related brain regions. This is especially true for early AD, at which stage the disease-related regions are most likely to be weak-effect regions that are difficult to be detected from a single modality alone. We propose a sparse composite linear discriminant analysis model (SCLDA) for identification of disease-related brain regions of early AD from multi-modality data. SCLDA uses a novel formulation that decomposes each LDA parameter into a product of a common parameter shared by all the modalities and a parameter specific to each modality, which enables joint analysis of all the modalities and borrowing strength from one another. We prove that this formulation is equivalent to a penalized likelihood with non-convex regularization, which can be solved by the DC (difference of convex functions) programming. We show that in using the DC programming, the property of the non-convex regularization in terms of preserving weak-effect features can be nicely revealed. We perform extensive simulations to show that SCLDA outperforms existing competing algorithms on feature selection, especially on the ability for identifying weak-effect features. We apply SCLDA to the Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) images of 49 AD patients and 67 normal controls (NC). Our study identifies disease-related brain regions consistent with findings in the AD literature."
2214151,14133,422,Multi-domain active learning for text classification,2012,"Active learning has been proven to be effective in reducing labeling efforts for supervised learning. However, existing active learning work has mainly focused on training models for a single domain. In practical applications, it is common to simultaneously train classifiers for multiple domains. For example, some merchant web sites (like Amazon.com) may need a set of classifiers to predict the sentiment polarity of product reviews collected from various domains (e.g., electronics, books, shoes). Though different domains have their own unique features, they may share some common latent features. If we apply active learning on each domain separately, some data instances selected from different domains may contain duplicate knowledge due to the common features. Therefore, how to choose the data from multiple domains to label is crucial to further reducing the human labeling efforts in multi-domain learning. In this paper, we propose a novel  multi-domain active learning  framework to jointly select data instances from all domains with duplicate information considered. In our solution, a shared subspace is first learned to represent common latent features of different domains. By considering the common and the domain-specific features together, the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part. In this way, the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying. We compare our method with the state-of-the-art active learning approaches on several text classification tasks: sentiment classification, newsgroup classification and email spam filtering. The experiment results show that our method reduces the human labeling efforts by 33.2%, 42.9% and 68.7% on the three tasks, respectively."
1825267,14133,20358,Towards semantic knowledge propagation from text corpus to web images,2011,"In this paper, we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains. The problem of classification of image data is often much more challenging than text data because of the following two reasons: (a) Labeled text data is very widely available for classification purposes. On the other hand, this is often not the case for image data, in which a lot of images are available from many sources, but many of them are often not labeled. (b) The image features are not directly related to semantic concepts inherent in class labels. On the other hand, since text data tends to have natural semantic interpretability (because of their human origins), they are often more directly related to class labels. Therefore, the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results.     The semantic challenges of image features are glaringly evident, when we attempt to recognize complex abstract concepts, and the visual features often fail to discriminate such concepts. However, the copious availability of bridging relationships between text and images in the context of web and social network data can be used in order to design for effective classifiers for image data. One of our goals in this paper is to develop a mathematical model for the functional relationships between text and image features, so as indirectly transfer semantic knowledge through feature transformations. This feature transformation is accomplished by mapping instances from different domains into a common space of unspecific topics. This is used as a bridge to semantically connect the two heterogeneous spaces. This is also helpful for the cases where little image data is available for the classification process. We evaluate our knowledge transfer techniques on an image classification task with labeled text corpora and show the effectiveness with respect to competing algorithms."
946315,14133,422,From micro to macro: data driven phenotyping by densification of longitudinal electronic medical records,2014,"Inferring phenotypic patterns from population-scale clinical data is a core computational task in the development of personalized medicine. One important source of data on which to conduct this type of research is patient Electronic Medical Records (EMR). However, the patient EMRs are typically sparse and noisy, which creates significant challenges if we use them directly to represent patient phenotypes. In this paper, we propose a data driven phenotyping framework called Pacifier (PAtient reCord densIFIER), where we interpret the longitudinal EMR data of each patient as a sparse matrix with a feature dimension and a time dimension, and derive more robust patient phenotypes by exploring the latent structure of those matrices. Specifically, we assume that each derived phenotype is composed of a subset of the medical features contained in original patient EMR, whose value evolves smoothly over time. We propose two formulations to achieve such goal. One is Individual Basis Approach (IBA), which assumes the phenotypes are different for every patient. The other is Shared Basis Approach (SBA), which assumes the patient population shares a common set of phenotypes. We develop an efficient optimization algorithm that is capable of resolving both problems efficiently. Finally we validate Pacifier on two real world EMR cohorts for the tasks of early prediction of Congestive Heart Failure (CHF) and End Stage Renal Disease (ESRD). Our results show that the predictive performance in both tasks can be improved significantly by the proposed algorithms (average AUC score improved from 0.689 to 0.816 on CHF, and from 0.756 to 0.838 on ESRD respectively, on diagnosis group granularity). We also illustrate some interesting phenotypes derived from our data."
1970441,14133,23735,RSSI-based physical layout classification and target tethering in mobile ad-hoc networks,2011,"We investigate mobile ad-hoc indoor networks consisting of simple inexpensive robots, LANdroids, with limited wireless communication range and without any range or location sensors. We focus on the problem of using the mobile LANdroids to take responsibility for maintaining connectivity between a static Gateway and mobile Targets that move beyond the communication range of an established network. We refer to such a tracking task as Target Tethering. This type of network commonly uses IEEE 802.11 wireless protocols for communication, with Received Signal Strength Indicator (RSSI) as a measure of radio signal strength. RSSI data is noisy and poorly relates to distance in indoor environments, leading to a challenging Target Tethering task. Some algorithms use the trace of single-source RSSI data to infer distance between two nodes and use it to compute a Target Tethering policy. However, such distance estimates are poor. We instead aim at inferring physical network layout from RSSI data among multiple nodes. We introduce a novel approach based on Cluster Geometries, classes of network nodes corresponding to rotation-invariant physical layouts of LANdroids and a mobile Target, with the conjecture that multi-robot RSSI data can distinguish the Cluster Geometries and therefore the physical layouts. We proceed with extensive experiments and support our conjecture by showing successful classification of the designed Cluster Geometries given the multi-robot RSSI-based data. We then combine the estimated Geometries with motion patterns of the moving Targets to show that suitable multi-robot Target Tethering policies for unknown indoor environments can be learned using multi-agent reinforcement-learning. Specifically, we use an interesting variation of Q-learning where we first learn offline base policies in general open environments and later specialize the policies seamlessly during online execution to account for obstacles in the indoor environment."
1716288,14133,23836,MIC-SVM: Designing a Highly Efficient Support Vector Machine for Advanced Modern Multi-core and Many-Core Architectures,2014,"Support Vector Machine (SVM) has been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4-84x and 18-47x speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, run on a top of the line NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns."
1836605,14133,8927,Enhanced email spam filtering through combining similarity graphs,2011,"Over the last decade  Email Spam  has evolved from being just an irritant to users to being truly dangerous. This has led web-mail providers and academic researchers to dedicate considerable resources towards tackling this problem [9, 21, 22, 24, 26]. However, we argue that some aspects of the spam filtering problem are not handled appropriately in existing work. Principal among these are adversarial spammer efforts -- spammers routinely tune their spam emails to bypass spam-filters, and contaminate ground truth via fake HAM/SPAM votes -- and the scale and sparsity of the problem, which essentially precludes learning with a very large set of parameters.   In this paper we propose an approach that learns to filter spam by striking a balance between generalizing HAM/SPAM votes across users and emails (to alleviate sparsity) and learning local models for each user (to limit effect of adversarial votes); votes are shared only amongst users and emails that are similar to one another. Moreover, we define user-user and email-email similarities using spam-resilient features that are extremely difficult for spammers to fake. We give a methodology that learns to combine multiple features into similarity values while directly optimizing the objective of better spam filtering. A useful side effect of this methodology is that the number of parameters that need to be estimated is very small: this helps us use off-the-shelf learning algorithms to achieve good accuracy while preventing over-training to the adversarial noise in the data. Finally, our approach gives a systematic way to incorporate existing spam-fighting technologies such as IP blacklists, keyword based classifiers, etc into one framework. Experiments on a real-world email dataset show that our approach leads to significant improvements compared to two state-of-the-art baselines."
1584173,14133,11166,Dimensionality Reduction on Heterogeneous Feature Space,2012,"Combining correlated data sources may help improve the learning performance of a given task. For example, in recommendation problems, one can combine (1) user profile database ({\eg} genders, age, {\etc}), (2) users' log data ({\eg}, click through data, purchasing records, {\etc}), and (3) users' social network (useful in social targeting) to build a recommendation model. All these data sources provide informative but heterogeneous features. For instance, user profile database usually has nominal features reflecting users' background, log data provides term-based features about users' historical behaviors, and social network database has graph relational features. Given multiple heterogeneous data sources, one important challenge is to find a unified feature subspace that captures the knowledge from all sources. To this aim, we propose a principle of \emph{collective component analysis} (CoCA), in order to handle dimensionality reduction across a mixture of vector-based features and graph relational features. The CoCA principle is to find a feature subspace with maximal variance under two constraints. First, there should be consensus among the projections from different feature spaces. Second, the similarity between connected data (in any of the network databases) should be maximized. The optimal solution is obtained by solving an eigenvalue problem. Moreover, we discuss how to use prior knowledge to distinguish informative data sources, and optimally weight them in CoCA. Since there is no previous model that can be directly applied to solve the problem, we devised a straightforward comparison method by performing dimension reduction on the concatenation of the data sources. Three sets of experiments show that CoCA substantially outperforms the comparison method."
121393,14133,11052,A unified view on deformable shape factorizations,2012,"Multiple-view geometry and structure-from-motion are well established techniques to compute the structure of a moving rigid object. These techniques are all based on strong algebraic constraints imposed by the rigidity of the object. Unfortunately, many scenes of interest, e.g. faces or cloths, are dynamic and the rigidity constraint no longer holds. Hence, there is a need for non-rigid structure-from-motion (NRSfM) methods which can deal with dynamic scenes. A prominent framework to model deforming and moving non-rigid objects is the factorization technique where the measurements are assumed to lie in a low-dimensional subspace. Many different formulations and variations for factorization-based NRSfM have been proposed in recent years. However, due to the complex interactions between several subspaces, the distinguishing properties between two seemingly related approaches are often unclear. For example, do two approaches just vary in the optimization method used or is really a different model beneath?#R##N##R##N#In this paper, we show that these NRSfM factorization approaches are most naturally modeled with tensor algebra. This results in a clear presentation which subsumes many previous techniques. In this regard, this paper brings several strings of research together and provides a unified point of view. Moreover, the tensor formulation can be extended to the case of a camera network where multiple static affine cameras observe the same deforming and moving non-rigid object. Thanks to the insights gained through this tensor notation, a closed-form and an efficient iterative algorithm can be derived which provide a reconstruction even if there are no feature point correspondences at all between different cameras. An evaluation of the theory and algorithms on motion capture data show promising results."
868683,14133,422,A pattern discovery approach to retail fraud detection,2011,"A major source of revenue shrink in retail stores is the intentional or unintentional failure of proper checking out of items by the cashier. More recently, a few automated surveillance systems have been developed to monitor cashier lanes and detect non-compliant activities such as fake item checkouts or scans done with the intention of deriving monetary benefit. These systems use data from surveillance video cameras and transaction logs (TLog) recorded at the Point-of-Sale (POS). In this paper, we present a pattern discovery based approach to detect fraudulent events at the POS. Our approach is based on mining time-ordered text streams, representing retail transactions, formed from a combination of visually detected checkout related activities called primitives and barcodes from TLog data. Patterns representing single item checkouts, i.e. anchored around a single barcode, are discovered from these text streams using an efficient pattern discovery technique called Teiresias. The discovered patterns are used to build models for true and fake item scans by retaining or discarding the anchoring barcodes in those patterns respectively. A pattern matching and classification scheme is designed to robustly detect non-compliant cashier activities in the presence of noise in either the TLog or the video data. Different weighting schemes for quantifying the relative importance of the discovered patterns are explored: Frequency, Support Vector Machine (SVM) and Frequency+SVM. Using a large scale dataset recorded from retail stores, our approach discovers semantically meaningful cashier scan patterns. Our experiments also suggest that different weighting schemes result in varied false and true positive performances on the task of fake scan detection."
2503124,14133,8960,Fast Prediction for Large-Scale Kernel Machines,2014,"Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding pseudo landmark points to the classical Nystrom kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystrom kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%)."
1007701,14133,9099,Fused one-vs-all mid-level features for fine-grained visual categorization,2014,"As an emerging research topic, fine-grained visual categorization has been attracting growing attentions in recent years. Due to the large inter-class similarity and intra-class variance, recognizing objects in fine-grained domains is extremely challenging, and sometimes even humans can not recognize them accurately. Traditional bag-of-words model could obtain desirable results for basic-level category classification by weak alignment using spatial pyramid matching model, but may easily fail in fine-grained domains since the discriminative features are not only subtle but also extremely localized. The fine differences often get swamped by those irrelevant features, and it is virtually impossible to distinguish them. To address the problems above, we propose a new framework for fine-grained visual categorization. We strengthen the spatial correspondence among parts by including foreground segmentation and part localization. Based on the part representations of the images, we learn a large set of mid-level features which are more suitable for fine-grained tasks. Comparing with the low level features directly extracted from the images, the learned one-vs-all mid-level features enjoy the following advantages. First, the dimension of the mid-level features is relatively small. In order to obtain high classification accuracy, the dimension of the low level features usually reaches several thousand to tens of thousand, and becomes even larger when introducing spatial pyramid model. However, the dimension of our mid-level features is related to the number of classes, which is far less. Second, each entry of the proposed mid-level features is meaningful, which forms a more compact representation of the image. Third, the mid-level features are more robust than the low level ones, which is helpful for classification. Fourth, the learning process of the mid-level features is independent and can be easily combined with other techniques to boost the performance. We evaluate the proposed approach on the extensive fine-grained dataset CUB 200-2011 and Stanford Dogs, by learning the mid-level features based on the popular Fisher vectors and convolutional neural network, we boost the classification accuracy by a considerable margin and advance the state-of-the-art performance in fine-grained visual categorization."
1511377,14133,22130,Computational Modeling of Top-down Visual Attention in Interactive Environments,2011,"Modeling how visual saliency guides the deployment of attention over visual scenes has attracted much interest recently — among both computer vision and experimental/computational researchers — since visual attention is a key function of both machine and biological vision systems. Research efforts in computer vision have mostly been focused on modeling bottom-up saliency. Strong influences o n attention and eye movements, however, come from instantaneous task demands. Here, we propose models of top-down visual guidance considering task influences. The n ew models estimate the state of a human subject performing a task (here, playing video games), and map that state to an eye position. Factors influencing state come from scene gi st, physical actions, events, and bottom-up saliency. Proposed models fall into two categories. In the first category, we use classical discriminative classifiers, including Reg ression, kNN and SVM. In the second category, we use Bayesian Networks to combine all the multi-modal factors in a unified framework. Our approaches significantly outperfor m 15 competing bottom-up and top-down attention models in predicting future eye fixat ions on 18,000 and 75,00 video frames and eye movement samples from a driving and a flig ht combat video game, respectively. We further test and validate our approaches on 1.4M video frames and 11M fixations samples and in all cases obtain higher prediction s cores that reference models."
1084594,14133,8927,An efficient framework for online advertising effectiveness measurement and comparison,2014,"In online advertising market it is crucial to provide advertisers with a reliable measurement of advertising effectiveness to make better marketing campaign planning. The basic idea for ad effectiveness measurement is to compare the performance (e.g., success rate) among users who were and who were not exposed to a certain treatment of ads. When a randomized experiment is not available, a naive comparison can be biased because exposed and unexposed populations typically have different features. One solid methodology for a fair comparison is to apply inverse propensity weighting with doubly robust estimation to the observational data. However the existing methods were not designed for the online advertising campaign, which usually suffers from huge volume of users, high dimensionality, high sparsity and imbalance. We propose an efficient framework to address these challenges in a real campaign circumstance. We utilize gradient boosting stumps for feature selection and gradient boosting trees for model fitting, and propose a subsampling-and-backscaling procedure that enables analysis on extremely sparse conversion data. The choice of features, models and feature selection scheme are validated with irrelevant conversion test. We further propose a parallel computing strategy, combined with the subsampling-and-backscaling procedure to reach computational efficiency. Our framework is applied to an online campaign involving millions of unique users, which shows substantially better model fitting and efficiency. Our framework can be further generalized to comparison of multiple treatments and more general treatment regimes, as sketched in the paper. Our framework is not limited to online advertising, but also applicable to other circumstances (e.g., social science) where a 'fair' comparison is needed with observational data."
2487183,14133,8840,Aligning context-based statistical models of language with brain activity during reading,2014,"Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neural networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity. Our novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secondly, the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain’s own representation of word i. Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brain’s own assessment of the probability of word i, as it can be used to predict the brain activity after the word i’s properties have been fetched from memory and the brain is in the process of integrating it into the context."
857731,14133,422,Multi-source domain adaptation and its application to early detection of fatigue,2011,"We consider the characterization of muscle fatigue through a noninvasive sensing mechanism such as Surface ElectroMyoGraphy (SEMG). While changes in the properties of SEMG signals with respect to muscle fatigue have been reported in the literature, the large variation in these signals across different individuals makes the task of modeling and classification of SEMG signals challenging. Indeed, the variation in SEMG parameters from subject to subject creates differences in the data distribution. In this article, we propose two transfer learning frameworks based on the multisource domain adaptation methodology for detecting different stages of fatigue using SEMG signals, that addresses the distribution differences. In the proposed frameworks, the SEMG data of a subject represent a domain; data from multiple subjects in the training set form the multiple source domains and the test subject data form the target domain. SEMG signals are predominantly different in conditional probability distribution across subjects. The key feature of the first framework is a novel weighting scheme that addresses the conditional probability distribution differences across multiple domains (subjects) and the key feature of the second framework is a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multisource domain adaptation formulation using the weighted Rademacher complexity measure. We have validated the proposed frameworks on Surface ElectroMyoGram signals collected from 8 people during a fatigue-causing repetitive gripping activity. Comprehensive experiments on the SEMG dataset demonstrate that the proposed method improves the classification accuracy by 20p to 30p over the cases without any domain adaptation method and by 13p to 30p over existing state-of-the-art domain adaptation methods."
2075575,14133,8960,Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths,2013,"Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difficult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million fixations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have significant influence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results."
178175,14133,20332,"The Mathematics of Aggregation, Interdependence, Organizations and Systems of Nash Equilibria: A Replacement for Game Theory",2012,"Abstract : Traditional social science research has been unable to satisfactorily aggregate individual level data to group, organization and systems levels, making it one of social science's biggest challenges, if not the most important (Giles, 2011). For game and social theory, we believe that the fault can be attributed to the lack of valid distance measures (e.g., the arbitrary ordering of cooperation and competition precludes a Hilbert space distance metric for gradations in these social behaviors, making theory normative). As an alternative, we offer a theory of social interdependence with countable mathematics based on bistable or multi-stable perspectives patterned after quantum information theory. The evidence that is available is supportive. It indicates that meaning is a one-sided, stable, classical interpretation, not only making the correspondence between beliefs and objective reality in social settings incomplete, but necessarily sweeping aside many static theories from earlier eras (e.g., Axelrod's evolution of cooperation; Simon's bounded rationality). This result alone indicates for democracies that system interpretations evolve to become orthogonal (Nash equilibria), that orthogonal interpretations generate the information that uniquely promotes social evolution, but that in dictatorships, dependent as they are on the enforcement of social cooperation and the suppression of opposing points of view, evolution stops or slows, such as in China, Iran or Cuba, causing capital and energy to be wasted, misdirected or misallocated as government leaders suppress the interpretations that they alone have the authority to label as unethical, immoral, or irreligious."
1852840,14133,507,Explore-by-example: an automatic query steering framework for interactive data exploration,2014,"Interactive Data Exploration (IDE) is a key ingredient of a diverse set of discovery-oriented applications, including ones from scientific computing and evidence-based medicine. In these applications, data discovery is a highly ad hoc interactive process where users execute numerous exploration queries using varying predicates aiming to balance the trade-off between collecting all relevant information and reducing the size of returned data. Therefore, there is a strong need to support these human-in-the-loop applications by assisting their navigation in the data to find interesting objects.   In this paper, we introduce AIDE, an Automatic Interactive Data Exploration framework, that iteratively steers the user towards interesting data areas and predicts a query that retrieves his objects of interest. Our approach leverages relevance feedback on database samples to model user interests and strategically collects more samples to refine the model while minimizing the user effort. AIDE integrates machine learning and data management techniques to provide effective data exploration results (matching the user's interests with high accuracy) as well as high interactive performance. It delivers highly accurate query predictions for very common conjunctive queries with very small user effort while, given a reasonable number of samples, it can predict with high accuracy complex conjunctive queries. Furthermore, it provides interactive performance by limiting the user wait time per iteration to less than a few seconds in average. Our user study indicates that AIDE is a practical exploration framework as it significantly reduces the user effort and the total exploration time compared with the current state-of-the-art approach of manual exploration."
1884462,14133,23634,Bandits with Knapsacks,2013,"Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called bandits with knapsacks, that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sub linear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems. We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel balanced exploration paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret, with respect to the optimal dynamic policy, is sub linear in the supply."
2807451,14133,9004,Identifying Genetic Associations with MRI-derived Measures via Tree-Guided Sparse Learning,2014,"In recent imaging genetic studies, much work has been focused on regression analysis that treats large-scale single nucleotide polymorphisms (SNPs) and quantitative traits (QTs) as association variables. To deal with the weak detection and high-throughput data problem, feature selection methods such as the least absolute shrinkage and selection operator (Lasso) are often used for selecting the most relevant SNPs associated with QTs. However, one problem of Lasso as well as many other feature selection methods for imaging genetics is that some useful prior information, i.e., the hierarchical structure among SNPs throughout the whole genome, are rarely used for designing more powerful model. In this paper, we propose to identify the associations between candidate genetic features (i.e., SNPs) and magnetic resonance imaging (MRI)- derived measures using a tree-guided sparse learning (TGSL) method. The advan- tage of our method is that it explicitly models the priori hierarchical grouping structure among the SNPs in the objective function for feature selection. Specifi- cally, two kinds of hierarchical structures, i.e., group by gene and group by lin- kage disequilibrium (LD) clusters, are imposed as a tree-guided regularization term in our sparse learning model. Experimental results on the Alzheimer's Dis- ease Neuroimaging Initiative (ADNI) database show that our method not only achieves better predictions on the two MRI measures (i.e., left and right hippo- campal formation), but also identifies the informative SNPs to guide the disease- induced interpretation compared with other reference methods."
659750,14133,20358,Multi-label learning with millions of labels: recommending advertiser bid phrases for web pages,2013,"Recommending phrases from web pages for advertisers to bid on against search engine queries is an important research problem with direct commercial impact. Most approaches have found it infeasible to determine the relevance of all possible queries to a given ad landing page and have focussed on making recommendations from a small set of phrases extracted (and expanded) from the page using NLP and ranking based techniques. In this paper, we eschew this paradigm, and demonstrate that it is possible to efficiently predict the relevant subset of queries from a large set of monetizable ones by posing the problem as a multi-label learning task with each query being represented by a separate label.   We develop Multi-label Random Forests to tackle problems with millions of labels. Our proposed classifier has prediction costs that are logarithmic in the number of labels and can make predictions in a few milliseconds using 10 Gb of RAM. We demonstrate that it is possible to generate training data for our classifier automatically from click logs without any human annotation or intervention. We train our classifier on tens of millions of labels, features and training points in less than two days on a thousand node cluster. We develop a sparse semi-supervised multi-label learning formulation to deal with training set biases and noisy labels harvested automatically from the click logs. This formulation is used to infer a belief in the state of each label for each training ad and the random forest classifier is extended to train on these beliefs rather than the given labels. Experiments reveal significant gains over ranking and NLP based techniques on a large test set of 5 million ads using multiple metrics."
1771885,14133,8960,Spectral methods for neural characterization using generalized quadratic models,2013,"We describe a set of fast, tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model (GQM). The GQM consists of a low-rank quadratic function followed by a point nonlinearity and exponential-family noise. The quadratic function characterizes the neuron's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule, and the invertible nonlinearity maps this output to the desired response range. Special cases of the GQM include the 2nd-order Volterra model [1, 2] and the elliptical Linear-Nonlinear-Poisson model [3]. Here we show that for canonical form GQMs, spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes moment-based estimators such as the spike-triggered co-variance, and, in the Gaussian noise case, provides closed-form estimators under a large class of non-Gaussian stimulus distributions. We show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood. Moreover, the GQM provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model. We show applications to both analog and spiking data using intracellular recordings of V1 membrane potential and extracellular recordings of retinal spike trains."
2282743,14133,9099,Submodular video hashing: a unified framework towards video pooling and indexing,2012,"This paper develops a novel framework for efficient large-scale video retrieval. We aim to find video according to higher level similarities, which is beyond the scope of traditional near duplicate search. Following the popular hashing technique we employ compact binary codes to facilitate nearest neighbor search. Unlike the previous methods which capitalize on only one type of hash code for retrieval, this paper combines heterogeneous hash codes to effectively describe the diverse and multi-scale visual contents in videos. Our method integrates feature pooling and hashing in a single framework. In the pooling stage, we cast video frames into a set of pre-specified components, which capture a variety of semantics of video contents. In the hashing stage, we represent each video component as a compact hash code, and combine multiple hash codes into hash tables for effective search. To speed up the retrieval while retaining most informative codes, we propose a graph-based influence maximization method to bridge the pooling and hashing stages. We show that the influence maximization problem is submodular, which allows a greedy optimization method to achieve a nearly optimal solution. Our method works very efficiently, retrieving thousands of video clips from TRECVID dataset in about 0.001 second. For a larger scale synthetic dataset with 1M samples, it uses less than 1 second in response to 100 queries. Our method is extensively evaluated in both unsupervised and supervised scenarios, and the results on TRECVID Multimedia Event Detection and Columbia Consumer Video datasets demonstrate the success of our proposed technique."
1366140,14133,11104,Evaluating transfer learning approaches for image information mining applications,2011,"The recent explosion of data from various Earth observation (EO) systems requires new ways to rapidly harness the information and synthesize it for decision making. Currently several image information mining (IIM) systems have some form of supervised statistical learning models that relate the image content to the various semantic classes. However, this kind of approach is constrained by the paucity of training information in several EO domains due to limited ground truth. Although, semi-supervised learning methods alleviate this problem to a certain extent by using unlabelled data from various spatial databases, however these methods require that the training data and future unseen data should conform to the same statistical distribution and feature space. To overcome this problem a more recent approach is focused on using small amounts of labeled information from closely related or similar learning task and somehow adapt that information in developing new semantic models. The above methodology called transfer learning can be applied in several processes of supervised and unsupervised learning. In this paper, we propose Transfer learning methods for IIM and discuss various techniques and their implications for content-based retrieval in the EO domain. Specifically, we explore traffer learning application in a rapid disaster response scenarios during coastal events. The adopted methodology for knowledge transfer is based on harnessing prior knowledge from similar concepts to learn new ones and uses a modified weighted least squares support vector machine (SVM)."
85302,14133,11321,On p-norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection,2014,"Our objective is to develop formulations and algorithms for efficiently computing the feature selection path - i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to lp≤1 regularization (lp-MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art lp-MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path.#R##N##R##N#We propose a novel conjecture which states that, for certain lp-MKL formulations, the number of features selected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of p thereby reducing optimization costs while simultaneously providing approximation guarantees.#R##N##R##N#We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other lp-MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art lp-MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware. Entire path generation for such data set is well beyond the scaling capabilities of other methods."
1738298,14133,20411,QUADS: question answering for decision support,2014,"As the scale of available on-line data grows ever larger, individuals and businesses must cope with increasing complexity in decision-making processes which utilize large volumes of unstructured, semi-structured and/or structured data to satisfy multiple, interrelated information needs which contribute to an overall decision. Traditional decision support systems (DSSs) have been developed to address this need, but such systems are typically expensive to build, and are purpose-built for a particular decision-making scenario, making them difficult to extend or adapt to new decision scenarios. In this paper, we propose a novel decision representation which allows decision makers to formulate and organize natural language questions or assertions into an analytic hierarchy, which can be evaluated as part of an ad hoc decision process or as a documented, repeatable analytic process. We then introduce a new decision support framework, QUADS, which takes advantage of automatic question answering (QA) technologies to automatically understand and process a decision representation, producing a final decision by gathering and weighting answers to individual questions using a Bayesian learning and inference process. An open source framework implementation is presented and applied to two real world applications: target validation, a fundamental decision-making task for the pharmaceutical industry, and product recommendation from review texts, an everyday decision-making situation faced by on-line consumers. In both applications, we implemented and compared a number of decision synthesis algorithms, and present experimental results which demonstrate the performance of the QUADS approach versus other baseline approaches."
1831811,14133,8960,Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics,2012,"Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Their applications range from modeling brain dynamics to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations."
2186700,14133,20358,Ad impression forecasting for sponsored search,2013,"A typical problem for a search engine (hosting sponsored search service) is to provide the advertisers with a forecast of the number of impressions his/her ad is likely to obtain for a given bid. Accurate forecasts have high business value, since they enable advertisers to select bids that lead to better returns on their investment. They also play an important role in services such as automatic campaign optimization. Despite its importance the problem has remained relatively unexplored in literature. Existing methods typically overfit to the training data, leading to inconsistent performance. Furthermore, some of the existing methods cannot provide predictions for new ads, i.e., for ads that are not present in the logs. In this paper, we develop a generative model based approach that addresses these drawbacks. We design a Bayes net to capture inter-dependencies between the query traffic features and the competitors in an auction. Furthermore, we account for variability in the volume of query traffic by using a dynamic linear model. Finally, we implement our approach on a production grade MapReduce framework and conduct extensive large scale experiments on substantial volumes of sponsored search data from Bing. Our experimental results demonstrate significant advantages over existing methods as measured using several accuracy/error criteria, improved ability to provide estimates for new ads and more consistent performance with smaller variance in accuracies. Our method can also be adapted to several other related forecasting problems such as predicting average position of ads or the number of clicks under budget constraints."
750265,14133,11166,Collective Response Spike Prediction for Mutually Interacting Consumers,2013,"Modeling how marketing actions in various channels influence or cause consumer purchase decisions is crucial for marketing decision-making. Marketing campaigns stimulate consumer awareness, interest and help drive interactions such as the browsing of product web pages, ultimately impacting an individual's purchase decision. In addition, some successful campaigns stimulate word-of-mouth and social trends among consumers, and such collective behavior of consumers result in concurrent and correlated responses over a short term. Though each consumer's response should be attributed with both the same individual's experiences and the collective factors, unobservability of most word-of-mouth events makes the estimation challenging. The authors propose a new continuous-time predictive model for time-dependent response rates of each consumer, which can incorporate both the individual and the collective factors without explicit word-of-mouth observations. The individual factor is modeled as staircase functions associated with the experienced events by each consumer, and provides a clear psychological interpretation about how marketing advertising communications impact short-term and mid-term memories of consumers. The collective factor is modeled with aggregate response frequencies for mutually-interacting groups that are automatically estimated from data. The key idea to mine the mutually-interacting groups exists in a three-step estimator, which initially performs a Poisson regression without the collective factor, then does clustering of the residual time-series in the initial regression, and finally performs another Poisson regression involving the collective factor. The proposed collective factor robustly incorporates the underlying trends even when causality from one consumer's event spikes to another consumer's response is weak. High predictive accuracy of the proposed approach is empirically validated using real-world data provided by an online retailer in Europe."
2609095,14133,20332,Designing a Sunshade Installation for the UT Zero Energy House: An Exploration in Generative Modeling Technology,2011,"The subjective quality of architectural design requires all designers to consider an infinite set of possibilities to a project design. Logically, the faster a designer can visualize and communicate possible ideas within a given time frame, the better their opportunity to discover the best design solution. Ultimately, perfection in architectural design can never be achieved, but its pursuit does lead to a more refined solution.                  Computer-aided drafting and modeling has provided designers more efficient visualization of project designs, increased productivity, and enhanced workflow; however, with the increasing complexity of contemporary designs, new generative modeling technology must be employed to sustain efficient productivity in the design process. Generative modeling is a computer software technology which allows the designer to provide a set of parameters in which a programming script can be written to generate elements within a given domain. This technology allows the designer to model highly customizable and complex elements which can be generated at the speed of a computer calculation.                  What are some benefits of generative modeling technology, and does it support practicing sustainable design? This paper will discuss the implementation of Grasshopper (revised for version 0.8.0003), a generative modeling plug-in for the NURBS modeling program Rhinoceros (version 4.0 SR8), and its role in creating a sunshade installation by a research design team for the zero energy house project at the University of Tennessee. More importantly, the instructive description of the design and fabrication process will show that the same steps can be adapted to other architectural projects. Then, the paper will conclude with a brief case study, the CBD Media Tower, and its use of Grasshopper to provide enhanced modeling and visualization techniques."
1897231,14133,21106,The power of comparative reasoning,2011,"Rank correlation measures are known for their resilience to perturbations in numeric values and are widely used in many evaluation metrics. Such ordinal measures have rarely been applied in treatment of numeric features as a representational transformation. We emphasize the benefits of ordinal representations of input features both theoretically and empirically. We present a family of algorithms for computing ordinal embeddings based on partial order statistics. Apart from having the stability benefits of ordinal measures, these embeddings are highly nonlinear, giving rise to sparse feature spaces highly favored by several machine learning methods. These embeddings are deterministic, data independent and by virtue of being based on partial order statistics, add another degree of resilience to noise. These machine-learning-free methods when applied to the task of fast similarity search outperform state-of-the-art machine learning methods with complex optimization setups. For solving classification problems, the embeddings provide a nonlinear transformation resulting in sparse binary codes that are well-suited for a large class of machine learning algorithms. These methods show significant improvement on VOC 2010 using simple linear classifiers which can be trained quickly. Our method can be extended to the case of polynomial kernels, while permitting very efficient computation. Further, since the popular Min Hash algorithm is a special case of our method, we demonstrate an efficient scheme for computing Min Hash on conjunctions of binary features. The actual method can be implemented in about 10 lines of code in most languages (2 lines in MAT-LAB), and does not require any data-driven optimization."
1384506,14133,8927,Pairwise cross-domain factor model for heterogeneous transfer ranking,2012,"Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation systems. Traditional ranking mainly focuses on one type of data source, and effective modeling relies on a sufficiently large number of labeled examples, which require expensive and time-consuming labeling process. However, in many real-world applications, ranking over multiple related heterogeneous domains becomes a common situation, where in some domains we may have a relatively large amount of training data while in some other domains we can only collect very little. Theretofore, how to leverage labeled information from related heterogeneous domain to improve ranking in a target domain has become a problem of great interests. In this paper, we propose a novel probabilistic model, pairwise cross-domain factor model, to address this problem. The proposed model learns latent factors(features) for multi-domain data in partially-overlapped heterogeneous feature spaces. It is capable of learning homogeneous feature correlation, heterogeneous feature correlation, and pairwise preference correlation for cross-domain knowledge transfer. We also derive two PCDF variations to address two important special cases. Under the PCDF model, we derive a stochastic gradient based algorithm, which facilitates distributed optimization and is flexible to adopt different loss functions and regularization functions to accommodate different data distributions. The extensive experiments on real world data sets demonstrate the effectiveness of the proposed model and algorithm."
1526080,14133,422,An integrated data mining approach to real-time clinical monitoring and deterioration warning,2012,"Clinical study found that early detection and intervention are essential for preventing clinical deterioration in patients, for patients both in intensive care units (ICU) as well as in general wards but under real-time data sensing (RDS). In this paper, we develop an integrated data mining approach to give early deterioration warnings for patients under real-time monitoring in ICU and RDS.   Existing work on mining real-time clinical data often focus on certain single vital sign and specific disease. In this paper, we consider an integrated data mining approach for general sudden deterioration warning. We synthesize a large feature set that includes first and second order time-series features, detrended fluctuation analysis (DFA), spectral analysis, approximative entropy, and cross-signal features. We then systematically apply and evaluate a series of established data mining methods, including forward feature selection, linear and nonlinear classification algorithms, and exploratory undersampling for class imbalance.   An extensive empirical study is conducted on real patient data collected between 2001 and 2008 from a variety of ICUs. Results show the benefit of each of the proposed techniques, and the final integrated approach significantly improves the prediction quality. The proposed clinical warning system is currently under integration with the electronic medical record system at Barnes-Jewish Hospital in preparation for a clinical trial. This work represents a promising step toward general early clinical warning which has the potential to significantly improve the quality of patient care in hospitals."
981858,14133,11491,Compact hashing for mixed image-keyword query over multi-label images,2012,"Recently locality-sensitive hashing (LSH) algorithms have attracted much attention owing to its empirical success and theoretic guarantee in large-scale visual search. In this paper we address the new topic of  hashing with multi-label data , in which images in the database are assumed to be associated with missing or noisy multiple labels and each query consists of a query image and several textual search terms, similar to the new Search with Image function introduced by the Google Image Search. The returned images are judged based on the combination of visual similarity and semantic information conveyed by search terms. In most of the state-of-the-art approaches, the learned hashing functions are universal for all labels. To further enhance the hashing efficiency for such multi-label data, we propose a novel scheme  boosted shared hashing . Our basic observation is that image labels typically form cliques in the feature space. Hashing efficacy can be greatly improved by making each hashing function more targeted at and only shared across such cliques instead of all labels in conventional hashing methods. In other words, each hashing function is deliberately designed such that it is especially effective for a subset of labels. The targeted, but sparse association between labels and hash bits reduces the computation and storage when indexing a new datum, since only a small number of relevant hashing functions become active given the labels. We develop a Boosting-style algorithm for simultaneously optimizing the label subset and hashing function in a unified framework. Experimental results on standard image benchmarks like CIFAR-10 and NUS-WIDE show that the proposed hashing scheme achieves substantially superior performances over conventional methods in terms of accuracy under the same hash bit budget."
2215458,14133,8960,Context-sensitive active sensing in humans,2013,"Humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment. Understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artificial systems. Recently, we proposed a goal-directed, context-sensitive, Bayesian control strategy for active sensing, C-DAC (Context-Dependent Active Controller) (Ahmad & Yu, 2013). In contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, C-DAC directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions. However, C-DAC is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations. Here, we propose a myopic approximation to C-DAC, which also takes behavioral costs into account, but achieves a significant reduction in complexity by looking only one step ahead. We also present data from a human active visual search experiment, and compare the performance of the various models against human behavior. We find that C-DAC and its myopic variant both achieve better fit to human data than Infomax (Butko & Movellan, 2010), which maximizes expected cumulative future information gain. In summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving significant computational savings."
1144848,14133,422,Fast coordinate descent methods with variable selection for non-negative matrix factorization,2011,"Nonnegative Matrix Factorization (NMF) is an effective dimension reduction method for non-negative dyadic data, and has proven to be useful in many areas, such as text mining, bioinformatics and image processing. NMF is usually formulated as a constrained non-convex optimization problem, and many algorithms have been developed for solving it. Recently, a coordinate descent method, called FastHals, has been proposed to solve least squares NMF and is regarded as one of the state-of-the-art techniques for the problem. In this paper, we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus, performs unneeded descent steps on unimportant variables. We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method. Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees. Moreover when the solution is sparse, as is often the case in real applications, our new method benefits by selecting important variables to update more often, thus resulting in higher speed. As an example, on a text dataset RCV1, our method is 7 times faster than FastHals, and more than 15 times faster when the sparsity is increased by adding an L1 penalty. We also develop new coordinate descent methods when error in NMF is measured by KL-divergence by applying the Newton method to solve the one-variable sub-problems. Experiments indicate that our algorithm for minimizing the KL-divergence is faster than the Lee & Seung multiplicative rule by a factor of 10 on the CBCL image dataset."
2180372,14133,23922,Follow the Leader with Dropout Perturbations,2014,"We consider online prediction with expert advice. Over the course of many trials, the goal of the learning algorithm is to achieve small additional loss (i.e. regret) compared to the loss of the best from a set of K experts. The two most popular algorithms are Hedge/Weighted Majority and Follow the Perturbed Leader (FPL). The latter algorithm first perturbs the loss of each expert by independent additive noise drawn from a fixed distribution, and then predicts with the expert of minimum perturbed loss (“the leader”) where ties are broken uniformly at random. To achieve the optimal worst-case regret as a function of the lossL of the best expert in hindsight, the two types of algorithms need to tune their learning rate or noise magnitude, respectively, as a function ofL . Instead of perturbing the losses of the experts with additive noise, we randomly set them to 0 or 1 before selecting the leader. We show that our perturbations are an instance of dropout — because experts may be interpreted as features — although for non-binary losses the dropout probability needs to be made dependent on the losses to get good regret bounds. We show that this simple, tuning-free version of the FPL algorithm achieves two feats: optimal worst-case O( p L lnK + lnK) regret as a function ofL , and optimalO(lnK) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a gap between the expected loss of the best expert and all others. A number of recent algorithms from the Hedge family (AdaHedge and FlipFlop) also achieve this, but they employ sophisticated tuning regimes. The dropout perturbation of the losses of the experts result in different noise distributions for each expert (because they depend on the expert’s total loss) and curiously enough no additional tuning is needed: the choice of dropout probability only affects the constants."
1178473,14133,11166,Divide-and-Conquer Anchoring for Near-Separable Nonnegative Matrix Factorization and Completion in High Dimensions,2013,"Nonnegative matrix factorization (NMF) becomes tractable in polynomial time with unique solution under separability assumption, which postulates all the data points are contained in the conical hull of a few anchor data points. Recently developed linear programming and greedy pursuit methods can pick out the anchors from noisy data and results in a near-separable NMF. But their efficiency could be seriously weakened in high dimensions. In this paper, we show that the anchors can be precisely located from low-dimensional geometry of the data points even when their high dimensional features suffer from serious incompleteness. Our framework, entitled divide-and-conquer anchoring (DCA), divides the high-dimensional anchoring problem into a few cheaper sub-problems seeking anchors of data projections in low-dimensional random spaces, which can be solved in parallel by any near-separable NMF, and combines all the detected low-dimensional anchors via a fast hypothesis testing to identify the original anchors. We further develop two non-iterative anchoring algorithms in 1D and 2D spaces for data in convex hull and conical hull, respectively. These two rapid algorithms in the ultra low dimensions suffice to generate a robust and efficient near-separable NMF for high-dimensional or incomplete data via DCA. Compared to existing methods, two vital advantages of DCA are its scalability for big data, and capability of handling incomplete and high-dimensional noisy data. A rigorous analysis proves that DCA is able to find the correct anchors of a rank-k matrix by solving math cal O(klog k) sub-problems. Finally, we show DCA outperforms state-of-the-art methods on various datasets and tasks."
2648801,14133,20332,Dynamic matching via weighted myopia with application to kidney exchange,2012,"In many dynamic matching applications--especially high-stakes ones--the competitive ratios of prior-free online algorithms are unacceptably poor. The algorithm should take distributional information about possible futures into account in deciding what action to take now. This is typically done by drawing sample trajectories of possible futures at each time period, but may require a prohibitively large number of trajectories or prohibitive memory and/or computation to decide what action to take. Instead, we propose to learn potentials of elements (e.g., vertices) of the current problem. Then, at run time, we simply run an offline matching algorithm at each time period, but subtracting out in the objective the potentials of the elements used up in the matching.#R##N##R##N#We apply the approach to kidney exchange. Kidney exchanges enable willing but incompatible patient-donor pairs (vertices) to swap donors. These swaps typically include cycles longer than two pairs and chains triggered by altruistic donors. Fielded exchanges currently match myopically, maximizing the number of patients who get kidneys in an offline fashion at each time period. Myopic matching is sub-optimal; the clearing problem is dynamic since patients, donors, and altruists appear and expire over time. We theoretically compare the power of using potentials on increasingly large elements: vertices, edges, cycles, and the entire graph (optimum). Then, experiments show that by learning vertex potentials, our algorithm matches more patients than the current practice of clearing myopically. It scales to exchanges orders of magnitude beyond those handled by the prior dynamic algorithm."
1591545,14133,422,Online chinese restaurant process,2014,"Processing large volumes of streaming data in near-real-time is becoming increasingly important as the Internet, sensor networks and network traffic grow. Online machine learning is a typical means of dealing with streaming data, since it allows the classification model to learn one instance of data at a time. Although many online learning methods have been developed since the development of the Perceptron algorithm, existing online methods assume that the number of classes is available in advance of classification process. However, this assumption is unrealistic for large scale or streaming data sets. This work proposes an online Chinese restaurant process (CRP) algorithm, which is an online and nonparametric algorithm, to tackle this problem. This work proposes a relaxing function as part of the prior and updates the parameters with the likelihood function in terms of the consistency between the true label information and predicted result. This work presents two Gibbs sampling algorithms to perform posterior inference. In the experiments, the online CRP is applied to three massive data sets, and compared with several online learning and batch learning algorithms. One of the data sets is obtained from Wikipedia, which comprises approximately two million documents. The experimental results reveal that the proposed online CRP performs well and efficiently on massive data sets. Finally, this work proposes two methods to update the hyperparameter $\alpha$ of the online CRP. The first method is based on the posterior distribution of $\alpha$, and the second exploits the property of online learning, namely adapting to change, to adjust $\alpha$ dynamically."
2663488,14133,20332,Ensemble Classification for Relational Domains.,2011,"Ensemble classification methods have been shown to produce more accurate predictions than the base component models. Due to their effectiveness, ensemble approaches have been applied in a wide range of domains to improve classification. The expected prediction error of classification models can be decomposed into bias and variance. Ensemble methods that independently construct component models (e.g., bagging) can improve performance by reducing the error due to variance, while methods that dependently construct component models (e.g., boosting) can improve performance by reducing the error due to bias and variance. Although ensemble methods were initially developed for classification of independent and identically distributed (i.i.d.) data, they can be directly applied for relational data by using a relational classifier as the base component model. This straightforward approach can improve classification for network data, but suffers from a number of limitations. First, relational data characteristics will only be exploited by the base relational classifier, and not by the ensemble algorithm itself. We note that explicitly accounting for the structured nature of relational data by the ensemble mechanism can significantly improve ensemble classification. Second, ensemble learning methods that assume i.i.d. data can fail to preserve the relational structure of non-i.i.d. data, which will (1) prevent the relational base classifiers from exploiting these structures, and (2) fail to accurately capture properties of the dataset, which can lead to inaccurate models and classifications. Third, ensemble mechanisms that assume i.i.d. data are limited to reducing errors associated with i.i.d. models and fail to reduce additional sources of error associated with more powerful (e.g., collective classification models. Our key observation is that collective classification methods have error due to variance in inference. This has been overlooked by current ensemble methods that assume exact inference methods and only focus on the typical goal of reducing errors due to learning, even if the methods explicitly consider relational data. Here we study the problem of ensemble classification for relational domains by focusing on the reduction of error due to variance. We propose a relational ensemble framework that explicitly accounts for the structured nature of relational data during both learning and inference. Our proposed framework consists of two components. (1) A method for learning accurate ensembles from relational data, focusing on the reduction of error due to variance in learning, while preserving the relational characteristics in the data. (2) A method for applying ensembles in collective classification contexts, focusing on further reduction of the error due to variance in inference, which has not been considered in state of the art ensemble methods."
1501215,14133,9616,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,2014,"Recently, many deep networks are proposed to learn hierarchical image representation to replace traditional hand-designed features. To enhance the ability of the generative model to tackle discriminative computer vision tasks (e.g. image classification), we propose a hierarchical deconvolutional network with two biologically inspired properties incorporated, i.e., non-negative sparsity and selectivity. First, we propose a single layer deconvolutional model with a raw image as input, attempting to decompose the input as a weighted sum of feature maps convolving with filters. Here, the filters are the model parameters common to all the inputs, while the feature maps and the summing weights are specific to the input. The non-negative sparsity is formulated as the /i-norm regularizer on the feature map, which is used to generate feature representations for image classification. And the selectivity is forced on the filters to make different filters active different inputs, through requiring the sparsity on the summing weights specifically. The two properties are summarized into an overall cost function, which can be solved with an alternatively iterative algorithm. Then, we build multiple layer deconvolutional network by stacking the single models, where the next-layer inputs are the results of a 3D max-pooling operation on the inferred feature maps of the front layer, and train the network in a greedy layer wise scheme. Finally, we explore the feature maps of each layer to generate the image representations and input them to a SVM classifier for the classification task. Experiments on two image benchmark datasets of Caltech-101 and Caltech-256 demonstrate the encouraging performance of our model compared with other deep feature learning models as well as some hand-designed features."
974238,14133,422,Modeling disease progression via fused sparse group lasso,2012,"Alzheimer's Disease (AD) is the most common neurodegenerative disorder associated with aging. Understanding how the disease progresses and identifying related pathological biomarkers for the progression is of primary importance in Alzheimer's disease research. In this paper, we develop novel multi-task learning techniques to predict the disease progression measured by cognitive scores and select biomarkers predictive of the progression. In multi-task learning, the prediction of cognitive scores at each time point is considered as a task, and multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. Specifically, we propose a novel convex fused sparse group Lasso (cFSGL) formulation that allows the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and in the meantime incorporates the temporal smoothness using the fused Lasso penalty. The proposed formulation is challenging to solve due to the use of several non-smooth penalties. We show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed efficiently; thus cFSGL can be solved efficiently using the accelerated gradient method. To further improve the model, we propose two non-convex formulations to reduce the shrinkage bias inherent in the convex formulation. We employ the difference of convex programming technique to solve the non-convex formulations. Our extensive experiments using data from the Alzheimer's Disease Neuroimaging Initiative demonstrate the effectiveness of the proposed progression models in comparison with existing methods for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression."
2309994,14133,422,Cost-sensitive online active learning with application to malicious URL detection,2013,"Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in internet security. In literature, many existing studies have attempted to formulate the problem as a regular supervised binary classification task, which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To solve these issues, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art cost-insensitive and cost-sensitive online classification algorithms using a huge amount of labeled data."
1099439,14133,11166,Active Matrix Completion,2013,"Recovering a matrix from a sampling of its entries is a problem of rapidly growing interest and has been studied under the name of matrix completion. It occurs in many areas of engineering and applied science. In most machine learning and data mining applications, it is possible to leverage the expertise of human oracles to improve the performance of the system. It is therefore natural to extend this idea of human-in-the-loop to the matrix completion problem. However, considering the enormity of data in the modern era, manually completing all the entries in a matrix will be an expensive process in terms of time, labor and human expertise, human oracles can only provide selective supervision to guide the solution process. Thus, appropriately identifying a subset of missing entries (for manual annotation) in an incomplete matrix is of paramount practical importance, this can potentially lead to better reconstructions of the incomplete matrix with minimal human effort. In this paper, we propose novel algorithms to address this issue. Since the query locations are actively selected by the algorithms, we refer to these methods as active matrix completion algorithms. The proposed techniques are generic and the same frameworks can be used in a wide variety of applications including recommendation systems, transductive / multi-label active learning, active learning in regression and active feature acquisition among others. Our extensive empirical analysis on several challenging real-world datasets certify the merit and versatility of the proposed frameworks in efficiently exploiting human intelligence in data mining / machine learning applications."
2032326,14133,20332,Graphical Models for Integrated Intelligent Robot Architectures,2012,"The theoretically elegant yet broadly functional capability of graphical models shows intriguing potential to span in a uniform manner perception, cognition and action; and thus to ultimately yield simpler yet more powerful integrated architectures for intelligent robots and other comparable systems. This position paper explores this potential, with initial support from an effort underway to develop a graphical architecture that is based on factor graphs (with piecewise continuous functions). Robots require a close coupling of (multiple forms of) perception and action. Intelligent robots go beyond this to require a further coupling with cognition. From the perspective of robotics ‐ with its focus on behavior in the world ‐ the construction of intelligent robots generally emphasizes a tightly integrated perceptuomotor system that is then loosely connected to some limited form of cognitive system (such as a planner); as for example in (Bonasso et al. 1997). From the perspective of cognitive architectures ‐ with their focus on integrated embodiments of hypotheses concerning the fixed structure underlying intelligent behavior ‐ the construction of intelligent robots generally emphasizes a highly functional cognitive system that is then loosely connected to limited perceptual and motor modules; as for example in (Laird and Rosenbloom 1990). Neither perspective typically strives for a deep integration across the signal-to-symbol divide that separates the perceptuomotor and cognitive systems, nor even to do full justice to what is on the other side. Other approaches are possible though. One such is a form of graphical architecture that leverages the broadly functional yet theoretically elegant construct of graphical models (Koller and Friedman 2009) to support, among other things, a uniform approach to signal and symbol"
2345450,14133,20332,"Real-Time Extraction and Analysis of Key Morphological Features in the Electrocardiogram, for Data Compression and Clinical Decision Support",2011,"Massive amounts of clinical data can now be collected by stand-alone or wearable monitors over extended periods of time. One key challenge is to convert the volumes of raw data into clinically relevant and actionable information, ideally in real-time. This becomes imperative especially in the domain of wearable monitors, where power and memory constraints prevent continuous communication of raw, uncompressed data to a base station for a health care provider. We focus here on algorithmic approaches to extract clinically meaningful information from the electrocardiogram (ECG) in realtime. We use a curve-length transform to identify, and aggregate from beat to beat, physiologically relevant timing information, such as the onsets and offsets of P-waves, QRS complexes, and T-waves, along with their respective magnitudes. Each heartbeat is thus parametrized in terms of 12 variables. Assuming a nominal heart-rate of 70 beats per minute, and a sampling frequency of 250 Hz, each beat has approximately 215 samples. Reducing each beat to 12 samples thus gives an 18-fold compression. An exponentially-weighted sliding average of the identified morphological features over the preceding twenty beats is also stored. Whenever any feature deviates significantly from its stored weighted average, the algorithm registers an alarm and also retains the raw ECG data of the 5 beats immediately preceding and following the anomalous occurrence, for a later review by a clinician."
1543090,14133,23836,Portfolio-Based Selection of Robust Dynamic Loop Scheduling Algorithms Using Machine Learning,2014,"The execution of computationally intensive parallel applications in heterogeneous environments, where the quality and quantity of computing resources available to a single user continuously change, often leads to irregular behavior, in general due to variations of algorithmic and systemic nature. To improve the performance of scientific applications, loop scheduling algorithms are often employed for load balancing of their parallel loops. However, it is a challenge to select the most robust scheduling algorithms for guaranteeing optimized performance of scientific applications on large-scale computing systems that comprise resources which are widely distributed, highly heterogeneous, often shared among multiple users, and have computing availabilities that cannot always be guaranteed or predicted. To address this challenge, in this work we focus on a portfolio-based approach to enable the dynamic selection and use of the most robust dynamic loop scheduling (DLS) algorithm from a portfolio of DLS algorithms, depending on the given application and current system characteristics including workload conditions. Thus, in this paper we provide a solution to the algorithm selection problem and experimentally evaluate its quality. We propose the use of supervised machine learning techniques to build empirical robustness prediction models that are used to predict DLS algorithm's robustness for given scientific application characteristics and system availabilities. Using simulated scientific applications characteristics and system availabilities, along with empirical robustness prediction models, we show that the proposed portfolio-based approach enables the selection of the most robust DLS algorithm that satisfies a user-specified tolerance on the given application's performance obtained in the particular computing system with a certain variable availability. We also show that the portfolio-based approach offers higher guarantees regarding the robust performance of the application using the automatically selected DLS algorithms when compared to the robust performance of the same application using a manually selected DLS algorithm."
48482,14133,11321,Categorized EEG neurofeedback performance unveils simultaneous fMRI deep brain activation,2011,"Decades of Electroencephalogram-NeuroFeedback (EEG-NF) practice have proven that people can be effectively trained to selectively regulate their brain activity, thus potentially improving performance. A common protocol of EEG-NF training aims to guide people via a closed-loop operation shifting from high-amplitude of alpha (8-14Hz) to high-amplitude of theta (4-7 Hz) oscillations resulting in greater theta/alpha ratio (T/A). The induction of such a shift in EEG oscillations has been shown to be useful in reaching a state of relaxation in psychiatric conditions of anxiety and mood disorders. However, the clinical implication of this practice remains elusive and is considered to have relatively low therapeutic yield, possibly due to its poor specificity to a unique brain mechanism. The current project aims to use simultaneous acquisition of Functional Magnetic Resonance Imaging (fMRI) and EEG in order to unfold in high spatial and temporal resolutions, respectively the neural modulations induced via T/A EEG-NF. We used real time EEG preprocessing and analysis during the simultaneous T/A EEG-NF/fMRI. A data driven algorithm was implemented off-line to categorize individual scans into responders and non-responders to the EEG-NF practice via a temporal signature of T/A continuous modulation. Comparing the two groups along with their parasympathetic Heart-Rate reactivity profile verified the relaxed state of the responders. Projection of responders variations in the T/A power to the fMRI whole brain maps revealed networks of correlated and inversely correlated activity reflecting induced relaxation, uniquely among responders."
2646381,14133,20332,Three controversial hypotheses concerning computation in the primate cortex,2012,"We consider three hypotheses concerning the primate neocortex which have influenced computational neuroscience in recent years. Is the mind modular in terms of its being profitably described as a collection of relatively independent functional units? Does the regular structure of the cortex imply a single algorithm at work, operating on many different inputs in parallel? Can the cognitive differences between humans and our closest primate relatives be explained in terms of a scalable cortical architecture? We bring to bear diverse sources of evidence to argue that the answers to each of these questions -- with some judicious qualifications -- are in the affirmative. In particular, we argue that while our higher cognitive functions may interact in a complicated fashion, many of the component functions operate through well-defined interfaces and, perhaps more important, are built on a neural substrate that scales easily under the control of a modular genetic architecture. Processing in the primary sensory cortices seem amenable to similar algorithmic principles, and, even for those cases where alternative principles are at play, the regular structure of cortex allows the same or greater advantages as the architecture scales. Similar genetic machinery to that used by nature to scale body plans has apparently been applied to scale cortical computations. The resulting replicated computing units can be used to build larger working memory and support deeper recursions needed to qualitatively improve our abilities to handle language, abstraction and social interaction."
114633,14133,11187,Cortically Inspired Sensor Fusion Network for Mobile Robot Heading Estimation,2013,"All physical systems must reliably extract information from their noisily and partially observable environment, such as distances to objects. Biology has developed reliable mechanisms to combine multi-modal sensory information into a coherent belief about the underlying environment that caused the percept; a process called sensor fusion. Autonomous technical systems (such as mobile robots) employ compute-intense algorithms for sensor fusion, which hardly work in real-time; yet their results in complex unprepared environments are typically inferior to human performance. Despite the little we know about cortical computing principles for sensor fusion, an obvious difference between biological and technical information processing lies in the way information flows: computer algorithms are typically designed as feed-forward filter-banks, whereas in Cortex we see vastly recurrent connected networks with intertwined information processing, storage, and exchange. In this paper we model such information processing as distributed graphical network, in which independent neural computing nodes obtain and represent sensory information, while processing and exchanging exclusively local data. Given various external sensory stimuli, the network relaxes into the best possible explanation of the underlying cause, subject to the inferred reliability of sensor signals. We implement a simple test-case scenario with a 4 dimensional sensor fusion task on an autonomous mobile robot and demonstrate its performance. We expect to be able to expand this sensor fusion principle to vastly more complex tasks."
1915112,14133,8960,Neurally Plausible Reinforcement Learning of Working Memory Tasks,2012,"A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: by learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity [1]. It is however not well known how such neurons acquire these task-relevant working memories. Here we introduce a biologically plausible learning scheme grounded in Reinforcement Learning (RL) theory [2] that explains how neurons become selective for relevant information by trial and error learning. The model has memory units which learn useful internal state representations to solve working memory tasks by transforming partially observable Markov decision problems (POMDP) into MDPs. We propose that synaptic plasticity is guided by a combination of attentional feedback signals from the action selection stage to earlier processing levels and a globally released neuromodulatory signal. Feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. The neuromodulatory signal interacts with tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to 1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks [1, 3, 4] and 2) learn to optimally integrate probabilistic evidence for perceptual decision making [5, 6]."
1573651,14133,11166,Adaptive Windowing for Online Learning from Multiple Inter-related Data Streams,2011,"Relational reinforcement learning is a promising branch of reinforcement learning research that deals with structured environments. In these environments, states and actions are differentiated by the presence of certain types of objects and the relations between them and the objects that are involved in the actions. This makes it ultimately suited for tasks that require the manipulation of multiple, interacting objects, such as tasks that a future house-holding robot can be expected to perform like cleaning up a dinner table or storing away done dishes. However, the application of relational reinforcement learning to robotics has been hindered by assumptions such as discrete and atomic state observations. Typical robotic observation systems work in a streaming setup, where objects are discovered and recognized and their placement within their surroundings is determined in a quasi continuous manner instead of a state based one. The resulting information stream can be compared to a set of multiple inter-related data streams. In this paper, we propose an adaptive windowing strategy for generating a stream of learning examples and enabling relational learning from this kind of data. Our approach is independent from the learning algorithm and is based on a gradient search over the space of parameter values, i.e., window sizes, guided by the estimation of the testing error. The proposed algorithm performs online and is data driven and flexible. To the best of our knowledge, this is the first work addressing this problem. Our ideas are empirically supported by an extensive experimental evaluation in a controlled setup using artificial data."
1347243,14133,422,Capacitated team formation problem on social networks,2012,"In a team formation problem, one is required to find a group of users that can match the requirements of a collaborative task. Example of such collaborative tasks abound, ranging from software product development to various participatory sensing tasks in knowledge creation. Due to the nature of the task, team members are often required to work on a co-operative basis. Previous studies [1, 2] have indicated that co-operation becomes effective in presence of social connections. Therefore, effective team selection requires the team members to be  socially close  as well as a division of the task among team members so that no user is overloaded by the assignment. In this work, we investigate how such teams can be formed on a social network.   Since our team formation problems are proven to be NP-hard, we design efficient approximate algorithms for finding near optimum teams with provable guarantees. As traditional data-sets from on-line social networks (e.g. Twitter, Facebook etc) typically do not contain instances of large scale collaboration, we have crawled millions of software repositories spanning a period of four years and hundreds of thousands of developers from GitHub, a popular open-source  social coding network . We perform large scale experiments on this data-set to evaluate the accuracy and efficiency of our algorithms. Experimental results suggest that our algorithms achieve significant improvement in finding effective teams, as compared to naive strategies and scale well with the size of the data. Finally, we provide a validation of our techniques by comparing with existing software teams."
784090,14133,8927,Sentiment analysis on evolving social streams: how self-report imbalances can help,2014,"Real-time sentiment analysis is a challenging machine learning task, due to scarcity of labeled data and sudden changes in sentiment caused by real-world events that need to be instantly interpreted. In this paper we propose solutions to acquire labels and cope with concept drift in this setting, by using findings from social psychology on how humans prefer to disclose some types of emotions. In particular, we use findings that humans are more motivated to report positive feelings rather than negative feelings and also prefer to report extreme feelings rather than average feelings.   We map each of these self-report imbalances on two machine learning sub-tasks. The preference on the disclosure of positive feelings can be explored to generate labeled data on polarizing topics, where a positive event for one group usually induces negative feelings from the opposing group, generating an imbalance on user activity that unveils the current dominant sentiment.   Based on the knowledge that extreme experiences are more reported than average experiences, we propose a feature representation strategy that focus on terms which appear at spikes in the social stream. When comparing to a static text representation (TF-IDF), we found that our feature representation is more capable of detecting new informative features that capture the sudden changes on sentiment stream caused by real-world events.   We show that our social psychology-inspired framework produces accuracies up to 84% while analyzing live reactions in the debate of two popular sports on Twitter - soccer and football - despite requiring no human effort in generating supervisory labels."
25446,14133,10994,Adaptive unsupervised multi-view feature selection for visual concept recognition,2012,"To reveal and leverage the correlated and complemental information between different views, a great amount of multi-view learning algorithms have been proposed in recent years. However, unsupervised feature selection in multi-view learning is still a challenge due to lack of data labels that could be utilized to select the discriminative features. Moreover, most of the traditional feature selection methods are developed for the single-view data, and are not directly applicable to the multi-view data. Therefore, we propose an unsupervised learning method called Adaptive Unsupervised Multi-view Feature Selection (AUMFS) in this paper. AUMFS attempts to jointly utilize three kinds of vital information, i.e., data cluster structure, data similarity and the correlations between different views, contained in the original data together for feature selection. To achieve this goal, a robust sparse regression model with the l2,1-norm penalty is introduced to predict data cluster labels, and at the same time, multiple view-dependent visual similar graphs are constructed to flexibly model the visual similarity in each view. Then, AUMFS integrates data cluster labels prediction and adaptive multi-view visual similar graph learning into a unified framework. To solve the objective function of AUMFS, a simple yet efficient iterative method is proposed. We apply AUMFS to three visual concept recognition applications (i.e., social image concept recognition, object recognition and video-based human action recognition) on four benchmark datasets. Experimental results show the proposed method significantly outperforms several state-of-the-art feature selection methods. More importantly, our method is not very sensitive to the parameters and the optimization method converges very fast."
2025495,14133,9099,Query-driven iterated neighborhood graph search for large scale indexing,2012,"In this paper, we address the approximate nearest neighbor (ANN) search problem over large scale visual descriptors. We investigate a simple but very effective approach, neighborhood graph search, which constructs a neighborhood graph to index the data points and conducts a local search, expanding neighborhoods with a best-first manner, for ANN search. Our empirical analysis shows that neighborhood expansion is very efficient, with  O(1)  cost, for a new NN candidate location, and has high chances to locate true NNs and hence it usually performs well. However, it often gets sub-optimal solutions since local search only checks the neighborhood of the current solution, or conducts exhaustive and continuous neighborhood expansions to find better solutions, which deteriorates the query efficiency.   In this paper, we propose a query-driven iterated neighborhood graph search approach to improve the performance. We follow the iterated local search (ILS) strategy, widely-used in combinatorial optimization, to find a solution beyond a local optimum. We handle the key challenge in making neighborhood graph search adapt to ILS, Perturbation, which generates a new pivot to restart a local search. To this end, we present a criterion to check if the local search over a neighborhood graph arrives at the local solution. Moreover, we exploit the query and search history to design the perturbation scheme, resulting in a more effective search. The major benefit is avoiding unnecessary neighborhood expansions and hence more efficiently finding true NNs. Experimental results on large scale SIFT matching, similar image search, and shape retrieval with non-metric distance measures, show that our approach performs much better than previous state-of-the-art ANN search approaches."
2268781,14133,20358,"Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter",2011,"There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread.   Our results show that this variation is not attributable simply to differences in stickiness, the probability of adoption based on one or more exposures, but also to a quantity that could be viewed as a kind of persistence - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects. We find that hashtags on politically controversial topics are particularly persistent, with repeated exposures continuing to have unusually large marginal effects on adoption; this provides, to our knowledge, the first large-scale validation of the complex contagion principle from sociology, which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious. Among other findings, we discover that hashtags representing the natural analogues of Twitter idioms and neologisms are particularly non-persistent, with the effect of multiple exposures decaying rapidly relative to the first exposure.   We also study the subgraph structure of the initial adopters for different widely-adopted hashtags, again finding structural differences across topics. We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads."
116601,14133,11321,Automatic morphological classification of lung cancer subtypes with boosting algorithms for optimizing therapy,2011,"Patient-targeted therapies have recently been highlighted as important. An important development in the treatment of metastatic non-small cell lung cancer (NSCLC) has been the tailoring of therapy on the basis of histology. A pathology diagnosis of non-specified NSCLC is no longer routinely acceptable; an effective approach for classification of adenocarcinoma (AC) and squamous carcinoma (SC) histotypes is needed for optimizing therapy. In this study, we present a robust and objective automatic classification system for real time classification of AC and SC based on morphological tissue pattern of H&E images alone to assist medical experts in diagnosis of lung cancer. Various original and extended Densitometric and Haralick's texture features are used to extract image features, and a Boosting algorithm is utilized to train the classifier, together with alternative decision tree as the base learner. For evaluation, 369 tissue samples were collected in tissue microarray format, including 97 adenocarcinoma and 272 squamous carcinoma samples. Using 10-fold cross validation, the technique achieved high accuracy of 92.41%, and we also found that the two Boosting algorithms (cw-Boost and AdaBoost.M1) perform consistently well in comparison with other popularly adopted machine learning methods, including support vector machine, neural network, single decision tree and alternative decision tree. This approach offers a robust, objective and rapid procedure for optimized patient-targeted therapies."
621097,14133,20552,Discriminative Learning via Semidefinite Probabilistic Models,2012,"Discriminative linear models are a popular tool in machine learning. These can be generally divided into two types: The first is linear classifiers, such as support vector machines, which are well studied and provide state-of-the-art results. One shortcoming of these models is that their output (known as the 'margin') is not calibrated, and cannot be translated naturally into a distribution over the labels. Thus, it is difficult to incorporate such models as components of larger systems, unlike probabilistic based approaches. The second type of approach constructs class conditional distributions using a nonlinearity (e.g. log-linear models), but is occasionally worse in terms of classification error. We propose a supervised learning method which combines the best of both approaches. Specifically, our method provides a distribution over the labels, which is a linear function of the model parameters. As a consequence, differences between probabilities are linear functions, a property which most probabilistic models (e.g. log-linear) do not have. #R##N#Our model assumes that classes correspond to linear subspaces (rather than to half spaces). Using a relaxed projection operator, we construct a measure which evaluates the degree to which a given vector 'belongs' to a subspace, resulting in a distribution over labels. Interestingly, this view is closely related to similar concepts in quantum detection theory. The resulting models can be trained either to maximize the margin or to optimize average likelihood measures. The corresponding optimization problems are semidefinite programs which can be solved efficiently. We illustrate the performance of our algorithm on real world datasets, and show that it outperforms 2nd order kernel methods."
2123263,14133,8960,Dual-Space Analysis of the Sparse Linear Model,2012,"Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients. These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters. Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation. The underlying cost functions can be related via a dual-space framework from [22], which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space. This perspective is useful because some analyses or extensions are more conducive to development in one space or the other. Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit. As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I. In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II. For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular l1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations."
2206649,14133,8960,Bayesian entropy estimation for binary spike train data using parametric prior knowledge,2013,"Shannon's entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefficient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to flexibly exploit the statistical structure of simultaneously-recorded spike responses. We define two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efficient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods."
2006550,14133,8960,Orthogonal Matching Pursuit with Replacement,2011,"In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number of fixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residual. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structure, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hash, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrate that for large-scale problems our proposed methods are more robust and faster than existing methods."
3050795,14133,20332,Adversarial Patrolling Games.,2012,"Defender-Attacker Stackelberg games are the foundations of toolsdeployed for computing optimal patrolling strategies in adversarialdomains such as the United states Federal Air Marshals Service and the UnitedStates Coast Guard, among others.In Stackelberg game models of these systems the attacker knows only theprobability that each target is covered by the defender, but isoblivious to the detailed timing of the coverage schedule.In many real-world situations, however, the attacker can observe thecurrent location of the defender and can exploit this knowledge toreason about the defender's future moves.We study Stackelberg security games in which the defender sequentiallymoves between targets, with moves constrained by an exogenouslyspecified graph, while the attacker can observe the defender's currentlocation and his (stochastic) policy concerning future moves. We offerfive contributions: (1) We model this adversarial patrolling  game (APG) as a stochastic game with special structure and presentseveral alternative formulations that leverage the general non-linearprogramming (NLP) approach for computing equilibria in zero-sumstochastic games. We show that our formulations yield significantlybetter solutions than previous approaches. (2) We extend theNLP formulation for APG allow for attacks that may take multiple timesteps to unfold.(3) We provide anapproximate MILP formulation that uses discrete defender moveprobabilities. (4) We experimentally demonstrate the efficacy of anNLP-based approach, and systematically study the impact of networktopology on the results.(5) We extend our model to allow the defender to construct the graph constraining his moves, at some cost, and offer novel algorithms for this setting, finding that a MILP approximation is much more effective than the exact NLP in this setting."
2736282,14133,20332,Kernelized Bayesian transfer learning,2014,"Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace. Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets. In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source."
2695191,14133,20332,The deployment-to-saturation ratio in security games,2012,"Stackelberg security games form the backbone of systems like ARMOR, IRIS and PROTECT, which are in regular use by the Los Angeles International Police, US Federal Air Marshal Service and the US Coast Guard respectively. An understanding of the runtime required by algorithms that power such systems is critical to furthering the application of game theory to other real-world domains. This paper identifies the concept of the deployment-to-saturation ratio in random Stackelberg security games, and shows that problem instances for which this ratio is 0:5 are computationally harder than instances with other deployment-to-saturation ratios for a wide range of different equilibrium computation methods, including (i) previously published different MIP algorithms, and (ii) different underlying solvers and solution mechanisms. This finding has at least two important implications. First, it is important for new algorithms to be evaluated on the hardest problem instances. We show that this has often not been done in the past, and introduce a publicly available benchmark suite to facilitate such comparisons. Second, we provide evidence that this computationally hard region is also one where optimization would be of most benefit to security agencies, and thus requires significant attention from researchers in this area. Furthermore, we use the concept of phase transitions to better understand this computationally hard region. We define a decision problem related to security games, and show that the probability that this problem has a solution exhibits a phase transition as the deployment-to-saturation ratio crosses 0:5. We also demonstrate that this phase transition is invariant to changes both in the domain and the domain representation, and that the phase transition point corresponds to the computationally hardest instances."
2624657,14133,20332,Robust active learning using crowdsourced annotations for activity recognition,2011,"Recognizing human activities from wearable sensor data is an important problem, particularly for health and eldercare applications. However, collecting sufficient labeled training data is challenging, especially since interpreting IMU traces is difficult for human annotators. Recently, crowdsourcing through services such as Amazon's Mechanical Turk has emerged as a promising alternative for annotating such data, with active learning (Cohn, Ghahramani, and Jordan 1996) serving as a natural method for affordably selecting an appropriate subset of instances to label. Unfortunately, since most active learning strategies are greedy methods that select the most uncertain sample, they are very sensitive to annotation errors (which corrupt a significant fraction of crowdsourced labels). This paper proposes methods for robust active learning under these conditions. Specifically, we make three contributions: 1) we obtain better initial labels by asking labelers to solve a related task; 2) we propose a new principled method for selecting instances in active learning that is more robust to annotation noise; 3) we estimate confidence scores for labels acquired from MTurk and ask workers to relabel samples that receive low scores under this metric. The proposed method is shown to significantly outperform existing techniques both under controlled noise conditions and in real active learning scenarios. The resulting method trains classifiers that are close in accuracy to those trained using ground-truth data."
2453850,14133,8960,Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights,2014,"Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a mean-field factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs.#R##N##R##N#Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude.#R##N##R##N#We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior."
655391,14133,8231,Multi-Task Metric Learning on Network Data,2014,"Multi-task learning (MTL) improves prediction performance in different contexts by learning models jointly on multiple different, but related tasks. Network data, which are a priori data with a rich relational structure, provide an important context for applying MTL. In particular, the explicit relational structure implies that network data is not i.i.d. data. Network data also often comes with significant metadata (i.e., attributes) associated with each entity (node). Moreover, due to the diversity and variation in network data (e.g., multi-relational links or multi-category entities), various tasks can be performed and often a rich correlation exists between them. Learning algorithms should exploit all of these additional sources of information for better performance. In this work we take a metric-learning point of view for the MTL problem in the network context. Our approach builds on structure preserving metric learning (SPML). In particular SPML learns a Mahalanobis distance metric for node attributes using network structure as supervision, so that the learned distance function encodes the structure and can be used to predict link patterns from attributes. SPML is described for single-task learning on single network. Herein, we propose a multi-task version of SPML, abbreviated as MT-SPML, which is able to learn across multiple related tasks on multiple networks via shared intermediate parametrization. MT-SPML learns a specific metric for each task and a common metric for all tasks. The task correlation is carried through the common metric and the individual metrics encode task specific information. When combined together, they are structure-preserving with respect to individual tasks. MT-SPML works on general networks, thus is suitable for a wide variety of problems. In experiments, we challenge MT-SPML on two real-word problems, where MT-SPML achieves significant improvement."
1445297,14133,422,Scalable inference in max-margin topic models,2013,"Topic models have played a pivotal role in analyzing large collections of complex data. Besides discovering latent semantics, supervised topic models (STMs) can make predictions on unseen test data. By marrying with advanced learning techniques, the predictive strengths of STMs have been dramatically enhanced, such as max-margin supervised topic models, state-of-the-art methods that integrate max-margin learning with topic models. Though powerful, max-margin STMs have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale categorization tasks.   In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1)  a new formulation of Gibbs max-margin supervised topic models  for both multi-class and multi-label classification; 2)  a simple ``augment-and-collapse Gibbs sampling algorithm  without making restricting assumptions on the posterior distributions; 3)  an efficient parallel implementation  that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided at: http://www.ml-thu.net/~jun/medlda."
746032,14133,20796,Recommendation in the end-to-end encrypted domain,2011,"In recommendation systems, a central host typically requires access to user profiles in order to generate useful recommendations. This access, however, undermines user privacy; the more information is revealed to the host, the more the user's privacy is compromised. In this paper, we propose a novel end-to-end encrypted recommendation mechanism which encrypts sensitive private data at the user end, without ever exposing plaintext private data to the host server. Unlike previously proposed privacy-preserving recommendation mechanisms, the data in this proposed system are lossless - a pivotal feature to many applications, e.g., in health informatics, business analytics, cyber security, etc. We achieve this goal by developing encrypted-domain polynomial ring homomorphism cryptographic algorithms to compute similarity of encrypted scores on the server, so that collaborative recommendations can be computed in the encryption domain and only an authorized person can decrypt the exact results. We also propose a novel key management system to make sure private information retrieval and recommendation computations can be executed in the encrypted domain in practice. Our experiments show that the proposed scheme offers robust security and lossless accurate recommendation, as well as high efficiency. Our preliminary results show the recommendation accuracy is 21% better than the existing statistical lossy privacy-preserving mechanisms based on random perturbation and user profile distribution. This new approach can potentially be applied to various data mining and cloud computing environments and significantly alleviates the privacy concerns of users."
88004,14133,23922,Surrogate Regret Bounds for the Area Under the ROC Curve via Strongly Proper Losses,2013,"The area under the ROC curve (AUC) is a widely used performance measure in machine learning, and has been widely studied in recent years particularly in the context of bipartite ranking. A dominant theoretical and algorithmic framework for AUC optimization/bipartite ranking has been to reduce the problem to pairwise classication; in particular, it is well known that the AUC regret can be formulated as a pairwise classication regret, which in turn can be upper bounded using usual regret bounds for binary classication. Recently, Kotlowski et al. (2011) showed AUC regret bounds in terms of the regret associated with ‘balanced’ versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we obtain such (non-pairwise) surrogate regret bounds for the AUC in terms of a broad class of proper (composite) losses that we term strongly proper. Our proof technique is considerably simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2009, 2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact AUC-consistent; moreover, our results allow us to quantify the AUC regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate regret bounds under certain low-noise conditions via a recent result of Cl emen con and Robbiano (2011)."
2531464,14133,8960,Learning Gaussian Graphical Models with Observed or Latent FVSs,2013,"Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity O(k2n) using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of highly influential nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate with complexity O(kn2 + n2 log n) if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing an inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank corrections with complexity O(kn2 + n2 log n) per iteration. We perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes."
2733731,14133,20332,Strategic advice provision in repeated human-agent interactions,2012,"This paper addresses the problem of automated advice provision in settings that involve repeated interactions between people and computer agents. This problem arises in many real world applications such as route selection systems and office assistants. To succeed in such settings agents must reason about how their actions in the present influence people's future actions. This work models such settings as a family of repeated bilateral games of incomplete information called choice selection processes, in which players may share certain goals, but are essentially self-interested. The paper describes several possible models of human behavior that were inspired by behavioral economic theories of people's play in repeated interactions. These models were incorporated into several agent designs to repeatedly generate offers to people playing the game. These agents were evaluated in extensive empirical investigations including hundreds of subjects that interacted with computers in different choice selections processes. The results revealed that an agent that combined a hyperbolic discounting model of human behavior with a social utility function was able to outperform alternative agent designs, including an agent that approximated the optimal strategy using continuous MDPs and an agent using epsilongreedy strategies to describe people's behavior. We show that this approach was able to generalize to new people as well as choice selection processes that were not used for training. Our results demonstrate that combining computational approaches with behavioral economics models of people in repeated interactions facilitates the design of advice provision strategies for a large class of real-world settings."
85857,14133,10994,Benchmarking still-to-video face recognition via partial and local linear discriminant analysis on COX-S2V dataset,2012,"In this paper, we explore the real-world Still-to-Video (S2V) face recognition scenario, where only very few (single, in many cases) still images per person are enrolled into the gallery while it is usually possible to capture one or multiple video clips as probe. Typical application of S2V is mug-shot based watch list screening. Generally, in this scenario, the still image(s) were collected under controlled environment, thus of high quality and resolution, in frontal view, with normal lighting and neutral expression. On the contrary, the testing video frames are of low resolution and low quality, possibly with blur, and captured under poor lighting, in non-frontal view. We reveal that the S2V face recognition has been heavily overlooked in the past. Therefore, we provide a benchmarking in terms of both a large scale dataset and a new solution to the problem. Specifically, we collect (and release) a new dataset named COX-S2V, which contains 1,000 subjects, with each subject a high quality photo and four video clips captured simulating video surveillance scenario. Together with the database, a clear evaluation protocol is designed for benchmarking. In addition, in addressing this problem, we further propose a novel method named Partial and Local Linear Discriminant Analysis (PaLo-LDA). We then evaluated the method on COX-S2V and compared with several classic methods including LDA, LPP, ScSR. Evaluation results not only show the grand challenges of the COX-S2V, but also validate the effectiveness of the proposed PaLo-LDA method over the competitive methods."
1257064,14133,422,Robust multi-task feature learning,2012,"Multi-task learning (MTL) aims to improve the performance of multiple related tasks by exploiting the intrinsic relationships among them. Recently, multi-task feature learning algorithms have received increasing attention and they have been successfully applied to many applications involving high dimensional data. However, they assume that all tasks share a common set of features, which is too restrictive and may not hold in real-world applications, since outlier tasks often exist. In this paper, we propose a Robust Multi-Task Feature Learning algorithm (rMTFL) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks. Specifically, we decompose the weight (model) matrix for all tasks into two components. We impose the well-known group Lasso penalty on row groups of the first component for capturing the shared features among relevant tasks. To simultaneously identify the outlier tasks, we impose the same group Lasso penalty but on column groups of the second component. We propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rMTFL, and show that the proposed algorithm is scalable to large-size problems. In addition, we provide a detailed theoretical analysis on the proposed rMTFL formulation. Specifically, we present a theoretical bound to measure how well our proposed rMTFL approximates the true evaluation, and provide bounds to measure the error between the estimated weights of rMTFL and the underlying true weights. Moreover, by assuming that the underlying true weights are above the noise level, we present a sound theoretical result to show how to obtain the underlying true shared features and outlier tasks (sparsity patterns). Empirical studies on both synthetic and real-world data demonstrate that our proposed rMTFL is capable of simultaneously capturing shared features among tasks and identifying outlier tasks."
60670,14133,21106,Neural mechanisms for form and motion detection and integration: biology meets machine vision,2012,"General-purpose vision systems, either biological or technical, rely on the robust processing of visual data from the sensor array. Such systems need to adapt their processing capabilities to varying conditions, have to deal with noise, and also need to learn task-relevant representations. Here, we describe models of early and mid-level vision. These models are motivated by the layered and hierarchical processing of form and motion information in primate cortex. Core cortical processing principles are: (i) bottom-up processing to build representations of increasing feature specificity and spatial scale, (ii) selective amplification of bottom-up signals by feedback that utilizes spatial, temporal, or task-related context information, and (iii) automatic gain control via center-surround competitive interaction and activity normalization. We use these principles as a framework to design and develop bio-inspired models for form and motion processing. Our models replicate experimental findings and, furthermore, provide a functional explanation for psychophysical and physiological data. In addition, our models successfully process natural images or videos. We show mechanism that group items into boundary representations or estimate visual motions from opaque or transparent surfaces. Our framework suggests a basis for designing bio-inspired models that solve typical computer vision problems and enable the development of neural technology for vision."
2551778,14133,8960,Online and Stochastic Gradient Methods for Non-decomposable Loss Functions,2014,"Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method."
2632622,14133,8231,An Iterative Fusion Approach to Graph-Based Semi-Supervised Learning from Multiple Views,2014,"Often, a data object described by many features can be nat- urally decomposed into multiple views, where each view consists of a subset of features. For example, a video clip may have a video view and an audio view. Given a set of training data objects with multiple views, where some objects are labeled and the others are not, semi-supervised learning with graphs from multi-views tries to learn ac lassif ier by treat- ing each view as a similarity graph on all objects, where edges are defined by the similarity on object pairs based on the view attributes. Labels and label relevance ranking scores of labeled objects can be propagated from labeled objects to unlabeled objects on the similarity graphs so that similar objects receive similar labels. The state-of-the-art, one-combo- fits-all methods linearly and independently combine either the metrics or the label propagation results from multi-views and then build a model based on the combined results. However, the similarities between various objects may be manifested differently by different views. In such situa- tions, the one-combo-fits-all methods may not perform well. To tackle the problem, we develop an iterative Semi-Supervised Metric Fusion (SSMF) approach in this paper. SSMF fuses metrics and label propagation results from multi-views iteratively until the fused metric and label propagation results converge simultaneously. Views are weighted dynamically dur- ing the fusion process so that the adversary effect of irrelevant views, identified at each iteration of fusion process, can be reduced effectively. To evaluate the effectiveness of SSMF, we apply it on multi-view based and content based image retrieval and multi-view based multi-label im- age classification on real world data set, which demonstrates that our method outperforms the state-of-the-art methods."
1416637,14133,422,Serendipitous learning: learning beyond the predefined label space,2011,"Most traditional supervised learning methods are developed to learn a model from labeled examples and use this model to classify the unlabeled ones into the same label space predefined by the models. However, in many real world applications, the label spaces for both the labeled/training and unlabeled/testing examples can be different. To solve this problem, this paper proposes a novel notion of Serendipitous Learning (SL), which is defined to address the learning scenarios in which the label space can be enlarged during the testing phase. In particular, a large margin approach is proposed to solve SL. The basic idea is to leverage the knowledge in the labeled examples to help identify novel/unknown classes, and the large margin formulation is proposed to incorporate both the classification loss on the examples within the known categories, as well as the clustering loss on the examples in unknown categories. An efficient optimization algorithm based on CCCP and the bundle method is proposed to solve the optimization problem of the large margin formulation of SL. Moreover, an efficient online learning method is proposed to address the issue of large scale data in online learning scenario, which has been shown to have a guaranteed learning regret. An extensive set of experimental results on two synthetic datasets and two datasets from real world applications demonstrate the advantages of the proposed method over several other baseline algorithms. One limitation of the proposed method is that the number of unknown classes is given in advance. It may be possible to remove this constraint if we model it by using a non-parametric way. We also plan to do experiments on more real world applications in the future."
2553189,14133,8960,Bayesian inference for low rank spatiotemporal neural receptive fields,2013,"The receptive field (RF) of a sensory neuron describes how the neuron integrates sensory stimuli over time and space. In typical experiments with naturalistic or flickering spatiotemporal stimuli, RFs are very high-dimensional, due to the large number of coefficients needed to specify an integration profile across time and space. Estimating these coefficients from small amounts of data poses a variety of challenging statistical and computational problems. Here we address these challenges by developing Bayesian reduced rank regression methods for RF estimation. This corresponds to modeling the RF as a sum of space-time separable (i.e., rank-1) filters. This approach substantially reduces the number of parameters needed to specify the RF, from 1K-10K down to mere 100s in the examples we consider, and confers substantial benefits in statistical power and computational efficiency. We introduce a novel prior over low-rank RFs using the restriction of a matrix normal prior to the manifold of low-rank matrices, and use localized row and column covariances to obtain sparse, smooth, localized estimates of the spatial and temporal RF components. We develop two methods for inference in the resulting hierarchical model: (1) a fully Bayesian method using blocked-Gibbs sampling; and (2) a fast, approximate method that employs alternating ascent of conditional marginal likelihoods. We develop these methods for Gaussian and Poisson noise models, and show that low-rank estimates substantially outperform full rank estimates using neural data from retina and V1."
304766,14133,20552,Use of Dempster-Shafer Conflict Metric to Detect Interpretation Inconsistency,2012,"A model of the world built from sensor data may be incorrect even if the sensors are functioning correctly. Possible causes include the use of inappropriate sensors (e.g. a laser looking through glass walls), sensor inaccuracies accumulate (e.g. localization errors), the a priori models are wrong, or the internal representation does not match the world (e.g. a static occupancy grid used with dynamically moving objects). We are interested in the case where the constructed model of the world is flawed, but there is no access to the ground truth that would allow the system to see the discrepancy, such as a robot entering an unknown environment. This paper considers the problem of determining when something is wrong using only the sensor data used to construct the world model. It proposes 11 interpretation inconsistency indicators based on the Dempster-Shafer conflict metric, Con, and evaluates these indicators according to three criteria: ability to distinguish true inconsistency from sensor noise (classification), estimate the magnitude of discrepancies (estimation), and determine the source(s) (if any) of sensing problems in the environment (isolation). The evaluation is conducted using data from a mobile robot with sonar and laser range sensors navigating indoor environments under controlled conditions. The evaluation shows that the Gambino indicator performed best in terms of estimation (at best 0.77 correlation), isolation, and classification of the sensing situation as degraded (7% false negative rate) or normal (0% false positive rate)."
2690790,14133,22113,Improving combinatorial optimization: extended abstract,2013,"Combinatorial Optimization is an important area of computer science that has many theoretical and practical applications. In the thesis [Chu, 2011], we present important contributions to several different areas of combinatorial optimization, including nogood learning, symmetry breaking, dominance, relaxations and parallelization. We develop a new nogood learning technique based on constraint projection that allows us to exploit subproblem dominances that arise when two different search paths lead to subproblems which are identical on the remaining unfixed variables. We present a new symmetry breaking technique called SBDS-1UIP, which extends Symmetry Breaking During Search (SBDS) by using the more powerful 1UIP nogoods generated by Lazy Clause Generation (LCG) solvers. We present two new general methods for exploiting almost symmetries by modifying SBDS-1UIP and by using conditional symmetry breaking constraints. We solve the Minimization of Open Stacks Problem, the Talent Scheduling Problem (CSPLib prob039), and the Maximum Density Still Life Problem (CSPLib prob032) many orders of magnitude faster than the previous state of the art by applying various powerful techniques such as nogood learning, dynamic programming, dominance and relaxations. We present cache aware data structures for SAT solvers which allows sequential and parallel versions of SAT solvers to run more quickly. And we present a new load balancing scheme for parallel search called confidence based work stealing, which allows the parallel search to make use of the information contained in the branching heuristic."
2545850,14133,20332,"The Aggregative Contingent Estimation System: Selecting, Rewarding, and Training Experts in a Wisdom of Crowds Approach to Forecasting",2012,"We describe the Aggregative Contingent Estimation System (http://www.forecastingace.com), which is designed to elicit and aggregate forecasts from large, diverse groups of individuals. The Aggregative Contingent Estimation System (ACES; see http://www.forecastingace.com) is a project funded by the Intelligence Advanced Research Projects Activity. The project, which is a collaboration between seven universities and a private company (Applied Research Associates), utilizes a crowdsourcing approach to forecast global events such as the outcome of presidential elections in Taiwan and the potential of a downgrade of Greek sovereign debt. The main project goal is to develop new methods for collecting and combining forecasts of many widely-dispersed individuals in order to increase aggregated forecasts’ predictive accuracy. A future goal of this project will involve the development of methods for effectively communicating forecast results to decision makers, the end users of the forecasts. To test our methods, we are engaging members of the general public to voluntarily provide web-based forecasts at their convenience. Our engagement of the general public in this endeavor has brought up a host of issues that involve translation of basic research to the applied problem of global forecasting. In this case study, we focus on three aspects of the project that have general crowdsourcing implications: strategies for rewarding the contributors, strategies for training contributors to be better forecasters, and methods for selecting experts (i.e., estimating the extent to which one is an expert for the purpose of weighting forecasts). We also provide an overview of our statistical aggregation models that are consistently beating the baseline forecasts (the unweighted average forecasts)."
1889381,14133,23757,Modeling and Learning Context-Aware Recommendation Scenarios Using Tensor Decomposition,2011,"The task of recommending items, like movies, to users is a core feature of many social networks. Standard approaches either use item or user similarity to suggest the next items users might be interested in. Recently, multivariate models like matrix factorization have become popular to combine the advantages of both perspectives. In addition, extensions have been proposed to capture the dynamics of user interests over time, like trends or recurrent user needs. While offering good predictive performance, so far those models do not exploit possibly available rich semantic context. Typically, only one implicit feature, like user ratings, is tracked to give personalized recommendations. However, with semantic data sources, like linked data, wealthy background knowledge becomes available that could be leveraged to improve predictive performance. We argue, that a more flexible framework is needed to model and learn a greater class of recommendation scenarios where rich context is available. Thus, we propose a generic approach which generalizes state-of-the-art methods based on pair wise interaction tensor factorization by leveraging arbitrary background knowledge related to the recommendation situation. Our experiments on streamed semantic data from a social network show that by adding varying sets of context - like user information, sequential information or time information - the ranking of potential items can be personalized and the predictive performance can be improved."
1095129,14133,9078,Twofold video hashing with automatic synchronization,2014,"Video hashing finds a wide array of applications in content authentication, robust retrieval and anti-piracy search. While much of the existing research has focused on extracting robust and secure content descriptors, a significant open challenge still remains: Most existing video hashing methods are fallible to temporal desynchronization. That is, when the query video results by deleting or inserting some frames from the reference video, most existing methods assume the positions of the deleted (or inserted) frames are either perfectly known or reliably estimated. This assumption may be okay under typical transcoding and frame-rate changes but is highly inappropriate in adversarial scenarios such as anti-piracy video search. For example, an illegal uploader will try to bypass the ‘piracy check’ mechanism of YouTube/Dailymotion etc by performing a cleverly designed non-uniform resampling of the video. We present a new solution based on dynamic time warping (DTW), which can implement automatic synchronization and can be used together with existing video hashing methods. The second contribution of this paper is to propose a new robust feature extraction method called flow hashing (FH), based on frame averaging and optical flow descriptors. Finally, a fusion mechanism called distance boosting is proposed to combine the information extracted by DTW and FH. Experiments on real video collections show that such a hash extraction and comparison enables unprecedented robustness under both spatial and temporal attacks."
34422,14133,20358,Researcher homepage classification using unlabeled data,2013,"A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.   We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying irrelevant pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end,  we design novel URL-based features  and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.   In addition,  we propose a novel technique for learning a conforming pair of classifiers  using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make similar predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set."
2079057,14133,23593,FPGA-based biophysically-meaningful modeling of olivocerebellar neurons,2014,"The Inferior-Olivary nucleus (ION) is a well-charted region of the brain, heavily associated with sensorimotor control of the body. It comprises ION cells with unique properties which facilitate sensory processing and motor-learning skills. Various simulation models of ION-cell networks have been written in an attempt to unravel their mysteries. However, simulations become rapidly intractable when biophysically plausible models and meaningful network sizes (>=100 cells) are modeled. To overcome this problem, in this work we port a highly detailed ION cell network model, originally coded in Matlab, onto an FPGA chip. It was first converted to ANSI C code and extensively profiled. It was, then, translated to HLS C code for the Xilinx Vivado toolflow and various algorithmic and arithmetic optimizations were applied. The design was implemented in a Virtex 7 (XC7VX485T) device and can simulate a 96-cell network at real-time speed, yielding a speedup of x700 compared to the original Matlab code and x12.5 compared to the reference C implementation running on a Intel Xeon 2.66GHz machine with 20GB RAM. For a 1,056-cell network (non-real-time), an FPGA speedup of x45 against the C code can be achieved, demonstrating the design's usefulness in accelerating neuroscience research. Limited by the available on-chip memory, the FPGA can maximally support a 14,400-cell network (non-real-time) with online parameter configurability for cell state and network size. The maximum throughput of the FPGA ION-network accelerator can reach 2.13 GFLOPS."
2574763,14133,20332,"Just Keep Tweeting, Dear: Web-Mining Methods for Helping a Social Robot Understand User Needs",2011,"An intelligent system of the future should make its user feel comfortable, which is impossible without understanding context they coexist in. However, our past research did not treat language information as a part of the context a robot works in, and data about reasons why the user had made his decisions was not obtained. Therefore, we decided to utilize the Web as a knowledge source to discover context information that could suggest a robot’s behavior when it acquires verbal information from its user or users. By comparing user utterances (blogs, Twitter or Facebook entries, not direct orders) with other people’s written experiences (mostly blogs), a system can judge whether it is a situation in which the robot can perform or improve its performance. In this paper we introduce several methods that can be applied to a simple floor-cleaning robot. We describe basic experiments showing that text processing is helpful when dealing with multiple users who are not willing to give rich feedback. For example, we describe a method for finding usual reasons for cleaning on the Web by using Okapi BM25 to extract feature words from sentences retrieved by the query word ”cleaning”. Then, we introduce our ideas for dealing with conflicts of interest in multiuser environments and possible methods for avoiding such conflicts by achieving better situation understanding. Also, an emotion recognizer for guessing user needs and moods and a method to calculate situation naturalness are described."
606016,14133,22113,Meta-interpretive learning of higher-order dyadic datalog: predicate invention revisited,2013,"Since the late 1990s predicate invention has been under-explored within inductive logic programming due to difficulties in formulating efficient search mechanisms. However, a recent paper demonstrated that both predicate invention and the learning of recursion can be efficiently implemented for regular and context-free grammars, by way of metalogical substitutions with respect to a modified Prolog meta-interpreter which acts as the learning engine. New predicate symbols are introduced as constants representing existentially quantified higher-order variables. The approach demonstrates that predicate invention can be treated as a form of higher-order logical reasoning. In this paper we generalise the approach of meta-interpretive learning (MIL) to that of learning higher-order dyadic datalog programs. We show that with an infinite signature the higher-order dyadic datalog class $$H^2_2$$H22 has universal Turing expressivity though $$H^2_2$$H22 is decidable given a finite signature. Additionally we show that Knuth---Bendix ordering of the hypothesis space together with logarithmic clause bounding allows our MIL implementation Metagol$$_{D}$$D to PAC-learn minimal cardinality $$H^2_2$$H22 definitions. This result is consistent with our experiments which indicate that Metagol$$_{D}$$D efficiently learns compact $$H^2_2$$H22 definitions involving predicate invention for learning robotic strategies, the East---West train challenge and NELL. Additionally higher-order concepts were learned in the NELL language learning domain. The Metagol code and datasets described in this paper have been made publicly available on a website to allow reproduction of results in this paper."
2114442,14133,20332,Towards pareto descent directions in sampling experts for multiple tasks in an on-line learning paradigm,2013,"In many real-life design problems, there is a requirement to simultaneously balance multiple tasks or objectives in the system that are conflicting in nature, where minimizing one objective causes another to increase in value, thereby resulting in trade-offs between the objectives. For example, in embedded multi-core mobile devices and very large scale data centers, there is a continuous problem of simultaneously balancing interfering goals of maximal power savings and minimal performance delay with varying trade-off values for different application workloads executing on them. Typically, the optimal trade-offs for the executing workloads, lie on a difficult to determine optimal Pareto front. The nature of the problem requires learning over the lifetime of the mobile device or server with continuous evaluation and prediction of the trade-off settings on the system that balances the interfering objectives optimally. Towards this, we propose an on-line learning method, where the weights of experts for addressing the objectives are updated based on a convex combination of their relative performance in addressing all objectives simultaneously. An additional importance vector that assigns relative importance to each objective at every round is used, and is sampled from a convex cone pointed at the origin Our preliminary results show that the convex combination of the importance vector and the gradient of the potential functions of the learner's regret with respect to each objective ensure that in the next round, the drift (instantaneous regret vector), is the Pareto descent direction that enables better convergence to the optimal Pareto front."
1571233,14133,8235,Learning Stochastic Models of Information Flow,2012,"An understanding of information flow has many applications, including for maximizing marketing impact on social media, limiting malware propagation, and managing undesired disclosure of sensitive information. This paper presents scalable methods for both learning models of information flow in networks from data, based on the Independent Cascade Model, and predicting probabilities of unseen flow from these models. Our approach is based on a principled probabilistic construction and results compare favourably with existing methods in terms of accuracy of prediction and scalable evaluation, with the addition that we are able to evaluate a broader range of queries than previously shown, including probability of joint and/or conditional flow, as well as reflecting model uncertainty. Exact evaluation of flow probabilities is exponential in the number of edges and naive sampling can also be expensive, so we propose sampling in an efficient Markov-Chain Monte-Carlo fashion using the Metropolis-Hastings algorithm -- details described in the paper. We identify two types of data, those where the paths of past flows are known -- attributed data, and those where only the endpoints are known -- unattributed data. Both data types are addressed in this paper, including training methods, example real world data sets, and experimental evaluation. In particular, we investigate flow data from the Twitter microblogging service, exploring the flow of messages through retweets (tweet forwards) for the attributed case, and the propagation of hash tags (metadata tags) and urls for the unattributed case."
647170,14133,20552,Continuous Time Markov Networks,2012,"A central task in many applications is reasoning about processes that change in a continuous time. The mathematical framework of Continuous Time Markov Processes provides the basic foundations for modeling such systems. Recently, Nodelman et al introduced continuous time Bayesian networks (CTBNs), which allow a compact representation of continuous-time processes over a factored state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics. In many real life processes, such as biological and chemical systems, the dynamics of the process can be naturally described as an interplay between two forces - the tendency of each entity to change its state, and the overall fitness or energy function of the entire system. In our model, the first force is described by a continuous-time proposal process that suggests possible local changes to the state of the system at different rates. The second force is represented by a Markov network that encodes the fitness, or desirability, of different states; a proposed local change is then accepted with a probability that is a function of the change in the fitness distribution. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. This allows us to naturally capture a different type of structure in complex dynamical processes, such as evolving biological sequences. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide algorithms for learning such models from data, and discuss its applicability to biological sequence evolution."
2797332,14133,20332,Robust cuts over time: combatting the spread of invasive species with unreliable biological control,2012,"Widespread accounts of the harmful effects of invasive species have stimulated both practical and theoretical studies on how the spread of these destructive agents can be contained. Inpractice, a widely used method is the deployment of biological control agents, that is, the release of an additional species (which may also spread) that creates a hostile environment for the invader. Seeding colonies of these protective biological control agents can be used to build a kind of living barrier against the spread of the harmful invader, but the ecological literature documents that attempts to establish colonies of biological control agents often fail (opening gaps in the barrier). Further, the supply of the protective species is limited, and the full supply may not be available immediately. This problem has a natural temporal component: biological control is deployed as the extent of the harmful invasion grows. How can a limited supply of unreliable biological control agents best be deployed over time to protect the landscape against the spread of a harmful invasive species?#R##N##R##N#To explore this question we introduce a new family of stochastic graph vaccination problems that generalizes ideas from social networks and multistage graph vaccination. We point out a deterministic (1 - 1/e,)-approximation algorithm for a deterministic base case studied in the social networks literature (matching the previous best randomized (1 - 1/e,) guarantee for that problem). Next, we show that the randomized (1 - 1/e,) guarantee (and a deterministic 1/2 guarantee) can be extended to our much more general family of stochastic graph vaccination problems in which vaccinations (a. k. a. biological control colonies) spread but may be unreliable. For the non-spreading vaccination case with unreliable vaccines, we give matching results in trees. Qualitatively, our extension is from computing cuts over time to computing robust cuts over time.#R##N##R##N#Our new family of problems captures the key tensions we identify for containing invasive species spread with unreliable biological control agents: a robust barrier is built over time with unreliable resources to contain an expanding invasion."
2503569,14133,8960,Learning convolution filters for inverse covariance estimation of neural network connectivity,2014,"We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution."
1941973,14133,11321,Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques,2014,"In many recommendation applications such as news recommendation, the items that can be recommended come and go at a very fast pace. This is a challenge for recommender systems (RS) to face this setting. Online learning algorithms seem to be the most straight forward solution. The contextual bandit framework was introduced for that very purpose. In general the evaluation of a RS is a critical issue. Live evaluation is often avoided due to the potential loss of revenue, hence the need for offline evaluation methods. Two options are available. Model based methods are biased by nature and are thus difficult to trust when used alone. Data driven methods are therefore what we consider here. Evaluating online learning algorithms with past data is not simple but some methods exist in the literature. Nonetheless their accuracy is not satisfactory mainly due to their mechanism of data rejection that only allow the exploitation of a small fraction of the data. We precisely address this issue in this paper. After highlighting the limitations of the previous methods, we present a new method, based on bootstrapping techniques. This new method comes with two important improvements: it is much more accurate and it provides a measure of quality of its estimation. The latter is a highly desirable property in order to minimize the risks entailed by putting online a RS for the first time. We provide both theoretical and experimental proofs of its superiority compared to state-of-the-art methods, as well as an analysis of the convergence of the measure of quality."
1790617,14133,20358,Factorizing YAGO: scalable machine learning for linked data,2012,"Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 ⋅ 10 14  possible triples in the YAGO~2 core ontology."
940361,14133,8927,Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited,2011,"We consider the problem of jointly training structured models for extraction from multiple web sources whose records enjoy  partial content overlap . This has important applications in open-domain extraction, e.g. a user materializing a table of interest from multiple relevant unstructured sources; or a site like Freebase augmenting an incomplete relation by extracting more rows from web sources. Such applications require extraction over arbitrary domains, so one cannot use a pre-trained extractor or demand a huge labeled dataset. We propose to overcome this lack of supervision by using content overlap across the related web sources. Existing methods of exploiting overlap have been developed under settings that do not generalize easily to the scale and diversity of overlap seen on Web sources.   We present an agreement-based learning framework that jointly trains the models by biasing them to agree on the  agreement regions , i.e. shared text segments. We present alternatives within our framework to trade-off tractability, robustness to noise, and extent of agreement enforced; and propose a scheme of partitioning agreement regions that leads to efficient training while maximizing overall accuracy. Further, we present a principled scheme to discover low-noise agreement regions in unlabeled data across multiple sources.   Through extensive experiments over 58 different extraction domains, we establish that our framework provides significant boosts over uncoupled training, and scores over alternatives such as collective inference, staged training, and multi-view learning."
1699512,14133,23757,Using social network knowledge for detecting spider constructions in social security fraud,2013,"As social networks offer a vast amount of additional information to enrich standard learning algorithms, the most challenging part is extracting relevant information from networked data. Fraudulent behavior is imperceptibly concealed both in local and relational data, making it even harder to define useful input for prediction models. Starting from expert knowledge, this paper succeeds to efficiently incorporate social network effects to detect fraud for the Belgian governmental social security institution, and to improve the performance of traditional non-relational fraud prediction tasks. As there are many types of social security fraud, this paper concentrates on payment fraud, predicting which companies intentionally disobey their payment duties to the government. We introduce a new fraudulent structure, the so-called spider constructions, which can easily be translated in terms of social networks and included in the learning algorithms. Focusing on the egonet of each company, the proposed method can handle large scale networks. In order to face the skewed class distribution, the SMOTE approach is applied to rebalance the data. The models were trained on different timestamps and evaluated on varying time windows. Using techniques as Random Forest, logistic regression and Naive Bayes, this paper shows that the combined relational model improves the AUC score and the precision of the predictions in comparison to the base scenario where only local variables are used."
1100171,14133,20796,Collective prediction with latent graphs,2011,"Collective classification in relational data has become an important and active research topic in the last decade. It exploits the dependencies of instances in a network to improve predictions. Related applications include hyperlinked document classification, social network analysis and collaboration network analysis. Most of the traditional collective classification models mainly study the scenario that there exists a large amount of labeled examples (labeled nodes). However, in many real-world applications, labeled data are extremely difficult to obtain. For example, in network intrusion detection, there may be only a limited number of identified intrusions whereas there are a huge set of unlabeled nodes. In this situation, most of the data have no connection to labeled nodes; hence, no supervision knowledge can be obtained from the local connections. In this paper, we propose to explore various latent linkages among the nodes and judiciously integrate the linkages to generate a latent graph. This is achieved by finding a graph that maximizes the linkages among the training data with the same label, and maximizes the separation among the data with different labels. The objective is further cast into an optimization problem and is solved with quadratic programming. Finally, we apply label propagation on the latent graph to make prediction. Experiments show that the proposed model LNP (Latent Network Propagation) can improve the learning accuracy significantly. For instance, when there are only 10% of labeled examples, the accuracies of all the comparison models are less than 63%, while that of the proposed model is 74%."
855474,14133,20515,"Appearance, context and co-occurrence ensembles for identity recognition in personal photo collections",2013,"While modern research in face recognition has focused on new feature representations, alternate learning methods for fusion of features, most have ignored the issue of unmodeled correlations in face data when combining diverse features such as similar visual regions, attributes, appearance frequency, etc. Conventional wisdom is that by using sufficient data and machine, one can learn the systematic correlations and use the data to form a more robust basis for core recognition tasks like verification, identification, and clustering. This however, takes large amounts of training data which is not really available for personal consumer photo collections. We address the fusion/correlation issue differently by proposing an ensemble-based approach that is built on different information sources such as facial appearance, visual context, and social (or co-occurrence) information of samples in a dataset, to provide higher classification accuracy for face recognition in consumer photo collections. To evaluate the utility of our ensembles and simultaneously generate stronger generic features, we perform two experiments - (i) a verification experiment on the standard unconstrained LFW (Labeled Faces in the Wild) dataset where by using an ensemble of appearance related features we report comparable results with recently reported state-of-the-art results and 2.9% better classification accuracy than the previous best method, and(ii) experiment on the Gallagher personal photo collection where we demonstrate at least 17% relative performance gain using visual context and social co-occurrence ensembles."
721623,14133,422,Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization,2014,"The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately, EHR data do not always directly and reliably map to phenotypes, or medical concepts, that clinical researchers need or use. Existing phenotyping approaches typically require labor intensive supervision from medical experts. We propose Marble, a novel sparse non-negative tensor factorization method to derive phenotype candidates with virtually no human supervision. Marble decomposes the observed tensor into two terms, a bias tensor and an interaction tensor. The bias tensor represents the baseline characteristics common amongst the overall population and the interaction tensor defines the phenotypes. We demonstrate the capability of our proposed model on both simulated and patient data from a publicly available clinical database. Our results show that Marble derived phenotypes provide at least a 42.8% reduction in the number of non-zero element and also retains predictive power for classification purposes. Furthermore, the resulting phenotypes and baseline characteristics from real EHR data are consistent with known characteristics of the patient population. Thus it can potentially be used to rapidly characterize, predict, and manage a large number of diseases, thereby promising a novel, data-driven solution that can benefit very large segments of the population."
1111404,14133,422,Comparing apples to oranges: a scalable solution with heterogeneous hashing,2013,"Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain  Facebook  user. In this paper, we address the problem of ``comparing apples to oranges'' under the large scale setting. Specifically, we propose a novel  Relation-aware Heterogeneous Hashing  (RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites,  Tencent Weibo , and the other is an open dataset of  Flickr (NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains."
2136321,14133,8960,What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach,2013,"We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the first time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We first investigated encodings learned by the model using artificial data with mutually occluding components. We find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive fields associated with the model's hidden units. We find many Gabor-like or globular receptive fields as well as fields sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex."
1098493,14133,11166,Multi-task Semi-supervised Semantic Feature Learning for Classification,2012,"Multi-task learning has proven to be useful to boost the learning of multiple related but different tasks. Meanwhile, latent semantic models such as LSA and LDA are popular and effective methods to extract discriminative semantic features of high dimensional dyadic data. In this paper, we present a method to combine these two techniques together by introducing a new matrix tri-factorization based formulation for semi-supervised latent semantic learning, which can incorporate labeled information into traditional unsupervised learning of latent semantics. Our inspiration for multi-task semantic feature learning comes from two facts, i.e., 1) multiple tasks generally share a set of common latent semantics, and 2) a semantic usually has a stable indication of categories no matter which task it is from. Thus to make multiple tasks learn from each other we wish to share the associations between categories and those common semantics among tasks. Along this line, we propose a novel joint Nonnegative matrix tri-factorization framework with the aforesaid associations shared among tasks in the form of a semantic-category relation matrix. Our new formulation for multi-task learning can simultaneously learn (1) discriminative semantic features of each task, (2) predictive structure and categories of unlabeled data in each task, (3) common semantics shared among tasks and specific semantics exclusive to each task. We give alternating iterative algorithm to optimize our objective and theoretically show its convergence. Finally extensive experiments on text data along with the comparison with various baselines and three state-of-the-art multi-task learning algorithms demonstrate the effectiveness of our method."
2549528,14133,11166,BibClus: A Clustering Algorithm of Bibliographic Networks by Message Passing on Center Linkage Structure,2011,"Multi-type objects with multi-type relations are ubiquitous in real-world networks, e.g. bibliographic networks. Such networks are also called heterogeneous information networks. However, the research on clustering for heterogeneous information networks is little. A new algorithm, called NetClus, has been proposed in recent two years. Although NetClus is applied on a heterogeneous information network with a star network schema, considering the relations between center objects and all attribute objects linking to them, it ignores the relations between center objects such as citation relations, which also contain rich information. Hence, we think the star network schema cannot be used to characterize all possible relations without integrating the linkage structure among center objects, which we call the Center Linkage Structure, and there has been no practical way good enough to solve it. In this paper, we present a novel algorithm, BibClus, for clustering heterogeneous objects with center linkage structure by taking a bibliographic information network as an example. In BibClus, we build a probabilistic model of pair wise hidden Markov random field (P-HMRF) to characterize the center linkage structure, and convert it to a factor graph. We further combine EM algorithm with factor graph theory, and design an efficient way based on message passing algorithm to inference marginal probabilities and estimate parameters at each iteration of EM. We also study how factor functions affect clustering performance with different function forms and constraints. For evaluating our proposed method, we have conducted thorough experiments on a real dataset that we had crawled from ACM Digital Library. The experimental results show that BibClus is effective and has a much higher quantity than the recently proposed algorithm, NetClus, in both recall and precision."
1121690,14133,11166,Healing Sample Selection Bias by Source Classifier Selection,2011,"Domain Adaptation (DA) methods are usually carried out by means of simply reducing the marginal distribution differences between the source and target domains, and subsequently using the resultant trained classifier, namely source classifier, for use in the target domain. However, in many cases, the true predictive distributions of the source and target domains can be vastly different especially when their class distributions are skewed, causing the issues of sample selection bias in DA. Hence, DA methods which leverage the source labeled data may suffer from poor generalization in the target domain, resulting in negative transfer. In addition, we observed that many DA methods use either a source classifier or a linear combination of source classifiers with a fixed weighting for predicting the target unlabeled data. Essentially, the labels of the target unlabeled data are spanned by the prediction of these source classifiers. Motivated by these observations, in this paper, we propose to construct many source classifiers of diverse biases and learn the weight for each source classifier by directly minimizing the structural risk defined on the target unlabeled data so as to heal the possible sample selection bias. Since the weights are learned by maximizing the margin of separation between opposite classes on the target unlabeled data, the proposed method is established here as Maximal Margin Target Label Learning (MMTLL), which is in a form of Multiple Kernel Learning problem with many label kernels. Extensive experimental studies of MMTLL against several state-of-the-art methods on the Sentiment and Newsgroups datasets with various imbalanced class settings showed that MMTLL exhibited robust accuracies on all the settings considered and was resilient to negative transfer, in contrast to other counterpart methods which suffered significantly in prediction accuracy."
1075976,14133,8927,CMAP: effective fusion of quality and relevance for multi-criteria recommendation,2011,"The research issue of recommender systems has been treated as a classical regression problem over the decades and has obtained a great success. In the next generation of recommender systems, multi-criteria recommendation has been predicted as an important direction. Different from traditional recommender systems that aim particularly at recommending high-quality items evaluated by users' ratings, inmulti-criteria recommendation, quality only serves as one criterion, and many other criteria such as relevance, coverage, and diversity should be simultaneously optimized. Although recently there is work investigating each single criterion, there is rarely any literature that reports how each single criterion impacts each other and how to combine them in real applications. Thus in this paper, we study the relationship of two criteria, quality and relevance, as a preliminary work in multi-criteria recommendation. We first give qualitative and quantitative analysis of competitive quality-based and relevance-based algorithms in these two criteria to show that both algorithms cannot work well in the opposite criteria. Then we propose an integrated metric and finally investigate how to combine previous work together into an unified model. In the combination, we introduce a Continuous-time MArkov Process (CMAP) algorithm for ranking, which enables principled and natural integration with features derived from both quality-based and relevance-based algorithms. Through experimental verification, the combined methods can significantly outperform either single quality-based or relevance-based algorithms in the integrated metric and the CMAP model outperforms traditional combination methods by around 3%. Its linear complexity with respect to the number of users and items leads to satisfactory performance, as demonstrated by the around 7-hour computational time for over 480k users and almost 20k items."
2052712,14133,20358,Dynamic learning-based mechanism design for dependent valued exchange economies,2011,"Learning private information from multiple strategic agents poses challenge in many Internet applications. Sponsored search auctions, crowdsourcing, Amazon's mechanical turk, various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers. The common thread in these decision problems is that the optimal outcome depends on the  private  information of all the agents, while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents. The other important trait of these applications is their dynamic nature. The advertisers in an online auction or the users of mechanical turk arrive and depart, and when present, interact with the system repeatedly, giving the opportunity to  learn  their types. Dynamic mechanisms, which learn from the past interactions and make present decisions depending on the expected future evolution of the game, has been shown to improve performance over repeated versions of static mechanisms. In this paper, we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers, known as  exchange economies , and agents having value interdependency, which are relevant in applications illustrated through examples. We show that known results of dynamic mechanisms with independent value settings cannot guarantee certain desirable properties in this new significantly different setting. In the future work, we propose to analyze similar settings with dynamic types and population."
2704310,14133,20332,Policies to optimize work performance and thermal safety in exercising humans,2013,"Emergency workers engaged in strenuous work in hot environments risk overheating and mission failure. We describe a real-time application that would reduce these risks in terms of a real-time thermal-work strain index (SI) estimator; and a Markov Decision Process (MDP) to compute optimal work rate policies. We examined the thermo-physiological responses of 14 experienced U.S. Army Ranger students (26±4 years 1.77±0.04 m; 78.3±7.3 kg) who participated in a strenuous 8 mile time-restricted pass/fail road march conducted under thermally stressful conditions. A thermoregulatory model was used to derive SI state transition probabilities and model the students' observed and policy driven movement rates. We found that policy end-state SI was significantly lower than SI when modeled using the student's own movement rates (3.94±0.88 vs. 5.62±1.20, P<0.001). We also found an inverse relationship between our policy impact and maximum SI (r=0.64 P<0.05). These results suggest that modeling real world missions as an MDP can provide optimal work rate policies that improve thermal safety and allow students to finish in a fresher state. Ultimately, SI state estimation and MDP models incorporated into wearable physiological monitoring systems could provide real-time work rate guidance, thus minimizing thermal work-strain while maximizing the likelihood of accomplishing mission tasks."
2309631,14133,8960,How transferable are features in deep neural networks,2014,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
2513045,14133,21106,A Convex Optimization Framework for Active Learning,2013,"In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the family of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets."
1839338,14133,8960,Learning to Search Efficiently in High Dimensions,2011,"High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees)."
2275698,14133,8960,Multi-Scale Spectral Decomposition of Massive Graphs,2014,"Computing the k dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when k is reasonably large. In this paper, we propose and analyze a novel multi-scale spectral decomposition method (MSEIGS), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently. We show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately. Thus, eigenvectors of the clusters serve as good initializations to a block Lanczos algorithm that is used to compute spectral decomposition of the original graph. We further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations. Our method outperforms widely used solvers in terms of convergence speed and approximation quality. Furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings. For example, on a graph with more than 82 million nodes and 3.6 billion edges, MSEIGS takes less than 3 hours on a single-core machine while Randomized SVD takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors. Using 16 cores, we can reduce this time to less than 40 minutes."
766865,14133,422,A time-dependent topic model for multiple text streams,2011,"In recent years social media have become indispensable tools for information dissemination, operating in tandem with traditional media outlets such as newspapers, and it has become critical to understand the interaction between the new and old sources of news. Although social media as well as traditional media have attracted attention from several research communities, most of the prior work has been limited to a single medium. In addition temporal analysis of these sources can provide an understanding of how information spreads and evolves. Modeling temporal dynamics while considering multiple sources is a challenging research problem. In this paper we address the problem of modeling text streams from two news sources - Twitter and Yahoo! News. Our analysis addresses both their individual properties (including temporal dynamics) and their inter-relationships. This work extends standard topic models by allowing each text stream to have both local topics and shared topics. For temporal modeling we associate each topic with a time-dependent function that characterizes its popularity over time. By integrating the two models, we effectively model the temporal dynamics of multiple correlated text streams in a unified framework. We evaluate our model on a large-scale dataset, consisting of text streams from both Twitter and news feeds from Yahoo! News. Besides overcoming the limitations of existing models, we show that our work achieves better perplexity on unseen data and identifies more coherent topics. We also provide analysis of finding real-world events from the topics obtained by our model."
1184942,14133,21106,Efficient variational inference in large-scale Bayesian compressed sensing,2011,"We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring."
1246260,14133,21106,On One-Shot Similarity Kernels: Explicit Feature Maps and Properties,2013,"Kernels have been a common tool of machine learning and computer vision applications for modeling non-linearities and/or the design of robust Robustness may refer to either the presence of outliers and noise or to the robustness to a class of transformations (e.g., translation). similarity measures between objects. Arguably, the class of positive semi-definite (psd) kernels, widely known as Mercer's Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space mathcal H, the so-called feature space. The main reason behind psd kernels' popularity is the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in mathcal H, without an explicit definition of the feature map, only by using the kernel (the so-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research towards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms."
1817374,14133,8502,Matching image sets via adaptive multi convex hull,2014,"Traditional nearest points methods use all the samples in an image set to construct a single convex or affine hull model for classification. However, strong artificial features and noisy data may be generated from combinations of training samples when significant intra-class variations and/or noise occur in the image set. Existing multi-model approaches extract local models by clustering each image set individually only once, with fixed clusters used for matching with various image sets. This may not be optimal for discrimination, as undesirable environmental conditions (eg. illumination and pose variations) may result in the two closest clusters representing different characteristics of an object (eg. frontal face being compared to non-frontal face). To address the above problem, we propose a novel approach to enhance nearest points based methods by integrating affine/convex hull classification with an adapted multi-model approach. We first extract multiple local convex hulls from a query image set via maximum margin clustering to diminish the artificial variations and constrain the noise in local convex hulls. We then propose adaptive reference clustering (ARC) to constrain the clustering of each gallery image set by forcing the clusters to have resemblance to the clusters in the query image set. By applying ARC, noisy clusters in the query set can be discarded. Experiments on Honda, MoBo and ETH-80 datasets show that the proposed method outperforms single model approaches and other recent techniques, such as Sparse Approximated Nearest Points, Mutual Subspace Method and Manifold Discriminant Analysis."
1969694,14133,8960,Learning invariant representations and applications to face verification,2013,"One approach to computer object recognition and modeling the brain's ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformation-invariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations. The model's wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically confirm theoretical predictions (from [1]) for the case of 2D affine transformations. Next, we apply the model to non-affine transformations; as expected, it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter transformations which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well."
2184428,14133,20796,Active Exploration in Networks: Using Probabilistic Relationships for Learning and Inference,2014,"Many interesting domains in machine learning can be viewed as networks, with relationships (e.g., friendships) connecting items (e.g., individuals). The Active Exploration (AE) task is to identify all items in a network with a desired trait (i.e., positive labels) given only partial information about the network. The AE process iteratively queries for labels or network structure within a limited budget; thus, accurate predictions prior to making each query is critical to maximizing the number of positives gathered. However, the targeted AE query process produces partially observed networks that can create difficulties for predictive modeling. In particular, we demonstrate that these partial networks can exhibit extreme label correlation bias, which makes it difficult for conventional relational learning methods to accurately estimate relational parameters. To overcome this issue, we model the joint distribution of possible edges and labels to improve learning and inference. Our proposed method, Probabilistic Relational Expectation Maximization (PR-EM), is the first AE approach to accurately learn the complex dependencies between attributes, labels, and structure to improve predictions. PR-EM utilizes collective inference over the missing relationships in the partial network to jointly infer unknown item traits. Further, we develop a linear inference algorithm to facilitate efficient use of PR-EM in large networks. We test our approach on four real world networks, showing that AE with PR-EM gathers significantly more positive items compared to state-of-the-art methods."
1861270,14133,8960,Scaling-up Importance Sampling for Markov Logic Networks,2014,"Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach."
1815441,14133,8927,Latent factor models with additive and hierarchically-smoothed user preferences,2013,"Items in recommender systems are usually associated with annotated attributes: for e.g., brand and price for products; agency for news articles, etc. Such attributes are highly informative and must be exploited for accurate recommendation. While learning a user preference model over these attributes can result in an interpretable recommender system and can hands the cold start problem, it suffers from two major drawbacks: data sparsity and the inability to model random effects. On the other hand, latent-factor collaborative filtering models have shown great promise in recommender systems; however, its performance on rare items is poor. In this paper we propose a novel model LFUM, which provides the advantages of both of the above models. We learn user preferences (over the attributes) using a personalized Bayesian hierarchical model that uses a combination(additive model) of a globally learned preference model along with user-specific preferences. To combat data-sparsity, we smooth these preferences over the item-taxonomy using an efficient forward-filtering and backward-smoothing inference algorithm. Our inference algorithms can handle both discrete attributes (e.g., item brands) and continuous attributes (e.g., item prices). We combine the user preferences with the latent-factor models and train the resulting collaborative filtering system end-to-end using the successful BPR ranking algorithm. In our extensive experimental analysis, we show that our proposed model outperforms several commonly used baselines and we carry out an ablation study showing the benefits of each component of our model."
2016838,14133,8960,Slice Normalized Dynamic Markov Logic Networks,2012,"Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the size of the discretized time-domain typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efficient online inference, and can directly model influences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks."
1424332,14133,20358,Mining photo-sharing websites to study ecological phenomena,2012,"The popularity of social media websites like Flickr and Twitter has created enormous collections of user-generated content online. Latent in these content collections are observations of the world: each photo is a visual snapshot of what the world looked like at a particular point in time and space, for example, while each tweet is a textual expression of the state of a person and his or her environment. Aggregating these observations across millions of social sharing users could lead to new techniques for large-scale monitoring of the state of the world and how it is changing over time. In this paper we step towards that goal, showing that by analyzing the tags and image features of geo-tagged, time-stamped photos we can measure and quantify the occurrence of ecological phenomena including ground snow cover, snow fall and vegetation density. We compare several techniques for dealing with the large degree of noise in the dataset, and show how machine learning can be used to reduce errors caused by misleading tags and ambiguous visual content. We evaluate the accuracy of these techniques by comparing to ground truth data collected both by surface stations and by Earth-observing satellites. Besides the immediate application to ecology, our study gives insight into how to accurately crowd-source other types of information from large, noisy social sharing datasets."
2711910,14133,20332,Hedge Detection Using a Rewards and Penalties Approach,2013,"Semantic and syntactic features found in text can be used in combination to statistically predict linguistic devices such as hedges in online chat. Some features are better indicators than others, and there are cases when multiple features need to be considered together to be useful. Once the features are identified, it becomes an optimization problem to find the best division of data. We have devised a genetic algorithm approach towards detecting hedges in online multi-party chat discourse. A system was created using rewards and penalties for matching features in tokenized text, so optimizing the reward and penalty amounts are the main challenge. Genetic algorithms, a subset of Evolutionary Algorithms, are great for optimization; as they are massively parallel directed searches, and therefore suited to finding the best ratio of integer rewards and penalties. “Evolutionary algorithms (EAs) utilize principles of natural selection and are robust adaptive search schemes suitable for searching nonlinear, discontinuous, and high-dimensional spaces. This class of algorithms is being increasingly applied to obtain optimal or near-optimal solutions to many complex real-world optimization problems” (Bonissone, et. al. 2006) We show results using 10-fold cross validation as commonly used in traditional machine learning. The best performance without further fine tuning is 79% in classifying whether an utterance in chat contains a hedge or not."
2761939,14133,9804,IsNL? A Discriminative Approach to Detect Natural Language Like Queries for Conversational Understanding,2013,"While data-driven methods for spoken language understanding (SLU) provide state of the art performances and reduce maintenance and model adaptation costs compared to handcrafted parsers, the collection and annotation of domain-specific natural language utterances for training remains a time-consuming task. A recent line of research has focused on enriching the training data with in-domain utterances by mining search engine query logs to improve the SLU tasks. However genre mismatch is a big obstacle as search queries are typically keywords. In this paper, we present an efficient discriminative binary classification method that filters large collection of online web search queries only to select the natural language like queries. The training data used to build this classifier is mined from search query click logs, represented as a bipartite graph. Starting from queries which contain natural language salient phrases, random graph walk algorithms are employed to mine corresponding keyword queries. Then an active learning method is employed for quickly improving on top of this automatically mined data. The results show that our method is robust to noise in search queries by improving over a baseline model previously used for SLU data collection. We also show the effectiveness of detected natural language like queries in extrinsic evaluations on domain detection and slot filling tasks. Index Terms: natural language, keyword search, natural language understanding, web search, semantic parsing."
1453934,14133,23735,Real-time super-resolution Sound Source Localization for robots,2012,"Sound Source Localization (SSL) is an essential function for robot audition and yields the location and number of sound sources, which are utilized for post-processes such as sound source separation. SSL for a robot in a real environment mainly requires noise-robustness, high resolution and real-time processing. A technique using microphone array processing, that is, Multiple Signal Classification based on Standard Eigen-Value Decomposition (SEVD-MUSIC) is commonly used for localization. We improved its robustness against noise with high power by incorporating Generalized EigenValue Decomposition (GEVD). However, GEVD-based MUSIC (GEVD-MUSIC) has mainly two issues: 1) the resolution of pre-measured Transfer Functions (TFs) determines the resolution of SSL, 2) its computational cost is expensive for real-time processing. For the first issue, we propose a TF interpolation method integrating time-domain-based and frequency-domain-based interpolation. The interpolation achieves super-resolution SSL, whose resolution is higher than that of the pre-measured TFs. For the second issue, we propose two methods, MUSIC based on Generalized Singular Value Decomposition (GSVD-MUSIC), and Hierarchical SSL (H-SSL). GSVD-MUSIC drastically reduces the computational cost while maintaining noise-robustness in localization. H-SSL also reduces the computational cost by introducing a hierarchical search algorithm instead of using greedy search in localization. These techniques are integrated into an SSL system using a robot embedded microphone array. The experimental result showed: the proposed interpolation achieved approximately 1 degree resolution although we have only TFs at 30 degree intervals, GSVD-MUSIC attained 46.4% and 40.6% of the computational cost compared to SEVD-MUSIC and GEVD-MUSIC, respectively, H-SSL reduces 59.2% computational cost in localization of a single sound source."
2471547,14133,8960,Analog Memories in a Balanced Rate-Based Network of E-I Neurons,2014,"The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory."
191901,14133,22051,Classifying Imbalanced Data Streams via Dynamic Feature Group Weighting with Importance Sampling.,2014,"Data stream classification and imbalanced data learning are two important areas of data mining research. Each has been well studied to date with many interesting algorithms developed. However, only a few approaches reported in literature address the intersection of these two fields due to their complex interplay. In this work, we proposed an importance sampling driven, dynamic feature group weighting framework (DFGW-IS) for classifying data streams of imbalanced distribution. Two components are tightly incorporated into the proposed approach to address the intrinsic characteristics of concept-drifting, imbalanced streaming data. Specifically, the ever-evolving concepts are tackled by a weighted ensemble trained on a set of feature groups with each sub-classifier (i.e. a single classifier or an ensemble) weighed by its discriminative power and stable level. The un-even class distribution, on the other hand, is typically battled by the sub-classifier built in a specific feature group with the underlying distribution rebalanced by the importance sampling technique. We derived the theoretical upper bound for the generalization error of the proposed algorithm. We also studied the empirical performance of our method on a set of benchmark synthetic and real world data, and significant improvement has been achieved over the competing algorithms in terms of standard evaluation metrics and parallel running time. Algorithm implementations and datasets are available upon request."
2411198,14133,8960,Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions,2013,"We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves O(√T log |II| + log |II|) regret with respect to a comparison set of policies II. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set II has polynomial size, this algorithm is efficient.#R##N##R##N#We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes. Finally, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs."
1513633,14133,11166,Structural Bregman Distance Functions Learning to Rank with Self-Reinforcement,2014,"Learning to rank is an important task for many data mining applications. Essentially, the goal of learning to rank is to learn an appropriate similarity or distance metric to determine the relevance relationships among data points. However, most of the existing approaches for distance metric learning are limited in three aspects. First, they often assume a fixed form of distance metric for the entire input space. Second, the assumed distance functions are often computationally expensive or even intractable to learn for high dimensional data, such as Mahalanobis distance. Third, most of these approaches lack robustness to noisily labeled data, which is pervasive in many real-world applications. In this paper, we study learning to rank as a problem of distance metric learning to address the above three problems. We choose Bregman distance as the target distance function, due to its general functional form as a generalization of a wide class of distance functions, and its capacity of exploiting complicated nonlinear patterns underlying the data. Under the framework of structural SVM, we formulate the problem of learning Bregman distance functions for ranking as a QP problem by a nonparametric approach, and present an effective algorithm. Furthermore, we propose a self-reinforcement scheme that adaptively differentiates each data point in the role of learning to secure the robustness. We emphasize that the proposed method SBLR-S (Structural Bregman distance functions Learning to Rank with Self-reinforcement) is more general than the conventional distance metric learning approaches, and is able to handle high dimensional data as well as noisily labeled data. The experiments of data ranking on real-world datasets show the superiority of this method to the state-of-the-art literature."
1486028,14133,9475,Structured covariance estimation for state prediction,2014,"In this paper we propose a structurally constrained expectation-maximization (EM) algorithm for estimating noise covariances in state-space models, for the purpose of state prediction and control. More specifically, we generalize the problem of covariance estimation on the basis of given i.i.d sample sequence to the dynamic setting where the samples (i.e. state and observation noises) are observed only through the measurement data, or equivalently, drawn from the conditional distribution governed by the dynamic model. By applying the expectation maximization (EM) algorithm to the innovation model representation, we view the resulting ML covariance estimates as the conditional sample covariances, and augment the negative log-likelihood function with matrix norm penalty terms that enforce low-rank and low cardinality structure in the estimated covariances or their inverses. These constraints serve to reflect realistic problem structure expected from model knowledge, yet are still general and flexible enough to be broadly applicable. In addition, the new derivation of the EM algorithm based on the innovation representation gives the common sufficient statistic for both the process and observation noise covariances. This illustrates the coupling between the two covariance estimates, and in simulated cases, enables the calculation of an upper performance bound against which the EM estimates can be compared. The use of the innovation representation also provides a tractable connection to the existing techniques such as the Autocovariance Least Squares (ALS) algorithm. Numerical results comparing the constrained EM and the ALS algorithms are also provided, showing favorable performance for the EM covariance estimates."
2670305,14133,10174,Computing Solutions in Infinite-Horizon Discounted Adversarial Patrolling Games.,2014,"Stackelberg games form the core of a number of tools deployed for computing optimal patrolling strategies in adversarial domains, such as the US Federal Air Marshall Service and the US Coast Guard. In traditional Stackelberg security game models the attacker knows only the probability that each target is covered by the defender, but is oblivious to the detailed timing of the coverage schedule. In many real-world situations, however, the attacker can observe the current location of the defender and can exploit this knowledge to reason about the defender’s future moves. We show that this general modeling framework can be captured using adversarial patrolling games (APGs) in which the defender sequentially moves between targets, with moves constrained by a graph, while the attacker can observe the defender’s current location and his (stochastic) policy concerning future moves. We offer a very general model of infinite-horizon discounted adversarial patrolling games. Our first contribution is to show that defender policies that condition only on the previous defense move (i.e., Markov stationary policies) can be arbitrarily suboptimal for general APGs. We then offer a mixed-integer non-linear programming (MINLP) formulation for computing optimal randomized policies for the defender that can condition on history of bounded, but arbitrary, length, as well as a mixed-integer linear programming (MILP) formulation to approximate these, with provable quality guarantees. Additionally, we present a non-linear programming (NLP) formulation for solving zero-sum APGs. We show experimentally that MILP significantly outperforms the MINLP formulation, and is, in turn, significantly outperformed by the NLP specialized to zero-sum games."
537858,14133,20552,"Learning with Scope, with Application to Information Extraction and Classification",2012,"In probabilistic approaches to classification and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data. In many data sets, however, there are scope­ limited features whose predictive power is only applicable to a certain subset of the data. For example, in information extrac­ tion from web pages, word formatting may be indicative of extraction category in differ­ ent ways on different web pages. The dif­ ficulty with using such features is capturing and exploiting the new regularities encoun­ tered in previously unseen data. In this pa­ per, we propose a hierarchical probabilistic model that uses both local /scope-limited fea­ tures, such as word formatting, and global features, such as word content. The local regularities are modeled as an unobserved random parameter which is drawn once for each local data set. This random parame­ ter is estimated during the inference process and then used to perform classification with both the local and global features- a proce­ dure which is akin to automatically retuning the classifier to the local regularities on each newly encountered web page. Exact inference is intractable and we present approxi mations via point estimates and variational methods. Empirical results on large collections of web data demonstrate that this method signifi­ cantly improves performance from traditional models of global features alone."
2657889,14133,20332,A Study of Phase Transitions in Security Games.,2012,"Stackelberg security games form the backbone of systems like ARMOR, IRIS and PROTECT, which are in regular use by the Los Angeles International Police, US Federal Air Marshal Service and the US Coast Guard respectively. An understanding of the runtime required by algorithms that power such systems is critical to furthering the application of game theory to other real-world domains. This paper identifies the concept of the deployment-to-saturation ratio in random Stackelberg security games, and shows that in a decision problem related to these games, the probability that a solution exists exhibits a phase transition as the ratio crosses 0.5. We demonstrate that this phase transition is invariant to changes both in the domain and the domain representation. Moreover, problem instances at this phase transition point are computationally harder than instances with other deployment-to-saturation ratios for a wide range of different equilibrium computation methods, including (i) previously published different MIP algorithms, and (ii) different underlying solvers and solution mechanisms. Our findings have at least two important implications. First, it is important for new algorithms to be evaluated on the hardest problem instances. We show that this has often not been done in the past, and introduce a publicly available benchmark suite to facilitate such comparisons. Second, we provide evidence that this phase transition region is also one where optimization would be of most benefit to security agencies, and thus requires significant attention from researchers in this area."
2137580,14133,8960,Structure learning of antiferromagnetic Ising models,2014,"In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of Ω(pd/2) for learning general graphical models on p nodes of maximum degree d, for the class of so-called statistical algorithms recently introduced by Feldman et al. [1]. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound suggests that the O(pd+2) runtime required by Bresler, Mossel, and Sly's [2] exhaustive-search algorithm cannot be significantly improved without restricting the class of models.#R##N##R##N#Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari [3] showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time O(p2). We provide an algorithm whose performance interpolates between O(p2) and O(pd+2) depending on the strength of the repulsion."
2118993,14133,8960,Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors,2014,"Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians.#R##N##R##N#In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems."
2470888,14133,20411,From one tree to a forest: a unified solution for structured web data extraction,2011,"Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution."
733830,14133,422,"Pleasing the advertising oracle: Probabilistic prediction from sampled, aggregated ground truth",2014,"Most video advertising campaigns today are still evaluated based on aggregate demographic audience metrics, rather than measures of individual impact or even individual demographic reach. To fit in with advertisers' evaluations, campaigns must be optimized toward validation by third-party measurement companies, which act as oracles in assessing ground truth. However, information is only available from such oracles in aggregate, leading to a setting with incomplete ground truth. We explore methods for building probabilistic classification models using these aggregate data. If they perform well, such models can be used to create new engineered segments that perform better than existing segments, in terms of lift and/or reach. We focus on the setting where companies already have machinery in place for high-performance predictive modeling from traditional, individual-level data. We show that model building, evaluation, and selection can be reliably carried out even with access only to aggregate ground truth data. We show various concrete results, highlighting confounding aspects of the problem, such as the tendency for pre-existing in-target segments actually to comprise biased subpopulations, which has implications both for campaign performance and modeling performance. The paper's main results show that these methods lead to engineered segments that can substantially improve lift and/or reach---as verified by a leading third-party oracle. For example, for lifts of 2-3X, segment reach can be increased to 57 times that of comparable, pre-existing segments."
2475227,14133,23922,Opportunistic Strategies for Generalized No-Regret Problems,2013,"This paper considers a generalized no-regret problem with vector-valued rewards, defined in terms of a desired reward set of the agent. For each mixed action q of the opponent, the agent has a set R � (q) where the average reward should reside. In addition, the agent has a response mixed action p which brings the expected reward under these two actions, r(p;q), to R � (q). If a strategy of the agent ensures that the average reward converges to R � (¯qn), where ¯ qn is the empirical distribution of the opponent’s actions, for any strategy of the opponent, we say that it is a no-regret strategy with respect to R � (q). When the multifunction q 㜡 R � (q) is convex, as is the case in the standard no-regret problem, noregret strategies can be devised. Our main interest in this paper is in cases where this convexity property does not hold. The best that can be guaranteed in general then is the convergence of the average reward to R c (¯qn), the convex hull of R � (¯qn). However, as the game unfolds, it may turn out that the opponent’s choices of actions are limited in some way. If these restrictions were known in advance, the agent could possibly ensure convergence of the average reward to some desired subset of R c (¯qn), or even approach R � (¯qn) itself. We formulate appropriate goals for opportunistic no-regret strategies, in the sense that they may exploit such limitations on the opponent’s action sequence in an on-line manner, without knowing them beforehand. As the main technical tool, we propose a class of approachability algorithms that rely on a calibrated forecast of the opponent’s actions, which are opportunistic in the above mentioned sense. As an application, we consider the online no-regret problem with average cost constraints, introduced in Mannor, Tsitsiklis, and Yu (2009). We show, in particular, that our algorithm does attain the best-responsein-hindsight for this problem if the opponent’s play happens to be stationary, or close to stationary in a certain sense."
1238080,14133,20358,Attributed graph models: modeling network structure with correlated attributes,2014,"Online social networks have become ubiquitous to today's society and the study of data from these networks has improved our understanding of the processes by which relationships form. Research in statistical relational learning focuses on methods to exploit correlations among the attributes of linked nodes to predict user characteristics with greater accuracy. Concurrently, research on generative graph models has primarily focused on modeling network structure without attributes, producing several models that are able to replicate structural characteristics of networks such as power law degree distributions or community structure. However, there has been little work on how to generate networks with real-world structural properties and correlated attributes. In this work, we present the Attributed Graph Model (AGM) framework to jointly model network structure and vertex attributes. Our framework learns the attribute correlations in the observed network and exploits a generative graph model, such as the Kronecker Product Graph Model (KPGM) and Chung Lu Graph Model (CL), to compute structural edge probabilities. AGM then combines the attribute correlations with the structural probabilities to sample networks conditioned on attribute values, while keeping the expected edge probabilities and degrees of the input graph model. We outline an efficient method for estimating the parameters of AGM, as well as a sampling method based on Accept-Reject sampling to generate edges with correlated attributes. We demonstrate the efficiency and accuracy of our AGM framework on two large real-world networks, showing that AGM scales to networks with hundreds of thousands of vertices, as well as having high attribute correlation."
1504373,14133,21106,"Analysis of Scores, Datasets, and Models in Visual Saliency Prediction",2013,"Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scan path sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scan path sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling."
2048879,14133,21106,Learning Hash Codes with Listwise Supervision,2013,"Hashing techniques have been intensively investigated in the design of highly efficient search engines for large-scale computer vision applications. Compared with prior approximate nearest neighbor search approaches like tree-based indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. However, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pair wise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage list wise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via list wise supervision can provide superior search accuracy without incurring heavy computational overhead."
2622451,14133,20332,"Search more, disclose less",2013,"The blooming of comparison shopping agents (CSAs) in recent years enables buyers in today's markets to query more than a single CSA while shopping, thus substantially expanding the list of sellers whose prices they obtain. From the individual CSA point of view, however, the multi-CSAs querying is definitely non-favorable as most of today's CSAs benefit depends on payments they receive from sellers upon transferring buyers to their websites (and making a purchase). The most straightforward way for the CSA to improve its competence is through spending more resources on getting more sellers' prices, potentially resulting in a more attractive best price. In this paper we suggest a complementary approach that improves the attractiveness of the best price returned to the buyer without having to extend the CSAs' price database. This approach, which we term selective price disclosure relies on removing some of the prices known to the CSA from the list of results returned to the buyer. The advantage of this approach is in the ability to affect the buyer's beliefs regarding the probability of obtaining more attractive prices if querying additional CSAs. The paper presents two methods for choosing the subset of prices to be presented to a fully-rational buyer, attempting to overcome the computational complexity associated with evaluating all possible subsets. The effectiveness and efficiency of the methods are demonstrated using real data, collected from five CSAs for four products. Furthermore, since people are known to have an inherently bounded rationality, the two methods are also evaluated with human buyers, demonstrating that selective price-disclosing can be highly effective with people, however the subset of prices that needs to be used should be extracted in a different (and more simplistic) manner."
2601296,14133,20332,A Multitask Representation Using Reusable Local Policy Templates,2012,"Constructing robust controllers to perform tasks in large, continually changing worlds is a difficult problem. A long-lived agent placed in such a world could be required to perform a variety of different tasks. For this to be possible, the agent needs to be able to abstract its experiences in a reusable way. This paper addresses the problem of online multitask decision making in such complex worlds, with inherent incompleteness in models of change. A fully general version of this problem is intractable but many interesting domains are rendered manageable by the fact that all instances of tasks may be described using a finite set of qualitatively meaningful contexts. We suggest an approach to solving the multitask problem through decomposing the domain into a set of capabilities based on these local contexts. Capabilities resemble the options of hierarchical reinforcement learning, but provide robust behaviours capable of achieving some subgoal with the associated guarantee of achieving at least a particular aspiration level of performance. This enables using these policies within a planning framework, and they become a level of abstraction which factorises an otherwise large domain into task-independent sub-problems, with well-defined interfaces between the perception, control and planning problems. This is demonstrated in a stochastic navigation example, where an agent reaches different goals in different world instances without relearning."
2151389,14133,8960,Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time,2014,"We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees.#R##N##R##N#In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping.#R##N##R##N#Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of 1/√t within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level s*, dimension d and sample size n. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees."
1763182,14133,11166,Low-Rank Common Subspace for Multi-view Learning,2014,"Multi-view data is very popular in real-world applications, as different view-points and various types of sensors help to better represent data when fused across views or modalities. Samples from different views of the same class are less similar than those with the same view but different class. We consider a more general case that prior view information of testing data is inaccessible in multi-view learning. Traditional multi-view learning algorithms were designed to obtain multiple view-specific linear projections and would fail without this prior information available. That was because they assumed the probe and gallery views were known in advance, so the correct view-specific projections were to be applied in order to better learn low-dimensional features. To address this, we propose a Low-Rank Common Subspace (LRCS) for multi-view data analysis, which seeks a common low-rank linear projection to mitigate the semantic gap among different views. The low-rank common projection is able to capture compatible intrinsic information across different views and also well-align the within-class samples from different views. Furthermore, with a low-rank constraint on the view-specific projected data and that transformed by the common subspace, the within-class samples from multiple views would concentrate together. Different from the traditional supervised multi-view algorithms, our LRCS works in a weakly supervised way, where only the view information gets observed. Such a common projection can make our model more flexible when dealing with the problem of lacking prior view information of testing data. Two scenarios of experiments, robust subspace learning and transfer learning, are conducted to evaluate our algorithm. Experimental results on several multi-view datasets reveal that our proposed method outperforms state-of-the-art, even when compared with some supervised learning methods."
1995944,14133,23922,Stochastic Regret Minimization via Thompson Sampling,2014,"The Thompson Sampling (TS) policy is a widely implemented algorithm for the stochastic multiarmed bandit (MAB) problem. Given a prior distribution over possible parameter settings of the underlying reward distributions of the arms, at each time instant, the policy plays an arm with probability equal to the probability that this arm has largest mean reward conditioned on the current posterior distributions of the arms. This policy generalizes the celebrated “probability matching” heuristic which has been experimentally and widely observed in human decision making. However, despite its ubiquity, the Thompson Sampling policy is poorly understood. Our goal in this paper is to make progress towards understanding the empirical success of this policy. We proceed using the lens of approximation algorithms and problem definitions from stochastic optimization. We focus on an objective function termed stochastic regret that captures the expected number of times the policy plays an arm that is not the eventual best arm, where the expectation is over the prior distribution. Given such a definition, we show that TS is a 2‐ approximation to the optimal decision policy in two extreme but canonical scenarios. One such scenario is the two-armed bandit problem which is used as a calibration point in all bandit literature. The second scenario is stochastic optimization where the outcome of a random variable is revealed in a single play to a high or low deterministic value. We show that the 2 approximation is tight in both these scenarios. We provide an uniform analysis framework that in theory is capable of proving our conjecture that the TS policy is a 2‐approximation to the optimal decision policy for minimizing stochastic regret, for any prior distribution and any time horizon."
1879633,14133,8960,Active learning of neural response functions with Gaussian processes,2011,"A sizeable literature has focused on the problem of estimating a low-dimensional feature space for a neuron's stimulus sensitivity. However, comparatively little work has addressed the problem of estimating the nonlinear function from feature space to spike rate. Here, we use a Gaussian process (GP) prior over the infinite-dimensional space of nonlinear functions to obtain Bayesian estimates of the non-linearity in the linear-nonlinear-Poisson (LNP) encoding model. This approach offers increased flexibility, robustness, and computational tractability compared to traditional methods (e.g., parametric forms, histograms, cubic splines). We then develop a framework for optimal experimental design under the GP-Poisson model using uncertainty sampling. This involves adaptively selecting stimuli according to an information-theoretic criterion, with the goal of characterizing the nonlinearity with as little experimental data as possible. Our framework relies on a method for rapidly updating hyperparameters under a Gaussian approximation to the posterior. We apply these methods to neural data from a color-tuned simple cell in macaque V1, characterizing its nonlinear response function in the 3D space of cone contrasts. We find that it combines cone inputs in a highly nonlinear manner. With simulated experiments, we show that optimal design substantially reduces the amount of data required to estimate these nonlinear combination rules."
2931764,14133,11166,Incremental Ensemble Classifier Addressing Non-stationary Fast Data Streams,2014,"Classification of data points in a data stream is a fundamentally different set of challenges than data mining on static data. While streaming data is often placed into the context of Big Data (or more specifically Fast Data) wherein one-pass algorithms are used, true data streams offer additional hurdles due to their dynamic, evolving, and non-stationary nature. During the stream, the available labels (or concepts) often change, and a concept's definition in the feature space can also evolve (or drift) over time. The core issue is that the hidden generative function of the data is not a constant function, but rather evolves over time. This is known as a non-stationary distribution. In this paper, we describe a new approach to using ensembles for stream classification. While the core method is straightforward, it is specifically designed to adapt quickly with very little overhead to the dynamic and evolving nature of data streams generated from non-stationary functions. Our method, M3, is based on a weighted majority ensemble of heterogeneous model types where model weights are updated on-line using Reinforcement Learning techniques. We compare our method with current leading algorithms as implemented in the Massive Online Analysis (MOA) framework using UCI benchmark and synthetic stream generator data sets, and find that our method shows particularly strong gain over the baseline method when ground truth is of limited availability to the classifiers."
791026,14133,422,ComSoc: adaptive transfer of user behaviors over composite social network,2012,"Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization and recommendation, etc. A major challenge lies in that, the available behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., >= 99.9% empty). Many previous works model user behavior from only historical user logs. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter and Tencent's QQ. Importantly, their behaviors and interests in different networks influence one another. This gives us an opportunity to leverage the knowledge of user behaviors in different networks, in order to alleviate the data sparsity problem, and enhance the predictive performance of user modeling. Combining different networks simply and naively does not work well. Instead, we formulate the problem to model multiple networks as composite network knowledge transfer. We first select the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users, and then build topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior model significantly improve the predictive accuracy over a number of existing approaches on several real world applications, such as a very large social-networking dataset from Tencent Inc."
1957583,14133,8228,Utilizing Social Influence in Content Distribution Networks,2011,"Online social networks (OSNs) provide new means of disseminating information about applications and contents served by network providers. OSN members often reveal their usage information and opinion about applications and contents to their neighbors within their social networks. Consequently, sudden popularity and viral propagation of applications among OSN members can put significant burden on network resources and degrade network performance. Further, viral exchanges might propagate malicious applications and such propagation might need to be kept in check. Accordingly, we propose a novel content distribution architecture that controls the resource utilization within an operator's network by utilizing the existing social connections between users and building a model of information diffusion within the social network. Our method is based upon computing a reward function that takes into account the influence of users over each other in a given social network in terms of application adoption and content consumption. Based on this reward function and assuming that not only the users but also the network operator can limit the exposure of application usage and content consumption over the online social networks, we present algorithms to slow down or speed up the adoption/consumption of different applications/contents given the current state of both the physical network and the social network. We evaluate the effectiveness of this method over a Flickr data set. Results suggest that such a control is indeed possible and the proposed method significantly outperforms other approaches that employs mainly degree-based mechanisms for controlling the information dissemination over the social graphs."
2478342,14133,8960,MAP Inference in Chains using Column Generation,2012,"Linear chains and trees are basic building blocks in many applications of graphical models, and they admit simple exact maximum a-posteriori (MAP) inference algorithms based on message passing. However, in many cases this computation is prohibitively expensive, due to quadratic dependence on variables' domain sizes. The standard algorithms are inefficient because they compute scores for hypotheses for which there is strong negative local evidence. For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate results. This paper presents new exact inference algorithms based on the combination of column generation and pre-computed bounds on terms of the model's scoring function. While we do not improve worst-case performance, our method substantially speeds real-world, typical-case inference in chains and trees. Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task. Our algorithm is also extendable to new techniques for approximate inference, to faster 0/1 loss oracles, and new opportunities for connections between inference and learning. We encourage further exploration of high-level reasoning about the optimization problem implicit in dynamic programs."
1513937,14133,422,Scalable hands-free transfer learning for online advertising,2014,"Internet display advertising is a critical revenue source for publishers and online content providers, and is supported by massive amounts of user and publisher data. Targeting display ads can be improved substantially with machine learning methods, but building many models on massive data becomes prohibitively expensive computationally. This paper presents a combination of strategies, deployed by the online advertising firm Dstillery, for learning many models from extremely high-dimensional data efficiently and without human intervention. This combination includes: (i)~A method for simple-yet-effective transfer learning where a model learned from data that is relatively abundant and cheap is taken as a prior for Bayesian logistic regression trained with stochastic gradient descent (SGD) from the more expensive target data. (ii)~A new update rule for automatic learning rate adaptation, to support learning from sparse, high-dimensional data, as well as the integration with adaptive regularization. We present an experimental analysis across 100 different ad campaigns, showing that the transfer learning indeed improves performance across a large number of them, especially at the start of the campaigns. The combined hands-free method needs no fiddling with the SGD learning rate, and we show that it is just as effective as using expensive grid search to set the regularization parameter for each campaign."
851092,14133,11166,The PerfSim Algorithm for Concept Drift Detection in Imbalanced Data,2012,"There is currently a surge of interest in adaptive learning algorithms for applications ranging from ozone level peak predictions, learning stock market indicators, and detecting smart phone usage patterns. In such scenarios, the detection of change (or drift) in the concept being learned is important to ensure that correct, timely and relevant models are constructed. In addition, such data is often imbalanced and, to further complicate the issue, we are frequently interested in learning the minority class. It follows that ignoring these two aspects during learning may lead to unreliable, or even incorrect, models being built. In this research we discuss the interplay between concept drift detection and imbalanced data sets in order to ensure reliable results. We introduce a novel algorithm that, rather than considering a single performance evaluation measure such as accuracy for change detection, considers all the components of a confusion matrix and employs the cosine similarity coefficient. We evaluate our algorithm against a real world mobile phone database, as well as benchmarking datasets, and we compare it with two other state-of-the-art methods. The results show that our approach is particularly sensitive to concept drifts occurring in imbalanced data sets. Our evaluation indicates that our algorithm is able to detect concept drift reliably. Further, our method is shown to perform very well compared to the other techniques, especially when the drift occurs in the minority class of a class imbalance problem."
561118,14133,20332,Parameterized complexity of problems in coalitional resource games,2011,"Coalition formation is a key topic in multi-agent systems. Coalitions enable agents to achieve goals that they may not have been able to achieve on their own. Previous work has shown problems in coalition games to be computationally hard. Wooldridge and Dunne (Artifi. Intell. 2006) studied the classical computational complexity of several natural decision problems in Coalitional Resource Games (CRG) - games in which each agent is endowed with a set of resources and coalitions can bring about a set of goals if they are collectively endowed with the necessary amount of resources. The input of coalitional resource games bundles together several elements, e.g., the agent set Ag, the goal set G, the resource set R, etc. Shrot et al. (AAMAS 2009) examine coalition formation problems in the CRG model using the theory of Parameterized Complexity. Their refined analysis shows that not all parts of input act equal - some instances of the problem are indeed tractable while others still remain intractable.#R##N##R##N#We answer an important question left open by Shrot, Aumann, and Kraus by showing that the SC Problem (checking whether a Coalition is Successful) is W[1]-hard when parameterized by the size of the coalition. Then via a single theme of reduction from SC, we are able to show that various problems related to resources, resource bounds, and resource conflicts introduced by Wooldridge et al. are (i) W[1]-hard or co-W[1]-hard w.r.t the size of the coalition; and (ii) Para-NP-hard or co-Para-NP-hard w.r.t |R|. When parameterized by |G| or |R| + |Ag|, we give a general algorithm which proves that these problems are indeed tractable."
1701698,14133,20411,Latent semantic sparse hashing for cross-modal similarity search,2014,"Similarity search methods based on hashing for effective and efficient cross-modal retrieval on large-scale multimedia databases with massive text and images have attracted considerable attention. The core problem of cross-modal hashing is how to effectively construct correlation between multi-modal representations which are heterogeneous intrinsically in the process of hash function learning. Analogous to Canonical Correlation Analysis (CCA), most existing cross-modal hash methods embed the heterogeneous data into a joint abstraction space by linear projections. However, these methods fail to bridge the semantic gap more effectively, and capture high-level latent semantic information which has been proved that it can lead to better performance for image retrieval. To address these challenges, in this paper, we propose a novel Latent Semantic Sparse Hashing (LSSH) to perform cross-modal similarity search by employing Sparse Coding and Matrix Factorization. In particular, LSSH uses Sparse Coding to capture the salient structures of images, and Matrix Factorization to learn the latent concepts from text. Then the learned latent semantic features are mapped to a joint abstraction space. Moreover, an iterative strategy is applied to derive optimal solutions efficiently, and it helps LSSH to explore the correlation between multi-modal representations efficiently and automatically. Finally, the unified hashcodes are generated through the high level abstraction space by quantization. Extensive experiments on three different datasets highlight the advantage of our method under cross-modal scenarios and show that LSSH significantly outperforms several state-of-the-art methods."
2714176,14133,20332,Regret transfer and parameter optimization,2014,"Regret matching is a widely-used algorithm for learning how to act. We begin by proving that regrets on actions in one setting (game) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters. We prove how this can be done by carefully discounting the prior regrets. This provides, to our knowledge, the first principled warm-starting method for no-regret learning. It also extends to warm-starting the widely-adopted counterfactual regret minimization (CFR) algorithm for large incomplete-information games; we show this experimentally as well. We then study optimizing a parameter vector for a player in a two-player zero-sum game (e.g., optimizing bet sizes to use in poker). We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step. It optimizes the parameter vector while simultaneously finding an equilibrium. We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing. This amounts to the first action abstraction algorithm (algorithm for selecting a small number of discrete actions to use from a continuum of actions--a key preprocessing step for solving large games using current equilibrium-finding algorithms) with convergence guarantees for extensive-form games."
1098135,14133,422,Multi-task copula by sparse graph regression,2014,"This paper proposes multi-task copula (MTC) that can handle a much wider class of tasks than mean regression with Gaussian noise in most former multi-task learning (MTL). While former MTL emphasizes shared structure among models, MTC aims at joint prediction to exploit inter-output correlation. Given input, the outputs of MTC are allowed to follow arbitrary joint continuous distribution. MTC captures the joint likelihood of multi-output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function. While the former can be achieved by classical MTL, learning graphs dynamically varying with input is quite a challenge. We address this issue by developing sparse graph regression (SpaGraphR), a non-parametric estimator incorporating kernel smoothing, maximum likelihood, and sparse graph structure to gain fast learning algorithm. It starts from a few seed graphs on a few input points, and then updates the graphs on other input points by a fast operator via coarse-to-fine propagation. Due to the power of copula in modeling semi-parametric distributions, SpaGraphR can model a rich class of dynamic non-Gaussian correlations. We show that MTC can address more flexible and difficult tasks that do not fit the assumptions of former MTL nicely, and can fully exploit their relatedness. Experiments on robotic control and stock price prediction justify its appealing performance in challenging MTL problems."
628906,14133,22113,Towards understanding global spread of disease from everyday interpersonal interactions,2013,"Monitoring and forecast of global spread of infectious diseases is difficult, mainly due to lack of fine-grained and timely data. Previous work in computational epidemiology has shown that mining data from the web can improve the predictability of high-level aggregate patterns of epidemics. By contrast, this paper explores how individuals contribute to the global spread of disease. We consider the important task of predicting the prevalence of flu-like illness in a given city based on interpersonal interactions of the city's residents with the outside world. We use the geo-tagged status updates of traveling Twitter users to infer properties of the flow of individuals between cities. While previous research considered only the raw volume of passengers, we estimate a number of latent variables, including the number of sick (symptomatic) travelers and the number of sick individuals to whom each traveler was exposed. We show that AI techniques provide insights into the mechanisms of disease spread and significantly improve predictability of future flu outbreaks. Our experiments involve over 51,000 individuals traveling between 75 cities prior and during a severe ongoing flu epidemic (October 2012 - January 2013). Our model leverages the text and interpersonal interactions recorded in over 6.5 million online status updates without any active user participation, enabling scalable public health applications."
2378564,14133,22260,Approximation and Inapproximation for the Influence Maximization Problem in Social Networks under Deterministic Linear Threshold Model,2011,"Influence Maximization is the problem of finding a certain amount of people in a social network such that their aggregation influence through the network is maximized. In the past this problem has been widely studied under a number of different models. In 2003, Kempe \emph{et al.} gave a $(1-{1 \over e})$-approximation algorithm for the \emph{linear threshold model} and the \emph{independent cascade model}, which are the two main models in the social network analysis. In addition, Chen \emph{et al.} proved that the problem of exactly computing the influence given a seed set in the two models is $\#$P-hard. Both the \emph{linear threshold model} and the \emph{independent cascade model} are based on randomized propagation. However such information might be obtained by surveys or data mining techniques, which makes great difference on the properties of the problem. In this paper, we study the Influence Maximization problem in the \emph{deterministic linear threshold model}. As a contrast, we show that in the \emph{deterministic linear threshold model}, there is no polynomial time $n^{1-\epsilon}$-approximation unless P=NP even at the simple case that one person needs at most two active neighbors to become active. This inapproximability result is derived with self-contained proofs without using PCP theorem. In the case that a person can be activated when one of its neighbors become active, there is a polynomial time ${e\over e-1}$-approximation, and we prove it is the best possible approximation under a reasonable assumption in the complexity theory, $NP \not\subset DTIME(n^{\log\log n})$. We also show that the exact computation of the final influence given a seed set can be solved in linear time in the \emph{deterministic linear threshold model}. The Least Seed Set problem, which aims to find a seed set with least number of people to activate all the required people in a given social network, is discussed. Using an analysis framework based on Set Cover, we show a $O($log$n)$-approximation in the case that a people become active when one of its neighbors is activated."
2408556,14133,20796,"Can irrelevant data help semi-supervised learning, why and how?",2011,"Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unlabeled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact of the irrelevant data while relying more on the labeled data and the relevant unlabeled data. This loss function can therefore achieve the maximum entropy principle. The 3C-SVM can then generalize standard SVMs, Semi-supervised SVMs, and SVMs learned from the universum as its special cases. We further analyze the property of 3C-SVM on why the irrelevant data can help to improve the model performance. For implementation, we make relaxation and approximate the objective by the convex-concave procedure, which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic programming problems. Empirical results are reported to demonstrate the advantages of our 3C-SVM model."
269726,14133,20552,Efficient regret bounds for online bid optimisation in budget-limited sponsored search auctions,2014,"We study the problem of an advertising agent who needs to intelligently distribute her budget across a sequence of online keyword bidding auctions. We assume the closing price of each auction is governed by the same unknown distribution, and study the problem of making provably optimal bidding decisions. Learning the distribution is done under censored observations, i.e. the closing price of an auction is revealed only if the bid we place is above it. We consider three algorithms, namely epsilon-First, Greedy Product-Limit (GPL) and LuekerLearn, respectively, and we show that these algorithms provably achieve Hannan-consistency. In particular, we show that the regret bound of epsilon-FIrst is at most O(T^2\3) with high probability. For the other two algorithms, we first prove that, by using a censored data distribution estimator proposed by Zeng[19], the empirical distribution of the closing market price converges in probability to its true distribution with a O(1\sqrt t) rate, where t is the number of updates. Based on this result, we prove that both GPL and LuekerLearn achieve O(\sqrt T) regret bound with high probability. This in fact provides an affirmative answer to the research question raised in [1]. We also evaluate the abovementioned algorithms using real bidding data, and show that although GPL achieves the best performance on average (up to 90% of the optimal solution), its long running time may limit its suitability in practice. By contrast, LuekerLearn and epsilon-First proposed in this paper achieve up to 85% of the optimal, but with an exponential reduction in computational complexity (a saving up to 95%, compared to GPL)."
2818514,14133,9804,Detecting Out-Of-Domain Utterances Addressed to a Virtual Personal Assistant,2014,"Conversational understanding systems, especially virtual personal assistants (VPAs), perform “targeted” natural language understanding, assuming their users stay within the walled gardens of covered domains, and back-off to generic web search otherwise. However, users usually do not know the concept of domains and sometimes simply do not distinguish the system from simple voice search. Hence it becomes an important problem to identify these rejected out-of-domain utterances which are actually intended for the VPA. This paper presents a study tackling this new task, showing that how one utters a request is more important for this task than what is uttered, resembling addressee detection or dialog act tagging. To this end, syntactic and semantic parse “structure” features are extracted in addition to lexical features to train a binary SVM classifier using a large number of random web search queries and VPA utterances from multiple domains. We present controlled experiments leaving one domain out and check the precision of the model when combined with unseen queries. Our results indicate that such structured features result in higher precision especially when the test domain bears little resemblance to the existing domains. Index Terms: conversational understanding, semantic parsing, keyword search, out-of-domain detection, machine learning, virtual personal assistants"
2112284,14133,8960,A mechanistic model of early sensory processing based on subtracting sparse representations,2012,"Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics."
2041117,14133,8960,Hierarchical spike coding of sound,2012,"Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important first step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The first layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of first layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while fine-scale structure is captured by recurrent interactions within the first layer. When fit to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a significant improvement over standard methods."
526970,14133,8960,Prediction strategies without loss,2011,"Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say 'predict 0' or 'predict 1', and our payoff is +1 if the prediction is correct and –1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14∊T and loss 2√T e−∊∊T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors.#R##N##R##N#Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the 'special' expert has been studied by Even-Dar et al. (COLT'07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O(√n(log N + log T)), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property."
242314,14133,11330,Cross-Validation and Mean-Square Stability,2011,"A popular practical method of obtaining a good estimate of the error rate of a learning algorithm is k-fold cross-validation. Here, the set of examples is flrst partitioned into k equal-sized folds. Each fold acts as a test set for evaluating the hypothesis learned on the other ki1 folds. The average error across the k hypotheses is used as an estimate of the error rate. Although widely used, especially with small values of k (such as 10), the cross-validation method has heretofore resisted theoretical analysis due to the fact that the k distinct estimates have inherent correlations between them. With only sanity-check bounds known, there is no compelling reason to use the k-fold cross-validation estimate over a simpler holdout estimate. Conventional wisdom is that the averaging in cross-validation leads to a tighter concentration of the estimate of the error around its mean. In this paper, we show that the conventional wisdom is essentially correct. We analyze the reduction in variance of the gap between the cross-validation estimate and the true error rate, and show that for a large family of stable algorithms cross-validation achieves a near optimal variance reduction factor of (1 + o(1))k. In these cases, the k difierent estimates are essentially independent of each other. To proceed with the analysis, we deflne a new measure of algorithm stability, called mean-square stability. This measure is weaker than most stability notions described in the literature, and encompasses a large class of algorithms including bounded SVM regression and regularized least-squares regression. For slightly less stable algorithms such as t-nearest-neighbor, we show that cross-validation leads to an O(1 p k) reduction in the variance of the generalization error."
876147,14133,10162,Differentially-private release of check-in data for venue recommendation,2014,"Recommender systems suggesting venues offer very useful services to people on the move and a great business opportunity for advertisers. These systems suggest venues by matching the current context of the user with the venue features, and consider the popularity of venues, based on the number of visits (“check-ins”) that they received. Check-ins may be explicitly communicated by users to geo-social networks, or implicitly derived by analysing location data collected by mobile services. In general, the visibility of explicit check-ins is limited to friends in the social network, while the visibility of implicit check-ins limited to the service provider. Exposing check-ins to unauthorized users is a privacy threat since recurring presence in given locations may reveal political opinions, religious beliefs, or sexual orientation, as well as absence from other locations where the user is supposed to be. Hence, on one side mobile app providers host valuable information that recommender system providers would like to buy and use to improve their systems, and on the other we recognize serious privacy issues in releasing that information. In this paper, we solve this dilemma by providing formal privacy guarantees to users and trusted mobile providers while preserving the utility of check-in information for recommendation purposes. Our technique is based on the use of differential privacy methods integrated with a pre-filtering process, and protects against both an untrusted recommender system and its users, willing to infer the venues and sensitive locations visited by other users. Extensive experiments with a large dataset of real users' check-ins show the effectiveness of our methods."
1740948,14133,8502,A relational kernel-based approach to scene classification,2013,"Real-world scenes involve many objects that interact with each other in complex semantic patterns. For example, a bar scene can be naturally described as having a variable number of chairs of similar size, close to each other and aligned horizontally. This high-level interpretation of a scene relies on semantically meaningful entities and is most generally described using relational representations or (hyper-) graphs. Popular in early work on syntactic and structural pattern recognition, relational representations are rarely used in computer vision due to their pure symbolic nature. Yet, today recent successes in combining them with statistical learning principles motivates us to reinvestigate their use. In this paper we show that relational techniques can also improve scene classification. More specifically, we employ a new relational language for learning with kernels, called kLog. With this language we define higher-order spatial relations among semantic objects. When applied to a particular image, they characterize a particular object arrangement and provide discriminative cues for the scene category. The kernel allows us to tractably learn from such complex features. Thus, our contribution is a principled and interpretable approach to learn from symbolic relations how to classify scenes in a statistical framework. We obtain results comparable to state-of-the-art methods on 15 Scenes and a subset of the MIT indoor dataset."
238871,14133,22113,Towards active event recognition,2013,"Directing robot attention to recognise activities and to anticipate events like goal-directed actions is a crucial skill for human-robot interaction. Unfortunately, issues like intrinsic time constraints, the spatially distributed nature of the entailed information sources, and the existence of a multitude of unobservable states affecting the system, like latent intentions, have long rendered achievement of such skills a rather elusive goal. The problem tests the limits of current attention control systems. It requires an integrated solution for tracking, exploration and recognition, which traditionally have been seen as separate problems in active vision. We propose a probabilistic generative framework based on information gain maximisation and a mixture of Kalman Filters that uses predictions in both recognition and attention-control. This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration. Interestingly, the sensors control policy, directly derived from first principles, represents the intuitive trade-off between finding the most discriminative clues and maintaining overall awareness. Experiments on a simulated humanoid robot observing a human executing goal-oriented actions demonstrated improvement on recognition time and precision over baseline systems."
4115,14133,11321,Faster segmentation algorithm for optical coherence tomography images with guaranteed smoothness,2011,"This paper considers the problem of segmenting an accurate and smooth surface from 3D volumetric images. Despite extensive studies in the past, the segmentation problem remains challenging in medical imaging, and becomes even harder in highly noisy and edge-weak images. In this paper we present a highly efficient graph-theoretical approach for segmenting a surface from 3D OCT images. Our approach adopts an objective function that combines the weight and the smoothness of the surface so that the resulting segmentation achieves global optimality and smoothness simultaneously. Based on a volumetric graph representation of the 3D images that incorporates curvature information, our approach first generates a set of 2D local optimal segmentations, and then iteratively improves the solution by fast local computation at regions where significant improvement can be achieved. It can be shown that our approach monotonically improves the quality of solution and converges rather quickly to the global optimal solution. To evaluate the convergence and performance of our method, we test it on both artificial data sets and a set of 14 3D OCT images. Our experiments suggest that the proposed method yields optimal (or almost optimal) solutions in 3 to 5 iterations. Comparing to the existing approaches, our method has a much improved running time, yields almost the same global optimality but with much better smoothness, which makes it especially suitable for segmenting highly noisy images. Our approach can be easily generalized to multi-surface detection."
158657,14133,10994,Efficient discriminative learning of class hierarchy for many class prediction,2012,"Recently the maximum margin criterion has been employed to learn a discriminative class hierarchical model, which shows promising performance for rapid multi-class prediction. Specifically, at each node of this hierarchy, a separating hyperplane is learned to split its associated classes from all of the corresponding training data, leading to a time-consuming training process in computer vision applications with many classes such as large-scale object recognition and scene classification. To address this issue, in this paper we propose a new efficient discriminative class hierarchy learning approach for many class prediction. We first present a general objective function to unify the two state-of-the-art methods for multi-class tasks. When there are many classes, this objective function reveals that some classes are indeed redundant. Thus, omitting these redundant classes will not degrade the prediction performance of the learned class hierarchical model. Based on this observation, we decompose the original optimization problem into a sequence of much smaller sub-problems by developing an adaptive classifier updating method and an active class selection strategy. Specifically, we iteratively update the separating hyperplane by efficiently using the training samples only from a limited number of selected classes that are well separated by the current separating hyperplane. Comprehensive experiments on three large-scale datasets demonstrate that our approach can significantly accelerate the training process of the two state-of-the-art methods while achieving comparable prediction performance in terms of both classification accuracy and testing speed."
261354,14133,20332,Agent based intelligent decluttering enhancements,2011,"Model-driven visualization (MDV) is a novel framework that supports more effective, intelligent user interfaces to improve decision making in complex environments by coupling cognitive and perceptual theories of information processing with advanced artificial intelligence methods. It embeds empirical and theory driven approaches for identifying and prioritizing data based on the information requirements and needs of the human decision maker within intelligent agents. The agents automatically deliver and present information based on its likely value using visualizations that best convey that information to the user(s) of the system. Agents also reason about the context and constraints of the user, environment, and display to enable a higher degree of personalization within an interactive user interface (e.g., by drawing a user's attention to interesting aspects of the data such as trends, anomalies, and patterns). We apply cognitive systems engineering processes to help identify the information available to individuals and/or teams, where it resides, where it is needed, and ultimately how to create the mappings required in connecting critical information to those who need it with innovative visualizations that most effectively support the end user. This paper describes the application of MDV to intelligently deliver timely, mission-critical information by adapting a Common Tactical Picture (CTP) display used for maritime situation awareness, threat assessment, and decision support."
1827095,14133,8960,On Poisson Graphical Models,2013,"Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its infinite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution. While this model can accommodate a wider range of conditional dependencies, some limitations still remain. To address this, we investigate two additional novel variants of the Poisson distribution and their corresponding joint graphical model distributions. Our three novel approaches provide classes of Poisson-like graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our models via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-sequencing data."
1812874,14133,9099,Online multimodal deep similarity learning with application to image retrieval,2013,"Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique."
1948560,14133,20411,IMRank: influence maximization via finding self-consistent ranking,2014,"Influence maximization, fundamental for word-of-mouth marketing and viral marketing, aims to find a set of seed nodes maximizing influence spread on social network. Early methods mainly fall into two paradigms with certain benefits and drawbacks: (1) Greedy algorithms, selecting seed nodes one by one, give a guaranteed accuracy relying on the accurate approximation of influence spread with high computational cost; (2) Heuristic algorithms, estimating influence spread using efficient heuristics, have low computational cost but unstable accuracy. We first point out that greedy algorithms are essentially finding a self-consistent ranking, where nodes' ranks are consistent with their ranking-based marginal influence spread. This insight motivates us to develop an iterative ranking framework, i.e., IMRank, to efficiently solve influence maximization problem under independent cascade model. Starting from an initial ranking, e.g., one obtained from efficient heuristic algorithm, IMRank finds a self-consistent ranking by reordering nodes iteratively in terms of their ranking-based marginal influence spread computed according to current ranking. We also prove that IMRank definitely converges to a self-consistent ranking starting from any initial ranking. Furthermore, within this framework, a last-to-first allocating strategy and a generalization of this strategy are proposed to improve the efficiency of estimating ranking-based marginal influence spread for a given ranking. In this way, IMRank achieves both remarkable efficiency and high accuracy by leveraging simultaneously the benefits of greedy algorithms and heuristic algorithms. As demonstrated by extensive experiments on large scale real-world social networks, IMRank always achieves high accuracy comparable to greedy algorithms, while the computational cost is reduced dramatically, about 10-100 times faster than other scalable heuristics."
2610422,14133,20332,Exploring the Mind with the Aid of Personal Genome — Citizen Science Genetics to Promote Positive Well-Being,2013,"Understanding the human mind and increasing individual happiness are important goals in artificial intelligence   (AI)   and well-being science. The recent revolution in portable self-tracking devices in the data-driven wellness movement and participatory-driven wellness communities, such as the Quantified Self community, provides us with new opportunities to collect psychological or physiological data for understanding the human mind.  While new technologies make it possible to track our daily behavior and various biological signals such as physiological or genetic data more easily, one of the important remaining challenges is to discover our own truly meaningful personal values. Citizen science, scientific research by crowdsourcing or human-based computation, is a new and challenging framework that promotes interdisciplinary research in the fields of computer science, life/brain science, and social psychological/behavioral science,   which   may introduce new paradigms to the AI community. We have been working on citizen science projects related to the area of personal genomics and have developed a personal genomics information environment named MyFinder. The developed platform supports the search for our inherited talents and maximizes our potential for a meaningful life. In particular, we are interested in the human mind and the personal genome. In this paper, we introduce our MyFinder Project and present the results of a recent study on “social intelligence genomics and empathy building”, and discuss issues involved in exploring our mind within the context of personal genomics."
2626028,14133,20332,Designing Intelligent Robots for Human-Robot Teaming in Urban Search and Rescue,2012,"The paper describes ongoing integrated research on designing intelligent robots that can assist humans in making a situation assessment during Urban Search & Rescue (USAR) missions. These robots (rover, microcopter) are deployed during the early phases of an emergency response. The aim is to explore those areas of the disaster hotzone which are too dangerous or too difficult for a human to enter at that point. This requires the robots to be  intelligent  in the sense of being capable of various degrees of autonomy in acting and perceiving in the environment. At the same time, their intelligence needs to go beyond mere task-work. Robots and humans are interdependent. Human operators are dependent on these robots to provide information for a situation assessment. And robots are dependent on humans to help them operate (shared control) and perceive (shared assessment) in what are typically highly dynamic, largely unknown environments. Robots and humans need to form a team. The paper describes how various insights from robotics and Artificial Intelligence are combined , to develop new approaches for modeling human robot teaming. These approaches range from new forms of mod-eling situation awareness (to model distributed acting in dynamic space), human robot interaction (to model communication in teams), flexible planning (to model team coordination and joint action), and cognitive system design (to integrate different forms of functionality in a single system)."
2337062,14133,8960,Why The Brain Separates Face Recognition From Object Recognition,2011,"Many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes. Recent electro-physiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpoint-specific cells projecting to downstream viewpoint-invariant identity-specific cells [1]. A separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-specific. In particular, the 2D images evoked by a face undergoing a 3D rotation are not produced by the same image transformation (2D) that would produce the images evoked by an object of another class undergoing the same 3D rotation. However, within the class of faces, knowledge of the image transformation evoked by 3D rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint. We show, through computational simulations, that an architecture which applies this method of gaining invariance to class-specific transformations is effective when restricted to faces and fails spectacularly when applied to other object classes. We argue here that in order to accomplish viewpoint-invariant face identification from a single example view, visual cortex must separate the circuitry involved in discounting 3D rotations of faces from the generic circuitry involved in processing other objects. The resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network."
637817,14133,10174,Information-Theoretic Approach to Efficient Adaptive Path Planning for Mobile Robotic Environmental Sensing,2013,"Recent research in robot exploration and mapping has focused on sampling environmental hotspot fields. This exploration task is formalized by Low, Dolan, and Khosla (2008) in a sequential decision-theoretic planning under uncertainty framework called MASP. The time complexity of solving MASP approximately depends on the map resolution, which limits its use in large-scale, high-resolution exploration and mapping. To alleviate this computational difficulty, this paper presents an information-theoretic approach to MASP (iMASP) for efficient adaptive path planning; by reformulating the cost-minimizing iMASP as a reward-maximizing problem, its time complexity becomes independent of map resolution and is less sensitive to increasing robot team size as demonstrated both theoretically and empirically. Using the reward-maximizing dual, we derive a novel adaptive variant of maximum entropy sampling, thus improving the induced exploration policy performance. It also allows us to establish theoretical bounds quantifying the performance advantage of optimal adaptive over non-adaptive policies and the performance quality of approximately optimal vs. optimal adaptive policies. We show analytically and empirically the superior performance of iMASP-based policies for sampling the log-Gaussian process to that of policies for the widely-used Gaussian process in mapping the hotspot field. Lastly, we provide sufficient conditions that, when met, guarantee adaptivity has no benefit under an assumed environment model."
1996275,14133,8960,Inverse Density as an Inverse Problem: the Fredholm Equation Approach,2013,"We address the problem of estimating the ratio q/p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration often referred to as importance sampling in statistical inference. It is also closely related to the problem of covariate shift in transfer learning. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization leads to a principled framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities defined on ℝd and smooth d-dimensional sub-manifolds of the Euclidean space.#R##N##R##N#Model selection for unsupervised or semi-supervised inference is generally a difficult problem. It turns out that in the density ratio estimation setting, when samples from both distributions are available, simple completely unsupervised model selection methods are available. We call this mechanism CD-CV for Cross-Density Cross-Validation. We show encouraging experimental results including applications to classification within the covariate shift framework."
1529965,14133,8960,Confidence Sets for Network Structure,2011,"Latent variable models are frequently used to identify structure in dichotomous network data, in part, because they give rise to a Bernoulli product likelihood that is both well understood and consistent with the notion of exchangeable random graphs. In this article, we propose conservative confidence sets that hold with respect to these underlying Bernoulli parameters as a function of any given partition of network nodes, enabling us to assess estimates of residual network structure, that is, structure that cannot be explained by known covariates and thus cannot be easily verified by manual inspection. We demonstrate the proposed methodology by analyzing student friendship networks from the National Longitudinal Survey of Adolescent Health that include race, gender, and school year as covariates. We employ a stochastic expectation-maximization algorithm to fit a logistic regression model that includes these explanatory variables as well as a latent stochastic blockmodel component and additional node-specific effects. Although maximum-likelihood estimates do not appear consistent in this context, we are able to evaluate confidence sets as a function of different blockmodel partitions, which enables us to qualitatively assess the significance of estimated residual network structure relative to a baseline, which models covariates but lacks block structure. © 2011 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 4: 461–469, 2011 © 2011 Wiley Periodicals, Inc."
924894,14133,23735,A transfer learning approach for multi-cue semantic place recognition,2013,"As researchers are striving for developing robotic systems able to move into the `the wild', the interest towards novel learning paradigms for domain adaptation has increased. In the specific application of semantic place recognition from cameras, supervised learning algorithms are typically adopted. However, once learning has been performed, if the robot is moved to another location, the acquired knowledge may be not useful, as the novel scenario can be very different from the old one. The obvious solution would be to retrain the model updating the robot internal representation of the environment. Unfortunately this procedure involves a very time consuming data-labeling effort at the human side. To avoid these issues, in this paper we propose a novel transfer learning approach for place categorization from visual cues. With our method the robot is able to decide automatically if and how much its internal knowledge is useful in the novel scenario. Differently from previous approaches, we consider the situation where the old and the novel scenario may differ significantly (not only the visual room appearance changes but also different room categories are present). Importantly, our approach does not require labeling from a human operator. We also propose a strategy for improving the performance of the proposed method by fusing two complementary visual cues. Our extensive experimental evaluation demonstrates the advantages of our approach on several sequences from publicly available datasets."
2137887,14133,22260,Least Cost Rumor Blocking in Social Networks,2013,"In many real-world scenarios, social network serves as a platform for information diffusion, alongside with positive information (truth) dissemination, negative information (rumor) also spread among the public. To make the social network as a reliable medium, it is necessary to have strategies to control rumor diffusion. In this article, we address the Least Cost Rumor Blocking (LCRB) problem where rumors originate from a community Cr in the network and a notion of protectors are used to limit the bad influence of rumors. The problem can be summarized as identifying a minimal subset of individuals as initial protectors to minimize the number of people infected in neighbor communities of Cr at the end of both diffusion processes. Observing the community structure property, we pay attention to a kind of vertex set, called bridge end set, in which each node has at least one direct in-neighbor in Cr and is reachable from rumors. Under the OOAO model, we study LCRB-P problem, in which α (0 <; α <; 1) fraction of bridge ends are required to be protected. We prove that the objective function of this problem is submodular and a greedy algorithm is adopted to derive a (1-1/e)-approximation. Furthermore, we study LCRB-D problem over the DOAA model, in which all the bridge ends are required to be protected, we prove that there is no polynomial time o(ln n)-approximation for the LCRB-D problem unless P = NP, and propose a Set Cover Based Greedy (SCBG) algorithm which achieves a O(ln n)-approximation ratio. Finally, to evaluate the efficiency and effectiveness of our algorithm, we conduct extensive comparison simulations in three real-world datasets, and the results show that our algorithm outperforms other heuristics."
734860,14133,11166,The Pairwise Gaussian Random Field for High-Dimensional Data Imputation,2013,"In this paper, we consider the problem of imputation (recovering missing values) in very high-dimensional data with an arbitrary covariance structure. The modern solution to this problem is the Gaussian Markov random field (GMRF). The problem with applying a GMRF to very high-dimensional data imputation is that while the GMRF model itself can be useful even for data having tens of thousands of dimensions, utilizing a GMRF requires access to a sparsified, inverse covariance matrix for the data. Computing this matrix using even state-of-the-art methods is very costly, as it typically requires first estimating the covariance matrix from the data (at a O(nm2) cost for m dimensions and n data points) and then performing a regularized inversion of the estimated covariance matrix, which is also very expensive. This is impractical for even moderately-sized, high-dimensional data sets. In this paper, we propose a very simple alternative to the GMRF called the pair wise Gaussian random field or PGRF for short. The PGRF is a graphical, factor-based model. Unlike traditional Gaussian or GMRF models, a PGRF does not require a covariance or correlation matrix as input. Instead, a PGRF takes as input a set of p (dimension, dimension) pairs for which the user suspects there might be a strong correlation or anti-correlation. This set of pairs defines the graphical structure of the model, with a simple Gaussian factor associated with each of the p (dimension, dimension) pairs. Using this structure, it is easy to perform simultaneous inference and imputation of the model. The key benefit of the approach is that the time required for the PGRF to perform inference is approximately linear with respect to p, where p will typically be much smaller than the number of entries in a m×m covariance or precision matrix."
126625,14133,11187,It's as easy as ABC: introducing anthropology-based computing,2013,"The evolution and adaptation of humans is intractably intertwined with the evolution and adaptation of our technology. This was true when we added wooden handles to stone adzes, and it is true today. Weiser and Brown warned that ubiquitous computing would require the development of Calm Technology, a total change to the way in which we interact with computers, so that the entire process could become more suitable to human perceptual abilities and limitations. Our failure to do so is responsible for a daily onslaught of injury and death, from Carpal Tunnel Syndrome to plane crashes. We propose a solution based on one of the underlying concepts of Artificial Neural Networks. For decades, attempts have been made to recreate the basic physiological step of human information processing. It is time to go one step further and consider the basic human parameters of input and output, as proposed by Weiser and Brown. Their term Calm Technology has been modified and re-defined over the past twenty years and their true intent has been lost. In order to avoid the territorial battles that surround the term, and in an attempt to assist engineers and human factors specialists in their efforts to preserve health and save lives, we introduce the concept of Anthropology-Based Computing (ABC). We define ABC as any input and output design parameters based on the basic physiological, psychological and social requirements of the human animal in our natural habitat."
1408420,14133,10162,Scheduling sensors for monitoring sentient spaces using an approximate POMDP policy,2013,"We present a framework for sensor actuation and control in sentient spaces, in which sensors are used to observe a physical phenomena. We focus on sentient spaces that enable pervasive computing applications, such as smart video surveillance and situational awareness in instrumented office environments. Our framework utilizes the spatio-temporal statistical properties of an observed phenomena, with the goal of maximizing an application-specified reward. Specifically, we define an observation of a phenomena by assigning it a discrete value (state) and we model its semantics as the transition between these values (states). This semantic model is used to predict the future states in which the phenomena is likely to be at, based on partially-observed past states. To accomplish real-time agility, we designed an approximate, adaptive-grid solution for Partially Observable Markov Decision Processes (POMDPs) that yields practically good results, and in some cases, guarantees on the quality of the approximation. We use our framework to control and actuate a large-scale camera network so as to maximize the number and type of captured events. To enable real-time control, we implement an action schedule using a table lookup and make use of a factored probability model to capture state semantics. To the best of our knowledge, we are the first to address the problem of actuating a large-scale sensor network based on a real-time POMDP formulation."
2590448,14133,22113,A hierarchical architecture for adaptive brain-computer interfacing,2011,"Brain-computer interfaces (BCIs) allow a user to directly control devices such as cursors and robots using brain signals. Non-invasive BCIs, e.g., those based on electroencephalographic (EEG) signals recorded from the scalp, suffer from low signal-to-noise ratio which limits the bandwidth of control. Invasive BCIs allow fine-grained control but can leave users exhausted since control is typically exerted on a moment-by-moment basis. In this paper, we address these problems by proposing a new adaptive hierarchical architecture for brain-computer interfacing. The approach allows a user to teach the BCI new skills on-the-fly; these learned skills are later invoked directly as high-level commands, relieving the user of tedious low-level control. We report results from four subjects who used a hierarchical EEG-based BCI to successfully train and control a humanoid robot in a virtual home environment. Gaussian processes were used for learning high-level commands, allowing a BCI to switch between autonomous and user-guided modes based on the current estimate of uncertainty. We also report the first instance of multi-tasking in a BCI, involving simultaneous control of two different devices by a single user. Our results suggest that hierarchical BCIs can provide a flexible and robust way of controlling complex robotic devices in real-world environments."
2376865,14133,8960,Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs,2014,"We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model [1] and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. [1] is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models [2, 3, 4, 5] and measures of model fitness [6] provide strong support that explicitly modeling word dependencies—as in APM—could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because O(p2) parameters must be estimated where p is the number of words ([1] could only provide results for datasets with p = 200). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle p = 104 as an important step towards scaling to large datasets. In addition, Inouye et al. [1] only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word brings to mind another word [7]). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)"
2529806,14133,22051,Balancing Prediction and Recommendation Accuracy: Hierarchical Latent Factors for Preference Data,2012,"Recent works in Recommender Systems (RS) have investigated the relationships between the prediction accuracy, i.e. the ability of a RS to minimize a cost function (for instance the RMSE measure) in estimating users’ preferences, and the accuracy of the recommendation list provided to users. State-of-the-art recommendation algorithms, which focus on the minimization of RMSE, have shown to achieve weak results from the recommendation accuracy perspective, and vice versa. In this work we present a novel Bayesian probabilistic hierarchical approach for users’ preference data, which is designed to overcome the limitation of current methodologies and thus to meet both prediction and recommendation accuracy. According to the generative semantics of this technique, each user is modeled as a random mixture over latent factors, which identify users community interests. Each individual user community is then modeled as a mixture of topics, which capture the preferences of the members on a set of items. We provide two dierent formalization of the basic hierarchical model: BH-Forced focuses on rating prediction, while BH-Free models both the popularity of items and the distribution over item ratings. The combined modeling of item popularity and rating provides a powerful framework for the generation of highly accurate recommendations. An extensive evaluation over two popular benchmark datasets reveals the eectiveness and the quality of the proposed algorithms, showing that BH-Free realizes the most satisfactory compromise between prediction and recommendation accuracy with respect to several stateof-the-art competitors."
967292,14133,20796,MaC: A Probabilistic Framework for Query Answering with Machine-Crowd Collaboration,2014,"The popularity of crowdsourcing has recently brought about brand new opportunities for engaging human intelligence in the process of data analysis. Most existing works on crowdsourcing have developed sophisticated methods to utilize the crowd as a new kind of processor, a.k.a. Human Processor Units (HPU). In this paper, we propose a framework, called MaC, to combine the powers of both CPUs and HPUs. In order to build MaC, we need to tackle the following two challenges: (1) HIT Selection: Selecting the right HITs (Human Intelligent Tasks) can help reducing the uncertainty significantly and the results can converge quickly. Thus, we propose an entropy-based model to evaluate the informativeness of HITs. Furthermore, we find that selecting HITs has factorial complexity and the optimization function is non-linear, thus, we propose an efficient approximation algorithm with a bounded error. (2) Uncertainty Management: Crowdsourced answers can be inaccurate. To address this issue, we provide effective solutions in three common scenarios of crowdsourcing: (a) the answer and the confidence of each worker are available; (b) the confidence of each worker and the voting score for each HIT are available; (c) only the answer of each worker is available. To verify the effectiveness of the MaC framework, we built a hybrid Machine-Crowd system and tested it on three real-world applications - data fusion, information extraction and pattern recognition. The experimental results verified the effectiveness and the applicability of our framework."
1404144,14133,422,Active-transductive learning with label-adapted kernels,2014,"This paper presents an efficient active-transductive approach for classification. A common approach of active learning algorithms is to focus on querying points near the class boundary in order to refine it. However, for certain data distributions, this approach has been shown to lead to uninformative samples. More recent approaches consider combining data exploration with traditional refinement techniques. These techniques typically require tuning sampling of unexplored regions with refinement of detected class boundaries. They also involve significant computational costs for the exploration of informative query candidates. We present a novel iterative active learning algorithm designed to overcome these shortcomings by using a linear running-time active-transductive learning approach that naturally switches from exploration to refinement. The passive classifier employed in our algorithm builds a random-walk on the data graph based on a modified graph geometry that combines the data distribution with current label hypothesis; while the query component uses the uncertainty of the evolving hypothesis. Our supporting theory draws the link between the spectral properties of our iteration matrix and a solution to the minimal-cut problem for a fused hypothesis-data graph. Experiments demonstrate computational complexity that is orders of magnitude lower than state-of-the-art, and competitive results on benchmark data and real churn prediction data."
327016,14133,11321,Segmenting hippocampus from 7.0 Tesla MR images by combining multiple atlases and auto-context models,2011,"In investigation of neurological diseases, accurate measurement of hippocampus is very important for differentiating inter-subject difference and subtle longitudinal change. Although many automatic segmentation methods have been developed, their performance can be limited by the poor image contrast of hippocampus in the MR images, acquired from either 1.5T or 3.0T scanner. Recently, the emergence of 7.0T scanner sheds new light on the study of hippocampus by providing much higher contrast and resolution. But the automatic segmentation algorithm for 7.0T images still lags behind the development of high-resolution imaging techniques. In this paper, we present a learning-based algorithm for segmenting hippocampi from 7.0T images, by using multi-atlases technique and auto-context models. Specifically, for each atlas (along with other aligned atlases), Auto-Context Model (ACM) is performed to iteratively construct a sequence of classifiers by integrating both image appearance and context features in the local patch. Since there exist plenty of texture information in 7.0T images, more advanced texture features are also extracted and incorporated into the ACM during the training stage. With the use of multiple atlases, multiple sequences of ACM-based classifiers will be trained, respectively in each atlas' space. Thus, in the application stage, a new image will be segmented by first applying the sequence of the learned classifiers of each atlas to it, and then fusing multiple segmentation results from multiple atlases (or multiple sequences of classifiers) by a label-fusion technique. Experimental results on the six 7.0T images with voxel size of 0.35 × 0.35 × 0.35mm3 show much better results obtained by our method than by the method using only the conventional auto-context model."
1896555,14133,8960,Optimal prior-dependent neural population codes under shared input noise,2014,"The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations."
627131,14133,11321,Modelling transition dynamics in MDPs with RKHS embeddings,2012,"We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments. Copyright 2012 by the author(s)/owner(s)."
2364623,14133,8960,Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging,2012,"Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefficients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q ≽ 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model's utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject's conversion to Alzheimer's Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values ≪ 10-3). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity)."
2536660,14133,20332,Sembler: ensembling crowd sequential labeling for improved quality,2012,"Many natural language processing tasks, such as named entity recognition (NER), part of speech (POS) tagging, word segmentation, and etc., can be formulated as sequential data labeling problems. Building a sound labeler requires very large number of correctly labeled training examples, which may not always be possible. On the other hand, crowdsourcing provides an inexpensive yet efficient alternative to collect manual sequential labeling from non-experts. However the quality of crowd labeling cannot be guaranteed, and three kinds of errors are typical: (1) incorrect annotations due to lack of expertise (e.g., labeling gene names from plain text requires corresponding domain knowledge); (2) ignored or omitted annotations due to carelessness or low confidence; (3) noisy annotations due to cheating or vandalism. To correct these mistakes, we present Sembler, a statistical model for ensembling crowd sequential labelings. Sembler considers three types of statistical information: (1) the majority agreement that proves the correctness of an annotation; (2) correct annotation that improves the credibility of the corresponding annotator; (3) correct annotation that enhances the correctness of other annotations which share similar linguistic or contextual features. We evaluate the proposed model on a real Twitter and a synthetical biological data set, and find that Sembler is particularly accurate when more than half of annotators make mistakes."
2037595,14133,8960,Consistent Binary Classification with Generalized Performance Metrics,2014,"Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives. Despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy, AM measure, F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets."
794506,14133,20796,Semi-supervised multi-task learning of structured prediction models for web information extraction,2011,"Extracting information from web pages is an important problem; it has several applications such as providing improved search results and construction of databases to serve user queries. In this paper we propose a novel structured prediction method to address two important aspects of the extraction problem: (1) labeled data is available only for a small number of sites and (2) a machine learned global model does not generalize adequately well across many websites. For this purpose, we propose a weight space based graph regularization method. This method has several advantages. First, it can use unlabeled data to address the limited labeled data problem and falls in the class of graph regularization based semi-supervised learning approaches. Second, to address the generalization inadequacy of a global model, this method builds a local model for each website. Viewing the problem of building a local model for each website as a task, we learn the models for a collection of sites jointly; thus our method can also be seen as a graph regularization based multi-task learning approach. Learning the models jointly with the proposed method is very useful in two ways: (1) learning a local model for a website can be effectively influenced by labeled and unlabeled data from other websites; and (2) even for a website with only unlabeled examples it is possible to learn a decent local model. We demonstrate the efficacy of our method on several real-life data; experimental results show that significant performance improvement can be obtained by combining semi-supervised and multi-task learning in a single framework."
2566153,14133,22113,Towards effective prioritizing water pipe replacement and rehabilitation,2013,"Water pipe failures can not only have a great impact on people's daily life but also cause significant waste of water which is an essential and precious resource to human beings. As a result, preventative maintenance for water pipes, particularly in urban-scale networks, is of great importance for a sustainable society. To achieve effective replacement and rehabilitation, failure prediction aims to proactively find those 'most-likely-to-fail' pipes becomes vital and has been attracting more attention from both academia and industry, especially from the civil engineering field. This paper presents an already-deployed industrial computational system for pipe failure prediction. As an alternative to risk matrix methods often depending on ad-hoc domain heuristics, learning based methods are adopted using the attributes with respect to physical, environmental, operational conditions and etc. Further challenge arises in practice when lacking of profile attributes. A dive into the failure records shows that the failure event sequences typically exhibit temporal clustering patterns, which motivates us to use the stochastic process to tackle the failure prediction task. Specifically, the failure sequence is formulated as a self-exciting stochastic process which is, to our best knowledge, a novel formulation for pipe failure prediction. And we show that it outperforms a baseline assuming the failure risk grows linearly with aging. Broad new problems and research points for the machine learning community are also introduced for future work."
2583308,14133,20332,Locality-constrained low-rank coding for image classification,2014,"Low-rank coding (LRC), originated from matrix decomposition, is recently introduced into image classification. Following the standard bag-of-words (BOW) pipeline, when coding the data matrix in the sense of low-rankness incorporates contextual information into the traditional BOW model, this can capture the dependency relationship among neighbor patches. It differs from the traditional sparse coding paradigms which encode patches independently. Current LRC-based methods use l1 norm to increase the discrimination and sparseness of the learned codes. However, such methods fail to consider the local manifold structure between data space and dictionary space. To solve this problem, we propose a locality-constrained low-rank coding (LCLR) algorithm for image representations. By using the geometric structure information as a regularization term, we can obtain more discriminative representations. In addition, we present a fast and stable online algorithm to solve the optimization problem. In the experiments, we evaluate LCLR with four benchmarks, including one face recognition dataset (extended Yale B), one handwritten digit recognition dataset (USPS), and two image datasets (Scene13 for scene recognition and Caltech101 for object recognition). Experimental results show that our approach outperforms many state-of-the-art algorithms even with a linear classifier."
1797179,14133,422,Automatic taxonomy construction from keywords,2012,"Taxonomies, especially the ones in specific domains, are becoming indispensable to a growing number of applications. State-of-the-art approaches assume there exists a text corpus to accurately characterize the domain of interest, and that a taxonomy can be derived from the text corpus using information extraction techniques. In reality, neither assumption is valid, especially for highly focused or fast-changing domains. In this paper, we study a challenging problem: Deriving a taxonomy from a set of keyword phrases. A solution can benefit many real life applications because i) keywords give users the flexibility and ease to characterize a specific domain; and ii) in many applications, such as online advertisements, the domain of interest is already represented by a set of keywords. However, it is impossible to create a taxonomy out of a keyword set itself. We argue that additional knowledge and contexts are needed. To this end, we first use a general purpose knowledgebase and keyword search to supply the required knowledge and context. Then we develop a Bayesian approach to build a hierarchical taxonomy for a given set of keywords. We reduce the complexity of previous hierarchical clustering approaches from  O ( n  2  log  n ) to  O ( n  log  n ), so that we can derive a domain specific taxonomy from one million keyword phrases in less than an hour. Finally, we conduct comprehensive large scale experiments to show the effectiveness and efficiency of our approach. A real life example of building an insurance-related query taxonomy illustrates the usefulness of our approach for specific domains."
2349401,14133,8960,Fast Kernel Learning for Multidimensional Pattern Extrapolation,2014,"The ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems. Kernel methods, such as Gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods. However, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing Gaussian process models for this purpose involves several challenges. A vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation. This difficulty is compounded by the fact that Gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard Gaussian process model. One faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset. In this paper, we propose a Gaussian process approach for large scale multidimensional pattern extrapolation. We recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points. The proposed method significantly outperforms alternative scalable and flexible Gaussian process methods, in speed and accuracy. Moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation."
2256410,14133,22164,A left-to-right HDP-HMM with HDPM emissions,2014,"Nonparametric Bayesian models use a Bayesian framework to learn the model complexity automatically from the data and eliminate the need for a complex model selection process. The Hierarchical Dirichlet Process hidden Markov model (HDP-HMM) is the nonparametric Bayesian equivalent of an HMM. However, HDP-HMM is restricted to an ergodic topology and uses a Dirichlet Process Model (DPM) to achieve a mixture distribution-like model. For applications such as speech recognition, where we deal with ordered sequences, it is desirable to impose a left-to-right structure on the model to improve its ability to model the sequential nature of the speech signal. In this paper, we introduce three enhancements to HDP-HMM: (1) a left-to-right structure: needed for sequential decoding of speech, (2) non-emitting initial and final states: required for modeling finite length sequences, (3) HDP mixture emissions: allows sharing of data across states. The latter is particularly important for speech recognition because Gaussian mixture models have been very effective at modeling speaker variability. Further, due to the nature of language, some models occur infrequently and have a small number of data points associated with them, even for large corpora. Sharing allows these models to be estimated more accurately. We demonstrate that this new HDP-HMM model produces a 15% increase in likelihoods and a 15% relative reduction in error rate on a phoneme classification task based on the TIMIT Corpus."
1127196,14133,422,Flexible and robust co-regularized multi-domain graph clustering,2013,"Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the  same  set of instances. Thus instances in different domains can be treated as having strict  one-to-one  relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports  many-to-many  cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach."
413608,14133,20332,Exploring the contribution of unlabeled data in financial sentiment analysis,2013,"With the proliferation of its applications in various industries, sentiment analysis by using publicly available web data has become an active research area in text classification during these years. It is argued by researchers that semi-supervised learning is an effective approach to this problem since it is capable to mitigate the manual labeling effort which is usually expensive and time-consuming. However, there was a long-term debate on the effectiveness of unlabeled data in text classification. This was partially caused by the fact that many assumptions in theoretic analysis often do not hold in practice. We argue that this problem may be further understood by adding an additional dimension in the experiment. This allows us to address this problem in the perspective of bias and variance in a broader view. We show that the well-known performance degradation issue caused by unlabeled data can be reproduced as a subset of the whole scenario. We argue that if the bias-variance trade-off is to be better balanced by a more effective feature selection method unlabeled data is very likely to boost the classification performance. We then propose a feature selection framework in which labeled and unlabeled training samples are both considered. We discuss its potential in achieving such a balance. Besides, the application in financial sentiment analysis is chosen because it not only exemplifies an important application, the data possesses better illustrative power as well. The implications of this study in text classification and financial sentiment analysis are both discussed."
1058517,14133,23757,Community detection by popularity based models for authored networked data,2013,"Community detection has emerged as an attractive topic due to the increasing need to understand and manage the networked data of tremendous magnitude. Networked data usually consists of links between the entities and the attributes for describing the entities. Various approaches have been proposed for detecting communities by utilizing the link information and/or attribute information. In this work, we study the problem of community detection for networked data with additional authorship information. By authorship, each entity in the network is authored by another type of entities (e.g., wiki pages are edited by users, products are purchased by customers), to which we refer as authors. Communities of entities are affected by their authors, e.g., two entities that are associated with the same author tend to belong to the same community. Therefore leveraging the authorship information would help us better detect the communities in the networked data. However, it also brings new challenges to community detection. The foremost question is how to model the correlation between communities and authorships. In this work, we address this question by proposing probabilistic models based on the popularity link model [1], which is demonstrated to yield encouraging results for community detection. We employ two methods for modeling the authorships: (i) the first one generates the authorships independently from links by community memberships and popularities of authors by analogy of the popularity link model; (ii) the second one models the links between entities based on authorships together with community memberships and popularities of nodes, which is an analog of previous author-topic model. Upon the basic models, we explore several extensions including (i) we model the community memberships of authors by that of their authored entities to reduce the number of redundant parameters; and (ii) we model the communities memberships of entities and/or authors by their attributes using a discriminative approach. We demonstrate the effectiveness of the proposed models by empirical studies."
1555328,14133,8494,Temporal processing with volatile memristors,2013,"Short-term synaptic plasticity (STP) is a mechanism identified in brain systems according to which the effective connection strength (synaptic strength) between two neurons varies dynamically with recent communication history. As a consequence, the amplitude of the post-synaptic potential in response to a single pre-synaptic event, so-called “spike”, may increase (short-term facilitation) or decrease (short-term depression) with consecutive presynaptic stimulation. However, in contrast to Long-term Synaptic plasticity, these changes are temporary and are typically restored in the absence of input. Interestingly, however, a single neuron which receives input via both facilitating and depressing synapses has improved discrimination capability, distinguishing, for instance, between a sequence of events and a sequence of the same events presented in the reversed order. We, therefore, studied the memory mechanisms in emerging non-CMOS devices with a view to application in temporal pattern recognition and detection, inspired by the STP mechanisms. In particular, we demonstrate that memristors can exhibit a resembling behavior to STP due to an inherent volatility and hysteresis. When stimulated by closely spaced pulse waves, the conductance of the device decreases similar to what a depressing synapse would do if presented with consecutive pre-synaptic spikes. This work paves the way for employing memristors in solving spatio-temporal sequence learning problems."
1712560,14133,422,Online active inference and learning,2011,"We present a generalized framework for active inference, the selective acquisition of labels for cases at prediction time in lieu of using the estimated labels of a predictive model. We develop techniques within this framework for classifying in an online setting, for example, for classifying the stream of web pages where online advertisements are being served. Stream applications present novel complications because (i) at the time of label acquisition, we don't know the set of instances that we will eventually see, (ii) instances repeat based on some unknown (and possibly skewed) distribution. We combine ideas from decision theory, cost-sensitive learning, and online density estimation. We also introduce a method for on-line estimation of the utility distribution, which allows us to manage the budget over the stream. The resulting model tells which instances to label so that by the end of each budget period, the budget is best spent (in expectation). The main results show that: (1) our proposed approach to active inference on streams can indeed reduce error costs substantially over alternative approaches, (2) more sophisticated online estimations achieve larger reductions in error. We next discuss simultaneously conducting active inference and active learning. We show that our expected-utility active inference strategy also selects good examples for learning. We close by pointing out that our utility-distribution estimation strategy can also be applied to convert pool-based active learning techniques into budget-sensitive online active learning techniques."
370513,14133,20552,Latent composite likelihood learning for the structured canonical correlation model,2012,"Latent variable models are used to estimate variables of interest - quantities which are observable only up to some measurement error. In many studies, such variables are known but not precisely quantifiable (such as job satisfaction in social sciences and marketing, analytical ability in educational testing, or inflation in economics). This leads to the development of measurement instruments to record noisy indirect evidence for such unobserved variables such as surveys, tests and price indexes. In such problems, there are postulated latent variables and a given measurement model. At the same time, other unantecipated latent variables can add further unmeasured confounding to the observed variables. The problem is how to deal with unantecipated latents variables. In this paper, we provide a method loosely inspired by canonical correlation that makes use of background information concerning the known latent variables. Given a partially specified structure, it provides a structure learning approach to detect unknown unknowns, the confounding effect of potentially infinitely many other latent variables. This is done without explicitly modeling such extra latent factors. Because of the special structure of the problem, we are able to exploit a new variation of composite likelihood fitting to efficiently learn this structure. Validation is provided with experiments in synthetic data and the analysis of a large survey done with a sample of over 100,000 staff members of the National Health Service of the United Kingdom."
2578688,14133,20332,Discovering life cycle assessment trees from impact factor databases,2011,"In recent years, environmental sustainability has received widespread attention due to continued depletion of natural resources and degradation of the environment. Life cycle assessment (LCA) is a methodology for quantifying multiple environmental impacts of a product, across its entire life cycle – from creation to use to discard. The key object of interest in LCA is the inventory tree, with the desired product as the root node and the materials and processes used across its life cycle as the children. The total impact of the parent in any environmental category is a linear combination of the impacts of the children in that category. LCA has generally been used in 'forward' mode: given an inventory tree and impact factors of its children, the task is to compute the impact factors of the root, i.e., the product being modeled. We propose a data mining approach to solve the inverse problem, where the task is to infer inventory trees from a database of environmental factors. This is an important problem with applications in not just understanding what parts and processes constitute a product but also in designing and developing more sustainable alternatives. Our solution methodology is one of feature selection but set in the context of a non-negative least squares problem. It organizes numerous non-negative least squares fits over the impact factor database into a set of pairwise membership relations which are then summarized into candidate trees in turn yielding a consensus tree. We demonstrate the applicability of our approach over real LCA datasets obtained from a large computer manufacturer."
1449158,14133,23735,Sigma hulls for Gaussian belief space planning for imprecise articulated robots amid obstacles,2013,"In many home and service applications, an emerging class of articulated robots such as the Raven and Baxter trade off precision in actuation and sensing to reduce costs and to reduce the potential for injury to humans in their workspaces. For planning and control of such robots, planning in belief ssigma hullpace, i.e., modeling such problems as POMDPs, has shown great promise but existing belief space planning methods have primarily been applied to cases where robots can be approximated as points or spheres. In this paper, we extend the belief space framework to treat articulated robots where the linkage can be decomposed into convex components. To allow planning and collision avoidance in Gaussian belief spaces, we introduce the concept of sigma hulls: convex hulls of robot links transformed according to the sigma standard deviation boundary points generated by the Unscented Kalman filter (UKF). We characterize the signed distances between sigma hulls and obstacles in the workspace to formulate efficient collision avoidance constraints compatible with the Gilbert-Johnson-Keerthi (GKJ) and Expanding Polytope Algorithms (EPA) within an optimization-based planning framework. We report results in simulation for planning motions for a 4-DOF planar robot and a 7-DOF articulated robot with imprecise actuation and inaccurate sensors. These experiments suggest that the sigma hull framework can significantly reduce the probability of collision and is computationally efficient enough to permit iterative re-planning for model predictive control."
1451339,14133,11166,Tensor Regression Based on Linked Multiway Parameter Analysis,2014,"Classical regression methods take vectors as covariates and estimate the corresponding vectors of regression parameters. When addressing regression problems on covariates of more complex form such as multi-dimensional arrays (i.e. Tensors), traditional computational models can be severely compromised by ultrahigh dimensionality as well as complex structure. By exploiting the special structure of tensor covariates, the tensor regression model provides a promising solution to reduce the model's dimensionality to a manageable level, thus leading to efficient estimation. Most of the existing tensor-based methods independently estimate each individual regression problem based on tensor decomposition which allows the simultaneous projections of an input tensor to more than one direction along each mode. As a matter of fact, multi-dimensional data are collected under the same or very similar conditions, so that data share some common latent components but can also have their own independent parameters for each regression task. Therefore, it is beneficial to analyse regression parameters among all the regressions in a linked way. In this paper, we propose a tensor regression model based on Tucker Decomposition, which identifies not only the common components of parameters across all the regression tasks, but also independent factors contributing to each particular regression task simultaneously. Under this paradigm, the number of independent parameters along each mode is constrained by a sparsity-preserving regulariser. Linked multiway parameter analysis and sparsity modeling further reduce the total number of parameters, with lower memory cost than their tensor-based counterparts. The effectiveness of the new method is demonstrated on real data sets."
1821887,14133,8927,Maximizing product adoption in social networks,2012,"One of the key objectives of viral marketing is to identify a small set of users in a social network, who when convinced to adopt a product will influence others in the network leading to a large number of adoptions in an expected sense. The seminal work of Kempe et al. [13] approaches this as the problem of influence maximization. This and other previous papers tacitly assume that a user who is influenced (or, informed) about a product necessarily adopts the product and encourages her friends to adopt it. However, an influenced user may not adopt the product herself, and yet form an opinion based on the experiences of her friends, and share this opinion with others. Furthermore, a user who adopts the product may not like the product and hence not encourage her friends to adopt it to the same extent as another user who adopted and liked the product. This is independent of the extent to which those friends are influenced by her. Previous works do not account for these phenomena.   We argue that it is important to distinguish product adoption from influence. We propose a model that factors in a user's experience (or projected experience) with a product. We adapt the classical Linear Threshold (LT) propagation model by defining an objective function that explicitly captures product adoption, as opposed to influence. We show that under our model, adoption maximization is NP-hard and the objective function is monotone and submodular, thus admitting an approximation algorithm. We perform experiments on three real popular social networks and show that our model is able to distinguish between influence and adoption, and predict product adoption much more accurately than approaches based on the classical LT model."
1963646,14133,11321,A Divide-and-Conquer Solver for Kernel Support Vector Machines,2014,"The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-andConquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10 6 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DCSVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM."
817726,14133,422,Representing documents through their readers,2013,"From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers? To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as vegetarian or liberal. By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty.   Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the politics label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective."
1463981,14133,20796,An effective latent networks fusion based model for event recommendation in offline ephemeral social networks,2013,"Offline ephemeral social networks (OffESNs) are the networks created ad-hoc at a specific location for a specific purpose and lasting for short period of time, relying on mobile social media such as Radio Frequency Identification (RFID) and Bluetooth devices. The primary purpose of people in the OffESNs is to acquire and share information via attending prescheduled events. Event Recommendation over this kind of networks can facilitate attendees on selecting the prescheduled events and organizers on making resource planning. However, because of lack of users' preference and rating information, as well as explicit social relations, the existing recommendation methods can no longer work well to recommend the events in the OffESNs. To address the challenges such as how to derive latent preferences and social relations and how to fuse the latent information in a unified model, we first construct two heterogeneous interaction social networks, an event participation network and a physical proximity network. Then, we use them to derive users' latent preferences and latent networks on social relations, including like-minded peers, co-attendees and friends. Finally, we propose an LNF (Latent Networks Fusion) model under a pairwise factor graph to infer event attendance probabilities for recommendation. Experiments on an RFID-based real conference dataset have demonstrated the effectiveness of the proposed model compared with typical solutions."
2504429,14133,8960,Large Scale Distributed Deep Networks,2012,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
154769,14133,11052,Multidimensional spectral hashing,2012,"With the growing availability of very large image databases, there has been a surge of interest in methods based on semantic hashing, i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods, we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding, we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints, rather than their distances. We show that this criterion is intractable to solve exactly, but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix, and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators, similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases, the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension, but introduce a kernel trick to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art, especially in the challenging regime of small distance thresholds."
2974721,14133,11166,Rényi Divergence Based Generalization for Learning of Classification Restricted Boltzmann Machines,2014,"As a derivative of Restricted Boltzmann Machine (RBM), classification RBM (Class RBM) is proved to be an effective classifier with a probabilistic interpretation. Several elegant learning methods/models related to Class RBM have been proposed. This paper proposes and analyzes a Renyi divergence based generalization for discriminative learning objective of Class RBM. Specifically, we extend the Conditional Log Likelihood (CLL) objective to a general learning criterion. We prove that, some existing popular training methods can be derived from this generalization, via adjusting the parameters to specific values. Intuitively, the regularization with different settings of parameters constrain the learned RBM distribution in different ways, and the parameter setting that provide a suitable distribution constraints for a particular sample set leads to the optimal performance. Moreover, we show that this generalized criterion actually extends the CLL objective with a Renyi divergence-based regularization. The uniform distribution used in this divergence-based regularization can be replaced by some sample-based distribution. This modification is applicable to any specific case of the general objective, and we call the appended loss as general margin. The proposed generalization enables an effective model selection procedure and experiments on human face recognition and document classification achieved significant performance improvement over the existing learning methods. It is also shown empirically that general margin loss is able to stabilize the parameter sensitivity and further improve the performance of the classifiers."
1803689,14133,11321,Optimization Equivalence of Divergences Improves Neighbor Embedding,2014,"Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two previously separate approaches are tied by an optimization equivalence, making it possible to relate methods from the two approaches and to build new methods that take the best of both worlds. In detail, we prove a theorem of optimization equivalences between - and -, as well as - and R´ enyi-divergences through a connection scalar. Through the equivalences we represent several nonlinear dimensionality reduction and graph drawing methods in a generalized stochastic neighbor embedding setting, where information divergences are minimized between similarities in input and output spaces, and the optimal connection scalar provides a natural choice for the tradeoff between attractive and repulsive forces. We give two examples of developing new visualization methods through the equivalences: 1) We develop weighted symmetric stochastic neighbor embedding (ws-SNE) from Elastic Embedding and analyze its benefits, good performance for both vectorial and network data; in experiments ws-SNE has good performance across data sets of different types, whereas comparison methods fail for some of the data sets; 2) we develop a divergence version of a PolyLog layout method; the new method is scale invariant in the output space and makes it possible to efficiently use large-scale smoothed neighborhoods."
2133551,14133,8960,Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection,2014,"Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of l1-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing l1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for l1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof."
1738814,14133,11104,Input-output-consistent domain adaptation algorithm for remote sensing data classification,2012,"A domain adaptation problem is dealt with where the marginal probability in a target domain is different from but correlated to the one in the source domain but the classification tasks are the same. This problem occurs frequently in classification of remote sensing data, e.g., when data are collected in the same area but at different dates or when data are acquired by the same sensor with the same class label set but in different locations. Traditional learning machines cannot deal with this problem in a satisfactory manner. In this paper, we propose a rationale input-output-consistency where samples in the same cluster and defined by spectral signatures (input space) should have the same class label (output space) if they are accurately classified. With the rationale, samples of high confidence in the target domain are selected to define a new prediction function. Since two domains that are related can have different distributions, the data in the source domain which cannot adapt to the distribution in the target domain are deleted from the training data set. Therefore, the proposed algorithm is denoted as input-consistent-output domain adaptation (iCODA) and works in an iterative way. After the selection of highly-confident target samples and the deletion of source data, a new training data set is used to define a new prediction model. The proposed iCODA algorithm was evaluated on EO-1 hyperspectral data sets from Botswana. Experimental results demonstrate much better classification accuracies when compared to a traditionally used supervised classifier."
2243570,14133,8960,Optimizing Instructional Policies,2013,"Psychologists are interested in developing instructional policies that boost student learning. An instructional policy specifies the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only difficult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more difficult (known as fading). We propose an alternative to the traditional methodology in which we define a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that specifies exemplar difficulty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as efficient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena."
1574198,14133,11166,Multi-task Learning for Classifying Proteins Using Dual Hierarchies,2012,"Several biological databases organize information in taxonomies/hierarchies. These databases differ in terms of curation process, input data, coverage and annotation errors. SCOP and CATH are examples of two databases that classify proteins hierarchically into structurally related groups based on experimentally determined structures. Given the large number of protein sequences with unavailable structure, there is a need to develop prediction methods to classify protein sequences into structural classes. We have developed a novel classification approach that utilizes the underlying relationships across multiple hierarchical source databases within a multi-task learning (MTL) framework. MTL is used to simultaneously learn multiple related tasks, and has been shown to improve generalization performance. Specifically, we have developed and evaluated an MTL approach for predicting the structural class, as defined by two hierarchical databases, CATH and SCOP, using protein sequence information only. We define one task per node of the hierarchies and formulate the MTL problem as a combination of these binary classification tasks. Our experimental evaluation demonstrates that the MTL approach that integrates both the hierarchies outperforms the base-line approach that trains independent models per task, as well as a MTL approach that integrates tasks across a single hierarchical database. We also performed extensive experiments that evaluate different regularization penalties and incorporate different task relationships that achieve superior classification performance."
1147628,14133,10237,Distributed rating prediction in user generated content streams,2011,"Recommender systems predict user preferences based on a range of available information. For systems in which users generate streams of content ( e.g.,  blogs, periodically-updated newsfeeds), users may rate the produced content that they read, and be given accurate predictions about future content they are most likely to prefer. We design a distributed mechanism for predicting user ratings that avoids the disclosure of information to a centralized authority or an untrusted third party: users disclose the rating they give to certain content only to the user that produced this content.   We demonstrate how rating prediction in this context can be formulated as a matrix factorization problem. Using this intuition, we propose a distributed gradient descent algorithm for its solution that abides with the above restriction on how information is exchanged between users. We formally analyse the convergence properties of this algorithm, showing that it reduces a weighted root mean square error of the accuracy of predictions. Although our algorithm may be used many different ways, we evaluate it on the Neflix data set and prediction problem as a benchmark. In addition to the improved privacy properties that stem from its distributed nature, our algorithm is competitive with current centralized solutions. Finally, we demonstrate the algorithm's fast convergence in practice by conducting an online experiment with a prototype user-generated content exchange system implemented as a Facebook application."
1688341,14133,11166,An Online Clustering Algorithm That Ignores Outliers: Application to Hierarchical Feature Learning from Sensory Data,2013,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. Hierarchical feature learning is at the crux to the problems of discovery and recognition. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. The bottom-up weights in each layer are learned to encode a hierarchy of over complete and sparse feature dictionaries from space- and time-varying sensory data by recursive layer-by-layer spherical clustering. This density-based clustering algorithm ignores outliers by the use of a unique adaptive threshold in each neuron's transfer function. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers, thereby possessing the capability to capture features at any level of abstraction. It is fully-learnable with only two manually tunable parameters. The model was deployed to learn meaningful feature hierarchies from audio, images and videos which can then be used for recognition and reconstruction. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications."
2583365,14133,20332,The Exploration of Engineering Hybrid Modeling Strategies Applied to World Cup Soccer,2011,"Given the challenges of modeling multi-scale social phenomena, hybrids may hold the key to unlocking social complexity dynamics. We introduce hybrid system modeling from engineering, as a means to capture complex dynamics within interacting, multi-scale, and global social systems. Whereby hybrid modeling is used in industrial processes and automated control systems, this research uses world cup soccer tournament simulations to demonstrate successful applications. Agent-based modeling for soccer games and cellular automatons for crowd and bettor emotional reactions are modeled on each side of a playing field. A predator-prey theoretical approach is applied with self-organizing soccer teams represented as predators and the soccer ball as prey. Simulations of multiple soccer tournaments of thirty-two teams were conducted with pregame betting and without betting as a pseudo-control measure. Tournaments conducted with pre-game betting resulted in the final tournament games having the wining team demonstrating strong defensive playing styles and scoring by a large margin. Divergence of playing styles did not develop in tournaments without pre-game betting. Hybrids offer a means to explore complexity with evolutionary learning by players, corresponding emotional reactions of spectators, and betting interacting, resulting in patterns of emergent behavior and unique evolutionary behavioral responses to complexity."
1421008,14133,422,Class-distribution regularized consensus maximization for alleviating overfitting in model combination,2014,"In data mining applications such as crowdsourcing and privacy-preserving data mining, one may wish to obtain consolidated predictions out of multiple models without access to features of the data. Besides, multiple models usually carry complementary predictive information, model combination can potentially provide more robust and accurate predictions by correcting independent errors from individual models. Various methods have been proposed to combine predictions such that the final predictions are maximally agreed upon by multiple base models. Though this maximum consensus principle has been shown to be successful, simply maximizing consensus can lead to less discriminative predictions and overfit the inevitable noise due to imperfect base models. We argue that proper regularization for model combination approaches is needed to alleviate such overfitting effect. Specifically, we analyze the hypothesis spaces of several model combination methods and identify the trade-off between model consensus and generalization ability. We propose a novel model called Regularized Consensus Maximization (RCM), which is formulated as an optimization problem to combine the maximum consensus and large margin principles. We theoretically show that RCM has a smaller upper bound on generalization error compared to the version without regularization. Experiments show that the proposed algorithm outperforms a wide spectrum of state-of-the-art model combination methods on 11 tasks."
35163,14133,23922,Open Problem: Shifting Experts on Easy Data,2014,"A number of online algorithms have been developed that have small additional loss (regret) compared to the best \shifting expert. In this model, there is a set of experts and the comparator is the best partition of the trial sequence into a small number of segments, where the expert of smallest loss is chosen in each segment. The regret is typically dened for worst-case data / loss sequences. There has been a recent surge of interest in online algorithms that combine good worstcase guarantees with much improved performance on easy data. A practically relevant class of easy data is the case when the loss of each expert is iid and the best and second best experts have a gap between their mean loss. In the full information setting, the FlipFlop algorithm by De Rooij et al. (2014) combines the best of the iid optimal Follow-The-Leader (FL) and the worst-case-safe Hedge algorithms, whereas in the bandit information case SAO by Bubeck and Slivkins (2012) competes with the iid optimal UCB and the worst-case-safe EXP3. We ask the same question for the shifting expert problem. First, we ask what are the simple and ecient algorithms for the shifting experts problem when the loss sequence in each segment is iid with respect to a xed but unknown distribution. Second, we ask how to eciently unite the performance of such algorithms on easy data with worst-case robustness. A particular intriguing open problem is the case when the comparator shifts within a small subset of experts from a large set under the assumption that the losses in each segment are iid."
2613822,14133,9804,Rapidly Building Domain-Specific Entity-Centric Language Models Using Semantic Web Knowledge Sources,2014,"For domain-specific speech recognition tasks, it is best if the statistical language model component is trained with text data that is content-wise and style-wise similar to the targeted domain for which the application is built. For state-of-the-art language modeling techniques that can be used in real-time within speech recognition engines during first-pass decoding (e.g., N-gram models), the above constraints have to be fulfilled in the training data. However collecting such data, even through crowd sourcing, is expensive and time consuming, and can still be not representative of how a much larger user population would interact with the recognition system. In this paper, we address this problem by employing several semantic web sources that already contain the domain-specific knowledge, such as query click logs and knowledge graphs. We build statistical language models that meet the requirements listed above for domain-specific recognition tasks where natural language is used and the user queries are about name entities in a specific domain. As a case study, in the movies domain where users’ voice queries are movie related, compared to a generic web language model, a language model trained with the above resources not only yields significant perplexity and word-errorrate improvements, but also presents an approach where such language models can be rapidly developed for other domains. Index Terms: speech recognition, language modeling, knowledge graphs, query click graphs, name entities, semantic web"
2742864,14133,20332,Collective Views of the Workings and Significance of Experiences,2012,"The ideal psychological realm that brings forth the best performance in sports is called “the zone.” Top athletes describe it as “the consciousness that exists within the unconsciousness,” a place where their state of mind is empty and their motion automated. In addition, various extraordinary experiences have been reported from the zone. States of mind become intuitive, and information processing capabilities at the unconscious level become optimized. People experience altered perceptions of time and space; they feel unified and fused with objects; they feel as if they were flowing and floating; they recognize their alter ego; they hear their internal voice, and they see a light. These transcendent experiences in the zone can be explained from the standpoint of “Kansei,” based on Carl Jung’s theory. Basically, the “zone” is another dimension which athletes can enter when their “Kansei” becomes transparent and sharpened and their consciousness stretches out to the realm of the collective unconsciousness. It is thought that the psychological “cocktail party” phenomenon can occur in the zone, and neuroscientists believe that Fm alpha waves can appear there. Examining the zone experience using Maslow’s self-actualization theory and Suzuki Daisetsu’s Zen viewpoint suggests that it is the experience of attaining self-actualization by fusing the egoistic and altruistic aspects of the self and triggering a spiritual awakening. The zone experience is an inspirational and spiritual experience concerning love, harmony and beauty, and it can contribute to the wellness and happiness of humankind."
533942,14133,11321,Temporally Dynamic Resting-State Functional Connectivity Networks for Early MCI Identification,2013,"Resting-state functional Magnetic Resonance Imaging (R-fMRI) scan provides a rich characterization of the dynamic changes or temporal variabilities caused by neural interactions that may happen within the scan duration. Multiple functional connectivity networks can be estimated from R-fMRI time series to effectively capture subtle yet short neural connectivity changes induced by disease pathologies. To effectively extract the temporally dynamic information, we utilize a sliding window approach to generate multiple shorter, yet overlapping sub-series from a full R-fMRI time series. Whole-brain sliding window correlations are computed based on these sub-series to generate a series of temporal networks, characterize the neural interactions between brain regions at different time scales. Individual estimation of these temporal networks overlooks the intrinsic temporal smoothness between successive overlapping R-fMRI sub-series. To handle this problem, we suggest to jointly estimate temporal networks by maximizing a penalized log likelihood via a fused lasso regularization: 1) l 1-norm penalty ensures a sparse solution; 2) fused regularization preserves the temporal smoothness while allows correlation variability. The estimated temporal networks were applied for early Mild Cognitive Impairment (eMCI) identification, and our results demonstrate the importance of including temporally dynamic R-fMRI scan information for accurate diagnosis of eMCI."
2020569,14133,21106,Search pruning in video surveillance systems: Efficiency-reliability tradeoff,2011,"In the setting of computer vision, algorithmic searches often aim to identify an object of interest inside large sets of images or videos. Towards reducing the often astronomical complexity of this search, one can use pruning to filter out objects that are sufficiently distinct from the object of interest, thus resulting in a pruning gain of an overall reduced search space. Motivated by practical computer vision based scenarios such as time-constrained human identification in biometric-based video surveillance systems, we analyze the stochastic behavior of time-restricted search pruning, over large and unstructured data sets which are furthermore random and varying, and where in addition, pruning itself is not fully reliable but is instead prone to errors. In this stochastic setting we apply the information theoretic method of types as well as information divergence techniques to explore the natural tradeoff that appears between pruning gain and reliability, and proceed to study the typical and atypical gainreliability behavior, giving insight on how often pruning might fail to substantially reduce the search space. The result, as is, applies to a plethora of computer vision based applications where efficiency and reliability are intertwined bottlenecks in the overall system performance, and the simplicity of the obtained expressions allows for rigorous and insightful assessment of the pruning gain-reliability behavior in such applications, as well as for intuition into designing general object recognition systems."
1790670,14133,9475,PRISCA: A policy search method for extreme trajectory following,2011,"Consider slide parking, given a desired demonstration, how to repeat it accurately? Many robotics tasks, such as slide parking, can be formulated in trajectory following, but not many dynamics of which can be easily modeled to facilitate a solving by the optimal control. Although an emerging stream in robotics is to learn the dynamics and policy from demonstrations, multiple, if not numerous, demonstrations are required. Therefore, learning a policy from scarce experience remains a difficult problem. In this paper, we proposed an online algorithm to learn a policy for control using only a desired demonstration and our intuitive knowledge of the dynamics system. Our approach is found on this observation: For trajectory following, even on a highly nonlinear and coupled dynamical system, so long as the state deviation is initially small, a policy can be updated online to keep the robot on track according to a very obvious and coarse model information (e.g., for driving, this information is simply: steer left to turn left). Our policy search is then devised as a function minimization problem, and is solved by gradient descent using the techniques of optimal baseline, least-state-deviation error, smoothing and in an inverse depreciation as a cost intensifier. Apart from guarantees on performance and convergence, we also demonstrated its performance in two simulations, and an extreme trajectory-following scenario - four-wheel-drive slide parking experiment. To our best knowledge, it is the state-of-the-art autonomous precision slide parking of a 4×4 brakeless RC car."
2119623,14133,9704,A cooperative coevolution-based pittsburgh learning classifier system embedded with memetic feature selection,2011,"Given that real-world classification tasks always have irrelevant or noisy features which degrade both prediction accuracy and computational efficiency, feature selection is an effective data reduction technique showing promising performance. This paper presents a cooperative coevolution framework to make the feature selection process embedded into the classification model construction within the genetic-based machine learning paradigm. The proposed approach utilizes the divide-and-conquer strategy to manage two populations in parallel, corresponding to the selected feature subsets and the rule sets of classifier respectively, in which a memetic feature selection algorithm is adopted to evolve the feature subset population while a Pittsburgh-style learning classifier system is used to carry out the classifier evolution. These two coevolving populations cooperate with each other regarding the fitness evaluation and the final solution is obtained via collaborations between the best individuals from each population. Empirical results on several benchmark data sets chosen from the UCI repository, together with a non-parametric statistical test, validate that the proposed approach is able to deliver classifiers of better prediction accuracy and higher stability with fewer selected features, compared with the original learning classifier system. In addition, the incorporated feature selection process is shown to help improve the computational efficiency as well."
1383626,14133,11166,Tensor-Based Multi-view Feature Selection with Applications to Brain Diseases,2014,"In the era of big data, we can easily access information from multiple views which may be obtained from different sources or feature subsets. Generally, different views provide complementary information for learning tasks. Thus, multi-view learning can facilitate the learning process and is prevalent in a wide range of application domains. For example, in medical science, measurements from a series of medical examinations are documented for each subject, including clinical, imaging, immunologic, serologic and cognitive measures which are obtained from multiple sources. Specifically, for brain diagnosis, we can have different quantitative analysis which can be seen as different feature subsets of a subject. It is desirable to combine all these features in an effective way for disease diagnosis. However, some measurements from less relevant medical examinations can introduce irrelevant information which can even be exaggerated after view combinations. Feature selection should therefore be incorporated in the process of multi-view learning. In this paper, we explore tensor product to bring different views together in a joint space, and present a dual method of tensor-based multi-view feature selection DUAL-TMFS based on the idea of support vector machine recursive feature elimination. Experiments conducted on datasets derived from neurological disorder demonstrate the features selected by our proposed method yield better classification performance and are relevant to disease diagnosis."
623275,14133,11052,Simultaneous image classification and annotation via biased random walk on tri-relational graph,2012,"Image annotation as well as classification are both critical and challenging work in computer vision research. Due to the rapid increasing number of images and inevitable biased annotation or classification by the human curator, it is desired to have an automatic way. Recently, there are lots of methods proposed regarding image classification or image annotation. However, people usually treat the above two tasks independently and tackle them separately. Actually, there is a relationship between the image class label and image annotation terms. As we know, an image with the sport class label rowing is more likely to be annotated with the terms water, boat and oar than the terms wall, net and floor, which are the descriptions of indoor sports.#R##N##R##N#In this paper, we propose a new method for jointly class recognition and terms annotation. We present a novel Tri-Relational Graph (TG) model that comprises the data graph, annotation terms graph, class label graph, and connect them by two additional graphs induced from class label as well as annotation assignments. Upon the TG model, we introduce a Biased Random Walk (BRW) method to jointly recognize class and annotate terms by utilizing the interrelations between two tasks. We conduct the proposed method on two benchmark data sets and the experimental results demonstrate our joint learning method can achieve superior prediction results on both tasks than the state-of-the-art methods."
2654008,14133,11321,Exploration and exploitation with insufficient resources,2011,"In physical experimentation, the resources available to discover new knowledge are typically extremely small in comparison to the size and dimensionality of the parameter spaces that can be searched. Additionally, due to the nature of physical experimentation, experimental errors will occur, particularly in biochemical experimentation where the reactants may undetectably denature, or reactant contamination could occur or equipment failure. These errors mean that not all experimental measurements and observations will be accurate or representative of the system being investigated. As the validity of observations is not guaranteed, resources must be split between exploration to discover new knowledge and exploitation to test the validity of the new knowledge. Currently we are investigating the automation of discovery in physical experimentation, with the aim of producing a fully autonomous closed-loop robotic machine capable of autonomous experimentation. This machine will build and evaluate hypotheses, determine experiments to perform and then perform them on an automated lab-on-chip experimentation platform for biochemical response characterisation. In the present work we examine how the trade-off between exploration and exploitation can occur in a situation where the number of experiments that can be performed is extremely small and where the observations returned are sometimes erroneous or unrepresentative of the behaviour being examined. To manage this trade-off we consider the use of a Bayesian notion of surprise, which is used to perform exploration experiments whilst observations are unsurprising from the predictions that can be made and exploits when observations are surprising as they do not match the predicted response."
1385541,14133,20411,TwiNER: named entity recognition in targeted twitter stream,2012,"Many private and/or public organizations have been reported to create and monitor targeted  Twitter  streams to collect and understand users' opinions about the organizations. Targeted  Twitter  stream is usually constructed by filtering tweets with user-defined selection criteria e.g. tweets published by users from a selected region, or tweets that match one or more predefined keywords. Targeted  Twitter  stream is then monitored to collect and understand users' opinions about the organizations. There is an emerging need for early crisis detection and response with such target stream. Such applications require a good named entity recognition (NER) system for  Twitter , which is able to automatically discover emerging named entities that is potentially linked to the crisis. In this paper, we present a novel 2-step unsupervised NER system for targeted  Twitter  stream, called TwiNER. In the first step, it leverages on the  global context  obtained from Wikipedia and Web N-Gram corpus to partition tweets into valid segments (phrases) using a dynamic programming algorithm. Each such tweet segment is a candidate named entity. It is observed that the named entities in the targeted stream usually exhibit a  gregarious  property, due to the way the targeted stream is constructed. In the second step, TwiNER constructs a random walk model to exploit the  gregarious  property in the  local context  derived from the  Twitter  stream. The highly-ranked segments have a higher chance of being true named entities. We evaluated TwiNER on two sets of real-life tweets simulating two targeted streams. Evaluated using labeled ground truth, TwiNER achieves comparable performance as with conventional approaches in both streams. Various settings of TwiNER have also been examined to verify our  global context + local context  combo idea."
2513164,14133,11321,Greedy Algorithms for Sparse Reinforcement Learning,2012,"Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L1 regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the L1 regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing L1 regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems."
2031371,14133,8960,Incremental Local Gaussian Regression,2014,"Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters."
1020501,14133,9704,A New Approach to Constraint Weight Learning for Variable Ordering in CSPs,2014,"A Constraint Satisfaction Problem (CSP) is a framework used for modeling and solving constrained problems. Tree-search algorithms like backtracking try to construct a solution to a CSP by selecting the variables of the problem one after another. The order in which these algorithm select the variables potentially have significant impact on the search performance. Various heuristics have been proposed for choosing good variable ordering. Many powerful variable ordering heuristics weigh the constraints first and then utilize the weights for selecting good order of the variables. Constraint weighting are basically employed to identify global bottlenecks in a CSP. #R##N#In this paper, we propose a new approach for learning weights for the constraints using competitive coevolutionary Genetic Algorithm (GA). Weights learned by the coevolutionary GA later help to make better choices for the first few variables in a search. In the competitive coevolutionary GA, constraints and candidate solutions for a CSP evolve together through an inverse fitness interaction process. We have conducted experiments on several random, quasi-random and patterned instances to measure the efficiency of the proposed approach. The results and analysis show that the proposed approach is good at learning weights to distinguish the hard constraints for quasi-random instances and forced satisfiable random instances generated with the Model RB. For other type of instances, RNDI still seems to be the best approach as our experiments show."
1803742,14133,8960,Online Decision-Making in General Combinatorial Spaces,2014,"We study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010), and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009)."
1972833,14133,11321,Exponential Family Matrix Completion under Structural Constraints,2014,"We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low‐rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin‐ tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data‐types, such as skewed‐continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low‐rank, such as block‐sparsity, or a superposition structure of low‐rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizerR(:). We propose a simple convex regularized M ‐estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets."
2532672,14133,21106,Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias,2013,"Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyper parameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validation performance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets."
2644516,14133,20332,Supervised hashing for image retrieval via image representation learning,2014,"Hashing is a popular approximate nearest neighbor search approach for large-scale image retrieval. Supervised hashing, which incorporates similarity/ dissimilarity information on entity pairs to improve the quality of hashing function learning, has recently received increasing attention. However, in the existing supervised hashing methods for images, an input image is usually encoded by a vector of handcrafted visual features. Such hand-crafted feature vectors do not necessarily preserve the accurate semantic similarities of images pairs, which may often degrade the performance of hashing function learning. In this paper, we propose a supervised hashing method for image retrieval, in which we automatically learn a good image representation tailored to hashing as well as a set of hash functions. The proposed method has two stages. In the first stage, given the pairwise similarity matrix S over training images, we propose a scalable coordinate descent method to decompose S into a product of HHT where H is a matrix with each of its rows being the approximate hash code associated to a training image. In the second stage, we propose to simultaneously learn a good feature representation for the input images as well as a set of hash functions, via a deep convolutional network tailored to the learned hash codes in H and optionally the discrete class labels of the images. Extensive empirical evaluations on three benchmark datasets with different kinds of images show that the proposed method has superior performance gains over several state-of-the-art supervised and unsupervised hashing methods."
2442490,14133,9099,Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification,2014,"Supervised learning using deep convolutional neural network has shown its promise in large-scale image classification task. As a building block, it is now well positioned to be part of a larger system that tackles real-life multimedia tasks. An unresolved issue is that such model is trained on a static snapshot of data. Instead, this paper positions the training as a continuous learning process as new classes of data arrive. A system with such capability is useful in practical scenarios, as it gradually expands its capacity to predict increasing number of new classes. It is also our attempt to address the more fundamental issue: a good learning system must deal with new knowledge that it is exposed to, much as how human do.   We developed a training algorithm that grows a network not only incrementally but also hierarchically. Classes are grouped according to similarities, and self-organized into levels. The newly added capacities are divided into component models that predict coarse-grained superclasses and those return final prediction within a superclass. Importantly, all models are cloned from existing ones and can be trained in parallel. These models inherit features from existing ones and thus further speed up the learning. Our experiment points out advantages of this approach, and also yields a few important open questions."
2178297,14133,8960,Flexible sampling of discrete data correlations without the marginal distributions,2013,"Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parameterization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size."
767788,14133,11166,Enabling Fast Lazy Learning for Data Streams,2011,"Lazy learning, such as k-nearest neighbor learning, has been widely applied to many applications. Known for well capturing data locality, lazy learning can be advantageous for highly dynamic and complex learning environments such as data streams. Yet its high memory consumption and low prediction efficiency have made it less favorable for stream oriented applications. Specifically, traditional lazy learning stores all the training data and the inductive process is deferred until a query appears, whereas in stream applications, data records flow continuously in large volumes and the prediction of class labels needs to be made in a timely manner. In this paper, we provide a systematic solution that overcomes the memory and efficiency limitations and enables fast lazy learning for concept drifting data streams. In particular, we propose a novel Lazy-tree (Ltree for short) indexing structure that dynamically maintains compact high-level summaries of historical stream records. L-trees are M-Tree [5] like, height-balanced, and can help achieve great memory consumption reduction and sub-linear time complexity for prediction. Moreover, L-trees continuously absorb new stream records and discard outdated ones, so they can naturally adapt to the dynamically changing concepts in data streams for accurate prediction. Extensive experiments on real-world and synthetic data streams demonstrate the performance of our approach."
1664067,14133,20411,Economically-efficient sentiment stream analysis,2014,"Text-based social media channels, such as Twitter, produce torrents of opinionated data about the most diverse topics and entities. The analysis of such data (aka. sentiment analysis) is quickly becoming a key feature in recommender systems and search engines. A prominent approach to sentiment analysis is based on the application of classification techniques, that is, content is classified according to the attitude of the writer. A major challenge, however, is that Twitter follows the data stream model, and thus classifiers must operate with limited resources, including labeled data and time for building classification models. Also challenging is the fact that sentiment distribution may change as the stream evolves. In this paper we address these challenges by proposing algorithms that select relevant training instances at each time step, so that training sets are kept small while providing to the classifier the capabilities to suit itself to, and to recover itself from, different types of sentiment drifts. Simultaneously providing capabilities to the classifier, however, is a conflicting-objective problem, and our proposed algorithms employ basic notions of Economics in order to balance both capabilities. We performed the analysis of events that reverberated on Twitter, and the comparison against the state-of-the-art reveals improvements both in terms of error reduction (up to 14%) and reduction of training resources (by orders of magnitude)."
1533965,14133,11491,Large vocabulary quantization for searching instances from videos,2012,"A very promising application involving video collections is to search for relevant video segments from a video database when given few visual examples of the specific instance, e.g. a person, object, or place. However, this problem is difficult due to the lighting variations, different viewpoints, partial occlusion, and large changes in appearance. In this paper, we focus on a kind of restricted instance searching task, where the region of a specific instance to be searched for is manually labeled on each query image. We formulate this problem in a large vocabulary quantization based Bag-of-Words framework, while putting more research emphasis on investigating to what extent we can benefit from these labeled instance regions. The contribution of this paper mainly lies in two aspects: first, we proposed an algorithm for instance search that outperformed all submissions on the instance search dataset TRECVID 2011. Secondly, after thoroughly analyzing the experiment results, we show that our top performance is mainly due to similar scene retrieval, instead of the same instance search. This observation reveals that in the current dataset background is more dominated than instance, and it also suggests that a promising direction in which to further improve the current algorithm, which may also be the breakthrough for achieving this challenge, is to investigate more about how to truly take advantage of additional labeled instance regions. We believe our research opens a window for future new methods for searching instance."
927170,14133,422,Fast flux discriminant for large-scale sparse nonlinear classification,2014,"In this paper, we propose a novel supervised learning method, Fast Flux Discriminant (FFD), for large-scale nonlinear classification. Compared with other existing methods, FFD has unmatched advantages, as it attains the efficiency and interpretability of linear models as well as the accuracy of nonlinear models. It is also sparse and naturally handles mixed data types. It works by decomposing the kernel density estimation in the entire feature space into selected low-dimensional subspaces. Since there are many possible subspaces, we propose a submodular optimization framework for subspace selection. The selected subspace predictions are then transformed to new features on which a linear model can be learned. Besides, since the transformed features naturally expect non-negative weights, we only require smooth optimization even with the L1 regularization. Unlike other nonlinear models such as kernel methods, the FFD model is interpretable as it gives importance weights on the original features. Its training and testing are also much faster than traditional kernel models. We carry out extensive empirical studies on real-world datasets and show that the proposed model achieves state-of-the-art classification results with sparsity, interpretability, and exceptional scalability. Our model can be learned in minutes on datasets with millions of samples, for which most existing nonlinear methods will be prohibitively expensive in space and time."
16501,14133,11187,Wind Power Resource Estimation with Deep Neural Networks,2013,"The measure-correlate-predict technique is state-of-the-art for assessing the quality of a wind power resource based on long term numerical weather prediction systems. On-site wind speed measurements are correlated to meteorological reanalysis data, which represent the best historical estimate available for the atmospheric state. The different variants of MCP more or less correct the statistical main attributes by making the meteorological reanalyses bias and scaling free using the on-site measurements. However, by neglecting the higher order correlations none of the variants utilize the full potential of the measurements. We show that deep neural networks make use of these higher order correlations. Our implementation is tailored to the requirements of MCP in the context of wind resource assessment. We show the application of this method to a set of different locations and compare the results to a simple linear fit to the wind speed frequency distribution as well as to a standard linear regression MCP, that represents the state-of-the-art in industrial aerodynamics. The neural network based MCP outperforms both other methods with respect to correlation, root-mean-square error and the distance in the wind speed frequency distribution. Site assessment can be considered one of the most important steps developing a wind energy project. To this end, the approach described can be regarded as a novel, high-quality tool for reducing uncertainties in the long-term reference problem of on-site measurements."
1250074,14133,9475,Kernel-based reinforcement learning for traffic signal control with adaptive feature selection,2014,"Reinforcement learning in a large-scale system is computationally challenging due to the curse of the dimensionality. One approach is to approximate the Q-function as a function of a state-action related feature vector, then learn the parameters instead. Although assumptions from the priori knowledge can potentially explore an appropriate feature vector, selecting a biased one that insufficiently represents the system usually leads to the poor learning performance. To avoid this disadvantage, this paper introduces kernel methods to implicitly propose a learnable feature vector instead of a pre-selected one. More specifically, the feature vector is estimated from a reference set which contains all critical state-action pairs observed so far, and it can be updated by either adding a new pair or replace an existing one in the reference set. Thus the approximate Q-function keeps adjusting itself as the knowledge about the system accumulates via observations. Our algorithm is designed in both batch mode and online mode in the context of the traffic signal control. In addition, the convergence of this algorithm is experimentally supported. Furthermore, some regularization methods are proposed to avoid overfitting of Q-function on the noisy observations. Finally, A simulation on the traffic signal control in a single intersection is provided, and the performance of this algorithm is compared with Q-learning, in which the Q-function is numerically estimated for each state-action pair without approximation."
2779738,14133,10174,Concurrent plan recognition and execution for human-robot teams,2014,"There is a strong demand for robots to work in environments, such as aircraft manufacturing, where they share tasks with humans and must quickly adapt to each other's needs. To do so, a robot must both infer the intent of humans, and must adapt accordingly. The literature to date has made great progress on these two tasks - recognition and adaptation - but largely as separate research activities. In this paper, we present a unified approach to these two problems, in which recognition and adaptation occur concurrently and holistically. Key to our approach is a task representation that uses choice to represent alternative plans for both the human and robot, allowing a single set of algorithms to simultaneously achieve recognition and adaptation. To achieve such fluidity, a labeled propagation mechanism is used where decisions made by the human and robot during execution are propagated to relevant future open choices, as determined by causal link analysis, narrowing the possible options that the human would reasonably take (hence achieving intent recognition) as well as the possible actions the robot could consistently take (adaptation). This paper introduces Pike, an executive for human-robot teamwork that quickly adapts and infers intent based on the preconditions of actions in the plan, temporal constraints, unanticipated disturbances, and choices made previously (by either robot or human). We evaluate Pike's performance and demonstrate it on a household task in a human-robot team testbed."
1151393,14133,20796,Cost-sensitive learning for large-scale hierarchical classification,2013,"We study hierarchical classification of products in electronic commerce, classifying a text description of a product into one of the leaf classes of a tree-structure taxonomy. In particular, we investigate two essential problems, performance evaluation and learning, in a synergistic way. Unless we know what is the appropriate performance evaluation metric for a task, we are not going to learn a classifier of maximum utility for the task. Given the characteristics of the task of hierarchical product classification, we shed insight into how and why common evaluation metrics such as error rate can be misleading, which is applicable for treating other real world applications. The analysis leads to a new performance evaluation metric that tailors this task to reflect a vendor's business goal of maximizing revenue. The proposed metric has an intuitive meaning as the average revenue loss, which depends on both the value of individual products and the hierarchical distance between the true class and the predicted class. Correspondingly, our learning algorithm uses multi-class SVM with margin re-scaling to optimize the proposed metric, instead of error rate or other common metrics. Margin re-scaling is sensitive to the scaling of loss functions. We propose a loss normalization approach to appropriately calibrating the scaling of loss functions, which is applicable to general classification and structured prediction tasks whenever using structured SVM with margin re-scaling. Experiments on a large dataset show that our approach outperforms standard multi-class SVM in terms of the proposed metric, effectively reducing the average revenue loss."
585245,14133,20552,Uncertain congestion games with assorted human agent populations,2012,"Congestion games model a wide variety of real-world resource congestion problems, such as selfish network routing, traffic route guidance in congested areas, taxi fleet optimization and crowd movement in busy areas. However, existing research in congestion games assumes: (a) deterministic movement of agents between resources; and (b) perfect rationality (i.e. maximizing their own expected value) of all agents. Such assumptions are not reasonable in dynamic domains where decision support has to be provided to humans. For instance, in optimizing the performance of a taxi fleet serving a city, movement of taxis can be involuntary or nondeterministic (decided by the specific customer who hires the taxi) and more importantly, taxi drivers may not follow advice provided by the decision support system (due to bounded rationality of humans). To that end, we contribute: (a) a general framework for representing congestion games under uncertainty for populations with assorted notions of rationality. (b) a scalable approach for solving the decision problem for perfectly rational agents which are in the mix with boundedly rational agents; and (c) a detailed evaluation on a synthetic and realworld data set to illustrate the usefulness of our new approach with respect to key social welfare metrics in the context of an assorted human-agent population. An interesting result from our experiments on a real-world taxi fleet optimization problem is that it is better (in terms of revenue and operational efficiency) for taxi drivers to follow perfectly rational strategies irrespective of the percentage of drivers not following the advice."
219091,14133,22051,Discriminative Density-ratio Estimation,2013,"The covariate shift is a challenging problem in supervised learning that results from the discrepancy between the training and test distributions. An effective approach which recently drew a considerable attention in the research community is to reweight the training samples to minimize that discrepancy. In specific, many methods are based on developing Density-ratio (DR) estimation techniques that apply to both regression and classification problems. Although these methods work well for regression problems, their performance on classification problems is not satisfactory. This is due to a key observation that these methods focus on matching the sample marginal distributions without paying attention to preserving the separation between classes in the reweighted space. In this paper, we propose a novel method for Discriminative Density-ratio (DDR) estimation that addresses the aforementioned problem and aims at estimating the density-ratio of joint distributions in a class-wise manner. The proposed algorithm is an iterative procedure that alternates between estimating the class information for the test data and estimating new density ratio for each class. To incorporate the estimated class information of the test data, a soft matching technique is proposed. In addition, we employ an effective criterion which adopts mutual information as an indicator to stop the iterative procedure while resulting in a decision boundary that lies in a sparse region. Experiments on synthetic and benchmark datasets demonstrate the superiority of the proposed method in terms of both accuracy and robustness."
1965037,14133,23735,"Towards mixed-initiative, multi-robot field experiments: Design, deployment, and lessons learned",2011,"With the advent of Autonomous Underwater Vehicles (AUVs) and other mobile platforms, marine robotics have had substantial impact on the oceanographic sciences. These systems have allowed scientists to collect data over temporal and spatial scales that would be logistically impossible or prohibitively expensive using traditional ship-based measurement techniques. Increased dependence of scientists on such robots has permeated scientific data gathering with future field campaigns involving these platforms as well as on entire infrastructure of people, processes and software, on shore and at sea. Recent field experiments carried out with a number of surface and underwater platforms give clues to how these technologies are coalescing and need to work together. We highlight one such confluence and describe a future trajectory of needs and desires for field experiments with autonomous marine robotic platforms. Our 2010 inter-disciplinary experiment in the Monterey Bay involved multiple platforms and collaborators with diverse science goals. One important goal was to enable situational awareness, planning and collaboration before, during and after this large-scale collaborative exercise. We present the overall view of the experiment and describe an important shore-side component, the Oceanographic Decision Support System (ODSS), its impact and future directions leveraging such technologies for field experiments."
154986,14133,11052,Sequential spectral learning to hash with multiple representations,2012,"Learning to hash involves learning hash functions from a set of images for embedding high-dimensional visual descriptors into a similarity-preserving low-dimensional Hamming space. Most of existing methods resort to a single representation of images, that is, only one type of visual descriptors is used to learn a hash function to assign binary codes to images. However, images are often described by multiple different visual descriptors (such as SIFT, GIST, HOG), so it is desirable to incorporate these multiple representations into learning a hash function, leading to multi-view hashing. In this paper we present a sequential spectral learning approach to multi-view hashing where a hash function is sequentially determined by solving the successive maximization of local variances subject to decorrelation constraints. We compute multi-view local variances by α-averaging view-specific distance matrices such that the best averaged distance matrix is determined by minimizing its α-divergence from view-specific distance matrices. We also present a scalable implementation, exploiting a fast approximate k-NN graph construction method, in which α-averaged distances computed in small partitions determined by recursive spectral bisection are gradually merged in conquer steps until whole examples are used. Numerical experiments on Caltech-256, CIFAR-20, and NUS-WIDE datasets confirm the high performance of our method, in comparison to single-view spectral hashing as well as existing multi-view hashing methods."
2244353,14133,8960,Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination,2012,"Hypothesis testing on signals defined on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere."
2384717,14133,8960,Computing Nash Equilibria in Generalized Interdependent Security Games,2014,"We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents' voluntary investment decisions when facing potential direct risk and transfer-risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP-complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets."
1798740,14133,8960,Sparse Approximate Manifolds for Differential Geometric MCMC,2012,"One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.#R##N##R##N#In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust Student-t error model, for which the Expected Fisher Information is analytically intractable."
687697,14133,20552,A Bayesian Probability Calculus for Density Matrices,2014,"One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions are a special case where the density matrix is restricted to be diagonal. Density matrices are mixtures of dyads, where a dyad has the form uu' for any any unit column vector u. These unit vectors are the elementary events of the generalized probability space. Perhaps the simplest case to see that something unusual is going on is the case of uniform density matrix, i.e. 1/n times identity. This matrix assigns probability 1/n to every unit vector, but of course there are infinitely many of them. The new normalization rule thus says that sum of probabilities over any orthonormal basis of directions is one. We develop a probability calculus based on these more general distributions that includes definitions of joints, conditionals and formulas that relate these, i.e. analogs of the theorem of total probability, various Bayes rules for the calculation of posterior density matrices, etc. The resulting calculus parallels the familiar 'classical' probability calculus and always retains the latter as a special case when all matrices are diagonal. #R##N#Whereas the classical Bayesian methods maintain uncertainty about which model is 'best', the generalization maintains uncertainty about which unit direction has the largest variance. Surprisingly the bounds also generalize: as in the classical setting we bound the negative log likelihood of the data by the negative log likelihood of the MAP estimator."
1144370,14133,23735,A benchmark for the evaluation of RGB-D SLAM systems,2012,"In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools."
1965391,14133,21106,Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies,2013,"Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branch-and-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy tree-based methods [14] on Caltech-256 [15], SUN [32] and Image Net 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them."
1851869,14133,11321,GEV-Canonical Regression for Accurate Binary Class Probability Estimation when One Class is Rare,2014,"We consider the problem of binary class probability estimation (CPE) when one class is rare compared to the other. It is well known that standard algorithms such as logistic regression do not perform well in this setting as they tend to underestimate the probability of the rare class. Common fixes include under-sampling and weighting, together with various correction schemes. Recently, Wang & Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the generalized extreme value (GEV) distribution, which has been used for modeling rare events in statistics. The approach showed promising initial results, but combined with the logarithmic CPE loss implicitly used in their work, it results in a non-convex composite loss that is difficult to optimize. In this paper, we use tools from the theory of proper composite losses (Buja et al., 2005; Reid & Williamson, 2010) to construct a canonical underlying CPE loss corresponding to the GEV link, which yields a convex proper composite loss that we call the GEV-canonical loss; this loss can be tailored to CPE settings where one class is rare, and is easily minimized using an IRLS-type algorithm similar to that used for logistic regression. Our experiments on both synthetic and real data suggest that the resulting algorithm ‐ which we term GEVcanonical regression ‐ performs well compared to common approaches such as under-sampling and weights-correction for this problem."
2578573,14133,9804,Leveraging Knowledge Graphs for Web-Scale Unsupervised Semantic Parsing,2013,"The past decade has seen the emergence of web-scale structured and linked semantic knowledge resources (e.g., Freebase, DBPedia). These semantic knowledge graphs provide a scalable “schema for the web”, representing a significant opportunity for the spoken language understanding (SLU) research community. This paper leverages these resources to bootstrap a web-scale semantic parser with no requirement for semantic schema design, no data collection, and no manual annotations. Our approach is based on an iterative graph crawl algorithm. From an initial seed node (entity-type), the method learns the related entity-types from the graph structure, and automatically annotates documents that can be linked to the node (e.g., Wikipedia articles, web search documents). Following the branches, the graph is crawled and the procedure is repeated. The resulting collection of annotated documents is used to bootstrap webscale conditional random field (CRF) semantic parsers. Finally, we use a maximum-a-posteriori (MAP) unsupervised adaptation technique on sample data from a specific domain to refine the parsers. The scale of the unsupervised parsers is on the order of thousands of domains and entity-types, millions of entities, and hundreds of millions of relations. The precision-recall of the semantic parsers trained with our unsupervised method approaches those trained with supervised annotations. Index Terms: semantic parsing, semantic web, semantic search, dialog, natural language understanding"
2457939,14133,8960,Sparse PCA via Covariance Thresholding,2014,"In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n x p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components v1,..., vr have at most k1, · · · , kq non-zero entries respectively, and study the high-dimensional regime in which p is of the same order as n.#R##N##R##N#In an influential paper, Johnstone and Lu [JL04] introduced a simple algorithm that estimates the support of the principal vectors v1,..., vr by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if kq ≤ C1 √n/ log p, and to fail with high probability if kq ≥ C2 √n/ log p for two constants 0 < C1, C2 < ∞. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees.#R##N##R##N#Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik [KNV13]. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for k of order √n. Recent conditional lower bounds [BR13] suggest that it might be impossible to do significantly better.#R##N##R##N#The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before."
2613042,14133,20332,Tractable probabilistic knowledge bases with existence uncertainty,2013,"A central goal of AI is to reason efficiently in domains that are both complex and uncertain. Most attempts toward this end add probability to a tractable subset of first-order logic, but this results in intractable inference. To address this, Domingos and Webb (2012) introduced tractable Markov logic (TML), the first tractable first-order probabilistic representation. Despite its surprising expressiveness, TML has a number of significant limitations. Chief among these is that it does not explicitly handle existence uncertainty, meaning that all possible worlds contain the same objects and relations. This leads to a number of conceptual problems, such as models that must contain meaningless combinations of attributes (e.g., horses with wheels). Here we propose a new formalism, tractable probabilistic knowledge bases (TPKBs), that overcomes this problem. Like TML, TPKBs use probabilistic class and part hierarchies to ensure tractability, but TPKBs have a much cleaner and user-friendly object-oriented syntax and a well-founded semantics for existence uncertainty. TML is greatly complicated by the use of probabilistic theorem proving, an inference procedure that is much more powerful than necessary. In contrast, we introduce an inference procedure specifically designed for TPKBs, which makes them far more transparent and amenable to analysis and implementation. TPKBs subsume TML and therefore essentially all tractable models, including many high-treewidth ones."
1880507,14133,8960,Information Rates and Optimal Decoding in Large Neural Populations,2011,"Many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of Shannon information rates in populations of spiking neurons. In this paper, we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities. We find that for large neural populations carrying a finite total amount of information, the full spiking population response is asymptotically as informative as a single observation from a Gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties. The Gaussian form of this asymptotic sufficient statistic allows us in certain cases to perform optimal Bayesian decoding by simple linear transformations, and to obtain closed-form expressions of the Shannon information carried by the network. One technical advantage of the theory is that it may be applied easily even to non-Poisson point process network models; for example, we find that under some conditions, neural populations with strong history-dependent (non-Poisson) effects carry exactly the same information as do simpler equivalent populations of non-interacting Poisson neurons with matched firing rates. We argue that our findings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design."
1645146,14133,20796,Collaborative online learning of user generated content,2011,"We study the problem of online classification of user generated content, with the goal of efficiently learning to categorize content generated by individual user. This problem is challenging due to several reasons. First, the huge amount of user generated content demands a highly efficient and scalable classification solution. Second, the categories are typically highly imbalanced, i.e., the number of samples from a particular useful class could be far and few between compared to some others (majority class). In some applications like spam detection, identification of the minority class often has significantly greater value than that of the majority class. Last but not least, when learning a classification model from a group of users, there is a dilemma: A single classification model trained on the entire corpus may fail to capture personalized characteristics such as language and writing styles unique to each user. On the other hand, a personalized model dedicated to each user may be inaccurate due to the scarcity of training data, especially at the very beginning; when users have written just a few articles. To overcome these challenges, we propose learning a global model over all users' data, which is then leveraged to continuously refine the individual models through a collaborative online learning approach. The class imbalance problem is addressed via a cost-sensitive learning approach. Experimental results show that our method is effective and scalable for timely classification of user generated content."
2068630,14133,11321,Deep Generative Stochastic Networks Trainable by Backprop,2014,"We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining."
2245831,14133,20358,Sparse online topic models,2013,"Topic models have shown great promise in discovering latent semantic structures from complex data corpora, ranging from text documents and web news articles to images, videos, and even biological data. In order to deal with massive data collections and dynamic text streams, probabilistic online topic models such as online latent Dirichlet allocation (OLDA) have recently been developed. However, due to normalization constraints, OLDA can be ineffective in controlling the sparsity of discovered representations, a desirable property for learning interpretable semantic patterns, especially when the total number of topics is large. In contrast, sparse topical coding (STC) has been successfully introduced as a non-probabilistic topic model for effectively discovering sparse latent patterns by using sparsity-inducing regularization. But, unfortunately STC cannot scale to very large datasets or deal with online text streams, partly due to its batch learning procedure. In this paper, we present a sparse online topic model, which directly controls the sparsity of latent semantic patterns by imposing sparsity-inducing regularization and learns the topical dictionary by an online algorithm. The online algorithm is efficient and guaranteed to converge. Extensive empirical results of the sparse online topic model as well as its collapsed and supervised extensions on a large-scale Wikipedia dataset and the medium-sized 20Newsgroups dataset demonstrate appealing performance."
1745184,14133,11491,Labelset anchored subspace ensemble (LASE) for multi-label annotation,2012,"In multimedia retrieval, multi-label annotation for image, text and video is challenging and attracts rapidly growing interests in past decades. The main crux of multi-label annotation lies on 1) how to reduce the model complexity when the label space expands exponentially with the increase of the number of labels; and 2) how to leverage the label correlations which have broadly believed useful for boosting annotation performance. In this paper, we propose labelsets anchored subspace ensemble (LASE) to solve both problems in an efficient scheme, whose training is a regularized matrix decomposition and prediction is an inference of group sparse representations. In order to shrink the label space, we firstly introduce label distilling extracting the frequent labelsets to replace the original labels. In the training stage, the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization, where each low-rank part defines a feature subspace mapped by a labelset. A manifold regularization is applied to map the labelset geometry to the geometry of the obtained subspaces. In the prediction stage, the group sparse representation of a new sample on the subspace ensemble is estimated by group  lasso . The selected subspaces indicate the labelsets that the sample should be annotated with. Experiments on several benchmark datasets of texts, images, web data and videos validate the appealing performance of LASE in multi-label annotation."
798446,14133,9099,Optimal batch selection for active learning in multi-label classification,2011,"Multi-label classification is a generalization of conventional classification, where it is possible for a single data point to have multiple labels. Manual annotation of a multi-label data point requires a human oracle to consider the presence/absence of every possible class separately, which involves significant labor. Active learning techniques are effective in reducing human labeling effort to induce a classification model. When exposed to large quantities of unlabeled data, such algorithms automatically select the salient and representative instances for manual annotation. Further, to address the high redundancy in data such as image or video sequences as well as the availability of multiple labeling agents, there have been recent attempts towards a batch mode form of active learning, where a batch of data points is selected simultaneously from an unlabeled set. In this work, we propose a novel optimization based batch mode active learning strategy to minimize human labeling effort in multi-label classification problems. To the best of our knowledge, this is the first attempt to develop such a scheme primarily intended for the multi-label context. The proposed framework is computationally simple, easy to implement and can be suitably modified to perform batch mode active learning in other formulations, such as single-label classification or problems involving hierarchical label spaces. Our results corroborate the efficacy of the proposed algorithm and certify the potential of the framework in being used for real world applications."
877645,14133,20796,Functional dirichlet process,2013,"Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its efficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical settings where samples are generated from a collection of dependent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a conference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on arbitrary covariate space. The approach is based on restricting and projecting a DP defined on a space of continuous functions with different domains, which results in a collection of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model inference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on different types of covariates."
888542,14133,20796,Active exploration: simultaneous sampling and labeling for large graphs,2013,"Modern information networks, such as social networks, are often characterized with large sizes and dynamic changing structures. To analyze these networks, existing solutions commonly rely on graph sampling techniques to reduce network sizes, and then carry out succeeding mining processes, such as labeling network nodes to build classification models. Such a  sampling-then-labeling  paradigm assumes that the whole network is available for sampling and the sampled network is useful for all subsequent tasks (such as network classification). Yet real-world networks are rarely immediately available unless the sampling process progressively crawls every single node and its connections. Meanwhile, without knowing the underlying analytic objective, the sampled network can hardly produce quality results. In this paper, we propose an  Active Exploration  framework for large graphs where the goal is to carry out network sampling and node labeling at the same time. To achieve this goal, we consider a network as a Markov chain and compute its stationary distribution by using supervised random walks. The stationary distribution of the sampled network help identify important nodes to be explored in the next step, and the labeling process labels the most informative node which in turn strengthens the sampling process. The mutually and simultaneously enhanced sampling and labeling processes ensure that the final network contains a maximum number of nodes directly related to the underlying mining tasks."
29085,14133,11187,A retina-inspired neurocomputing circuit for image representation,2013,"Biological vision systems have become highly optimized over millions of years of evolution, developing complex neural structures to represent and process stimuli. Moreover, biological systems of vision are typically far more efficient than current human-made machine vision systems. The present report describes a non-task-dependent image representation schema that simulates the early phase of a biological neural vision mechanism. We designed a neural model involving multiple types of computational units to simulate ganglion cells and their non-classical receptive fields, local feedback control circuits and receptive field dynamic self-adjustment mechanisms in the retina. We found that, beyond the pixel level, our model was able to represent images self-adaptively and rapidly. In addition, the improved representation was found to substantially facilitate contour detection. We propose that this improvement arose because ganglion cells can resize their receptive fields, enabling multiscale analysis functionality, a neighborhood referring function and a localized synthesis function. The ganglion cell layer is the starting point of subsequent diverse visual processing. The universality of this cell type and its functional mechanisms suggests that it will be useful for designing image processing algorithms in future."
1980011,14133,8960,Almost) No Label No Cry,2014,"In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to ≈300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels."
185826,14133,10174,Better time constrained search via randomization and postprocessing,2013,"Most of the satisficing planners which are based on heuristic search iteratively improve their solution quality through an anytime approach. Typically, the lowest-cost solution found so far is used to constrain the search. This avoids areas of the state space which cannot directly lead to lower cost solutions. However, in this paper we show that when used in conjunction with a post-processing plan improvement system such as ARAS, this bounding approach can harm a planner's performance since the bound may prevent the search from ever finding additional plans for the post-processor to improve.#R##N##R##N#The new anytime search framework of Diverse Any-Time Search addresses this issue through the use of restarts, randomization, and by not bounding as strictly as is done by previous approaches. Below, we will show that by using these techniques, the framework is able to generate a more diverse set of raw input plans for the post-processor to work on. We then show that when adding both Diverse Any-Time Search and the ARAS post-processor to LAMA-2011, the winner of the most recent IPC planning competition, the performance according to the IPC scoring metric improves from 511 points to over 570 points when tested on the 550 problems from IPC 2008 and IPC 2011. Performance gains are also seen when these techniques are added to Anytime Explicit Estimation Algorithm (AEES), as the performance improves from 440 points to over 513 points on the same problem set."
403087,14133,8960,Matrix Completion on Graphs,2014,"The problem of finding the missing values of a matrix given a few of its entries, called matrix completion, has gathered a lot of attention in the recent years. Al- though the problem under the standard low rank assumption is NP-hard, Cande`s and Recht showed that it can be exactly relaxed if the number of observed entries is sufficiently large. In this work, we introduce a novel matrix completion model that makes use of proximity information about rows and columns by assuming they form communities. This assumption makes sense in several real-world prob- lems like in recommender systems, where there are communities of people sharing preferences, while products form clusters that receive similar ratings. Our main goal is thus to find a low-rank solution that is structured by the proximities of rows and columns encoded by graphs. We borrow ideas from manifold learning to constrain our solution to be smooth on these graphs, in order to implicitly force row and column proximities. Our matrix recovery model is formulated as a con- vex non-smooth optimization problem, for which a well-posed iterative scheme is provided. We study and evaluate the proposed matrix completion on synthetic and real data, showing that the proposed structured low-rank recovery model outper- forms the standard matrix completion model in many situations."
746056,14133,11491,Multimodal feature generation framework for semantic image classification,2012,"The automatic attribution of semantic labels to unlabeled or weakly labeled images has received considerable attention but, given the complexity of the problem, remains a hard research topic. Here we propose a unified classification framework which mixes textual and visual information in a seamless manner. Unlike most recent previous works, computer vision techniques are used as inspiration to process textual information. To do so, we consider two types of complementary tag similarities, respectively computed from a conceptual hierarchy and from data collected from a photo sharing platform. Visual content is processed using recent techniques for bag-of visual-words feature generation. A central contribution of our work is to infer the coding step of the general bag-of-word framework with such similarities and to aggregate these tag-codes by max-pooling to obtain a single representative vector (signature). Final image annotations are obtained via late fusion, where the three modalities (two text-based and one visual-based) are merged during the classification step. Experimental results on the Pascal VOC 2007 and MIR Flickr datasets show an improvement over the state-of-the-art methods, while significantly decreasing the computational complexity of the learning system."
2655489,14133,10174,Chance-Constrained Consistency for Probabilistic Temporal Plan Networks,2014,"Unmanned deep-sea and planetary vehicles operate in highly uncertain environments. Autonomous agents often are not adopted in these domains due to the risk of mission failure, and loss of vehicles. Prior work on contingent plan execution addresses this issue by placing bounds on uncertain variables and by providing consistency guarantees for a ‘worst-case’ analysis, which tends to be too conservative for real-world applications. In this work, we unify features from trajectory optimization through risk-sensitive execution methods and high-level, contingent plan execution in order to extend existing guarantees of consistency for conditional plans to a chance-constrained setting. The result is a set of efficient algorithms for computing plan execution policies with explicit bounds on the risk of failure. To accomplish this, we introduce Probabilistic Temporal Plan Network (pTPN), which improve previous formulations, by incorporating probabilistic uncertainty and chance-constraints into the plan representation. We then introduce a novel method to the chance-constrained strong consistency problem, by leveraging a conflict-directed approach that searches for an execution policy that maximizes reward while meeting the risk constraint. Experimental results indicate that our approach for computing strongly consistent policies has an average scalability gain of about one order of magnitude, when compared to current methods based on chronological search."
500119,14133,22051,Influence maximization in social Networks when negative opinions may emerge and propagate,2011,"Influence maximization, defined by Kempe, Kleinberg, and#R##N#Tardos (2003), is the problem of finding a small set of seed#R##N#nodes in a social network that maximizes the spread of influence#R##N#under certain influence cascade models. In this paper,#R##N#we propose an extension to the independent cascade model#R##N#that incorporates the emergence and propagation of negative#R##N#opinions. The new model has an explicit parameter called#R##N#quality factor to model the natural behavior of people turning#R##N#negative to a product due to product defects. Our model#R##N#incorporates negativity bias (negative opinions usually dominate#R##N#over positive opinions) commonly acknowledged in#R##N#the social psychology literature. The model maintains some#R##N#nice properties such as submodularity, which allows a greedy#R##N#approximation algorithm for maximizing positive influence#R##N#within a ratio of 1 ��� 1=e. We define a quality sensitivity ratio#R##N#(qs-ratio) of influence graphs and show a tight bound of#R##N# (#R##N#p#R##N#n=k) on the qs-ratio, where n is the number of nodes#R##N#in the network and k is the number of seeds selected, which#R##N#indicates that seed selection is sensitive to the quality factor#R##N#for general graphs. We design an efficient algorithm to compute influence in tree structures, which is nontrivial due to#R##N#the negativity bias in the model. We use this algorithm as the#R##N#core to build a heuristic algorithm for influence maximization#R##N#for general graphs. Through simulations, we show that#R##N#our heuristic algorithm has matching influence with a standard#R##N#greedy approximation algorithm while being orders of#R##N#magnitude faster."
2002617,14133,8960,LSDA: Large Scale Detection through Adaptation,2014,"A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at lsda.berkeleyvision.org."
2356945,14133,8960,Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems,2013,"We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisfies all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisfied by this assignment. External evidence, or input to the network, can force variables to specific values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits."
1188610,14133,422,Modeling the dynamics of composite social networks,2013,"Modeling the dynamics of online social networks over time not only helps us understand the evolution of network structures and user behaviors, but also improves the performance of other analysis tasks, such as link prediction and community detection. Nowadays, users engage in multiple networks and form a composite social network by considering common users as the bridge. State-of-the-art network-dynamics analysis is performed in isolation for individual networks, but users' interactions in one network can influence their behaviors in other networks, and in an individual network, different types of user interactions also affect each other. Without considering the influences across networks, one may not be able to model the dynamics in a given network correctly due to the lack of information. In this paper, we study the problem of modeling the dynamics of composite networks, where the evolution processes of different networks are jointly considered. However, due to the difference in network properties, simply merging multiple networks into a single one is not ideal because individual evolution patterns may be ignored and network differences may bring negative impacts. The proposed solution is a nonparametric Bayesian model, which models each user's common latent features to extract the cross-network influences, and use network-specific factors to describe different networks' evolution patterns. Empirical studies on large-scale dynamic composite social networks demonstrate that the proposed approach improves the performance of link prediction over several state-of-the-art baselines and unfolds the network evolution accurately."
2107138,14133,20358,Towards realistic team formation in social networks based on densest subgraphs,2013,"Given a task  T , a set of experts  V  with multiple skills and a social network  G ( V ,  W ) reflecting the compatibility among the experts,  team formation  is the problem of identifying a team  C  ?  V  that is both competent in performing the task  T  and compatible in working together. Existing methods for this problem make too restrictive assumptions and thus cannot model practical scenarios. The goal of this paper is to consider the team formation problem in a realistic setting and present a novel formulation based on densest subgraphs. Our formulation allows modeling of many natural requirements such as (i) inclusion of a designated team leader and/or a group of given experts, (ii) restriction of the size or more generally cost of the team (iii) enforcing  locality  of the team, e.g., in a geographical sense or social sense, etc. The proposed formulation leads to a generalized version of the classical densest subgraph problem with cardinality constraints (DSP), which is an NP hard problem and has many applications in social network analysis. In this paper, we present a new method for (approximately) solving the generalized DSP (GDSP). Our method,  FORTE,  is based on solving an  equivalent  continuous relaxation of GDSP. The solution found by our method has a quality guarantee and always satisfies the constraints of GDSP. Experiments show that the proposed formulation (GDSP) is useful in modeling a broader range of team formation problems and that our method produces more coherent and compact teams of high quality. We also show, with the help of an LP relaxation of GDSP, that our method gives close to optimal solutions to GDSP."
1270536,14133,422,Online multiple kernel regression,2014,"Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these drawbacks, this paper presents a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fashion, and dynamically explore a pool of multiple diverse kernels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selection. The OMKR problem is more challenging than regular kernel-based regression tasks since we have to on-the-fly determine both the optimal kernel-based regressor for each individual kernel and the best combination of the multiple kernel regressors. In this paper, we propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks. We also analyze the theoretical bounds of the proposed OMKR method and conduct extensive experiments to evaluate its empirical performance on both real-world regression and times series prediction tasks."
2776472,14133,22113,GUARDS: innovative application of game theory for national airport security,2011,"We describe an innovative application of a novel game-theoretic approach for a national scale security deployment. Working with the United States Transportation Security Administration (TSA), we have developed a new application called GUARDS to allocate the TSA's limited resources across hundreds of security activities to provide protection at over 400 United States airports. Similar security applications (e.g., ARMOR and IRIS) have focused on one-off tailored applications and one security activity (e.g. checkpoints) per application, GUARDS on the other hand faces three new key issues: (i) reasoning about hundreds of heterogeneous security activities; (ii) reasoning over diverse potential threats; (iii) developing a system designed for hundreds of end-users. Since a national deployment precludes tailoring to specific airports, our key ideas are: (i) creating a new game-theoretic framework that allows for heterogeneous defender activities and compact modeling of a large number of threats; (ii) developing an efficient solution technique based on general purpose Stackelberg game solvers; (iii) taking a partially centralized approach for knowledge acquisition. The scheduling assistant has been delivered to the TSA and is currently undergoing evaluation for scheduling practices at an undisclosed airport. If successful, the TSA intends to incorporate the system into their unpredictable scheduling practices nationwide."
1857478,14133,8960,Empirical models of spiking in neural populations,2011,"Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing a latent dynamical model with realistic spiking observations to coupled generalised linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly better goodness-of-fit and more realistic population spike counts."
2386876,14133,422,Identifying users profiles from mobile calls habits,2012,"The huge quantity of positioning data registered by our mobile phones stimulates several research questions, mainly originating from the combination of this huge quantity of data with the extreme heterogeneity of the tracked user and the low granularity of the data. We propose a methodology to partition the users tracked by GSM phone calls into profiles like resident, commuters, in transit and tourists. The methodology analyses the phone calls with a combination of top-down and bottom up techniques where the top-down phase is based on a sequence of queries that identify some behaviors. The bottom-up is a machine learning phase to find groups of similar call behavior, thus refining the previous step. The integration of the two steps results in the partitioning of mobile traces into these four user categories that can be deeper analyzed, for example to understand the tourist movements in city or the traffic effects of commuters. An experiment on the identification of user profiles on a real dataset collecting call records from one month in the city of Pisa illustrates the methodology."
1752763,14133,8494,Live demonstration: A bio-inspired asynchronous pixel event tri-color vision sensor,2012,"We demonstrate the very first tri-color asynchronous pixel-event vision sensor, the latest addition of the growing family of bio-inspired AER (Address Event Representation) vision sensors. It is an asynchronous pulse/frequency density modulation (PDM/PFM) sensor (popularly known as octopus retina) that employs stacked photo diodes for color separation. Simple linear combination of the sensor's inherent pseudo color representation is employed to reconstruct approximate a RGB video stream. The 22×22 pixel array has been fabricated in the standard STM 90nm CMOS process."
1123359,14133,422,Predicting employee expertise for talent management in the enterprise,2014,"Strategic planning and talent management in large enterprises composed of knowledge workers requires complete, accurate, and up-to-date representation of the expertise of employees in a form that integrates with business processes. Like other similar organizations operating in dynamic environments, the IBM Corporation strives to maintain such current and correct information, specifically assessments of employees against job roles and skill sets from its expertise taxonomy. In this work, we deploy an analytics-driven solution that infers the expertise of employees through the mining of enterprise and social data that is not specifically generated and collected for expertise inference. We consider job role and specialty prediction and pose them as supervised classification problems. We evaluate a large number of feature sets, predictive models and postprocessing algorithms, and choose a combination for deployment. This expertise analytics system has been deployed for key employee population segments, yielding large reductions in manual effort and the ability to continually and consistently serve up-to-date and accurate data for several business functions. This expertise management system is in the process of being deployed throughout the corporation."
47951,14133,235,Multi-attribute Classification of Text Documents as a Tool for Ranking and Categorization of Educational Innovation Projects,2014,"We suggest a semi-automatic text processing method for ranking and categorization of educational innovation projects EIP. The EIP is a nation-wide program for strategic development of an university or a group of academic institutions. Our approach to the EIP evaluation is based on the multi-dimensional system ranking that uses quantitative indicators for three main missions of higher education institutions, namely, education, research, and knowledge transfer. The main part of this paper is devoted to the design of a semi-automatic method for ranking the EIPs exploiting multi-attribute document classification. The ranking methodology is based on the generalized Borda voting method."
1181392,14133,390,Patterning motor neurons in the Drosophila ventral nerve cord using latent state Conditional Random Fields,2012,"Type-specific dendritic arborization patterns dictate synaptic connectivity and are fundamental determinants of neuronal function. We exploit the morphological stereotypy and relative simplicity of the Drosophila nervous system to model the diverse neuronal morphologies of individual motor neurons (MNs) and understand underlying principles of synaptic connectivity in a motor circuit. In our analysis, we use images depicting single neurons labeled with green fluorescent protein (GFP) and serially imaged with laser scanning confocal microscopy. We model morphology with a novel formulation of Conditional Random Fields, a latent state CRF, to capture the highly varying compartment-based structure of the neurons (soma-axon-dendrites). We integrate a multi-class logistic model as the local potential function for combining compartment features. All parameters are learned in a single procedure, while L1-norm logistic model parameters are added in the maximum pseudo-likelihood model for learning with better scalability. The regularization hyper-parameters are chosen with a minimum cross-validation generalization error model."
1825069,14133,422,The mathematics of causal inference,2011,"I will review concepts, principles, and mathematical tools that were found useful in applications involving causal and counterfactual relationships. This semantical framework, enriched with a few ideas from logic and graph theory, gives rise to a complete, coherent, and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences. These include questions of causal effect estimation, policy analysis, and the integration of data from diverse studies. Of special interest to KDD researchers would be the following topics:   The Mediation Formula, and what it tells us about direct and indirect effects.   What mathematics can tell us about external validity or generalizing from experiments   What can graph theory tell us about recovering from sample-selection bias."
1660690,14133,422,Profit-maximizing cluster hires,2014,"Team formation has been long recognized as a natural way to acquire a diverse pool of useful skills, by combining experts with complementary talents. This allows organizations to effectively complete beneficial projects from different domains, while also helping individual experts position themselves and succeed in highly competitive job markets. Here, we assume a collection of projects ensuremath{P}, where each project requires a certain set of skills, and yields a different benefit upon completion. We are further presented with a pool of experts ensuremath{X}, where each expert has his own skillset and compensation demands. Then, we study the problem of hiring a cluster of experts T ⊆ X, so that the overall compensation (cost) does not exceed a given budget B, and the total benefit of the projects that this team can collectively cover is maximized. We refer to this as the ClusterHire problem. Our work presents a detailed analysis of the computational complexity and hardness of approximation of the problem, as well as heuristic, yet effective, algorithms for solving it in practice. We demonstrate the efficacy of our approaches through experiments on real datasets of experts, and demonstrate their advantage over intuitive baselines. We also explore additional variants of the fundamental problem formulation, in order to account for constraints and considerations that emerge in realistic cluster-hiring scenarios. All variants considered in this paper have immediate applications in the cluster hiring process, as it emerges in the context of different organizational settings."
2640036,14133,344,Undirected Machine Translation with Discriminative Reinforcement Learning,2014,"We present a novel Undirected Machine Translation model of Hierarchical MT that is not constrained to the standard bottomup inference order. Removing the ordering constraint makes it possible to condition on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art."
1383870,14133,422,"Leakage in data mining: formulation, detection, and avoidance",2011,"Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts."
579504,14133,256,Permutohedral Lattice CNNs,2014,"This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures."
200090,14133,422,Multi-agent based classification using argumentation from experience,2011,"An approach to multi-agent classification, using an Argumentation from Experience paradigm is describe, whereby individual agents argue for a given example to be classified with a particular label according to their local data. Arguments are expressed in the form of classification rules which are generated dynamically. The advocated argumentation process has been implemented in the PISA multi-agent framework, which is also described. Experiments indicate that the operation of PISA is comparable with other classification approaches and that it can be utilised for Ordinal Classification and Imbalanced Class problems."
706319,14133,422,Knowledge discovery from massive healthcare claims data,2013,"he role of big data in addressing the needs of the present healthcare system in US and rest of the world has been echoed by government, private, and academic sectors. There has been a growing emphasis to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from private and government health insurance providers. While the domain implications of such collaboration are well known, this type of data has been explored to a limited extent in the data mining community. The objective of this paper is two fold:  first , we introduce the emerging domain of big healthcare claims data to the KDD community, and  second , we describe the success and challenges that we encountered in analyzing this data using state of art analytics for massive data. Specifically, we translate the problem of analyzing healthcare data into some of the most well-known analysis problems in the data mining community,  social network analysis ,  text mining , and  temporal analysis and higher order feature construction , and describe how advances within each of these areas can be leveraged to understand the domain of healthcare. Each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost-care ratio by mining for opportunities to improve healthcare operations and reducing what seems to fall under fraud, waste, and abuse."
2619162,14133,235,Mining Rules for Rewriting States in a Transition-based Dependency Parser for English,2012,"Recently, methods for mining graph sequences have attracted considerable interest in datamining research. A graph sequence is a data structure used to represent changing networks. The aim of graph sequence mining is to enumerate common changing patterns appearing more frequently than a given threshold in graph sequences. Dependency analysis is recognized as a basic process in natural language processing. In transition-based parsers for dependency analysis, a transition sequence can be represented by a graph sequence, where each graph, vertex, and edge corresponds to a state, word, and dependency, respectively. In this paper, we propose a method for mining rules to rewrite states reaching incorrect final states to those reaching correct final states, from transition sequences of a dependency parser using a beam search. The proposed method is evaluated using an English corpus, and we demonstrate the design of effective feature templates based on knowledge obtained from the mined rules."
1573572,14133,339,Privacy Enhanced Personal Services for Smart Grids,2014,"Millions of people are now increasingly using smart devices at home to improve the quality of their lives. Unfortunately, the smart devices that we use at home for comfort and simplicity make our lives more complicated in terms of management due to several issues like mismatching interfaces and complexity of the micro-management. One approach to manage smart homes is to enable the utility provider, which has direct access to electrical devices via smart meters. It is expected that the data collected for the management can also be utilized for other personalized services using other business partners. In this paper, we address these personalized services and argue that privacy is a serious consideration for the deployment of the new business ideas. We provide a scientific method to provide new services for smart homes while protecting the privacy-sensitive data. To the best of our knowledge, privacy enhanced new services based on the utilization of smart meter data have not been considered by the research community."
2411313,14133,235,Why Gender and Age Prediction from Tweets is Hard: Lessons from a Crowdsourcing Experiment,2014,"There is a growing interest in automatically predicting the gender and age of authors from texts. However, most research so far ignores that language use is related to the social identity of speakers, which may be different from their biological identity. In this paper, we combine insights from sociolinguistics with data collected through an online game, to underline the importance of approaching age and gender as social variables rather than static biological variables. In our game, thousands of players guessed the gender and age of Twitter users based on tweets alone. We show that more than 10% of the Twitter users do not employ language that the crowd associates with their biological sex. It is also shown that older Twitter users are often perceived to be younger. Our findings highlight the limitations of current approaches to gender and age prediction from texts."
2570555,14133,422,Novel models and ensemble techniques to discriminate favorite items from unrated ones for personalized music recommendation,2011,"The Track 2 problem in KDD-Cup 2011 (music recommendation) is to discriminate between music tracks highly rated by a given user from those which are overall highly rated, but not rated by the given user. The training dataset consists of not only user rating history, but also the taxonomic information of track, artist, album, and genre. This paper describes the solution of the National Taiwan University team which ranked first place in the competition. We exploited a diverse of models (neighborhood models, latent models, Bayesian Personalized Ranking models, and random-walk models) with local blending and global ensemble to achieve 97.45% in accuracy on the testing dataset."
40295,14133,422,Spread of information in a social network using influential nodes,2012,"Viral marketing works with a social network as its backbone, where social interactions help spreading a message from one person to another. In social networks, a node with a higher degree can reach larger number of nodes in a single hop, and hence can be considered to be more influential than a node with lesser degree. For viral marketing with limited resources, initially the seller can focus on marketing the product to a certain influential group of individuals, here mentioned as  core  . If  k  persons are targeted for initial marketing, then the objective is to find the initial set of  k  active nodes, which will facilitate the spread most efficiently. We did a degree based scaling in graphs for making the edge weights suitable for degree based spreading. Then we detect the  core  from the maximum spanning tree (MST) of the graph by finding the top  k  influential nodes and the paths in MST that joins them. The paths within the  core  depict the key interaction sequences that will trigger the spread within the network. Experimental results show that the set of  k  influential nodes found by our  core  finding method spreads information faster than the greedy  k  -center method for the same  k  value."
1670993,14133,422,Contextual crowd intelligence,2014,"Most data analytics applications are industry/domain specific, e.g., predicting patients at high risk of being admitted to intensive care unit in the healthcare sector or predicting malicious SMSs in the telecommunication sector. Existing solutions are based on best practices, i.e., the systems' decisions are  knowledge-driven and/or data-driven . However, there are rules and exceptional cases that can only be precisely formulated and identified by subject-matter experts (SMEs) who have accumulated many years of experience. This paper envisions a more intelligent database management system (DBMS) that captures such knowledge to effectively address the industry/domain specific applications. At the core, the system is a hybrid human-machine database engine where the machine interacts with the SMEs as part of a feedback loop to gather, infer, ascertain and enhance the database knowledge and processing. We discuss the challenges towards building such a system through examples in healthcare predictive analysis -- a popular area for big data analytics."
1971263,14133,422,Heat pump detection from coarse grained smart meter data with positive and unlabeled learning,2013,"Recent advances in smart metering technology enable utility companies to have access to tremendous amount of smart meter data, from which the utility companies are eager to gain more insight about their customers. In this paper, we aim to detect electric heat pumps from coarse grained smart meter data for a heat pump marketing campaign. However, appliance detection is a challenging task, especially given a very low granularity and partial labeled even unlabeled data. Traditional methods install either a high granularity smart meter or sensors at every appliance, which is either too expensive or requires technical expertise. We propose a novel approach to detect heat pumps that utilizes low granularity smart meter data, prior sales data and weather data. In particular, motivated by the characteristics of heat pump consumption pattern, we extract novel features that are highly relevant to heat pump usage from smart meter data and weather data. Under the constraint that only a subset of heat pump users are available, we formalize the problem into a positive and unlabeled data classification and apply biased Support Vector Machine (BSVM) to our extracted features. Our empirical study on a real-world data set demonstrates the effectiveness of our method. Furthermore, our method has been deployed in a real-life setting where the partner electric company runs a targeted campaign for 292,496 customers. Based on the initial feedback, our detection algorithm can successfully detect substantial number of non-heat pump users who were identified heat pump users with the prior algorithm the company had used."
352305,14133,235,Temporal analysis of sentiment events: a visual realization and tracking,2011,"In recent years, extraction of temporal relations for events that express sentiments has drawn great attention of the Natural Language Processing (NLP) research communities. In this work, we propose a method that involves the association and contribution of sentiments in determining the event-event relations from texts. Firstly, we employ a machine learning approach based on Conditional Random Field (CRF) for solving the problem of Task C (identification of event-event relations) of TempEval-2007 within TimeML framework by considering sentiment as a feature of an event. Incorporating sentiment property, our system achieves the performance that is better than all the participated state-of-the-art systems of TempEval 2007. Evaluation results on the Task C test set yield the F-score values of 57.2% under the strict evaluation scheme and 58.6% under the relaxed evaluation scheme. The positive or negative coarse grained sentiments as well as the Ekman's six basic universal emotions (or, fine grained sentiments) are assigned to the events. Thereafter, we analyze the temporal relations between events in order to track the sentiment events. Representation of the temporal relations in a graph format shows the shallow visual realization path for tracking the sentiments over events. Manual evaluation of temporal relations of sentiment events identified in 20 documents sounds satisfactory from the purview of event-sentiment tracking."
51190,14133,235,Named entities in judicial transcriptions: extended conditional random fields,2013,"The progressive deployment of ICT technologies in the courtroom is leading to the development of integrated multimedia folders where the entire trial contents (documents, audio and video recordings) are available for online consultation via web-based platforms. The current amount of unstructured textual data available into the judicial domain, especially related to hearing transcriptions, highlights therefore the need to automatically extract structured data from the unstructured ones for improving the efficiency of consultation processes. In this paper we address the problem of extracting structured information from the transcriptions generated automatically using an ASR (Automatic Speech Recognition) system, by integrating Conditional Random Fields with available background information. The computational experiments show promising results in structuring ASR outputs, enabling a robust and efficient document consultation."
1998017,14133,422,Sequential entity group topic model for getting topic flows of entity groups within one document,2012,"Topic mining is regarded as a powerful method to analyze documents, and topic models are used to annotate relationships or to get a topic flow. The research aim in this paper is to get topic flows of entities and entity groups within one document. We propose two topic models: Entity Group Topic Model (EGTM) and Sequential Entity Group Topic Model (S-EGTM). These models provide two contributions. First, topic distributions of entities and entity groups can be analyzed. Second, the topic flow of each entity or each entity group can be captured, through segments in one document. We develop collapsed gibbs sampling methods for performing approximate inference of the models. By experiments, we demonstrate the models by showing the analysis of topics, prediction performance, and the topic flows over segments in one document."
1992278,14133,256,Herded Gibbs Sampling,2013,"Abstract: The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an $O(1/T)$ convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem."
1438205,14133,422,Discriminative clustering for market segmentation,2012,"We study discriminative clustering for market segmentation tasks. The underlying problem setting resembles discriminative clustering, however, existing approaches focus on the prediction of univariate cluster labels. By contrast, market segments encode complex (future) behavior of the individuals which cannot be represented by a single variable. In this paper, we generalize discriminative clustering to structured and complex output variables that can be represented as graphical models. We devise two novel methods to jointly learn the classifier and the clustering using alternating optimization and collapsed inference, respectively. The two approaches jointly learn a discriminative segmentation of the input space and a generative output prediction model for each segment. We evaluate our methods on segmenting user navigation sequences from Yahoo! News. The proposed collapsed algorithm is observed to outperform baseline approaches such as mixture of experts. We showcase exemplary projections of the resulting segments to display the interpretability of the solutions."
1540995,14133,422,Batch discovery of recurring rare classes toward identifying anomalous samples,2014,"We present a clustering algorithm for discovering rare yet significant recurring classes across a batch of samples in the presence of random effects. We model each sample data by an infinite mixture of Dirichlet-process Gaussian-mixture models (DPMs) with each DPM representing the noisy realization of its corresponding class distribution in a given sample. We introduce dependencies across multiple samples by placing a global Dirichlet process prior over individual DPMs. This hierarchical prior introduces a sharing mechanism across samples and allows for identifying local realizations of classes across samples. We use collapsed Gibbs sampler for inference to recover local DPMs and identify their class associations. We demonstrate the utility of the proposed algorithm, processing a flow cytometry data set containing two extremely rare cell populations, and report results that significantly outperform competing techniques. The source code of the proposed algorithm is available on the web via the link:http://cs.iupui.edu/~dundar/aspire.htm."
1357641,14133,65,Human behavior prediction for smart homes using deep learning,2013,"There is a growing interest in smart homes and predicting behaviors of inhabitants is a key element for the success of smart home services. In this paper, we propose two algorithms, DBN-ANN and DBN-R, based on the deep learning framework for predicting various activities in a home. We also address drawbacks of contrastive divergence, a widely used learning method for restricted Boltzmann machines, and propose an efficient online learning algorithm based on bootstrapping. From experiments using home activity datasets, we show that our proposed prediction algorithms outperform existing methods, such as a nonlinear SVM and k-means, in terms of prediction accuracy of newly activated sensors. In particular, DBN-R shows an accuracy of 43.9% (51.8%) for predicting newly activated sensors based on MIT home dataset 1 (dataset 2), while previous work based on the n-gram algorithm has shown an accuracy of 39% (43%) on the same dataset."
2675299,14133,235,Differential Evolution Based Feature Selection and Classifier Ensemble for Named Entity Recognition,2012,"In this paper, we propose a differential evolution (DE) based two-stage evolutionary approach for named entity recognition (NER). The first stage concerns with the problem of relevant feature selection for NER within the frameworks of two popular machine learning algorithms, namely Conditional Random Field (CRF) and Support Vector Machine (SVM). The solutions of the final best population provides different diverse set of classifiers; some are effective with respect to recall whereas some are effective with respect to precision. In the second stage we propose a novel technique for classifier ensemble for combining these classifiers. The approach is very general and can be applied for any classification problem. Currently we evaluate the proposed algorithm for NER in three popular Indian languages, namely Bengali, Hindi and Telugu. In order to maintain the domain-independence property the features are selected and developed mostly without using any deep domain knowledge and/or language dependent resources. Experimental results show that the proposed two stage technique attains the final F-measure values of 88.89%, 88.09% and 76.63% for Bengali, Hindi and Telugu, respectively. The key contributions of this work are two-fold, viz. (i). proposal of differential evolution (DE) based feature selection and classifier ensemble methods that can be applied to any classification problem; and (ii). scope of the development of language independent NER systems in a resource-poor"
60066,14133,344,Passive-Aggressive Sequence Labeling with Discriminative Post-Editing for Recognising Person Entities in Tweets,2014,"Recognising entities in social media text is difficult. NER on newswire text is conventionally cast as a sequence labeling problem. This makes implicit assumptions regarding its textual structure. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%."
1035338,14133,507,Database Modeling of Empirical Games,2014,"There is a growing interest in computational game theory, the study of algorithms for analyzing multi-agent strategic interaction cast as games. Work in this field has primarily focused on algorithmic complexity of solving given game forms, with relatively little attention paid to concerns about managing data underlying game models. We propose a model of game-play data implementable by a relational database system, designed to scale to the vast quantities of observations needed to describe real-world strategic scenarios with sufficient fidelity. Our model enables performing common game-theoretic analysis in the database, using a flexible, compact representation of games."
768684,14133,507,The SystemT IDE: an integrated development environment for information extraction rules,2011,"Information Extraction (IE)-the problem of extracting structured information from unstructured text - has become the key enabler for many enterprise applications such as semantic search, business analytics and regulatory compliance. While rule-based IE systems are widely used in practice due to their well-known explainability, developing high-quality information extraction rules is known to be a labor-intensive and time-consuming iterative process.   Our demonstration showcases SystemT IDE, the integrated development environment for SystemT, a state-of-the-art rule-based IE system from IBMResearch that has been successfully embedded in multiple IBM enterprise products. SystemT IDE facilitates the development, test and analysis of high-quality IE rules by means of sophisticated techniques, ranging from data management to machine learning. We show how to build high-quality IE annotators using a suite of tools provided by SystemT IDE, including computing data provenance, learning basic features such as regular expressions and dictionaries, and automatically refining rules based on labeled examples."
2138756,14133,422,Time-Evolving relational classification and ensemble methods,2012,"Relational networks often evolve over time by the addition, deletion, and changing of links, nodes, and attributes. However, accurately incorporating the full range of temporal dependencies into relational learning algorithms remains a challenge. We propose a novel framework for discovering  temporal-relational representations  for classification. The framework considers transformations over  all  the evolving relational components (attributes, edges, and nodes) in order to accurately incorporate temporal dependencies into relational models. Additionally, we propose  temporal ensemble methods  and demonstrate their effectiveness against traditional and relational ensembles on two real-world datasets. In all cases, the proposed temporal-relational models outperform competing models that ignore temporal information."
1411478,14133,422,Cross-task crowdsourcing,2013,"Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks."
1053438,14133,422,Random forests for metric learning with implicit pairwise position dependence,2012,"Metric learning makes it plausible to learn semantically meaningful distances for complex distributions of data using label or pairwise constraint information. However, to date, most metric learning methods are based on a single Mahalanobis metric, which cannot handle heterogeneous data well. Those that learn multiple metrics throughout the feature space have demonstrated superior accuracy, but at a severe cost to computational efficiency. Here, we adopt a new angle on the metric learning problem and learn a single metric that is able to implicitly adapt its distance function throughout the feature space. This metric adaptation is accomplished by using a random forest-based classifier to underpin the distance function and incorporate both absolute pairwise position and standard relative position into the representation. We have implemented and tested our method against state of the art global and multi-metric methods on a variety of data sets. Overall, the proposed method outperforms both types of method in terms of accuracy (consistently ranked first) and is an order of magnitude faster than state of the art multi-metric methods (16x faster in the worst case)."
1275627,14133,422,Matching users and items across domains to improve the recommendation quality,2014,"Given two homogeneous rating matrices with some overlapped users/items whose mappings are unknown, this paper aims at answering two questions. First, can we identify the unknown mapping between the users and/or items? Second, can we further utilize the identified mappings to improve the quality of recommendation in either domain? Our solution integrates a latent space matching procedure and a refining process based on the optimization of prediction to identify the matching. Then, we further design a transfer-based method to improve the recommendation performance. Using both synthetic and real data, we have done extensive experiments given different real life scenarios to verify the effectiveness of our models. The code and other materials are available at http://www.csie.ntu.edu.tw/~r00922051/matching/"
629198,14133,256,Learnable Pooling Regions for Image Classification,2013,"Biologically inspired, from the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms showing that the smooth regularization term is crucial to achieve strong performance using the presented architecture. Finally, we propose an efficient and parallel method to train the model. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter."
2477552,14133,422,Evaluating the crowd with confidence,2013,"Worker quality control is a crucial aspect of crowdsourcing systems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers."
541137,14133,256,Spectral Networks and Locally Connected Networks on Graphs,2014,"Abstract: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures."
2300771,14133,422,Selecting feature subset via constraint association rules,2012,"In this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the constraint association rules whose consequent is the target concept, and detects the redundant feature values with constraint association rules whose consequent and antecedent are both single feature value. After that, it eliminates the redundant feature values, and obtains the feature subset by mapping the relevant feature values to corresponding features. The efficiency and effectiveness of FEAST are tested upon both synthetic and real world data sets, and the classification results of the three different types of classifiers (including Naive Bayes, C4.5 and PART) with the other four representative feature subset selection algorithms (including CFS, FCBF, INTERACT and associative-based FSBAR) were compared. The results on synthetic data sets show that FEAST can effectively identify irrelevant and redundant features while reserving interactive ones. The results on the real world data sets show that FEAST outperformed other feature subset selection algorithms in terms of average classification accuracy and Win/Draw/Loss record."
1546018,14133,422,Active search on graphs,2013,"Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible. Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery. Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search.   Previous work on the active search problem [5] showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs. We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search. We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps."
1446553,14133,422,Networked bandits with disjoint linear payoffs,2014,"In this paper, we study `networked bandits', a new bandit problem where a set of interrelated arms varies over time and, given the contextual information that selects one arm, invokes other correlated arms. This problem remains under-investigated, in spite of its applicability to many practical problems. For instance, in social networks, an arm can obtain payoffs from both the selected user and its relations since they often share the content through the network. We examine whether it is possible to obtain multiple payoffs from several correlated arms based on the relationships. In particular, we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm, but also the relationships between arms. Our algorithm is `optimism in face of uncertainty' style, in that it decides an arm depending on integrated confidence sets constructed from historical data. We analyze the performance in simulation experiments and on two real-world offline datasets. The experimental results demonstrate our algorithm's effectiveness in the networked bandit setting."
1271138,14133,422,The setwise stream classification problem,2014,"In many applications, classification labels may not be associated with a single instance of records, but may be associated with a  data set  of records. The class behavior may not be possible to infer effectively from a single record, but may be only be inferred by an aggregate set of records. Therefore, in this problem, the class label is associated with a  set of instances both in the training and test data . Therefore, the problem may be understood to be that of classifying a  set of data sets . Typically, the classification behavior may only be inferred from the overall patterns of data distribution, and very little information is embedded in any given record for classification purposes. We refer to this problem as the setwise classification problem.   The problem can be extremely challenging in scenarios where the data is received in the form of a stream, and the records within any particular data set may not necessarily be received contiguously. In this paper, we present a first approach for real time and streaming classification of such data. We present experimental results illustrating the effectiveness of the approach."
1505785,14133,422,A dirichlet multinomial mixture model-based approach for short text clustering,2014,"Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models."
621089,14133,344,Learning Dictionaries for Named Entity Recognition using Minimal Supervision,2014,This paper describes an approach for automatic construction of dictionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings.
2430304,14133,344,Learning from evolving data streams: online triage of bug reports,2012,Open issue trackers are a type of social media that has received relatively little attention from the text-mining community. We investigate the problems inherent in learning to triage bug reports from time-varying data. We demonstrate that concept drift is an important consideration. We show the effectiveness of online learning algorithms by evaluating them on several bug report datasets collected from open issue trackers associated with large open-source projects. We make this collection of data publicly available.
2011705,14133,422,Active semi-supervised learning using sampling theory for graph signals,2014,"We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method."
1559890,14133,422,A probabilistic model for multimodal hash function learning,2012,"In recent years, both hashing-based similarity search and multimodal similarity search have aroused much research interest in the data mining and other communities. While hashing-based similarity search seeks to address the scalability issue, multimodal similarity search deals with applications in which data of multiple modalities are available. In this paper, our goal is to address both issues simultaneously. We propose a probabilistic model, called  multimodal latent binary embedding  (MLBE), to learn hash functions from multimodal data automatically. MLBE regards the binary latent factors as hash codes in a common Hamming space. Given data from multiple modalities, we devise an efficient algorithm for the learning of binary latent factors which corresponds to hash function learning. Experimental validation of MLBE has been conducted using both synthetic data and two realistic data sets. Experimental results show that MLBE compares favorably with two state-of-the-art models."
2683663,14133,22113,Analogico-deductive generation of Gödel's first incompleteness theorem from the liar paradox,2013,"Godel's proof of his famous first incompleteness theorem (G1) has quite understandably long been a tantalizing target for those wanting to engineer impressively intelligent computational systems. After all, in establishing G1, Godel did something that by any metric must be classified as stunningly intelligent. We observe that it has long been understood that there is some sort of analogical relationship between the Liar Paradox (LP) and G1, and that Godel himself appreciated and exploited the relationship. Yet the exact nature of the relationship has hitherto not been uncovered, by which we mean that the following question has not been answered: Given a description of LP, and the suspicion that it may somehow be used by a suitably programmed computing machine to find a proof of the incompleteness of Peano Arithmetic, can such a machine, provided this description as input, produce as output a complete and verifiably correct proof of G1? In this paper, we summarize engineering that entails an affirmative answer to this question. Our approach uses what we call analogico-deductive reasoning (ADR), which combines analogical and deductive reasoning to produce a full deductive proof of G1 from LP. Our engineering uses a form of ADR based on our META-R system, and a connection between the Liar Sentence in LP and Godel's Fixed Point Lemma, from which G1 follows quickly."
2368091,14133,390,Image fusion for autofocusing in fluorescence microscopy for tuberculosis screening,2011,Automatic microscopy for screening of sputum smears for tuberculosis would reduce the reliance on technicians in heavily burdened laboratories in poorly-resourced countries. Autofocusing is a key component of automated microscopy. We investigate the use of wavelet-based image fusion for automatic focusing of sputum smear slides as a component of automated fluorescence microscopy to identify Mycobacterium tuberculosis. We use manual focusing as ground truth for assessing the performance of the algorithm. Image fusion produces images with a degree of focus comparable to that of a human operator.
2161111,14133,422,OpenML: networked science in machine learning,2014,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners."
2491448,14133,422,Guilt by association: large scale malware detection by mining file-relation graphs,2014,"The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop's moral that a man is known by the company he keeps. We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate."
2409090,14133,369,Simple Channel Predictors for Lookahead Scheduling,2012,"To cope with highly loaded networks in diverse traffic situations, scheduling time slots in advance is an efficient approach. However, such Lookahead scheduling relies on future Channel Quality Information (CQI), which has to be accurately predicted. In this paper, we introduce a generic model for channel prediction errors. With this model, we study how the performance of lookahead scheduling depends on channel prediction's accuracy. Finally, we propose a simple channel predictor. This predictor provides high accuracy, is easy to implement and of low complexity. Consequently, it is a large step in making Lookahead scheduling feasible."
1922199,14133,235,Optimizing CRF-Based model for proper name recognition in polish texts,2012,"In this paper we present several optimizations introduced to Conditional Random Fields-based model for proper names recognition in Polish running texts. The proposed optimizations refer to word-level segmentation problems, gazetteers incompleteness, problem of unambiguous generalization features, feature construction and selection, and finally recognition of common proper names on the basis of external sources of knowledge. The problem of proper name recognition is limited to recognition of person first names and surnames, names of countries, cities and roads. The evaluation is performed in two ways: a single domain evaluation using 10-fold cross validation on a Corpus of Stock Exchange Reports and a cross-domain evaluation on a Corpus of Economic News. An additional corpus of Wikipedia articles, namely InfiKorp is used in the feature selection. Finally, we evaluate three configurations of proposed modifications. The top configuration improved the final result from 94.53% to 95.65% of F-measure for single domain and from 70.86% to 79.63% for cross-domain evaluation."
936932,14133,422,A data-driven method for in-game decision making in MLB: when to pull a starting pitcher,2013,"Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven. In this paper we show how machine learning can be applied to generate a model that could lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to help decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters.    For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model could have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time. For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time."
2421650,14133,422,Methods for exploring and mining tables on Wikipedia,2013,"Knowledge bases extracted automatically from the Web present new opportunities for data mining and exploration. Given a large, heterogeneous set of extracted relations, new tools are needed for searching the knowledge and uncovering relationships of interest. We present  WikiTables , a Web application that enables users to interactively explore tabular knowledge extracted from Wikipedia.   In experiments, we show that  WikiTables  substantially outperforms baselines on the novel task of automatically joining together disparate tables to uncover interesting relationships between table columns. We find that a Semantic Relatedness measure that leverages the Wikipedia link structure accounts for a majority of this improvement. Further, on the task of keyword search for tables, we show that  WikiTables  performs comparably to Google Fusion Tables despite using an order of magnitude fewer tables. Our work also includes the release of a number of public resources, including over 15 million tuples of extracted tabular data, manually annotated evaluation sets, and public APIs."
1571619,14133,422,FoodSIS: a text mining system to improve the state of food safety in singapore,2014,"Food safety is an important health issue in Singapore as the number of food poisoning cases have increased significantly over the past few decades. The National Environment Agency of Singapore (NEA) is the primary government agency responsible for monitoring and mitigating the food safety risks. In an effort to pro-actively monitor emerging food safety issues and to stay abreast with developments related to food safety in the world, NEA tracks the World Wide Web as a source of news feeds to identify food safety related articles. However, such information gathering is a difficult and time consuming process due to information overload. In this paper, we present FoodSIS, a system for end-to-end web information gathering for food safety. FoodSIS improves efficiency of such focused information gathering process with the use of machine learning techniques to identify and rank relevant content. We discuss the challenges in building such a system and describe how thoughtful system design and recent advances in machine learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operations. We conduct experiments and demonstrate that our classification approach results in improving the efficiency by average 35% compared to a conventional approach and the ranking approach leads to average 16% improvement in elevating the ranks of relevant articles."
506058,14133,256,Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines,2013,"Abstract: This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive."
227934,14133,235,Automatically Assessing Children's Writing Skills Based on Age-Supervised Datasets,2014,"In this paper, we propose an approach for predicting the age of the authors of narrative texts written by children between 6 and 13 years old. The features of the proposed model, which are lexical and syntactical part of speech, were normalized to avoid that the model uses the length of the text as a predictor. In addition, the initial features were extended using n-grams representations and combined using machine learning techniques for regression i.e. SMOreg. The proposed model was tested with collections of texts retrieved from Internet in Spanish, French and English, obtaining mean-absolute-error rates in the age-prediction task of 1.40, 1.20 and 1.72 years-old, respectively. Finally, we discuss the usefulness of this model to generate rankings of documents by written proficiency for each age."
2319587,14133,256,"Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",2013,"Abstract: Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free."
2611902,14133,235,Fast High-Accuracy Part-of-Speech Tagging by Independent Classifiers,2014,"Part-of-speech (POS) taggers can be quite accurate, but for practical use, accuracy often has to be sacrificed for speed. For example, the maintainers of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommend tagging with a model whose per tag error rate is 17% higher, relatively, than their most accurate model, to gain a factor of 10 or more in speed. In this paper, we treat POS tagging as a single-token independent multiclass classification task. We show that by using a rich feature set we can obtain high tagging accuracy within this framework, and by employing some novel feature-weight-combination and hypothesis-pruning techniques we can also get very fast tagging with this model. A prototype tagger implemented in Perl is tested and found to be at least 8 times faster than any publicly available tagger reported to have comparable accuracy on the standard Penn Treebank Wall Street Journal test set."
1868436,14133,235,A Study of using Syntactic and Semantic Structures for Concept Segmentation and Labeling,2014,"This paper presents an empirical study on using syntactic and semantic information for Concept Segmentation and Labeling (CSL), a well-known component in spoken language understanding. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We perform extensive experimentation by comparing different tree-based kernels with a variety of representations of the available linguistic information, including semantic concepts, words, POS tags, shallow and full syntax, and discourse trees. The results show that the structured representation with the semantic concepts yields significant improvement over the base CSL parser, much larger compared to learning with an explicit feature vector representation. We also show that shallow syntax helps improve the results and that discourse relations can be partially beneficial."
605854,14133,235,Ripple down rules for part-of-speech tagging,2011,This paper presents a new approach to learn a rule based system for the task of part of speech tagging. Our approach is based on an incremental knowledge acquisition methodology where rules are stored in an exception-structure and new rules are only added to correct errors of existing rules; thus allowing systematic control of interaction between rules. Experimental results of our approach on English show that we achieve in the best accuracy published to date: 97.095% on the Penn Treebank corpus. We also obtain the best performance for Vietnamese VietTreeBank corpus.
654230,14133,256,Bounding the Test Log-Likelihood of Generative Models,2014,"Abstract: Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison."
1804991,14133,422,Detecting adversarial advertisements in the wild,2011,"In a large online advertising system, adversaries may attempt to profit from the creation of low quality or harmful advertisements. In this paper, we present a large scale data mining effort that detects and blocks such adversarial advertisements for the benefit and safety of our users. Because both false positives and false negatives have high cost, our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classification. We also employ strategies to address the challenges of learning from highly skewed data at scale, allocating the effort of human experts, leveraging domain expert knowledge, and independently assessing the effectiveness of our system."
1711161,14133,422,NASA: achieving lower regrets and faster rates via adaptive stepsizes,2012,"The classic Stochastic Approximation (SA) method achieves optimal rates under the black-box model. This optimality does not rule out better algorithms when more information about functions and data is available.   We present a family of  Noise Adaptive Stochastic Approximation (NASA)  algorithms for online convex optimization and stochastic convex optimization. NASA is an adaptive variant of Mirror Descent Stochastic Approximation. It is novel in its practical variation-dependent stepsizes and better theoretical guarantees. We show that comparing with state-of-the-art adaptive and non-adaptive SA methods, lower regrets and faster rates can be achieved under low-variation assumptions."
2593669,14133,344,Projecting the Knowledge Graph to Syntactic Parsing,2014,"We present a syntactic parser training paradigm that learns from large scale Knowledge Bases. By utilizing the Knowledge Base context only during training, the resulting parser has no inference-time dependency on the Knowledge Base, thus not decreasing the speed during prediction. Knowledge Base information is injected into the model using an extension to the Augmented-loss training framework. We present empirical results that show this approach achieves a significant gain in accuracy for syntactic categories such as coordination and apposition."
628545,14133,256,Fast Training of Convolutional Networks through FFTs,2014,"Abstract: Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges."
638944,14133,256,Sparse similarity-preserving hashing,2014,"Abstract: In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method."
1604846,14133,422,Efficient mini-batch training for stochastic optimization,2014,"Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios."
